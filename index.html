<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2023-01-24T00:00:00Z">2023-01-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Watermark for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, Tom Goldstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Potential harms of large language models can be mitigated by watermarking
model output, i.e., embedding signals into generated text that are invisible to
humans but algorithmically detectable from a short span of tokens. We propose a
watermarking framework for proprietary language models. The watermark can be
embedded with negligible impact on text quality, and can be detected using an
efficient open-source algorithm without access to the language model API or
parameters. The watermark works by selecting a randomized set of whitelist
tokens before a word is generated, and then softly promoting use of whitelist
tokens during sampling. We propose a statistical test for detecting the
watermark with interpretable p-values, and derive an information-theoretic
framework for analyzing the sensitivity of the watermark. We test the watermark
using a multi-billion parameter model from the Open Pretrained Transformer
(OPT) family, and discuss robustness and security.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages in the main body. Code will be available at
  github.com/jwkirchenbauer/lm-watermarking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViHOS: Hate Speech Spans Detection for Vietnamese <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10186v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10186v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phu Gia Hoang, Canh Duc Luu, Khanh Quoc Tran, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise in hateful and offensive language directed at other users is one of
the adverse side effects of the increased use of social networking platforms.
This could make it difficult for human moderators to review tagged comments
filtered by classification systems. To help address this issue, we present the
ViHOS (Vietnamese Hate and Offensive Spans) dataset, the first human-annotated
corpus containing 26k spans on 11k comments. We also provide definitions of
hateful and offensive spans in Vietnamese comments as well as detailed
annotation guidelines. Besides, we conduct experiments with various
state-of-the-art models. Specifically, XLM-R$_{Large}$ achieved the best
F1-scores in Single span detection and All spans detection, while
PhoBERT$_{Large}$ obtained the highest in Multiple spans detection. Finally,
our error analysis demonstrates the difficulties in detecting specific types of
spans in our data for future research.
  Disclaimer: This paper contains real comments that could be considered
profane, offensive, or abusive.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Semantic Scholar Open Data Platform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodney Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg, Alexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Arman Cohan, Miles Crawford, Doug Downey, Jason Dunkelberger, Oren Etzioni, Rob Evans, Sergey Feldman, Joseph Gorney, David Graham, Fangzhou Hu, Regan Huff, Daniel King, Sebastian Kohlmeier, Bailey Kuehl, Michael Langan, Daniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner, Kelsey MacMillan, Tyler Murray, Chris Newell, Smita Rao, Shaurya Rohatgi, Paul Sayre, Zejiang Shen, Amanpreet Singh, Luca Soldaini, Shivashankar Subramanian, Amber Tanaka, Alex D. Wade, Linda Wagner, Lucy Lu Wang, Chris Wilhelm, Caroline Wu, Jiangjiang Yang, Angele Zamarron, Madeleine Van Zuylen, Daniel S. Weld
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The volume of scientific output is creating an urgent need for automated
tools to help scientists keep up with developments in their field. Semantic
Scholar (S2) is an open data platform and website aimed at accelerating science
by helping scholars discover and understand scientific literature. We combine
public and proprietary data sources using state-of-the-art techniques for
scholarly PDF content extraction and automatic knowledge graph construction to
build the Semantic Scholar Academic Graph, the largest open scientific
literature graph to-date, with 200M+ papers, 80M+ authors, 550M+
paper-authorship edges, and 2.4B+ citation edges. The graph includes advanced
semantic features such as structurally parsed text, natural language summaries,
and vector embeddings. In this paper, we describe the components of the S2 data
processing pipeline and the associated APIs offered by the platform. We will
update this living document to reflect changes as we add new data offerings and
improve existing services.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models as Fiduciaries: A Case Study Toward Robustly
  Communicating With Artificial Intelligence Through Legal Standards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John J. Nay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) is taking on increasingly autonomous roles,
e.g., browsing the web as a research assistant and managing money. But
specifying goals and restrictions for AI behavior is difficult. Similar to how
parties to a legal contract cannot foresee every potential "if-then"
contingency of their future relationship, we cannot specify desired AI behavior
for all circumstances. Legal standards facilitate the robust communication of
inherently vague and underspecified goals. Instructions (in the case of
language models, "prompts") that employ legal standards will allow AI agents to
develop shared understandings of the spirit of a directive that can adapt to
novel situations, and generalize expectations regarding acceptable actions to
take in unspecified states of the world. Standards have built-in context that
is lacking from other goal specification languages, such as plain language and
programming languages. Through an empirical study on thousands of evaluation
labels we constructed from U.S. court opinions, we demonstrate that large
language models (LLMs) are beginning to exhibit an "understanding" of one of
the most relevant legal standards for AI agents: fiduciary obligations.
Performance comparisons across models suggest that, as LLMs continue to exhibit
improved core capabilities, their legal standards understanding will also
continue to improve. OpenAI's latest LLM has 78% accuracy on our data, their
previous release has 73% accuracy, and a model from their 2020 GPT-3 paper has
27% accuracy (worse than random). Our research is an initial step toward a
framework for evaluating AI understanding of legal standards more broadly, and
for conducting reinforcement learning with legal feedback (RLLF).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2209.13020</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Inclusive Language to Gender-Neutral Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Piergentili, Dennis Fucci, Beatrice Savoldi, Luisa Bentivogli, Matteo Negri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gender inclusivity in language has become a central topic of debate and
research. Its application in the cross-lingual contexts of human and machine
translation (MT), however, remains largely unexplored. Here, we discuss
Gender-Neutral Translation (GNT) as a form of gender inclusivity in translation
and advocate for its adoption for MT models, which have been found to
perpetuate gender bias and discrimination. To this aim, we review a selection
of relevant institutional guidelines for Gender-Inclusive Language (GIL) to
collect and systematize useful strategies of gender neutralization. Then, we
discuss GNT and its scenarios of use, devising a list of desiderata. Finally,
we identify the main technical challenges to the implementation of GNT in MT.
Throughout these contributions we focus on translation from English into
Italian, as representative of salient linguistic transfer problems, due to the
different rules for gender marking in their grammar.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multitask Instruction-based <span class="highlight-title">Prompt</span>ing for Fallacy Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tariq Alhindi, Tuhin Chakrabarty, Elena Musi, Smaranda Muresan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fallacies are used as seemingly valid arguments to support a position and
persuade the audience about its validity. Recognizing fallacies is an
intrinsically difficult task both for humans and machines. Moreover, a big
challenge for computational models lies in the fact that fallacies are
formulated differently across the datasets with differences in the input format
(e.g., question-answer pair, sentence with fallacy fragment), genre (e.g.,
social media, dialogue, news), as well as types and number of fallacies (from 5
to 18 types per dataset). To move towards solving the fallacy recognition task,
we approach these differences across datasets as multiple tasks and show how
instruction-based prompting in a multitask setup based on the T5 model improves
the results against approaches built for a specific dataset such as T5, BERT or
GPT-3. We show the ability of this multitask prompting approach to recognize 28
unique fallacies across domains and genres and study the effect of model size
and prompt choice by analyzing the per-class (i.e., fallacy type) results.
Finally, we analyze the effect of annotation quality on model performance, and
the feasibility of complementing this approach with external knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the 2022 Conference on Empirical Methods in Natural
  Language Processing, pages 8172 - 8187</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Opportunities and Challenges in Neural Dialog Tutoring <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Macina, Nico Daheim, Lingzhi Wang, Tanmay Sinha, Manu Kapur, Iryna Gurevych, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing dialog tutors has been challenging as it involves modeling the
diverse and complex pedagogical strategies employed by human tutors. Although
there have been significant recent advances in neural conversational systems
using large language models and growth in available dialog corpora, dialog
tutoring has largely remained unaffected by these advances. In this paper, we
rigorously analyze various generative language models on two dialog tutoring
datasets for language learning using automatic and human evaluations to
understand the new opportunities brought by these advances as well as the
challenges we must overcome to build models that would be usable in real
educational settings. We find that although current approaches can model
tutoring in constrained learning scenarios when the number of concepts to be
taught and possible teacher strategies are small, they perform poorly in less
constrained scenarios. Our human quality evaluation shows that both models and
ground-truth annotations exhibit low performance in terms of equitable
tutoring, which measures learning opportunities for students and how engaging
the dialog is. To understand the behavior of our models in a real tutoring
setting, we conduct a user study using expert annotators and find a
significantly large number of model reasoning errors in 45% of conversations.
Finally, we connect our findings to outline future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EACL 2023 (main conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Applications and Challenges of Sentiment Analysis in Real-life Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diptesh Kanojia, Aditya Joshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sentiment analysis has benefited from the availability of lexicons and
benchmark datasets created over decades of research. However, its applications
to the real world are a driving force for research in SA. This chapter
describes some of these applications and related challenges in real-life
scenarios. In this chapter, we focus on five applications of SA: health, social
policy, e-commerce, digital humanities and other areas of NLP. This chapter is
intended to equip an NLP researcher with the `what', `why' and `how' of
applications of SA: what is the application about, why it is important and
challenging and how current research in SA deals with the application. We note
that, while the use of deep learning techniques is a popular paradigm that
spans these applications, challenges around privacy and selection bias of
datasets is a recurring theme across several applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Book Chapter (3rd Chapter in "Computational Intelligence Applications
  for Text and Sentiment Data Analysis" published by Elsevier)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conclusion-based Counter-Argument Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09911v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09911v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milad Alshomary, Henning Wachsmuth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world debates, the most common way to counter an argument is to
reason against its main point, that is, its conclusion. Existing work on the
automatic generation of natural language counter-arguments does not address the
relation to the conclusion, possibly because many arguments leave their
conclusion implicit. In this paper, we hypothesize that the key to effective
counter-argument generation is to explicitly model the argument's conclusion
and to ensure that the stance of the generated counter is opposite to that
conclusion. In particular, we propose a multitask approach that jointly learns
to generate both the conclusion and the counter of an input argument. The
approach employs a stance-based ranking component that selects the counter from
a diverse set of generated candidates whose stance best opposes the generated
conclusion. In both automatic and manual evaluation, we provide evidence that
our approach generates more relevant and stance-adhering counters than strong
baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1 figure, conference paper, eacl-23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-lingual German Biomedical Information Extraction: from Zero-shot
  to Human-in-the-Loop 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09908v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09908v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siting Liang, Mareike Hartmann, Daniel Sonntag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents our project proposal for extracting biomedical
information from German clinical narratives with limited amounts of
annotations. We first describe the applied strategies in transfer learning and
active learning for solving our problem. After that, we discuss the design of
the user interface for both supplying model inspection and obtaining user
annotations in the interactive environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Identification of Disaster News For Crisis Management Using
  Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09896v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09896v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lord Christian Carl H. Regacho, Ai Matsushita, Angie M. Ceniza-Canillo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A lot of news sources picked up on Typhoon Rai (also known locally as Typhoon
Odette), along with fake news outlets. The study honed in on the issue, to
create a model that can identify between legitimate and illegitimate news
articles. With this in mind, we chose the following machine learning algorithms
in our development: Logistic Regression, Random Forest and Multinomial Naive
Bayes. Bag of Words, TF-IDF and Lemmatization were implemented in the Model.
Gathering 160 datasets from legitimate and illegitimate sources, the machine
learning was trained and tested. By combining all the machine learning
techniques, the Combined BOW model was able to reach an accuracy of 91.07%,
precision of 88.33%, recall of 94.64%, and F1 score of 91.38% and Combined
TF-IDF model was able to reach an accuracy of 91.18%, precision of 86.89%,
recall of 94.64%, and F1 score of 90.60%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Stability Analysis of Fine-Tuning a <span class="highlight-title">Pre-Train</span>ed Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Fu, Anthony Man-Cho So, Nigel Collier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning a pre-trained model (such as BERT, ALBERT, RoBERTa, T5, GPT,
etc.) has proven to be one of the most promising paradigms in recent NLP
research. However, numerous recent works indicate that fine-tuning suffers from
the instability problem, i.e., tuning the same model under the same setting
results in significantly different performance. Many recent works have proposed
different methods to solve this problem, but there is no theoretical
understanding of why and how these methods work. In this paper, we propose a
novel theoretical stability analysis of fine-tuning that focuses on two
commonly used settings, namely, full fine-tuning and head tuning. We define the
stability under each setting and prove the corresponding stability bounds. The
theoretical bounds explain why and how several existing methods can stabilize
the fine-tuning procedure. In addition to being able to explain most of the
observed empirical discoveries, our proposed theoretical analysis framework can
also help in the design of effective and provable methods. Based on our theory,
we propose three novel strategies to stabilize the fine-tuning procedure,
namely, Maximal Margin Regularizer (MMR), Multi-Head Loss (MHLoss), and Self
Unsupervised Re-Training (SURT). We extensively evaluate our proposed
approaches on 11 widely used real-world benchmark datasets, as well as hundreds
of synthetic classification datasets. The experiment results show that our
proposed methods significantly stabilize the fine-tuning procedure and also
corroborate our theoretical analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Resource Compositional Semantic Parsing with Concept <span class="highlight-title">Pretrain</span>ing <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09809v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09809v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subendhu Rongali, Mukund Sridhar Harakere, Haidar Khan, Konstantine Arkoudas, Wael Hamza, Andrew McCallum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic parsing plays a key role in digital voice assistants such as Alexa,
Siri, and Google Assistant by mapping natural language to structured meaning
representations. When we want to improve the capabilities of a voice assistant
by adding a new domain, the underlying semantic parsing model needs to be
retrained using thousands of annotated examples from the new domain, which is
time-consuming and expensive. In this work, we present an architecture to
perform such domain adaptation automatically, with only a small amount of
metadata about the new domain and without any new training data (zero-shot) or
with very few examples (few-shot). We use a base seq2seq (sequence-to-sequence)
architecture and augment it with a concept encoder that encodes intent and slot
tags from the new domain. We also introduce a novel decoder-focused approach to
pretrain seq2seq models to be concept aware using Wikidata and use it to help
our model learn important concepts and perform well in low-resource settings.
We report few-shot and zero-shot results for compositional semantic parsing on
the TOPv2 dataset and show that our model outperforms prior approaches in
few-shot settings for the TOPv2 and SNIPS datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Very Large <span class="highlight-title">Pretrain</span>ed Language Models Learn Storytelling With A Few
  Examples? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuohan Xie, Trevor Cohn, Jey Han Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While pre-trained language models can generate individually fluent sentences
for automatic story generation, they struggle to generate stories that are
coherent, sensible and interesting. Current state-of-the-art (SOTA) story
generation models explore using higher-level features such as plots or
commonsense knowledge to improve the quality of generated stories. Prompt-based
learning using very large pre-trained language models (VLPLMs) such as GPT3 has
demonstrated impressive performance even across various NLP tasks. In this
paper, we present an extensive study using automatic and human evaluation to
compare the story generation capability of VLPLMs to those SOTA models in three
different datasets where stories differ in style, register and length. Our
results show that VLPLMs generate much higher quality stories than other story
generation models, and to a certain extent rival human authors, although
preliminary investigation also reveals that they tend to ``plagiarise'' real
stories in scenarios that involve world knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>-Patcher: One Mistake worth One Neuron <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong, Zhang Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Transformer-based Pretrained Language Models (PLMs) dominate almost all
Natural Language Processing (NLP) tasks. Nevertheless, they still make mistakes
from time to time. For a model deployed in an industrial environment, fixing
these mistakes quickly and robustly is vital to improve user experiences.
Previous works formalize such problems as Model Editing (ME) and mostly focus
on fixing one mistake. However, the one-mistake-fixing scenario is not an
accurate abstraction of the real-world challenge. In the deployment of AI
services, there are ever-emerging mistakes, and the same mistake may recur if
not corrected in time. Thus a preferable solution is to rectify the mistakes as
soon as they appear nonstop. Therefore, we extend the existing ME into
Sequential Model Editing (SME) to help develop more practical editing methods.
Our study shows that most current ME methods could yield unsatisfying results
in this scenario. We then introduce Transformer-Patcher, a novel model editor
that can shift the behavior of transformer-based models by simply adding and
training a few neurons in the last Feed-Forward Network layer. Experimental
results on both classification and generation tasks show that
Transformer-Patcher can successively correct up to thousands of errors
(Reliability) and generalize to their equivalent inputs (Generality) while
retaining the model's accuracy on irrelevant inputs (Locality). Our method
outperforms previous fine-tuning and HyperNetwork-based methods and achieves
state-of-the-art performance for Sequential Model Editing (SME). The code is
available at https://github.com/ZeroYuHuang/Transformer-Patcher.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted in ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Truveta Mapper: A Zero-shot Ontology Alignment Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mariyam Amir, Murchana Baruah, Mahsa Eslamialishah, Sina Ehsani, Alireza Bahramali, Sadra Naddaf-Sh, Saman Zarandioon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a new perspective is suggested for unsupervised Ontology
Matching (OM) or Ontology Alignment (OA) by treating it as a translation task.
Ontologies are represented as graphs, and the translation is performed from a
node in the source ontology graph to a path in the target ontology graph. The
proposed framework, Truveta Mapper (TM), leverages a multi-task
sequence-to-sequence transformer model to perform alignment across multiple
ontologies in a zero-shot, unified and end-to-end manner. Multi-tasking enables
the model to implicitly learn the relationship between different ontologies via
transfer-learning without requiring any explicit cross-ontology manually
labeled data. This also enables the formulated framework to outperform existing
solutions for both runtime latency and alignment quality. The model is
pre-trained and fine-tuned only on publicly available text corpus and
inner-ontologies data. The proposed solution outperforms state-of-the-art
approaches, Edit-Similarity, LogMap, AML, BERTMap, and the recently presented
new OM frameworks in Ontology Alignment Evaluation Initiative (OAEI22), offers
log-linear complexity in contrast to quadratic in the existing end-to-end
methods, and overall makes the OM task efficient and more straightforward
without much post-processing involving mapping extension or mapping repair.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-grained Early Frequency Attention for Deep Speaker Representation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2009.01822v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2009.01822v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirhossein Hajavi, Ali Etemad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning techniques have considerably improved speech processing in
recent years. Speaker representations extracted by deep learning models are
being used in a wide range of tasks such as speaker recognition and speech
emotion recognition. Attention mechanisms have started to play an important
role in improving deep learning models in the field of speech processing.
Nonetheless, despite the fact that important speaker-related information can be
embedded in individual frequency-bins of the input spectral representations,
current attention models are unable to attend to fine-grained information items
in spectral representations. In this paper we propose Fine-grained Early
Frequency Attention (FEFA) for speaker representation learning. Our model is a
simple and lightweight model that can be integrated into various CNN pipelines
and is capable of focusing on information items as small as frequency-bins. We
evaluate the proposed model on three tasks of speaker recognition, speech
emotion recognition, and spoken digit recognition. We use Three widely used
public datasets, namely VoxCeleb, IEMOCAP, and Free Spoken Digit Dataset for
our experiments. We attach FEFA to several prominent deep learning models and
evaluate its impact on the final performance. We also compare our work with
other related works in the area. Our experiments show that by adding FEFA to
different CNN architectures, performance is consistently improved by
substantial margins, and the models equipped with FEFA outperform all the other
attentive models. We also test our model against different levels of added
noise showing improvements in robustness and less sensitivity compared to the
backbone networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reasoning over Different Types of Knowledge Graphs: Static, Temporal and
  Multi-Modal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05767v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05767v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Liang, Lingyuan Meng, Meng Liu, Yue Liu, Wenxuan Tu, Siwei Wang, Sihang Zhou, Xinwang Liu, Fuchun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graph reasoning (KGR), aiming to deduce new facts from existing
facts based on mined logic rules underlying knowledge graphs (KGs), has become
a fast-growing research direction. It has been proven to significantly benefit
the usage of KGs in many AI applications, such as question answering and
recommendation systems, etc. According to the graph types, the existing KGR
models can be roughly divided into three categories, i.e., static models,
temporal models, and multi-modal models. The early works in this domain mainly
focus on static KGR and tend to directly apply general knowledge graph
embedding models to the reasoning task. However, these models are not suitable
for more complex but practical tasks, such as inductive static KGR, temporal
KGR, and multi-modal KGR. To this end, multiple works have been developed
recently, but no survey papers and open-source repositories comprehensively
summarize and discuss models in this important direction. To fill the gap, we
conduct a survey for knowledge graph reasoning tracing from static to temporal
and then to multi-modal KGs. Concretely, the preliminaries, summaries of KGR
models, and typical datasets are introduced and discussed consequently.
Moreover, we discuss the challenges and potential opportunities. The
corresponding open-source repository is shared on GitHub:
https://github.com/LIANGKE23/Awesome-Knowledge-Graph-Reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can large language models reason about medical questions? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08143v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08143v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Liévin, Christoffer Egeberg Hother, Ole Winther
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although large language models (LLMs) often produce impressive outputs, it
remains unclear how they perform in real-world scenarios requiring strong
reasoning skills and expert domain knowledge. We set out to investigate whether
GPT-3.5 (Codex and InstructGPT) can be applied to answer and reason about
difficult real-world-based questions. We utilize two multiple-choice medical
exam questions (USMLE and MedMCQA) and a medical reading comprehension dataset
(PubMedQA). We investigate multiple prompting scenarios: Chain-of-Thought (CoT,
think step-by-step), zero- and few-shot (prepending the question with
question-answer exemplars) and retrieval augmentation (injecting Wikipedia
passages into the prompt). For a subset of the USMLE questions, a medical
expert reviewed and annotated the model's CoT. We found that InstructGPT can
often read, reason and recall expert knowledge. Failure are primarily due to
lack of knowledge and reasoning errors and trivial guessing heuristics are
observed, e.g.\ too often predicting labels A and D on USMLE. Sampling and
combining many completions overcome some of these limitations. Using 100
samples, Codex 5-shot CoT not only gives close to well-calibrated predictive
probability but also achieves human-level performances on the three datasets.
USMLE: 60.2%, MedMCQA: 62.7% and PubMedQA: 78.2%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 6 figures, to be submitted. v1: results using InstructGPT,
  v2: added the Codex experiments, v3: added the missing test MedMCQA results
  for Codex 5-shot CoT and using k=100 samples</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HTMOT : Hierarchical Topic Modelling Over Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.03104v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.03104v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Judicael Poumay, Ashwin Ittoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the years, topic models have provided an efficient way of extracting
insights from text. However, while many models have been proposed, none are
able to model topic temporality and hierarchy jointly. Modelling time provide
more precise topics by separating lexically close but temporally distinct
topics while modelling hierarchy provides a more detailed view of the content
of a document corpus. In this study, we therefore propose a novel method,
HTMOT, to perform Hierarchical Topic Modelling Over Time. We train HTMOT using
a new implementation of Gibbs sampling, which is more efficient. Specifically,
we show that only applying time modelling to deep sub-topics provides a way to
extract specific stories or events while high level topics extract larger
themes in the corpus. Our results show that our training procedure is fast and
can extract accurate high-level topics and temporally precise sub-topics. We
measured our model's performance using the Word Intrusion task and outlined
some limitations of this evaluation method, especially for hierarchical models.
As a case study, we focused on the various developments in the space industry
in 2020.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robustness through Data Augmentation Loss Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.11205v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.11205v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianjian Huang, Shaunak Halbe, Chinnadhurai Sankar, Pooyan Amini, Satwik Kottur, Alborz Geramifard, Meisam Razaviyayn, Ahmad Beirami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep learning through empirical risk minimization (ERM) has succeeded
at achieving human-level performance at a variety of complex tasks, ERM is not
robust to distribution shifts or adversarial attacks. Synthetic data
augmentation followed by empirical risk minimization (DA-ERM) is a simple and
widely used solution to improve robustness in ERM. In addition, consistency
regularization can be applied to further improve the robustness of the model by
forcing the representation of the original sample and the augmented one to be
similar. However, existing consistency regularization methods are not
applicable to covariant data augmentation, where the label in the augmented
sample is dependent on the augmentation function. For example, dialog state
covaries with named entity when we augment data with a new named entity. In
this paper, we propose data augmented loss invariant regularization (DAIR), a
simple form of consistency regularization that is applied directly at the loss
level rather than intermediate features, making it widely applicable to both
invariant and covariant data augmentation regardless of network architecture,
problem setup, and task. We apply DAIR to real-world learning problems
involving covariant data augmentation: robust neural task-oriented dialog state
tracking and robust visual question answering. We also apply DAIR to tasks
involving invariant data augmentation: robust regression, robust classification
against adversarial attacks, and robust ImageNet classification under
distribution shift. Our experiments show that DAIR consistently outperforms ERM
and DA-ERM with little marginal computational cost and sets new
state-of-the-art results in several benchmarks involving covariant data
augmentation. Our code of all experiments is available at:
https://github.com/optimization-for-data-driven-science/DAIR.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grammar construction methods for extended deterministic expressions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.01621v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.01621v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoying Mou, Haiming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extended regular expressions with counting and interleaving are widely used
in practice. However the related theoretical studies for this kind of
expressions currently cannot meet the need of practical work. This paper
develops syntax definitions for extended deterministic expressions and their
subclasses, hope to completely solve the long-standing problem that there are
no syntax definitions for this kind of expressions, which has become an
important reason for restricting the use of extended expressions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in Chinese language</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Foresight -- Generative <span class="highlight-title">Pretrain</span>ed <span class="highlight-title">Transformer</span> (<span class="highlight-title">GPT</span>) for Modelling of
  Patient Timelines using EHRs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08072v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08072v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeljko Kraljevic, Dan Bean, Anthony Shek, Rebecca Bendayan, Harry Hemingway, Joshua Au Yeung, Alexander Deng, Alfie Baston, Jack Ross, Esther Idowu, James T Teo, Richard J Dobson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: Electronic Health Records hold detailed longitudinal information
about each patient's health status and general clinical history, a large
portion of which is stored within the unstructured text. Existing approaches
focus mostly on structured data and a subset of single-domain outcomes. We
explore how temporal modelling of patients from free text and structured data,
using deep generative transformers can be used to forecast a wide range of
future disorders, substances, procedures or findings. Methods: We present
Foresight, a novel transformer-based pipeline that uses named entity
recognition and linking tools to convert document text into structured, coded
concepts, followed by providing probabilistic forecasts for future medical
events such as disorders, substances, procedures and findings. We processed the
entire free-text portion from three different hospital datasets totalling
811336 patients covering both physical and mental health. Findings: On tests in
two UK hospitals (King's College Hospital, South London and Maudsley) and the
US MIMIC-III dataset precision@10 0.68, 0.76 and 0.88 was achieved for
forecasting the next disorder in a patient timeline, while precision@10 of
0.80, 0.81 and 0.91 was achieved for forecasting the next biomedical concept.
Foresight was also validated on 34 synthetic patient timelines by five
clinicians and achieved relevancy of 97% for the top forecasted candidate
disorder. As a generative model, it can forecast follow-on biomedical concepts
for as many steps as required. Interpretation: Foresight is a general-purpose
model for biomedical concept modelling that can be used for real-world risk
forecasting, virtual trials and clinical research to study the progression of
disorders, simulate interventions and counterfactuals, and educational
purposes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Agnostic Data-Driven Inverse Text Normalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08506v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08506v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Szu-Jui Chen, Debjyoti Paul, Yutong Pang, Peng Su, Xuedong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of automatic speech recognition (ASR) models, converting
the spoken form text (from ASR) to the written form is in urgent need. This
inverse text normalization (ITN) problem attracts the attention of researchers
from various fields. Recently, several works show that data-driven ITN methods
can output high-quality written form text. Due to the scarcity of labeled
spoken-written datasets, the studies on non-English data-driven ITN are quite
limited. In this work, we propose a language-agnostic data-driven ITN framework
to fill this gap. Specifically, we leverage the data augmentation in
conjunction with neural machine translated data for low resource languages.
Moreover, we design an evaluation method for language agnostic ITN model when
only English data is available. Our empirical evaluation shows this language
agnostic modeling approach is effective for low resource languages while
preserving the performance for high resource languages.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ K-Planes: Explicit Radiance Fields in Space, Time, and Appearance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Fridovich-Keil, Giacomo Meanti, Frederik Warburg, Benjamin Recht, Angjoo Kanazawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce k-planes, a white-box model for radiance fields in arbitrary
dimensions. Our model uses d choose 2 planes to represent a d-dimensional
scene, providing a seamless way to go from static (d=3) to dynamic (d=4)
scenes. This planar factorization makes adding dimension-specific priors easy,
e.g. temporal smoothness and multi-resolution spatial structure, and induces a
natural decomposition of static and dynamic components of a scene. We use a
linear feature decoder with a learned color basis that yields similar
performance as a nonlinear black-box MLP decoder. Across a range of synthetic
and real, static and dynamic, fixed and varying appearance scenes, k-planes
yields competitive and often state-of-the-art reconstruction fidelity with low
memory usage, achieving 1000x compression over a full 4D grid, and fast
optimization with a pure PyTorch implementation. For video results and code,
please see sarafridov.github.io/K-Planes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page https://sarafridov.github.io/K-Planes/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RangeViT: Towards Vision <span class="highlight-title">Transformer</span>s for 3D Semantic Segmentation in
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angelika Ando, Spyros Gidaris, Andrei Bursuc, Gilles Puy, Alexandre Boulch, Renaud Marlet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Casting semantic segmentation of outdoor LiDAR point clouds as a 2D problem,
e.g., via range projection, is an effective and popular approach. These
projection-based methods usually benefit from fast computations and, when
combined with techniques which use other point cloud representations, achieve
state-of-the-art results. Today, projection-based methods leverage 2D CNNs but
recent advances in computer vision show that vision transformers (ViTs) have
achieved state-of-the-art results in many image-based benchmarks. In this work,
we question if projection-based methods for 3D semantic segmentation can
benefit from these latest improvements on ViTs. We answer positively but only
after combining them with three key ingredients: (a) ViTs are notoriously hard
to train and require a lot of training data to learn powerful representations.
By preserving the same backbone architecture as for RGB images, we can exploit
the knowledge from long training on large image collections that are much
cheaper to acquire and annotate than point clouds. We reach our best results
with pre-trained ViTs on large image datasets. (b) We compensate ViTs' lack of
inductive bias by substituting a tailored convolutional stem for the classical
linear embedding layer. (c) We refine pixel-wise predictions with a
convolutional decoder and a skip connection from the convolutional stem to
combine low-level but fine-grained features of the the convolutional stem with
the high-level but coarse predictions of the ViT encoder. With these
ingredients, we show that our method, called RangeViT, outperforms existing
projection-based methods on nuScenes and SemanticKITTI. We provide the
implementation code at https://github.com/valeoai/rangevit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code at https://github.com/valeoai/rangevit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting and measuring human gastric peristalsis using magnetically
  controlled capsule endoscope 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueshen Li, Yu Gan, David Duan, Xiao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetically controlled capsule endoscope (MCCE) is an emerging tool for the
diagnosis of gastric diseases with the advantages of comfort, safety, and no
anesthesia. In this paper, we develop algorithms to detect and measure human
gastric peristalsis (contraction wave) using video sequences acquired by MCCE.
We develop a spatial-temporal deep learning algorithm to detect gastric
contraction waves and measure human gastric peristalsis periods. The quality of
MCCE video sequences is prone to camera motion. We design a camera motion
detector (CMD) to process the MCCE video sequences, mitigating the camera
movement during MCCE examination. To the best of our knowledge, we are the
first to propose computer vision-based solutions to detect and measure human
gastric peristalsis. Our methods have great potential in assisting the
diagnosis of gastric diseases by evaluating gastric motility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures, accepted by IEEE ISBI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple Adaptive Unfolding Network for Hyperspectral Image
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Wang, Shijie Wang, Wenyu Liu, Zengqiang Zheng, Xinggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a simple, efficient, and scalable unfolding network, SAUNet, to
simplify the network design with an adaptive alternate optimization framework
for hyperspectral image (HSI) reconstruction. SAUNet customizes a Residual
Adaptive ADMM Framework (R2ADMM) to connect each stage of the network via a
group of learnable parameters to promote the usage of mask prior, which greatly
stabilizes training and solves the accuracy degradation issue. Additionally, we
introduce a simple convolutional modulation block (CMB), which leads to
efficient training, easy scale-up, and less computation. Coupling these two
designs, SAUNet can be scaled to non-trivial 13 stages with continuous
improvement. Without bells and whistles, SAUNet improves both performance and
speed compared with the previous state-of-the-art counterparts, which makes it
feasible for practical high-resolution HSI reconstruction scenarios. We set new
records on CAVE and KAIST HSI reconstruction benchmarks. Code and models are
available at https://github.com/hustvl/SAUNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/hustvl/SAUNet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Sharp-GAN For Histopathology Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sujata Butte, Haotian Wang, Aleksandar Vakanski, Min Xian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Histopathology image synthesis aims to address the data shortage issue in
training deep learning approaches for accurate cancer detection. However,
existing methods struggle to produce realistic images that have accurate nuclei
boundaries and less artifacts, which limits the application in downstream
tasks. To address the challenges, we propose a novel approach that enhances the
quality of synthetic images by using nuclei topology and contour
regularization. The proposed approach uses the skeleton map of nuclei to
integrate nuclei topology and separate touching nuclei. In the loss function,
we propose two new contour regularization terms that enhance the contrast
between contour and non-contour pixels and increase the similarity between
contour pixels. We evaluate the proposed approach on the two datasets using
image quality metrics and a downstream task (nuclei segmentation). The proposed
approach outperforms Sharp-GAN in all four image quality metrics on two
datasets. By integrating 6k synthetic images from the proposed approach into
training, a nuclei segmentation model achieves the state-of-the-art
segmentation performance on TNBC dataset and its detection quality (DQ),
segmentation quality (SQ), panoptic quality (PQ), and aggregated Jaccard index
(AJI) is 0.855, 0.863, 0.691, and 0.683, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bipartite Graph Diffusion Model for Human Interaction Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baptiste Chopin, Hao Tang, Mohamed Daoudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The generation of natural human motion interactions is a hot topic in
computer vision and computer animation. It is a challenging task due to the
diversity of possible human motion interactions. Diffusion models, which have
already shown remarkable generative capabilities in other domains, are a good
candidate for this task. In this paper, we introduce a novel bipartite graph
diffusion method (BiGraphDiff) to generate human motion interactions between
two persons. Specifically, bipartite node sets are constructed to model the
inherent geometric constraints between skeleton nodes during interactions. The
interaction graph diffusion model is transformer-based, combining some
state-of-the-art motion methods. We show that the proposed achieves new
state-of-the-art results on leading benchmarks for the human interaction
generation task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Open-Set Semi-Supervised Learning with Self-Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Wallin, Lennart Svensson, Fredrik Kahl, Lars Hammarstrand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-set semi-supervised learning (OSSL) is a realistic setting of
semi-supervised learning where the unlabeled training set contains classes that
are not present in the labeled set. Many existing OSSL methods assume that
these out-of-distribution data are harmful and put effort into excluding data
from unknown classes from the training objective. In contrast, we propose an
OSSL framework that facilitates learning from all unlabeled data through
self-supervision. Additionally, we utilize an energy-based score to accurately
recognize data belonging to the known classes, making our method well-suited
for handling uncurated data in deployment. We show through extensive
experimental evaluations on several datasets that our method shows overall
unmatched robustness and performance in terms of closed-set accuracy and
open-set recognition compared with state-of-the-art for OSSL. Our code will be
released upon publication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using a Waffle Iron for Automotive Point Cloud Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gilles Puy, Alexandre Boulch, Renaud Marlet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation of point clouds in autonomous driving datasets requires
techniques that can process large numbers of points over large field of views.
Today, most deep networks designed for this task exploit 3D sparse convolutions
to reduce memory and computational loads. The best methods then further exploit
specificities of rotating lidar sampling patterns to further improve the
performance, e.g., cylindrical voxels, or range images (for feature fusion from
multiple point cloud representations). In contrast, we show that one can build
a well-performing point-based backbone free of these specialized tools. This
backbone, WaffleIron, relies heavily on generic MLPs and dense 2D convolutions,
making it easy to implement, and contains just a few parameters easy to tune.
Despite its simplicity, our experiments on SemanticKITTI and nuScenes show that
WaffleIron competes with the best methods designed specifically for these
autonomous driving datasets. Hence, WaffleIron is a strong, easy-to-implement,
baseline for semantic segmentation of sparse outdoor point clouds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model soups to increase inference without increasing compute time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles Dansereau, Milo Sobral, Maninder Bhogal, Mehdi Zalai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we compare Model Soups performances on three different models
(ResNet, ViT and EfficientNet) using three Soup Recipes (Greedy Soup Sorted,
Greedy Soup Random and Uniform soup) from arXiv:2203.05482, and reproduce the
results of the authors. We then introduce a new Soup Recipe called Pruned Soup.
Results from the soups were better than the best individual model for the
pre-trained vision transformer, but were much worst for the ResNet and the
EfficientNet. Our pruned soup performed better than the uniform and greedy
soups presented in the original paper. We also discuss the limitations of
weight-averaging that were found during the experiments. The code for our model
soup library and the experiments with different models can be found here:
https://github.com/milo-sobral/ModelSoup
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Planar Object Tracking via Weighted Optical Flow <span class="chip">WACV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10057v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10057v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Serych, Jiri Matas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose WOFT -- a novel method for planar object tracking that estimates a
full 8 degrees-of-freedom pose, i.e. the homography w.r.t. a reference view.
The method uses a novel module that leverages dense optical flow and assigns a
weight to each optical flow correspondence, estimating a homography by weighted
least squares in a fully differentiable manner. The trained module assigns zero
weights to incorrect correspondences (outliers) in most cases, making the
method robust and eliminating the need of the typically used non-differentiable
robust estimators like RANSAC. The proposed weighted optical flow tracker
(WOFT) achieves state-of-the-art performance on two benchmarks, POT-210 and
POIC, tracking consistently well across a wide range of scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Side Eye: Characterizing the Limits of POV Acoustic Eavesdropping from
  Smartphone Cameras with Rolling Shutters and Movable Lenses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Long, Pirouz Naghavi, Blas Kojusner, Kevin Butler, Sara Rampazzi, Kevin Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our research discovers how the rolling shutter and movable lens structures
widely found in smartphone cameras modulate structure-borne sounds onto camera
images, creating a point-of-view (POV) optical-acoustic side channel for
acoustic eavesdropping. The movement of smartphone camera hardware leaks
acoustic information because images unwittingly modulate ambient sound as
imperceptible distortions. Our experiments find that the side channel is
further amplified by intrinsic behaviors of Complementary
metal-oxide-semiconductor (CMOS) rolling shutters and movable lenses such as in
Optical Image Stabilization (OIS) and Auto Focus (AF). Our paper characterizes
the limits of acoustic information leakage caused by structure-borne sound that
perturbs the POV of smartphone cameras. In contrast with traditional
optical-acoustic eavesdropping on vibrating objects, this side channel requires
no line of sight and no object within the camera's field of view (images of a
ceiling suffice). Our experiments test the limits of this side channel with a
novel signal processing pipeline that extracts and recognizes the leaked
acoustic information. Our evaluation with 10 smartphones on a spoken digit
dataset reports 80.66%, 91.28%, and 99.67% accuracies on recognizing 10 spoken
digits, 20 speakers, and 2 genders respectively. We further systematically
discuss the possible defense strategies and implementations. By modeling,
measuring, and demonstrating the limits of acoustic eavesdropping from
smartphone camera image streams, our contributions explain the physics-based
causality and possible ways to reduce the threat on current and future devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Event Detection in Football using Graph Convolutional Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Sangram Singh Rana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The massive growth of data collection in sports has opened numerous avenues
for professional teams and media houses to gain insights from this data. The
data collected includes per frame player and ball trajectories, and event
annotations such as passes, fouls, cards, goals, etc. Graph Convolutional
Networks (GCNs) have recently been employed to process this highly unstructured
tracking data which can be otherwise difficult to model because of lack of
clarity on how to order players in a sequence and how to handle missing objects
of interest. In this thesis, we focus on the goal of automatic event detection
from football videos. We show how to model the players and the ball in each
frame of the video sequence as a graph, and present the results for graph
convolutional layers and pooling methods that can be used to model the temporal
context present around each action.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wise-IoU: Bounding Box Regression Loss with Dynamic Focusing Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zanjia Tong, Yuhang Chen, Zewei Xu, Rong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The loss function for bounding box regression (BBR) is essential to object
detection. Its good definition will bring significant performance improvement
to the model. Most existing works assume that the examples in the training data
are high-quality and focus on strengthening the fitting ability of BBR loss. If
we blindly strengthen BBR on low-quality examples, it will jeopardize
localization performance. Focal-EIoU v1 was proposed to solve this problem, but
due to its static focusing mechanism (FM), the potential of non-monotonic FM
was not fully exploited. Based on this idea, we propose an IoU-based loss with
a dynamic non-monotonic FM named Wise-IoU (WIoU). When WIoU is applied to the
state-of-the-art real-time detector YOLOv7, the AP-75 on the MS-COCO dataset is
improved from 53.03% to 54.50%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Optical Flow Guidance for <span class="highlight-title">Transformer</span>-Based Video Inpainting <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaidong Zhang, Jialun Peng, Jingjing Fu, Dong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have been widely used for video processing owing to the
multi-head self attention (MHSA) mechanism. However, the MHSA mechanism
encounters an intrinsic difficulty for video inpainting, since the features
associated with the corrupted regions are degraded and incur inaccurate self
attention. This problem, termed query degradation, may be mitigated by first
completing optical flows and then using the flows to guide the self attention,
which was verified in our previous work - flow-guided transformer (FGT). We
further exploit the flow guidance and propose FGT++ to pursue more effective
and efficient video inpainting. First, we design a lightweight flow completion
network by using local aggregation and edge loss. Second, to address the query
degradation, we propose a flow guidance feature integration module, which uses
the motion discrepancy to enhance the features, together with a flow-guided
feature propagation module that warps the features according to the flows.
Third, we decouple the transformer along the temporal and spatial dimensions,
where flows are used to select the tokens through a temporally deformable MHSA
mechanism, and global tokens are combined with the inner-window local tokens
through a dual perspective MHSA mechanism. FGT++ is experimentally evaluated to
be outperforming the existing video inpainting networks qualitatively and
quantitatively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript is a journal extension of our ECCV 2022 paper
  (arXiv:2208.06768)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffMotion: Speech-Driven Gesture Synthesis Using Denoising Diffusion
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10047v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10047v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Zhang, Naye Ji, Fuxing Gao, Yongping Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech-driven gesture synthesis is a field of growing interest in virtual
human creation. However, a critical challenge is the inherent intricate
one-to-many mapping between speech and gestures. Previous studies have explored
and achieved significant progress with generative models. Notwithstanding, most
synthetic gestures are still vastly less natural. This paper presents
DiffMotion, a novel speech-driven gesture synthesis architecture based on
diffusion models. The model comprises an autoregressive temporal encoder and a
denoising diffusion probability Module. The encoder extracts the temporal
context of the speech input and historical gestures. The diffusion module
learns a parameterized Markov chain to gradually convert a simple distribution
into a complex distribution and generates the gestures according to the
accompanied speech. Compared with baselines, objective and subjective
evaluations confirm that our approach can produce natural and diverse
gesticulation and demonstrate the benefits of diffusion-based models on
speech-driven gesture synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progressive Meta-Pooling Learning for Lightweight Image Classification
  Model <span class="chip">ICASSP23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peijie Dong, Xin Niu, Zhiliang Tian, Lujun Li, Xiaodong Wang, Zimian Wei, Hengyue Pan, Dongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Practical networks for edge devices adopt shallow depth and small
convolutional kernels to save memory and computational cost, which leads to a
restricted receptive field. Conventional efficient learning methods focus on
lightweight convolution designs, ignoring the role of the receptive field in
neural network design. In this paper, we propose the Meta-Pooling framework to
make the receptive field learnable for a lightweight network, which consists of
parameterized pooling-based operations. Specifically, we introduce a
parameterized spatial enhancer, which is composed of pooling operations to
provide versatile receptive fields for each layer of a lightweight model. Then,
we present a Progressive Meta-Pooling Learning (PMPL) strategy for the
parameterized spatial enhancer to acquire a suitable receptive field size. The
results on the ImageNet dataset demonstrate that MobileNetV2 using Meta-Pooling
achieves top1 accuracy of 74.6\%, which outperforms MobileNetV2 by 2.3\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, ICASSP23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-shot Font Generation by Learning Style Difference and Similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao He, Mingrui Zhu, Nannan Wang, Xinbo Gao, Heng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot font generation (FFG) aims to preserve the underlying global
structure of the original character while generating target fonts by referring
to a few samples. It has been applied to font library creation, a personalized
signature, and other scenarios. Existing FFG methods explicitly disentangle
content and style of reference glyphs universally or component-wisely. However,
they ignore the difference between glyphs in different styles and the
similarity of glyphs in the same style, which results in artifacts such as
local distortions and style inconsistency. To address this issue, we propose a
novel font generation approach by learning the Difference between different
styles and the Similarity of the same style (DS-Font). We introduce contrastive
learning to consider the positive and negative relationship between styles.
Specifically, we propose a multi-layer style projector for style encoding and
realize a distinctive style representation via our proposed Cluster-level
Contrastive Style (CCS) loss. In addition, we design a multi-task patch
discriminator, which comprehensively considers different areas of the image and
ensures that each style can be distinguished independently. We conduct
qualitative and quantitative evaluations comprehensively to demonstrate that
our approach achieves significantly better results than state-of-the-art
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-Aware Distillation for Semi-Supervised Few-Shot
  Class-Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yawen Cui, Wanxia Deng, Haoyu Chen, Li Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a model well-trained with a large-scale base dataset, Few-Shot
Class-Incremental Learning (FSCIL) aims at incrementally learning novel classes
from a few labeled samples by avoiding overfitting, without catastrophically
forgetting all encountered classes previously. Currently, semi-supervised
learning technique that harnesses freely-available unlabeled data to compensate
for limited labeled data can boost the performance in numerous vision tasks,
which heuristically can be applied to tackle issues in FSCIL, i.e., the
Semi-supervised FSCIL (Semi-FSCIL). So far, very limited work focuses on the
Semi-FSCIL task, leaving the adaptability issue of semi-supervised learning to
the FSCIL task unresolved. In this paper, we focus on this adaptability issue
and present a simple yet efficient Semi-FSCIL framework named Uncertainty-aware
Distillation with Class-Equilibrium (UaD-CE), encompassing two modules UaD and
CE. Specifically, when incorporating unlabeled data into each incremental
session, we introduce the CE module that employs a class-balanced self-training
to avoid the gradual dominance of easy-to-classified classes on pseudo-label
generation. To distill reliable knowledge from the reference model, we further
implement the UaD module that combines uncertainty-guided knowledge refinement
with adaptive distillation. Comprehensive experiments on three benchmark
datasets demonstrate that our method can boost the adaptability of unlabeled
data with the semi-supervised learning technique in FSCIL tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Neural Networks and Learning
  Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Interactive Lung Lesion Segmentation: A Framework for
  Annotating PET/CT Images based on Physiological and Anatomical Cues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Verena Jasmin Hallitschke, Tobias Schlumberger, Philipp Kataliakos, Zdravko Marinov, Moon Kim, Lars Heiliger, Constantin Seibold, Jens Kleesiek, Rainer Stiefelhagen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, deep learning enabled the accurate segmentation of various diseases
in medical imaging. These performances, however, typically demand large amounts
of manual voxel annotations. This tedious process for volumetric data becomes
more complex when not all required information is available in a single imaging
domain as is the case for PET/CT data. We propose a multimodal interactive
segmentation framework that mitigates these issues by combining anatomical and
physiological cues from PET/CT data. Our framework utilizes the geodesic
distance transform to represent the user annotations and we implement a novel
ellipsoid-based user simulation scheme during training. We further propose two
annotation interfaces and conduct a user study to estimate their usability. We
evaluated our model on the in-domain validation dataset and an unseen PET/CT
dataset. We make our code publicly available:
https://github.com/verena-hallitschke/pet-ct-annotate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ISBI 2023; 5 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transfer Learning for Olfactory Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathias Zinnen, Prathmesh Madhu, Peter Bell, Andreas Maier, Vincent Christlein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the effect of style and category similarity in multiple
datasets used for object detection pretraining. We find that including an
additional stage of object-detection pretraining can increase the detection
performance considerably. While our experiments suggest that style similarities
between pre-training and target datasets are less important than matching
categories, further experiments are needed to verify this hypothesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep learning-based method for segmenting epithelial layer of tubules in
  histopathological images of testicular tissue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azadeh Fakhrzadeh, Pouya Karimian, Mahsa Meyari, Cris L. Luengo Hendriks, Lena Holm, Christian Sonne, Rune Dietz, Ellinor Spörndly-Nees
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is growing concern that male reproduction is affected by environmental
chemicals. One way to determine the adverse effect of environmental pollutants
is to use wild animals as monitors and evaluate testicular toxicity using
histopathology. Automated methods are necessary tools in the quantitative
assessment of histopathology to overcome the subjectivity of manual evaluation
and accelerate the process. We propose an automated method to process histology
images of testicular tissue. Segmenting the epithelial layer of the
seminiferous tubule is a prerequisite for developing automated methods to
detect abnormalities in tissue. We suggest an encoder-decoder fully connected
convolutional neural network (F-CNN) model to segment the epithelial layer of
the seminiferous tubules in histological images. Using ResNet-34 modules in the
encoder adds a shortcut mechanism to avoid the gradient vanishing and
accelerate the network convergence. The squeeze & excitation (SE) attention
block is integrated into the encoding module improving the segmentation and
localization of epithelium. We applied the proposed method for the 2-class
problem where the epithelial layer of the tubule is the target class. The
f-score and IoU of the proposed method are 0.85 and 0.92. Although the proposed
method is trained on a limited training set, it performs well on an independent
dataset and outperforms other state-of-the-art methods. The pretrained
ResNet-34 in the encoder and attention block suggested in the decoder result in
better segmentation and generalization. The proposed method can be applied to
testicular tissue images from any mammalian species and can be used as the
first part of a fully automated testicular tissue processing pipeline. The
dataset and codes are publicly available on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to Journal of Medical Imaging, 16 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Augmentation Alone Can Improve Adversarial Training <span class="chip">ICLR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Li, Michael Spratling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial training suffers from the issue of robust overfitting, which
seriously impairs its generalization performance. Data augmentation, which is
effective at preventing overfitting in standard training, has been observed by
many previous works to be ineffective in mitigating overfitting in adversarial
training. This work proves that, contrary to previous findings, data
augmentation alone can significantly boost accuracy and robustness in
adversarial training. We find that the hardness and the diversity of data
augmentation are important factors in combating robust overfitting. In general,
diversity can improve both accuracy and robustness, while hardness can boost
robustness at the cost of accuracy within a certain limit and degrade them both
over that limit. To mitigate robust overfitting, we first propose a new crop
transformation, Cropshift, which has improved diversity compared to the
conventional one (Padcrop). We then propose a new data augmentation scheme,
based on Cropshift, with much improved diversity and well-balanced hardness.
Empirically, our augmentation method achieves the state-of-the-art accuracy and
robustness for data augmentations in adversarial training. Furthermore, when
combined with weight averaging it matches, or even exceeds, the performance of
the best contemporary regularization methods for alleviating robust
overfitting. Code is available at:
https://github.com/TreeLLi/DA-Alone-Improves-AT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>published at conference ICLR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ODOR: The ICPR2022 ODeuropa Challenge on Olfactory Object Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathias Zinnen, Prathmesh Madhu, Ronak Kosti, Peter Bell, Andreas Maier, Vincent Christlein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Odeuropa Challenge on Olfactory Object Recognition aims to foster the
development of object detection in the visual arts and to promote an olfactory
perspective on digital heritage. Object detection in historical artworks is
particularly challenging due to varying styles and artistic periods. Moreover,
the task is complicated due to the particularity and historical variance of
predefined target objects, which exhibit a large intra-class variance, and the
long tail distribution of the dataset labels, with some objects having only
very few training examples. These challenges should encourage participants to
create innovative approaches using domain adaptation or few-shot learning. We
provide a dataset of 2647 artworks annotated with 20 120 tightly fit bounding
boxes that are split into a training and validation set (public). A test set
containing 1140 artworks and 15 480 annotations is kept private for the
challenge evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Super-Resolution using Efficient Striped Window <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinpeng Shi, Hui Li, Tianle Liu, Yulong Liu, Mingjian Zhang, Jinchen Zhu, Ling Zheng, Shizhuang Weng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, transformer-based methods have made impressive progress in
single-image super-resolu-tion (SR). However, these methods are difficult to
apply to lightweight SR (LSR) due to the challenge of balancing model
performance and complexity. In this paper, we propose an efficient striped
window transformer (ESWT). ESWT consists of efficient transformation layers
(ETLs), allowing a clean structure and avoiding redundant operations. Moreover,
we designed a striped window mechanism to obtain a more efficient ESWT in
modeling long-term dependencies. To further exploit the potential of the
transformer, we propose a novel flexible window training strategy. Without any
additional cost, this strategy can further improve the performance of ESWT.
Extensive experiments show that the proposed method outperforms
state-of-the-art transformer-based LSR methods with fewer parameters, faster
inference, smaller FLOPs, and less memory consumption, achieving a better
trade-off between model performance and complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SOTA lightweight super-resolution transformer. 9 pages, 13 figures
  and tables. The Code is available at
  https://github.com/Fried-Rice-Lab/FriedRiceLab</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PowerQuant: Automorphism Search for Non-Uniform Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, Kevin Bailly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) are nowadays ubiquitous in many domains such as
computer vision. However, due to their high latency, the deployment of DNNs
hinges on the development of compression techniques such as quantization which
consists in lowering the number of bits used to encode the weights and
activations. Growing concerns for privacy and security have motivated the
development of data-free techniques, at the expanse of accuracy. In this paper,
we identity the uniformity of the quantization operator as a limitation of
existing approaches, and propose a data-free non-uniform method. More
specifically, we argue that to be readily usable without dedicated hardware and
implementation, non-uniform quantization shall not change the nature of the
mathematical operations performed by the DNN. This leads to search among the
continuous automorphisms of $(\mathbb{R}_+^*,\times)$, which boils down to the
power functions defined by their exponent. To find this parameter, we propose
to optimize the reconstruction error of each layer: in particular, we show that
this procedure is locally convex and admits a unique solution. At inference
time, we show that our approach, dubbed PowerQuant, only require simple
modifications in the quantized DNN activation functions. As such, with only
negligible overhead, it significantly outperforms existing methods in a variety
of configurations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RD-NAS: Enhancing One-shot Supernet Ranking Ability via Ranking
  Distillation from Zero-cost Proxies <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peijie Dong, Xin Niu, Lujun Li, Zhiliang Tian, Xiaodong Wang, Zimian Wei, Hengyue Pan, Dongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural architecture search (NAS) has made tremendous progress in the
automatic design of effective neural network structures but suffers from a
heavy computational burden. One-shot NAS significantly alleviates the burden
through weight sharing and improves computational efficiency. Zero-shot NAS
further reduces the cost by predicting the performance of the network from its
initial state, which conducts no training. Both methods aim to distinguish
between "good" and "bad" architectures, i.e., ranking consistency of predicted
and true performance. In this paper, we propose Ranking Distillation one-shot
NAS (RD-NAS) to enhance ranking consistency, which utilizes zero-cost proxies
as the cheap teacher and adopts the margin ranking loss to distill the ranking
knowledge. Specifically, we propose a margin subnet sampler to distill the
ranking knowledge from zero-shot NAS to one-shot NAS by introducing Group
distance as margin. Our evaluation of the NAS-Bench-201 and ResNet-based search
space demonstrates that RD-NAS achieve 10.7\% and 9.65\% improvements in
ranking ability, respectively. Our codes are available at
https://github.com/pprp/CVPR2022-NAS-competition-Track1-3th-solution
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, 4 tables, ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LDMIC: Learning-based Distributed Multi-view Image Coding <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinjie Zhang, Jiawei Shao, Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-view image compression plays a critical role in 3D-related
applications. Existing methods adopt a predictive coding architecture, which
requires joint encoding to compress the corresponding disparity as well as
residual information. This demands collaboration among cameras and enforces the
epipolar geometric constraint between different views, which makes it
challenging to deploy these methods in distributed camera systems with randomly
overlapping fields of view. Meanwhile, distributed source coding theory
indicates that efficient data compression of correlated sources can be achieved
by independent encoding and joint decoding, which motivates us to design a
learning-based distributed multi-view image coding (LDMIC) framework. With
independent encoders, LDMIC introduces a simple yet effective joint context
transfer module based on the cross-attention mechanism at the decoder to
effectively capture the global inter-view correlations, which is insensitive to
the geometric relationships between images. Experimental results show that
LDMIC significantly outperforms both traditional and learning-based MIC methods
while enjoying fast encoding speed. Code will be released at
https://github.com/Xinjie-Q/LDMIC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SIAN: Style-Guided Instance-Adaptive Normalization for Multi-Organ
  Histopathology Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.02412v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.02412v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotian Wang, Min Xian, Aleksandar Vakanski, Bryar Shareef
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing deep neural networks for histopathology image synthesis cannot
generate image styles that align with different organs, and cannot produce
accurate boundaries of clustered nuclei. To address these issues, we propose a
style-guided instance-adaptive normalization (SIAN) approach to synthesize
realistic color distributions and textures for histopathology images from
different organs. SIAN contains four phases, semantization, stylization,
instantiation, and modulation. The first two phases synthesize image semantics
and styles by using semantic maps and learned image style vectors. The
instantiation module integrates geometrical and topological information and
generates accurate nuclei boundaries. We validate the proposed approach on a
multiple-organ dataset, Extensive experimental results demonstrate that the
proposed method generates more realistic histopathology images than four
state-of-the-art approaches for five organs. By incorporating synthetic images
from the proposed approach to model training, an instance segmentation network
can achieve state-of-the-art performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain generalization in deep learning-based mass detection in
  mammography: A large-scale multi-center study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.11620v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.11620v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lidia Garrucho, Kaisar Kushibar, Socayna Jouide, Oliver Diaz, Laura Igual, Karim Lekadir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer-aided detection systems based on deep learning have shown great
potential in breast cancer detection. However, the lack of domain
generalization of artificial neural networks is an important obstacle to their
deployment in changing clinical environments. In this work, we explore the
domain generalization of deep learning methods for mass detection in digital
mammography and analyze in-depth the sources of domain shift in a large-scale
multi-center setting. To this end, we compare the performance of eight
state-of-the-art detection methods, including Transformer-based models, trained
in a single domain and tested in five unseen domains. Moreover, a single-source
mass detection training pipeline is designed to improve the domain
generalization without requiring images from the new domain. The results show
that our workflow generalizes better than state-of-the-art transfer
learning-based approaches in four out of five domains while reducing the domain
shift caused by the different acquisition protocols and scanner manufacturers.
Subsequently, an extensive analysis is performed to identify the covariate
shifts with bigger effects on the detection performance, such as due to
differences in patient age, breast density, mass size, and mass malignancy.
Ultimately, this comprehensive study provides key insights and best practices
for future research on domain generalization in deep learning-based breast
cancer detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Augmented Image Prior: Distilling 1000 Classes by Extrapolating from
  a Single Image <span class="chip">ICLR'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.00725v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.00725v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuki M. Asano, Aaqib Saeed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What can neural networks learn about the visual world when provided with only
a single image as input? While any image obviously cannot contain the
multitudes of all existing objects, scenes and lighting conditions - within the
space of all 256^(3x224x224) possible 224-sized square images, it might still
provide a strong prior for natural images. To analyze this `augmented image
prior' hypothesis, we develop a simple framework for training neural networks
from scratch using a single image and augmentations using knowledge
distillation from a supervised pretrained teacher. With this, we find the
answer to the above question to be: `surprisingly, a lot'. In quantitative
terms, we find accuracies of 94%/74% on CIFAR-10/100, 69% on ImageNet, and by
extending this method to video and audio, 51% on Kinetics-400 and 84% on
SpeechCommands. In extensive analyses spanning 13 datasets, we disentangle the
effect of augmentations, choice of data and network architectures and also
provide qualitative evaluations that include lucid `panda neurons' in networks
that have never even seen one.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR'23. Webpage:
  https://single-image-distill.github.io/, code:
  https://github.com/yukimasano/single-img-extrapolating</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-resolution synthesis of high-density breast mammograms: Application
  to improved fairness in deep learning based mass detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.09809v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.09809v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lidia Garrucho, Kaisar Kushibar, Richard Osuala, Oliver Diaz, Alessandro Catanese, Javier del Riego, Maciej Bobowicz, Fredrik Strand, Laura Igual, Karim Lekadir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer-aided detection systems based on deep learning have shown good
performance in breast cancer detection. However, high-density breasts show
poorer detection performance since dense tissues can mask or even simulate
masses. Therefore, the sensitivity of mammography for breast cancer detection
can be reduced by more than 20% in dense breasts. Additionally, extremely dense
cases reported an increased risk of cancer compared to low-density breasts.
This study aims to improve the mass detection performance in highdensity
breasts using synthetic high-density full-field digital mammograms (FFDM) as
data augmentation during breast mass detection model training. To this end, a
total of five cycle-consistent GAN (CycleGAN) models using three FFDM datasets
were trained for low-to-high-density image translation in highresolution
mammograms. The training images were split by breast density BIRADS categories,
being BI-RADS A almost entirely fatty and BI-RADS D extremely dense breasts.
Our results showed that the proposed data augmentation technique improved the
sensitivity and precision of mass detection in models trained with small
datasets and improved the domain generalization of the models trained with
large databases. In addition, the clinical realism of the synthetic images was
evaluated in a reader study involving two expert radiologists and one surgical
oncologist.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ESTAS: Effective and Stable Trojan Attacks in <span class="highlight-title">Self-supervised</span> Encoders
  with One Target Unlabelled Sample 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10908v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10908v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Xue, Qian Lou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emerging self-supervised learning (SSL) has become a popular image
representation encoding method to obviate the reliance on labeled data and
learn rich representations from large-scale, ubiquitous unlabelled data. Then
one can train a downstream classifier on top of the pre-trained SSL image
encoder with few or no labeled downstream data. Although extensive works show
that SSL has achieved remarkable and competitive performance on different
downstream tasks, its security concerns, e.g, Trojan attacks in SSL encoders,
are still not well-studied. In this work, we present a novel Trojan Attack
method, denoted by ESTAS, that can enable an effective and stable attack in SSL
encoders with only one target unlabeled sample. In particular, we propose
consistent trigger poisoning and cascade optimization in ESTAS to improve
attack efficacy and model accuracy, and eliminate the expensive target-class
data sample extraction from large-scale disordered unlabelled data. Our
substantial experiments on multiple datasets show that ESTAS stably achieves >
99% attacks success rate (ASR) with one target-class sample. Compared to prior
works, ESTAS attains > 30% ASR increase and > 8.3% accuracy improvement on
average.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-Supervised</span> Learning Through Efference Copies <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.09224v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.09224v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Franz Scherr, Qinghai Guo, Timoleon Moraitis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) methods aim to exploit the abundance of
unlabelled data for machine learning (ML), however the underlying principles
are often method-specific. An SSL framework derived from biological first
principles of embodied learning could unify the various SSL methods, help
elucidate learning in the brain, and possibly improve ML. SSL commonly
transforms each training datapoint into a pair of views, uses the knowledge of
this pairing as a positive (i.e. non-contrastive) self-supervisory sign, and
potentially opposes it to unrelated, (i.e. contrastive) negative examples.
Here, we show that this type of self-supervision is an incomplete
implementation of a concept from neuroscience, the Efference Copy (EC).
Specifically, the brain also transforms the environment through efference, i.e.
motor commands, however it sends to itself an EC of the full commands, i.e.
more than a mere SSL sign. In addition, its action representations are likely
egocentric. From such a principled foundation we formally recover and extend
SSL methods such as SimCLR, BYOL, and ReLIC under a common theoretical
framework, i.e. Self-supervision Through Efference Copies (S-TEC). Empirically,
S-TEC restructures meaningfully the within- and between-class representations.
This manifests as improvement in recent strong SSL baselines in image
classification, segmentation, object detection, and in audio. These results
hypothesize a testable positive influence from the brain's motor outputs onto
its sensory representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ To Trust or Not To Trust Prediction Scores for Membership Inference
  Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.09076v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.09076v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Hintersdorf, Lukas Struppek, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Membership inference attacks (MIAs) aim to determine whether a specific
sample was used to train a predictive model. Knowing this may indeed lead to a
privacy breach. Most MIAs, however, make use of the model's prediction scores -
the probability of each output given some input - following the intuition that
the trained model tends to behave differently on its training data. We argue
that this is a fallacy for many modern deep network architectures.
Consequently, MIAs will miserably fail since overconfidence leads to high
false-positive rates not only on known domains but also on out-of-distribution
data and implicitly acts as a defense against MIAs. Specifically, using
generative adversarial networks, we are able to produce a potentially infinite
number of samples falsely classified as part of the training data. In other
words, the threat of MIAs is overestimated, and less information is leaked than
previously assumed. Moreover, there is actually a trade-off between the
overconfidence of models and their susceptibility to MIAs: the more classifiers
know when they do not know, making low confidence predictions, the more they
reveal the training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures, 10 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Implicit k-Space for Binning-free Non-Cartesian Cardiac MR
  Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08479v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08479v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Huang, Hongwei Li, Jiazhen Pan, Gastao Cruz, Daniel Rueckert, Kerstin Hammernik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a novel image reconstruction framework that directly
learns a neural implicit representation in k-space for ECG-triggered
non-Cartesian Cardiac Magnetic Resonance Imaging (CMR). While existing methods
bin acquired data from neighboring time points to reconstruct one phase of the
cardiac motion, our framework allows for a continuous, binning-free, and
subject-specific k-space representation.We assign a unique coordinate that
consists of time, coil index, and frequency domain location to each sampled
k-space point. We then learn the subject-specific mapping from these unique
coordinates to k-space intensities using a multi-layer perceptron with
frequency domain regularization. During inference, we obtain a complete k-space
for Cartesian coordinates and an arbitrary temporal resolution. A simple
inverse Fourier transform recovers the image, eliminating the need for density
compensation and costly non-uniform Fourier transforms for non-Cartesian data.
This novel imaging framework was tested on 42 radially sampled datasets from 6
subjects. The proposed method outperforms other techniques qualitatively and
quantitatively using data from four and one heartbeat(s) and 30 cardiac phases.
Our results for one heartbeat reconstruction of 50 cardiac phases show improved
artifact removal and spatio-temporal resolution, leveraging the potential for
real-time CMR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08664v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08664v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingting Yuan, Liang Mi, Weijun Wang, Haipeng Dai, Xiaoming Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quality of the video stream is key to neural network-based video
analytics. However, low-quality video is inevitably collected by existing
surveillance systems because of poor quality cameras or over-compressed/pruned
video streaming protocols, e.g., as a result of upstream bandwidth limit. To
address this issue, existing studies use quality enhancers (e.g., neural
super-resolution) to improve the quality of videos (e.g., resolution) and
eventually ensure inference accuracy. Nevertheless, directly applying quality
enhancers does not work in practice because it will introduce unacceptable
latency. In this paper, we present AccDecoder, a novel accelerated decoder for
real-time and neural-enhanced video analytics. AccDecoder can select a few
frames adaptively via Deep Reinforcement Learning (DRL) to enhance the quality
by neural super-resolution and then up-scale the unselected frames that
reference them, which leads to 6-21% accuracy improvement. AccDecoder provides
efficient inference capability via filtering important frames using DRL for
DNN-based inference and reusing the results for the other frames via extracting
the reference relationship among frames and blocks, which results in a latency
reduction of 20-80% than baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2023 IEEE INFOCOM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robustness through Data Augmentation Loss Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.11205v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.11205v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianjian Huang, Shaunak Halbe, Chinnadhurai Sankar, Pooyan Amini, Satwik Kottur, Alborz Geramifard, Meisam Razaviyayn, Ahmad Beirami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep learning through empirical risk minimization (ERM) has succeeded
at achieving human-level performance at a variety of complex tasks, ERM is not
robust to distribution shifts or adversarial attacks. Synthetic data
augmentation followed by empirical risk minimization (DA-ERM) is a simple and
widely used solution to improve robustness in ERM. In addition, consistency
regularization can be applied to further improve the robustness of the model by
forcing the representation of the original sample and the augmented one to be
similar. However, existing consistency regularization methods are not
applicable to covariant data augmentation, where the label in the augmented
sample is dependent on the augmentation function. For example, dialog state
covaries with named entity when we augment data with a new named entity. In
this paper, we propose data augmented loss invariant regularization (DAIR), a
simple form of consistency regularization that is applied directly at the loss
level rather than intermediate features, making it widely applicable to both
invariant and covariant data augmentation regardless of network architecture,
problem setup, and task. We apply DAIR to real-world learning problems
involving covariant data augmentation: robust neural task-oriented dialog state
tracking and robust visual question answering. We also apply DAIR to tasks
involving invariant data augmentation: robust regression, robust classification
against adversarial attacks, and robust ImageNet classification under
distribution shift. Our experiments show that DAIR consistently outperforms ERM
and DA-ERM with little marginal computational cost and sets new
state-of-the-art results in several benchmarks involving covariant data
augmentation. Our code of all experiments is available at:
https://github.com/optimization-for-data-driven-science/DAIR.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Employing similarity to highlight differences: On the impact of
  anatomical assumptions in chest X-ray registration methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09338v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09338v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Astrid Berg, Eva Vandersmissen, Maria Wimmer, David Major, Theresa Neubauer, Dimitrios Lenis, Jeroen Cant, Annemiek Snoeckx, Katja Bühler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To facilitate both the detection and the interpretation of findings in chest
X-rays, comparison with a previous image of the same patient is very valuable
to radiologists. Today, the most common approach for deep learning methods to
automatically inspect chest X-rays disregards the patient history and
classifies only single images as normal or abnormal. Nevertheless, several
methods for assisting in the task of comparison through image registration have
been proposed in the past. However, as we illustrate, they tend to miss
specific types of pathological changes like cardiomegaly and effusion. Due to
assumptions on fixed anatomical structures or their measurements of
registration quality, they produce unnaturally deformed warp fields impacting
visualization of differences between moving and fixed images. We aim to
overcome these limitations, through a new paradigm based on individual rib pair
segmentation for anatomy penalized registration. Our method proves to be a
natural way to limit the folding percentage of the warp field to 1/6 of the
state of the art while increasing the overlap of ribs by more than 25%,
implying difference images showing pathological changes overlooked by other
methods. We develop an anatomically penalized convolutional multi-stage
solution on the National Institutes of Health (NIH) data set, starting from
less than 25 fully and 50 partly labeled training images, employing sequential
instance memory segmentation with hole dropout, weak labeling, coarse-to-fine
refinement and Gaussian mixture model histogram matching. We statistically
evaluate the benefits of our method and highlight the limits of currently used
metrics for registration of chest X-rays.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Study on the identification limits of craniofacial superimposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09461v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09461v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Óscar Ibáñez, Enrique Bermejo, Andrea Valsecchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Craniofacial Superimposition involves the superimposition of an image of a
skull with a number of ante-mortem face images of an individual and the
analysis of their morphological correspondence. Despite being used for one
century, it is not yet a mature and fully accepted technique due to the absence
of solid scientific approaches, significant reliability studies, and
international standards. In this paper we present a comprehensive
experimentation on the limitations of Craniofacial Superimposition as a
forensic identification technique. The study involves different experiments
over more than 1 Million comparisons performed by a landmark-based automatic
3D/2D superimposition method. The total sample analyzed consists of 320
subjects and 29 craniofacial landmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures. To be submitted to Scientific Reports</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Planckian Jitter: countering the color-crippling effects of color jitter
  on <span class="highlight-title">self-supervised</span> training <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.07993v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.07993v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Zini, Alex Gomez-Villa, Marco Buzzelli, Bartłomiej Twardowski, Andrew D. Bagdanov, Joost van de Weijer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several recent works on self-supervised learning are trained by mapping
different augmentations of the same image to the same feature representation.
The data augmentations used are of crucial importance to the quality of learned
feature representations. In this paper, we analyze how the color jitter
traditionally used in data augmentation negatively impacts the quality of the
color features in learned feature representations. To address this problem, we
propose a more realistic, physics-based color data augmentation - which we call
Planckian Jitter - that creates realistic variations in chromaticity and
produces a model robust to illumination changes that can be commonly observed
in real life, while maintaining the ability to discriminate image content based
on color information. Experiments confirm that such a representation is
complementary to the representations learned with the currently-used color
jitter augmentation and that a simple concatenation leads to significant
performance gains on a wide range of downstream datasets. In addition, we
present a color sensitivity analysis that documents the impact of different
training methods on model neurons and shows that the performance of the learned
features is robust with respect to illuminant variations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Eleventh International Conference on Learning
  Representations (ICLR 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A distribution-dependent Mumford-Shah model for unsupervised
  hyperspectral image segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.15058v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.15058v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan-Christopher Cohrs, Chandrajit Bajaj, Benjamin Berkels
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral images provide a rich representation of the underlying spectrum
for each pixel, allowing for a pixel-wise classification/segmentation into
different classes. As the acquisition of labeled training data is very
time-consuming, unsupervised methods become crucial in hyperspectral image
analysis. The spectral variability and noise in hyperspectral data make this
task very challenging and define special requirements for such methods.
  Here, we present a novel unsupervised hyperspectral segmentation framework.
It starts with a denoising and dimensionality reduction step by the
well-established Minimum Noise Fraction (MNF) transform. Then, the Mumford-Shah
(MS) segmentation functional is applied to segment the data. We equipped the MS
functional with a novel robust distribution-dependent indicator function
designed to handle the characteristic challenges of hyperspectral data. To
optimize our objective function with respect to the parameters for which no
closed form solution is available, we propose an efficient fixed point
iteration scheme. Numerical experiments on four public benchmark datasets show
that our method produces competitive results, which outperform three
state-of-the-art methods substantially on three of these datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FRAME: Fast and Robust Autonomous 3D point cloud Map-merging for
  Egocentric multi-robot exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09213v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09213v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Stathoulopoulos, Anton Koval, Ali-akbar Agha-mohammadi, George Nikolakopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article presents a 3D point cloud map-merging framework for egocentric
heterogeneous multi-robot exploration, based on overlap detection and
alignment, that is independent of a manual initial guess or prior knowledge of
the robots' poses. The novel proposed solution utilizes state-of-the-art place
recognition learned descriptors, that through the framework's main pipeline,
offer a fast and robust region overlap estimation, hence eliminating the need
for the time-consuming global feature extraction and feature matching process
that is typically used in 3D map integration. The region overlap estimation
provides a homogeneous rigid transform that is applied as an initial condition
in the point cloud registration algorithm Fast-GICP, which provides the final
and refined alignment. The efficacy of the proposed framework is experimentally
evaluated based on multiple field multi-robot exploration missions in
underground environments, where both ground and aerial robots are deployed,
with different sensor configurations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual <span class="highlight-title">Transformer</span>s: Redundancy-Free Attention for Online Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.06268v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.06268v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Hedegaard, Arian Bakhtiarnia, Alexandros Iosifidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers in their common form are inherently limited to operate on whole
token sequences rather than on one token at a time. Consequently, their use
during online inference on time-series data entails considerable redundancy due
to the overlap in successive token sequences. In this work, we propose novel
formulations of the Scaled Dot-Product Attention, which enable Transformers to
perform efficient online token-by-token inference on a continual input stream.
Importantly, our modifications are purely to the order of computations, while
the outputs and learned weights are identical to those of the original
Transformer Encoder. We validate our Continual Transformer Encoder with
experiments on the THUMOS14, TVSeries and GTZAN datasets with remarkable
results: Our Continual one- and two-block architectures reduce the floating
point operations per prediction by up to 63x and 2.6x, respectively, while
retaining predictive performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PaletteNeRF: Palette-based Appearance Editing of Neural Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10699v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10699v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengfei Kuang, Fujun Luan, Sai Bi, Zhixin Shu, Gordon Wetzstein, Kalyan Sunkavalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in neural radiance fields have enabled the high-fidelity 3D
reconstruction of complex scenes for novel view synthesis. However, it remains
underexplored how the appearance of such representations can be efficiently
edited while maintaining photorealism.
  In this work, we present PaletteNeRF, a novel method for photorealistic
appearance editing of neural radiance fields (NeRF) based on 3D color
decomposition. Our method decomposes the appearance of each 3D point into a
linear combination of palette-based bases (i.e., 3D segmentations defined by a
group of NeRF-type functions) that are shared across the scene. While our
palette-based bases are view-independent, we also predict a view-dependent
function to capture the color residual (e.g., specular shading). During
training, we jointly optimize the basis functions and the color palettes, and
we also introduce novel regularizers to encourage the spatial coherence of the
decomposition.
  Our method allows users to efficiently edit the appearance of the 3D scene by
modifying the color palettes. We also extend our framework with compressed
semantic features for semantic-aware appearance editing. We demonstrate that
our technique is superior to baseline methods both quantitatively and
qualitatively for appearance editing of complex real-world scenes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ E-NeRF: Neural Radiance Fields from a Moving Event Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.11300v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.11300v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Klenk, Lukas Koestler, Davide Scaramuzza, Daniel Cremers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating neural radiance fields (NeRFs) from "ideal" images has been
extensively studied in the computer vision community. Most approaches assume
optimal illumination and slow camera motion. These assumptions are often
violated in robotic applications, where images may contain motion blur, and the
scene may not have suitable illumination. This can cause significant problems
for downstream tasks such as navigation, inspection, or visualization of the
scene. To alleviate these problems, we present E-NeRF, the first method which
estimates a volumetric scene representation in the form of a NeRF from a
fast-moving event camera. Our method can recover NeRFs during very fast motion
and in high-dynamic-range conditions where frame-based approaches fail. We show
that rendering high-quality frames is possible by only providing an event
stream as input. Furthermore, by combining events and frames, we can estimate
NeRFs of higher quality than state-of-the-art approaches under severe motion
blur. We also show that combining events and frames can overcome failure cases
of NeRF estimation in scenarios where only a few input views are available
without requiring additional regularization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>revised RAL version + added suppl. material</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Contrastive Learning with Simple Transformation for 3D
  Point Cloud Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.06632v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.06632v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jincen Jiang, Xuequan Lu, Wanli Ouyang, Meili Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Though a number of point cloud learning methods have been proposed to handle
unordered points, most of them are supervised and require labels for training.
By contrast, unsupervised learning of point cloud data has received much less
attention to date. In this paper, we propose a simple yet effective approach
for unsupervised point cloud learning. In particular, we identify a very useful
transformation which generates a good contrastive version of an original point
cloud. They make up a pair. After going through a shared encoder and a shared
head network, the consistency between the output representations are maximized
with introducing two variants of contrastive losses to respectively facilitate
downstream classification and segmentation. To demonstrate the efficacy of our
method, we conduct experiments on three downstream tasks which are 3D object
classification (on ModelNet40 and ModelNet10), shape part segmentation (on
ShapeNet Part dataset) as well as scene segmentation (on S3DIS). Comprehensive
results show that our unsupervised contrastive representation learning enables
impressive outcomes in object classification and semantic segmentation. It
generally outperforms current unsupervised methods, and even achieves
comparable performance to supervised methods. Our source codes will be made
publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D UX-Net: A Large Kernel Volumetric ConvNet Modernizing Hierarchical
  <span class="highlight-title">Transformer</span> for Medical Image Segmentation <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.15076v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.15076v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ho Hin Lee, Shunxing Bao, Yuankai Huo, Bennett A. Landman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent 3D medical ViTs (e.g., SwinUNETR) achieve the state-of-the-art
performances on several 3D volumetric data benchmarks, including 3D medical
image segmentation. Hierarchical transformers (e.g., Swin Transformers)
reintroduced several ConvNet priors and further enhanced the practical
viability of adapting volumetric segmentation in 3D medical datasets. The
effectiveness of hybrid approaches is largely credited to the large receptive
field for non-local self-attention and the large number of model parameters. In
this work, we propose a lightweight volumetric ConvNet, termed 3D UX-Net, which
adapts the hierarchical transformer using ConvNet modules for robust volumetric
segmentation. Specifically, we revisit volumetric depth-wise convolutions with
large kernel size (e.g. starting from $7\times7\times7$) to enable the larger
global receptive fields, inspired by Swin Transformer. We further substitute
the multi-layer perceptron (MLP) in Swin Transformer blocks with pointwise
depth convolutions and enhance model performances with fewer normalization and
activation layers, thus reducing the number of model parameters. 3D UX-Net
competes favorably with current SOTA transformers (e.g. SwinUNETR) using three
challenging public datasets on volumetric brain and abdominal imaging: 1)
MICCAI Challenge 2021 FLARE, 2) MICCAI Challenge 2021 FeTA, and 3) MICCAI
Challenge 2022 AMOS. 3D UX-Net consistently outperforms SwinUNETR with
improvement from 0.929 to 0.938 Dice (FLARE2021) and 0.867 to 0.874 Dice
(Feta2021). We further evaluate the transfer learning capability of 3D UX-Net
with AMOS2022 and demonstrates another improvement of $2.27\%$ Dice (from 0.880
to 0.900). The source code with our proposed model are available at
https://github.com/MASILab/3DUX-Net.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Asteroid Detection in Microlensing <span class="highlight-title">Survey</span>s with Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.02239v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.02239v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Preeti Cowan, Ian A. Bond, Napoleon H. Reyes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Asteroids are an indelible part of most astronomical surveys though only a
few surveys are dedicated to their detection. Over the years, high cadence
microlensing surveys have amassed several terabytes of data while scanning
primarily the Galactic Bulge and Magellanic Clouds for microlensing events and
thus provide a treasure trove of opportunities for scientific data mining. In
particular, numerous asteroids have been observed by visual inspection of
selected images. This paper presents novel deep learning-based solutions for
the recovery and discovery of asteroids in the microlensing data gathered by
the MOA project. Asteroid tracklets can be clearly seen by combining all the
observations on a given night and these tracklets inform the structure of the
dataset. Known asteroids were identified within these composite images and used
for creating the labelled datasets required for supervised learning. Several
custom CNN models were developed to identify images with asteroid tracklets.
Model ensembling was then employed to reduce the variance in the predictions as
well as to improve the generalisation error, achieving a recall of 97.67%.
Furthermore, the YOLOv4 object detector was trained to localize asteroid
tracklets, achieving a mean Average Precision (mAP) of 90.97%. These trained
networks will be applied to 16 years of MOA archival data to find both known
and unknown asteroids that have been observed by the survey over the years. The
methodologies developed can be adapted for use by other surveys for asteroid
recovery and discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 17 figures, to be published in Astronomy and Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The 8-Point Algorithm as an Inductive Bias for Relative Pose Prediction
  by ViTs <span class="chip">3DV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.08988v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.08988v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Rockwell, Justin Johnson, David F. Fouhey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a simple baseline for directly estimating the relative pose
(rotation and translation, including scale) between two images. Deep methods
have recently shown strong progress but often require complex or multi-stage
architectures. We show that a handful of modifications can be applied to a
Vision Transformer (ViT) to bring its computations close to the Eight-Point
Algorithm. This inductive bias enables a simple method to be competitive in
multiple settings, often substantially improving over the state of the art with
strong performance gains in limited data regimes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 3DV 2022; Project Page:
  https://crockwell.github.io/rel_pose/ Revision: Fixed Epipolar Lines in
  Figure 3, Figure 10</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAIN: RegulArization on Input and Network for Black-Box Domain
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.10531v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.10531v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qucheng Peng, Zhengming Ding, Lingjuan Lyu, Lichao Sun, Chen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Source-Free domain adaptation transits the source-trained model towards
target domain without exposing the source data, trying to dispel these concerns
about data privacy and security. However, this paradigm is still at risk of
data leakage due to adversarial attacks on the source model. Hence, the
Black-Box setting only allows to use the outputs of source model, but still
suffers from overfitting on the source domain more severely due to source
model's unseen weights. In this paper, we propose a novel approach named RAIN
(RegulArization on Input and Network) for Black-Box domain adaptation from both
input-level and network-level regularization. For the input-level, we design a
new data augmentation technique as Phase MixUp, which highlights task-relevant
objects in the interpolations, thus enhancing input-level regularization and
class consistency for target models. For network-level, we develop a Subnetwork
Distillation mechanism to transfer knowledge from the target subnetwork to the
full target network via knowledge distillation, which thus alleviates
overfitting on the source domain by learning diverse target representations.
Extensive experiments show that our method achieves state-of-the-art
performance on several cross-domain benchmarks under both single- and
multi-source black-box domain adaptation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Superficial White Matter Analysis: An Efficient Point-cloud-based Deep
  Learning Framework with Supervised Contrastive Learning for Consistent
  Tractography Parcellation across Populations and dMRI Acquisitions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08975v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08975v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tengfei Xue, Fan Zhang, Chaoyi Zhang, Yuqian Chen, Yang Song, Alexandra J. Golby, Nikos Makris, Yogesh Rathi, Weidong Cai, Lauren J. O'Donnell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion MRI tractography is an advanced imaging technique that enables in
vivo mapping of the brain's white matter connections. White matter parcellation
classifies tractography streamlines into clusters or anatomically meaningful
tracts. It enables quantification and visualization of whole-brain
tractography. Currently, most parcellation methods focus on the deep white
matter (DWM), whereas fewer methods address the superficial white matter (SWM)
due to its complexity. We propose a novel two-stage deep-learning-based
framework, Superficial White Matter Analysis (SupWMA), that performs an
efficient and consistent parcellation of 198 SWM clusters from whole-brain
tractography. A point-cloud-based network is adapted to our SWM parcellation
task, and supervised contrastive learning enables more discriminative
representations between plausible streamlines and outliers for SWM. We train
our model on a large-scale tractography dataset including streamline samples
from labeled long- and medium-range (over 40 mm) SWM clusters and anatomically
implausible streamline samples, and we perform testing on six independently
acquired datasets of different ages and health conditions (including neonates
and patients with space-occupying brain tumors). Compared to several
state-of-the-art methods, SupWMA obtains highly consistent and accurate SWM
parcellation results on all datasets, showing good generalization across the
lifespan in health and disease. In addition, the computational speed of SupWMA
is much faster than other methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Medical Image Analysis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature-based Image Matching for Identifying Individual Kākā 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06678v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06678v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fintan O'Sullivan, Kirita-Rose Escott, Rachael C. Shaw, Andrew Lensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report investigates an unsupervised, feature-based image matching
pipeline for the novel application of identifying individual k\=ak\=a. Applied
with a similarity network for clustering, this addresses a weakness of current
supervised approaches to identifying individual birds which struggle to handle
the introduction of new individuals to the population. Our approach uses object
localisation to locate k\=ak\=a within images and then extracts local features
that are invariant to rotation and scale. These features are matched between
images with nearest neighbour matching techniques and mismatch removal to
produce a similarity score for image match comparison. The results show that
matches obtained via the image matching pipeline achieve high accuracy of true
matches. We conclude that feature-based image matching could be used with a
similarity network to provide a viable alternative to existing supervised
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, honour's report from Victoria University of Wellington</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does Search Engine Optimization come along with high-quality content? A
  comparison between optimized and non-optimized health-related web pages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Schultheiß, Helena Häußler, Dirk Lewandowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Searching for medical information is both a common and important activity
since it influences decisions people make about their healthcare. Using search
engine optimization (SEO), content producers seek to increase the visibility of
their content. SEO is more likely to be practiced by commercially motivated
content producers such as pharmaceutical companies than by non-commercial
providers such as governmental bodies. In this study, we ask whether content
quality correlates with the presence or absence of SEO measures on a web page.
We conducted a user study in which N = 61 participants comprising laypeople as
well as experts in health information assessment evaluated health-related web
pages classified as either optimized or non-optimized. The subjects rated the
expertise of non-optimized web pages as higher than the expertise of optimized
pages, justifying their appraisal by the more competent and reputable
appearance of non-optimized pages. In addition, comments about the website
operators of the non-optimized pages were exclusively positive, while optimized
pages tended to receive positive as well as negative assessments. We found no
differences between the ratings of laypeople and experts. Since non-optimized,
but high-quality content may be outranked by optimized content of lower
quality, trusted sources should be prioritized in rankings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How search engine marketing influences user knowledge gain: Development
  and empirical testing of an information search behavior model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Schultheiß
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  People use search engines to find answers to questions related to their
health, finances, or other socially relevant issues. However, most users are
unaware that search results are considerably influenced by search engine
marketing (SEM). SEM measures are driven by commercial, political, or other
motives. Due to these motivations, two questions arise: What information
quality is mediated through SEM? And how is collecting documents of different
quality affecting user knowledge gain? Both questions are not considered by
existing models of information behavior. Hence, the doctoral research project
described in this paper aims to develop and empirically test an information
search behavior model on the influences of SEM on user knowledge gain and
thereby contribute to the search as learning body of research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reasoning over Different Types of Knowledge Graphs: Static, Temporal and
  Multi-Modal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05767v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05767v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Liang, Lingyuan Meng, Meng Liu, Yue Liu, Wenxuan Tu, Siwei Wang, Sihang Zhou, Xinwang Liu, Fuchun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graph reasoning (KGR), aiming to deduce new facts from existing
facts based on mined logic rules underlying knowledge graphs (KGs), has become
a fast-growing research direction. It has been proven to significantly benefit
the usage of KGs in many AI applications, such as question answering and
recommendation systems, etc. According to the graph types, the existing KGR
models can be roughly divided into three categories, i.e., static models,
temporal models, and multi-modal models. The early works in this domain mainly
focus on static KGR and tend to directly apply general knowledge graph
embedding models to the reasoning task. However, these models are not suitable
for more complex but practical tasks, such as inductive static KGR, temporal
KGR, and multi-modal KGR. To this end, multiple works have been developed
recently, but no survey papers and open-source repositories comprehensively
summarize and discuss models in this important direction. To fill the gap, we
conduct a survey for knowledge graph reasoning tracing from static to temporal
and then to multi-modal KGs. Concretely, the preliminaries, summaries of KGR
models, and typical datasets are introduced and discussed consequently.
Moreover, we discuss the challenges and potential opportunities. The
corresponding open-source repository is shared on GitHub:
https://github.com/LIANGKE23/Awesome-Knowledge-Graph-Reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HTMOT : Hierarchical Topic Modelling Over Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.03104v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.03104v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Judicael Poumay, Ashwin Ittoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the years, topic models have provided an efficient way of extracting
insights from text. However, while many models have been proposed, none are
able to model topic temporality and hierarchy jointly. Modelling time provide
more precise topics by separating lexically close but temporally distinct
topics while modelling hierarchy provides a more detailed view of the content
of a document corpus. In this study, we therefore propose a novel method,
HTMOT, to perform Hierarchical Topic Modelling Over Time. We train HTMOT using
a new implementation of Gibbs sampling, which is more efficient. Specifically,
we show that only applying time modelling to deep sub-topics provides a way to
extract specific stories or events while high level topics extract larger
themes in the corpus. Our results show that our training procedure is fast and
can extract accurate high-level topics and temporally precise sub-topics. We
measured our model's performance using the Word Intrusion task and outlined
some limitations of this evaluation method, especially for hierarchical models.
As a case study, we focused on the various developments in the space industry
in 2020.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UserSimCRS: A User Simulation Toolkit for Evaluating Conversational
  Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.05544v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.05544v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jafar Afzali, Aleksander Mark Drzewiecki, Krisztian Balog, Shuo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an extensible user simulation toolkit to facilitate automatic
evaluation of conversational recommender systems. It builds on an established
agenda-based approach and extends it with several novel elements, including
user satisfaction prediction, persona and context modeling, and conditional
natural language generation. We showcase the toolkit with a pre-existing movie
recommender system and demonstrate its ability to simulate dialogues that mimic
real conversations, while requiring only a handful of manually annotated
dialogues as training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the Sixteenth ACM International Conference on Web
  Search and Data Mining</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Slate Recommendation with Reinforcement Learning <span class="chip">WSDM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08632v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08632v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romain Deffayet, Thibaut Thonet, Jean-Michel Renders, Maarten de Rijke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has employed reinforcement learning (RL) algorithms to
optimize long-term user engagement in recommender systems, thereby avoiding
common pitfalls such as user boredom and filter bubbles. They capture the
sequential and interactive nature of recommendations, and thus offer a
principled way to deal with long-term rewards and avoid myopic behaviors.
However, RL approaches are intractable in the slate recommendation scenario -
where a list of items is recommended at each interaction turn - due to the
combinatorial action space. In that setting, an action corresponds to a slate
that may contain any combination of items.
  While previous work has proposed well-chosen decompositions of actions so as
to ensure tractability, these rely on restrictive and sometimes unrealistic
assumptions. Instead, in this work we propose to encode slates in a continuous,
low-dimensional latent space learned by a variational auto-encoder. Then, the
RL agent selects continuous actions in this latent space, which are ultimately
decoded into the corresponding slates. By doing so, we are able to (i) relax
assumptions required by previous work, and (ii) improve the quality of the
action selection by modeling full slates instead of independent items, in
particular by enabling diversity. Our experiments performed on a wide array of
simulated environments confirm the effectiveness of our generative modeling of
slates over baselines in practical scenarios where the restrictive assumptions
underlying the baselines are lifted. Our findings suggest that representation
learning using generative models is a promising direction towards generalizable
RL-based slate recommendation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WSDM 2023, 9 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ New Metrics to Encourage Innovation and Diversity in Information
  Retrieval Approaches <span class="chip">ECIR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08062v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08062v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehmet Deniz Türkmen, Matthew Lease, Mucahid Kutlu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In evaluation campaigns, participants often explore variations of popular,
state-of-the-art baselines as a low-risk strategy to achieve competitive
results. While effective, this can lead to local "hill climbing" rather than
more radical and innovative departure from standard methods. Moreover, if many
participants build on similar baselines, the overall diversity of approaches
considered may be limited. In this work, we propose a new class of IR
evaluation metrics intended to promote greater diversity of approaches in
evaluation campaigns. Whereas traditional IR metrics focus on user experience,
our two "innovation" metrics instead reward exploration of more divergent,
higher-risk strategies finding relevant documents missed by other systems.
Experiments on four TREC collections show that our metrics do change system
rankings by rewarding systems that find such rare, relevant documents. This
result is further supported by a controlled, synthetic data experiment, and a
qualitative analysis. In addition, we show that our metrics achieve higher
evaluation stability and discriminative power than the standard metrics we
modify. To support reproducibility, we share our source code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 figures, to be published in ECIR 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Double Matching Under Complementary Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuantong Li, Guang Cheng, Xiaowu Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new algorithm for addressing the problem of
matching markets with complementary preferences, where agents' preferences are
unknown a priori and must be learned from data. The presence of complementary
preferences can lead to instability in the matching process, making this
problem challenging to solve. To overcome this challenge, we formulate the
problem as a bandit learning framework and propose the Multi-agent Multi-type
Thompson Sampling (MMTS) algorithm. The algorithm combines the strengths of
Thompson Sampling for exploration with a double matching technique to achieve a
stable matching outcome. Our theoretical analysis demonstrates the
effectiveness of MMTS as it is able to achieve stability at every matching
step, satisfies the incentive-compatibility property, and has a sublinear
Bayesian regret over time. Our approach provides a useful method for addressing
complementary preferences in real-world scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Watermark for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, Tom Goldstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Potential harms of large language models can be mitigated by watermarking
model output, i.e., embedding signals into generated text that are invisible to
humans but algorithmically detectable from a short span of tokens. We propose a
watermarking framework for proprietary language models. The watermark can be
embedded with negligible impact on text quality, and can be detected using an
efficient open-source algorithm without access to the language model API or
parameters. The watermark works by selecting a randomized set of whitelist
tokens before a word is generated, and then softly promoting use of whitelist
tokens during sampling. We propose a statistical test for detecting the
watermark with interpretable p-values, and derive an information-theoretic
framework for analyzing the sensitivity of the watermark. We test the watermark
using a multi-billion parameter model from the Open Pretrained Transformer
(OPT) family, and discuss robustness and security.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages in the main body. Code will be available at
  github.com/jwkirchenbauer/lm-watermarking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RangeViT: Towards Vision <span class="highlight-title">Transformer</span>s for 3D Semantic Segmentation in
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angelika Ando, Spyros Gidaris, Andrei Bursuc, Gilles Puy, Alexandre Boulch, Renaud Marlet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Casting semantic segmentation of outdoor LiDAR point clouds as a 2D problem,
e.g., via range projection, is an effective and popular approach. These
projection-based methods usually benefit from fast computations and, when
combined with techniques which use other point cloud representations, achieve
state-of-the-art results. Today, projection-based methods leverage 2D CNNs but
recent advances in computer vision show that vision transformers (ViTs) have
achieved state-of-the-art results in many image-based benchmarks. In this work,
we question if projection-based methods for 3D semantic segmentation can
benefit from these latest improvements on ViTs. We answer positively but only
after combining them with three key ingredients: (a) ViTs are notoriously hard
to train and require a lot of training data to learn powerful representations.
By preserving the same backbone architecture as for RGB images, we can exploit
the knowledge from long training on large image collections that are much
cheaper to acquire and annotate than point clouds. We reach our best results
with pre-trained ViTs on large image datasets. (b) We compensate ViTs' lack of
inductive bias by substituting a tailored convolutional stem for the classical
linear embedding layer. (c) We refine pixel-wise predictions with a
convolutional decoder and a skip connection from the convolutional stem to
combine low-level but fine-grained features of the the convolutional stem with
the high-level but coarse predictions of the ViT encoder. With these
ingredients, we show that our method, called RangeViT, outperforms existing
projection-based methods on nuScenes and SemanticKITTI. We provide the
implementation code at https://github.com/valeoai/rangevit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code at https://github.com/valeoai/rangevit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neuronal architecture extracts statistical temporal patterns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandra Nestler, Moritz Helias, Matthieu Gilson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neuronal systems need to process temporal signals. We here show how
higher-order temporal (co-)fluctuations can be employed to represent and
process information. Concretely, we demonstrate that a simple biologically
inspired feedforward neuronal model is able to extract information from up to
the third order cumulant to perform time series classification. This model
relies on a weighted linear summation of synaptic inputs followed by a
nonlinear gain function. Training both - the synaptic weights and the nonlinear
gain function - exposes how the non-linearity allows for the transfer of higher
order correlations to the mean, which in turn enables the synergistic use of
information encoded in multiple cumulants to maximize the classification
accuracy. The approach is demonstrated both on a synthetic and on real world
datasets of multivariate time series. Moreover, we show that the biologically
inspired architecture makes better use of the number of trainable parameters as
compared to a classical machine-learning scheme. Our findings emphasize the
benefit of biological neuronal architectures, paired with dedicated learning
algorithms, for the processing of information embedded in higher-order
statistical cumulants of temporal (co-)fluctuations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WEASEL 2.0 -- A Random Dilated Dictionary Transform for Fast, Accurate
  and Memory Constrained Time Series Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Schäfer, Ulf Leser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A time series is a sequence of sequentially ordered real values in time. Time
series classification (TSC) is the task of assigning a time series to one of a
set of predefined classes, usually based on a model learned from examples.
Dictionary-based methods for TSC rely on counting the frequency of certain
patterns in time series and are important components of the currently most
accurate TSC ensembles. One of the early dictionary-based methods was WEASEL,
which at its time achieved SotA results while also being very fast. However, it
is outperformed both in terms of speed and accuracy by other methods.
Furthermore, its design leads to an unpredictably large memory footprint,
making it inapplicable for many applications.
  In this paper, we present WEASEL 2.0, a complete overhaul of WEASEL based on
two recent advancements in TSC: Dilation and ensembling of randomized
hyper-parameter settings. These two techniques allow WEASEL 2.0 to work with a
fixed-size memory footprint while at the same time improving accuracy. Compared
to 15 other SotA methods on the UCR benchmark set, WEASEL 2.0 is significantly
more accurate than other dictionary methods and not significantly worse than
the currently best methods. Actually, it achieves the highest median accuracy
over all data sets, and it performs best in 5 out of 12 problem classes. We
thus believe that WEASEL 2.0 is a viable alternative for current TSC and also a
potentially interesting input for future ensembles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mesostructures: Beyond Spectrogram Loss in Differentiable Time-Frequency
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cyrus Vahidi, Han Han, Changhong Wang, Mathieu Lagrange, György Fazekas, Vincent Lostanlen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer musicians refer to mesostructures as the intermediate levels of
articulation between the microstructure of waveshapes and the macrostructure of
musical forms. Examples of mesostructures include melody, arpeggios,
syncopation, polyphonic grouping, and textural contrast. Despite their central
role in musical expression, they have received limited attention in deep
learning. Currently, autoencoders and neural audio synthesizers are only
trained and evaluated at the scale of microstructure: i.e., local amplitude
variations up to 100 milliseconds or so. In this paper, we formulate and
address the problem of mesostructural audio modeling via a composition of a
differentiable arpeggiator and time-frequency scattering. We empirically
demonstrate that time--frequency scattering serves as a differentiable model of
similarity between synthesis parameters that govern mesostructure. By exposing
the sensitivity of short-time spectral distances to time alignment, we motivate
the need for a time-invariant and multiscale differentiable time--frequency
model of similarity at the level of both local spectra and spectrotemporal
modulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Jellyfish Characterise Alternating Group Equivariant Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Pearce-Crump
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide a full characterisation of all of the possible alternating group
($A_n$) equivariant neural networks whose layers are some tensor power of
$\mathbb{R}^{n}$. In particular, we find a basis of matrices for the learnable,
linear, $A_n$-equivariant layer functions between such tensor power spaces in
the standard basis of $\mathbb{R}^{n}$. We also describe how our approach
generalises to the construction of neural networks that are equivariant to
local symmetries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages. arXiv admin note: text overlap with arXiv:2212.08648,
  arXiv:2212.08630</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Read the Signs: Towards Invariance to Gradient Descent's Hyperparameter
  Initialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davood Wadi, Marc Fredette, Sylvain Senecal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose ActiveLR, an optimization meta algorithm that localizes the
learning rate, $\alpha$, and adapts them at each epoch according to whether the
gradient at each epoch changes sign or not. This sign-conscious algorithm is
aware of whether from the previous step to the current one the update of each
parameter has been too large or too small and adjusts the $\alpha$ accordingly.
We implement the Active version (ours) of widely used and recently published
gradient descent optimizers, namely SGD with momentum, AdamW, RAdam, and
AdaBelief. Our experiments on ImageNet, CIFAR-10, WikiText-103, WikiText-2, and
PASCAL VOC using different model architectures, such as ResNet and
Transformers, show an increase in generalizability and training set fit, and
decrease in training time for the Active variants of the tested optimizers. The
results also show robustness of the Active variant of these optimizers to
different values of the initial learning rate. Furthermore, the detrimental
effects of using large mini-batch sizes are mitigated. ActiveLR, thus,
alleviates the need for hyper-parameter search for two of the most commonly
tuned hyper-parameters that require heavy time and computational costs to pick.
We encourage AI researchers and practitioners to use the Active variant of
their optimizer of choice for faster training, better generalizability, and
reducing carbon footprint of training deep neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Open-Set Semi-Supervised Learning with Self-Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Wallin, Lennart Svensson, Fredrik Kahl, Lars Hammarstrand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-set semi-supervised learning (OSSL) is a realistic setting of
semi-supervised learning where the unlabeled training set contains classes that
are not present in the labeled set. Many existing OSSL methods assume that
these out-of-distribution data are harmful and put effort into excluding data
from unknown classes from the training objective. In contrast, we propose an
OSSL framework that facilitates learning from all unlabeled data through
self-supervision. Additionally, we utilize an energy-based score to accurately
recognize data belonging to the known classes, making our method well-suited
for handling uncurated data in deployment. We show through extensive
experimental evaluations on several datasets that our method shows overall
unmatched robustness and performance in terms of closed-set accuracy and
open-set recognition compared with state-of-the-art for OSSL. Our code will be
released upon publication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inducing Point Allocation for Sparse Gaussian Processes in
  High-Throughput Bayesian Optimisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henry B. Moss, Sebastian W. Ober, Victor Picheny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse Gaussian Processes are a key component of high-throughput Bayesian
Optimisation (BO) loops; however, we show that existing methods for allocating
their inducing points severely hamper optimisation performance. By exploiting
the quality-diversity decomposition of Determinantal Point Processes, we
propose the first inducing point allocation strategy designed specifically for
use in BO. Unlike existing methods which seek only to reduce global uncertainty
in the objective function, our approach provides the local high-fidelity
modelling of promising regions required for precise optimisation. More
generally, we demonstrate that our proposed framework provides a flexible way
to allocate modelling capacity in sparse models and so is suitable broad range
of downstream sequential decision making tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Minimal Value-Equivalent Partial Models for Scalable and Robust Planning
  in Lifelong Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Safa Alver, Doina Precup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning models of the environment from pure interaction is often considered
an essential component of building lifelong reinforcement learning agents.
However, the common practice in model-based reinforcement learning is to learn
models that model every aspect of the agent's environment, regardless of
whether they are important in coming up with optimal decisions or not. In this
paper, we argue that such models are not particularly well-suited for
performing scalable and robust planning in lifelong reinforcement learning
scenarios and we propose new kinds of models that only model the relevant
aspects of the environment, which we call "minimal value-equivalent partial
models". After providing a formal definition for these models, we provide
theoretical results demonstrating the scalability advantages of performing
planning with such models and then perform experiments to empirically
illustrate our theoretical results. Then, we provide some useful heuristics on
how to learn these kinds of models with deep learning architectures and
empirically demonstrate that models learned in such a way can allow for
performing planning that is robust to distribution shifts and compounding model
errors. Overall, both our theoretical and empirical results suggest that
minimal value-equivalent partial models can provide significant benefits to
performing scalable and robust planning in lifelong reinforcement learning
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Robust Hypothesis Test for Tree Ensemble Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel de Marchi, Matthew Welch, Michael Kosorok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gradient boosted decision trees are some of the most popular algorithms in
applied machine learning. They are a flexible and powerful tool that can
robustly fit to any tabular dataset in a scalable and computationally efficient
way. One of the most critical parameters to tune when fitting these models are
the various penalty terms used to distinguish signal from noise in the current
model. These penalties are effective in practice, but are lacking in robust
theoretical justifications. In this paper we develop and present a novel
theoretically justified hypothesis test of split quality for gradient boosted
tree ensembles and demonstrate that using this method instead of the common
penalty terms leads to a significant reduction in out of sample loss.
Additionally, this method provides a theoretically well-justified stopping
condition for the tree growing algorithm. We also present several innovative
extensions to the method, opening the door for a wide variety of novel tree
pruning algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When does the student surpass the teacher? Federated Semi-supervised
  Learning with Teacher-Student EMA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jessica Zhao, Sayan Ghosh, Akash Bharadwaj, Chih-Yao Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-Supervised Learning (SSL) has received extensive attention in the domain
of computer vision, leading to development of promising approaches such as
FixMatch. In scenarios where training data is decentralized and resides on
client devices, SSL must be integrated with privacy-aware training techniques
such as Federated Learning. We consider the problem of federated image
classification and study the performance and privacy challenges with existing
federated SSL (FSSL) approaches. Firstly, we note that even state-of-the-art
FSSL algorithms can trivially compromise client privacy and other real-world
constraints such as client statelessness and communication cost. Secondly, we
observe that it is challenging to integrate EMA (Exponential Moving Average)
updates into the federated setting, which comes at a trade-off between
performance and communication cost. We propose a novel approach FedSwitch, that
improves privacy as well as generalization performance through Exponential
Moving Average (EMA) updates. FedSwitch utilizes a federated semi-supervised
teacher-student EMA framework with two features - local teacher adaptation and
adaptive switching between teacher and student for pseudo-label generation. Our
proposed approach outperforms the state-of-the-art on federated image
classification, can be adapted to real-world constraints, and achieves good
generalization performance with minimal communication cost overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PolarAir: A Compressed Sensing Scheme for Over-the-Air Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michail Gkagkos, Krishna R. Narayanan, Jean-Francois Chamberland, Costas N. Georghiades
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore a scheme that enables the training of a deep neural network in a
Federated Learning configuration over an additive white Gaussian noise channel.
The goal is to create a low complexity, linear compression strategy, called
PolarAir, that reduces the size of the gradient at the user side to lower the
number of channel uses needed to transmit it. The suggested approach belongs to
the family of compressed sensing techniques, yet it constructs the sensing
matrix and the recovery procedure using multiple access techniques. Simulations
show that it can reduce the number of channel uses by ~30% when compared to
conveying the gradient without compression. The main advantage of the proposed
scheme over other schemes in the literature is its low time complexity. We also
investigate the behavior of gradient updates and the performance of PolarAir
throughout the training process to obtain insight on how best to construct this
compression scheme based on compressed sensing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Autonomous particles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikola Andrejic, Vitaly Vanchurin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consider a reinforcement learning problem where an agent has access to a very
large amount of information about the environment, but it can only take very
few actions to accomplish its task and to maximize its reward. Evidently, the
main problem for the agent is to learn a map from a very high-dimensional space
(which represents its environment) to a very low-dimensional space (which
represents its actions). The high-to-low dimensional map implies that most of
the information about the environment is irrelevant for the actions to be
taken, and only a small fraction of information is relevant. In this paper we
argue that the relevant information need not be learned by brute force (which
is the standard approach), but can be identified from the intrinsic symmetries
of the system. We analyze in details a reinforcement learning problem of
autonomous driving, where the corresponding symmetry is the Galilean symmetry,
and argue that the learning task can be accomplished with very few relevant
parameters, or, more precisely, invariants. For a numerical demonstration, we
show that the autonomous vehicles (which we call autonomous particles since
they describe very primitive vehicles) need only four relevant invariants to
learn how to drive very well without colliding with other particles. The simple
model can be easily generalized to include different types of particles (e.g.
for cars, for pedestrians, for buildings, for road signs, etc.) with different
types of relevant invariants describing interactions between them. We also
argue that there must exist a field theory description of the learning system
where autonomous particles would be described by fermionic degrees of freedom
and interactions mediated by the relevant invariants would be described by
bosonic degrees of freedom.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Data-Driven Optimization: From Context to Decision and Back
  Again 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Forel, Axel Parmentier, Thibaut Vidal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven optimization uses contextual information and machine learning
algorithms to find solutions to decision problems with uncertain parameters.
While a vast body of work is dedicated to interpreting machine learning models
in the classification setting, explaining decision pipelines involving learning
algorithms remains unaddressed. This lack of interpretability can block the
adoption of data-driven solutions as practitioners may not understand or trust
the recommended decisions. We bridge this gap by introducing a counterfactual
explanation methodology tailored to explain solutions to data-driven problems.
We introduce two classes of explanations and develop methods to find nearest
explanations of random forest and nearest-neighbor predictors. We demonstrate
our approach by explaining key problems in operations management such as
inventory management and routing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intrinsic Motivation in Model-based Reinforcement Learning: A Brief
  <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artem Latyshev, Aleksandr I. Panov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reinforcement learning research area contains a wide range of methods for
solving the problems of intelligent agent control. Despite the progress that
has been made, the task of creating a highly autonomous agent is still a
significant challenge. One potential solution to this problem is intrinsic
motivation, a concept derived from developmental psychology. This review
considers the existing methods for determining intrinsic motivation based on
the world model obtained by the agent. We propose a systematic approach to
current research in this field, which consists of three categories of methods,
distinguished by the way they utilize a world model in the agent's components:
complementary intrinsic reward, exploration policy, and intrinsically motivated
goals. The proposed unified framework describes the architecture of agents
using a world model and intrinsic motivation to improve learning. The potential
for developing new techniques in this area of research is also examined.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inference of Continuous Linear Systems from Data with Guaranteed
  Stability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pawan Goyal, Igor Pontes Duff, Peter Benner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine-learning technologies for learning dynamical systems from data play
an important role in engineering design. This research focuses on learning
continuous linear models from data. Stability, a key feature of dynamic
systems, is especially important in design tasks such as prediction and
control. Thus, there is a need to develop methodologies that provide stability
guarantees. To that end, we leverage the parameterization of stable matrices
proposed in [Gillis/Sharma, Automatica, 2017] to realize the desired models.
Furthermore, to avoid the estimation of derivative information to learn
continuous systems, we formulate the inference problem in an integral form. We
also discuss a few extensions, including those related to control systems.
Numerical experiments show that the combination of a stable matrix
parameterization and an integral form of differential equations allows us to
learn stable systems without requiring derivative information, which can be
challenging to obtain in situations with noisy or limited data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Linear Reconstruction Approach for Attribute Inference Attacks against
  Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meenatchi Sundaram Muthu Selva Annamalai, Andrea Gadotti, Luc Rocher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personal data collected at scale from surveys or digital devices offers
important insights for statistical analysis and scientific research. Safely
sharing such data while protecting privacy is however challenging.
Anonymization allows data to be shared while minimizing privacy risks, but
traditional anonymization techniques have been repeatedly shown to provide
limited protection against re-identification attacks in practice. Among modern
anonymization techniques, synthetic data generation (SDG) has emerged as a
potential solution to find a good tradeoff between privacy and statistical
utility. Synthetic data is typically generated using algorithms that learn the
statistical distribution of the original records, to then generate "artificial"
records that are structurally and statistically similar to the original ones.
Yet, the fact that synthetic records are "artificial" does not, per se,
guarantee that privacy is protected. In this work, we systematically evaluate
the tradeoffs between protecting privacy and preserving statistical utility for
a wide range of synthetic data generation algorithms. Modeling privacy as
protection against attribute inference attacks (AIAs), we extend and adapt
linear reconstruction attacks, which have not been previously studied in the
context of synthetic data. While prior work suggests that AIAs may be effective
only on few outlier records, we show they can be very effective even on
randomly selected records. We evaluate attacks on synthetic datasets ranging
from 10^3 to 10^6 records, showing that even for the same generative model, the
attack effectiveness can drastically increase when a larger number of synthetic
records is generated. Overall, our findings prove that synthetic data is
subject to privacy-utility tradeoffs just like other anonymization techniques:
when good utility is preserved, attribute inference can be a risk for many data
subjects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Koopman neural operator as a mesh-free solver of non-linear partial
  differential equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Xiong, Xiaomeng Huang, Ziyang Zhang, Ruixuan Deng, Pei Sun, Yang Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The lacking of analytic solutions of diverse partial differential equations
(PDEs) gives birth to series of computational techniques for numerical
solutions. In machine learning, numerous latest advances of solver designs are
accomplished in developing neural operators, a kind of mesh-free approximators
of the infinite-dimensional operators that map between different
parameterization spaces of equation solutions. Although neural operators
exhibit generalization capacities for learning an entire PDE family
simultaneously, they become less accurate and explainable while learning
long-term behaviours of non-linear PDE families. In this paper, we propose
Koopman neural operator (KNO), a new neural operator, to overcome these
challenges. With the same objective of learning an infinite-dimensional mapping
between Banach spaces that serves as the solution operator of target PDE
family, our approach differs from existing models by formulating a non-linear
dynamic system of equation solution. By approximating the Koopman operator, an
infinite-dimensional linear operator governing all possible observations of the
dynamic system, to act on the flow mapping of dynamic system, we can
equivalently learn the solution of an entire non-linear PDE family by solving
simple linear prediction problems. In zero-shot prediction and long-term
prediction experiments on representative PDEs (e.g., the Navier-Stokes
equation), KNO exhibits notable advantages in breaking the tradeoff between
accuracy and efficiency (e.g., model size) while previous state-of-the-art
models are limited. These results suggest that more efficient PDE solvers can
be developed by the joint efforts from physics and machine learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solving the Discretised Neutron Diffusion Equations using Neural
  Networks: Applications in neutron transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09991v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09991v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        T. R. F. Phillips, C. E. Heaney, C. Boyang, A. G. Buchan, C. C. Pain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we solve the Boltzmann transport equation using AI libraries.
The reason why this is attractive is because it enables one to use the highly
optimised software within AI libraries, enabling one to run on different
computer architectures and enables one to tap into the vast quantity of
community based software that has been developed for AI and ML applications
e.g. mixed arithmetic precision or model parallelism. Here we take the first
steps towards developing this approach for the Boltzmann transport equation and
develop the necessary methods in order to do that effectively. This includes:
1) A space-angle multigrid solution method that can extract the level of
parallelism necessary to run efficiently on GPUs or new AI computers. 2) A new
Convolutional Finite Element Method (ConvFEM) that greatly simplifies the
implementation of high order finite elements (quadratic to quintic, say). 3) A
new non-linear Petrov-Galerkin method that introduces dissipation
anisotropically.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Membership Inference of Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hailong Hu, Jun Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed the tremendous success of diffusion models in
data synthesis. However, when diffusion models are applied to sensitive data,
they also give rise to severe privacy concerns. In this paper, we
systematically present the first study about membership inference attacks
against diffusion models, which aims to infer whether a sample was used to
train the model. Two attack methods are proposed, namely loss-based and
likelihood-based attacks. Our attack methods are evaluated on several
state-of-the-art diffusion models, over different datasets in relation to
privacy-sensitive data. Extensive experimental evaluations show that our
attacks can achieve remarkable performance. Furthermore, we exhaustively
investigate various factors which can affect attack performance. Finally, we
also evaluate the performance of our attack methods on diffusion models trained
with differential privacy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning To Dive In Branch And Bound 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max B. Paulus, Andreas Krause
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Primal heuristics are important for solving mixed integer linear programs,
because they find feasible solutions that facilitate branch and bound search. A
prominent group of primal heuristics are diving heuristics. They iteratively
modify and resolve linear programs to conduct a depth-first search from any
node in the search tree. Existing divers rely on generic decision rules that
fail to exploit structural commonality between similar problem instances that
often arise in practice. Therefore, we propose L2Dive to learn specific diving
heuristics with graph neural networks: We train generative models to predict
variable assignments and leverage the duality of linear programs to make diving
decisions based on the model's predictions. L2Dive is fully integrated into the
open-source solver SCIP. We find that L2Dive outperforms standard divers to
find better feasible solutions on a range of combinatorial optimization
problems. For real-world applications from server load balancing and neural
network verification, L2Dive improves the primal-dual integral by up to 7%
(35%) on average over a tuned (default) solver baseline and reduces average
solving time by 20% (29%).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solving the Discretised Neutron Diffusion Equations using Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09939v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09939v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        T. R. F. Phillips, C. E. Heaney, C. Boyang, A. G. Buchan, C. C. Pain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new approach which uses the tools within Artificial
Intelligence (AI) software libraries as an alternative way of solving partial
differential equations (PDEs) that have been discretised using standard
numerical methods. In particular, we describe how to represent numerical
discretisations arising from the finite volume and finite element methods by
pre-determining the weights of convolutional layers within a neural network. As
the weights are defined by the discretisation scheme, no training of the
network is required and the solutions obtained are identical (accounting for
solver tolerances) to those obtained with standard codes often written in
Fortran or C++. We also explain how to implement the Jacobi method and a
multigrid solver using the functions available in AI libraries. For the latter,
we use a U-Net architecture which is able to represent a sawtooth multigrid
method. A benefit of using AI libraries in this way is that one can exploit
their power and their built-in technologies. For example, their executions are
already optimised for different computer architectures, whether it be CPUs,
GPUs or new-generation AI processors. In this article, we apply the proposed
approach to eigenvalue problems in reactor physics where neutron transport is
described by diffusion theory. For a fuel assembly benchmark, we demonstrate
that the solution obtained from our new approach is the same (accounting for
solver tolerances) as that obtained from the same discretisation coded in a
standard way using Fortran. We then proceed to solve a reactor core benchmark
using the new approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Deep Reinforcement Learning: State of the Art and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George A. Vouros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpretability, explainability and transparency are key issues to
introducing Artificial Intelligence methods in many critical domains: This is
important due to ethical concerns and trust issues strongly connected to
reliability, robustness, auditability and fairness, and has important
consequences towards keeping the human in the loop in high levels of
automation, especially in critical cases for decision making, where both (human
and the machine) play important roles. While the research community has given
much attention to explainability of closed (or black) prediction boxes, there
are tremendous needs for explainability of closed-box methods that support
agents to act autonomously in the real world. Reinforcement learning methods,
and especially their deep versions, are such closed-box methods. In this
article we aim to provide a review of state of the art methods for explainable
deep reinforcement learning methods, taking also into account the needs of
human operators - i.e., of those that take the actual and critical decisions in
solving real-world problems. We provide a formal specification of the deep
reinforcement learning explainability problems, and we identify the necessary
components of a general explainable reinforcement learning framework. Based on
these, we provide a comprehensive review of state of the art methods,
categorizing them in classes according to the paradigm they follow, the
interpretable models they use, and the surface representation of explanations
provided. The article concludes identifying open questions and important
challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient learning of large sets of locally optimal classification rules 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Van Quoc Phuong Huynh, Johannes Fürnkranz, Florian Beck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional rule learning algorithms aim at finding a set of simple rules,
where each rule covers as many examples as possible. In this paper, we argue
that the rules found in this way may not be the optimal explanations for each
of the examples they cover. Instead, we propose an efficient algorithm that
aims at finding the best rule covering each training example in a greedy
optimization consisting of one specialization and one generalization loop.
These locally optimal rules are collected and then filtered for a final rule
set, which is much larger than the sets learned by conventional rule learning
algorithms. A new example is classified by selecting the best among the rules
that cover this example. In our experiments on small to very large datasets,
the approach's average classification accuracy is higher than that of
state-of-the-art rule learning algorithms. Moreover, the algorithm is highly
efficient and can inherently be processed in parallel without affecting the
learned rule set and so the classification accuracy. We thus believe that it
closes an important gap for large-scale classification rule induction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>article, 40 pages, Machine Learning journal (2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quadruple-star systems are not always nested triples: a machine learning
  approach to dynamical stability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pavan Vynatheya, Rosemary A. Mardling, Adrian S. Hamers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dynamical stability of quadruple-star systems has traditionally been
treated as a problem involving two `nested' triples which constitute a
quadruple. In this novel study, we employed a machine learning algorithm, the
multi-layer perceptron (MLP), to directly classify 2+2 and 3+1 quadruples based
on their stability (or long-term boundedness). The training data sets for the
classification, comprised of $5\times10^5$ quadruples each, were integrated
using the highly accurate direct $N$-body code MSTAR. We also carried out a
limited parameter space study of zero-inclination systems to directly compare
quadruples to triples. We found that both our quadruple MLP models perform
better than a `nested' triple MLP approach, which is especially significant for
3+1 quadruples. The classification accuracies for the 2+2 MLP and 3+1 MLP
models are 94% and 93% respectively, while the scores for the `nested' triple
approach are 88% and 66% respectively. This is a crucial implication for
quadruple population synthesis studies. Our MLP models, which are very simple
and almost instantaneous to implement, are available on GitHub, along with
Python3 scripts to access them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures; Submitted to MNRAS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A two stages Deep Learning Architecture for Model Reduction of
  Parametric Time-Dependent Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isabella Carla Gonnella, Martin W. Hess, Giovanni Stabile, Gianluigi Rozza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parametric time-dependent systems are of a crucial importance in modeling
real phenomena, often characterized by non-linear behaviors too. Those
solutions are typically difficult to generalize in a sufficiently wide
parameter space while counting on limited computational resources available. As
such, we present a general two-stages deep learning framework able to perform
that generalization with low computational effort in time. It consists in a
separated training of two pipe-lined predictive models. At first, a certain
number of independent neural networks are trained with data-sets taken from
different subsets of the parameter space. Successively, a second predictive
model is specialized to properly combine the first-stage guesses and compute
the right predictions. Promising results are obtained applying the framework to
incompressible Navier-Stokes equations in a cavity (Rayleigh-Bernard cavity),
obtaining a 97% reduction in the computational time comparing with its
numerical resolution for a new value of the Grashof number.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Labeler Bias in Face Annotation for Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luke Haliburton, Sinksar Ghebremedhin, Robin Welsch, Albrecht Schmidt, Sven Mayer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a world increasingly reliant on artificial intelligence, it is more
important than ever to consider the ethical implications of artificial
intelligence on humanity. One key under-explored challenge is labeler bias,
which can create inherently biased datasets for training and subsequently lead
to inaccurate or unfair decisions in healthcare, employment, education, and law
enforcement. Hence, we conducted a study to investigate and measure the
existence of labeler bias using images of people from different ethnicities and
sexes in a labeling task. Our results show that participants possess
stereotypes that influence their decision-making process and that labeler
demographics impact assigned labels. We also discuss how labeler bias
influences datasets and, subsequently, the models trained on them. Overall, a
high degree of transparency must be maintained throughout the entire artificial
intelligence training process to identify and correct biases in the data as
early as possible.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print currently under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probabilistic Bilevel Coreset Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09880v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09880v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Zhou, Renjie Pi, Weizhong Zhang, Yong Lin, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of coreset selection in supervised learning is to produce a weighted
subset of data, so that training only on the subset achieves similar
performance as training on the entire dataset. Existing methods achieved
promising results in resource-constrained scenarios such as continual learning
and streaming. However, most of the existing algorithms are limited to
traditional machine learning models. A few algorithms that can handle large
models adopt greedy search approaches due to the difficulty in solving the
discrete subset selection problem, which is computationally costly when coreset
becomes larger and often produces suboptimal results. In this work, for the
first time we propose a continuous probabilistic bilevel formulation of coreset
selection by learning a probablistic weight for each training sample. The
overall objective is posed as a bilevel optimization problem, where 1) the
inner loop samples coresets and train the model to convergence and 2) the outer
loop updates the sample probability progressively according to the model's
performance. Importantly, we develop an efficient solver to the bilevel
optimization problem via unbiased policy gradient without trouble of implicit
differentiation. We provide the convergence property of our training procedure
and demonstrate the superiority of our algorithm against various coreset
selection methods in various tasks, especially in more challenging label-noise
and class-imbalance scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Augmentation Alone Can Improve Adversarial Training <span class="chip">ICLR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Li, Michael Spratling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial training suffers from the issue of robust overfitting, which
seriously impairs its generalization performance. Data augmentation, which is
effective at preventing overfitting in standard training, has been observed by
many previous works to be ineffective in mitigating overfitting in adversarial
training. This work proves that, contrary to previous findings, data
augmentation alone can significantly boost accuracy and robustness in
adversarial training. We find that the hardness and the diversity of data
augmentation are important factors in combating robust overfitting. In general,
diversity can improve both accuracy and robustness, while hardness can boost
robustness at the cost of accuracy within a certain limit and degrade them both
over that limit. To mitigate robust overfitting, we first propose a new crop
transformation, Cropshift, which has improved diversity compared to the
conventional one (Padcrop). We then propose a new data augmentation scheme,
based on Cropshift, with much improved diversity and well-balanced hardness.
Empirically, our augmentation method achieves the state-of-the-art accuracy and
robustness for data augmentations in adversarial training. Furthermore, when
combined with weight averaging it matches, or even exceeds, the performance of
the best contemporary regularization methods for alleviating robust
overfitting. Code is available at:
https://github.com/TreeLLi/DA-Alone-Improves-AT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>published at conference ICLR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-specific kernel-based hidden Markov model for time series
  analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Puerto-Santana, Concha Bielza, Pedro Larrañaga, Gustav Eje Henter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional hidden Markov models have been a useful tool to understand and
model stochastic dynamic linear data; in the case of non-Gaussian data or not
linear in mean data, models such as mixture of Gaussian hidden Markov models
suffer from the computation of precision matrices and have a lot of unnecessary
parameters. As a consequence, such models often perform better when it is
assumed that all variables are independent, a hypothesis that may be
unrealistic. Hidden Markov models based on kernel density estimation is also
capable of modeling non Gaussian data, but they assume independence between
variables. In this article, we introduce a new hidden Markov model based on
kernel density estimation, which is capable of introducing kernel dependencies
using context-specific Bayesian networks. The proposed model is described,
together with a learning algorithm based on the expectation-maximization
algorithm. Additionally, the model is compared with related HMMs using
synthetic and real data. From the results, the benefits in likelihood and
classification accuracy from the proposed model are quantified and analyzed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Keywords: Hidden Markov models, Kernel density estimation, Bayesian
  networks, Adaptive models, Time series</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Same or Different? Diff-Vectors for Authorship Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silvia Corbara, Alejandro Moreo, Fabrizio Sebastiani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the effects on authorship identification tasks of a
fundamental shift in how to conceive the vectorial representations of documents
that are given as input to a supervised learner. In ``classic'' authorship
analysis a feature vector represents a document, the value of a feature
represents (an increasing function of) the relative frequency of the feature in
the document, and the class label represents the author of the document. We
instead investigate the situation in which a feature vector represents an
unordered pair of documents, the value of a feature represents the absolute
difference in the relative frequencies (or increasing functions thereof) of the
feature in the two documents, and the class label indicates whether the two
documents are from the same author or not. This latter (learner-independent)
type of representation has been occasionally used before, but has never been
studied systematically. We argue that it is advantageous, and that in some
cases (e.g., authorship verification) it provides a much larger quantity of
information to the training process than the standard representation. The
experiments that we carry out on several publicly available datasets (among
which one that we here make available for the first time) show that feature
vectors representing pairs of documents (that we here call Diff-Vectors) bring
about systematic improvements in the effectiveness of authorship identification
tasks, and especially so when training data are scarce (as it is often the case
in real-life authorship identification scenarios). Our experiments tackle
same-author verification, authorship verification, and closed-set authorship
attribution; while DVs are naturally geared for solving the 1st, we also
provide two novel methods for solving the 2nd and 3rd that use a solver for the
1st as a building block.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A predictive physics-aware hybrid reduced order model for reacting flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrián Corrochano, Rodolfo S. M. Freitas, Alessandro Parente, Soledad Le Clainche
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, a new hybrid predictive Reduced Order Model (ROM) is proposed
to solve reacting flow problems. This algorithm is based on a dimensionality
reduction using Proper Orthogonal Decomposition (POD) combined with deep
learning architectures. The number of degrees of freedom is reduced from
thousands of temporal points to a few POD modes with their corresponding
temporal coefficients. Two different deep learning architectures have been
tested to predict the temporal coefficients, based on recursive (RNN) and
convolutional (CNN) neural networks. From each architecture, different models
have been created to understand the behavior of each parameter of the neural
network. Results show that these architectures are able to predict the temporal
coefficients of the POD modes, as well as the whole snapshots. The RNN shows
lower prediction error for all the variables analyzed. The model was also found
capable of predicting more complex simulations showing transfer learning
capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neighborhood Homophily-Guided Graph Convolutional Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengbo Gong, Jiajun Zhou, Chenxuan Xie, Qi Xuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) have achieved remarkable advances in
graph-oriented tasks. However, many real-world graphs contain heterophily or
low homophily, challenging the homophily assumption of classical GNNs and
resulting in low performance. Although many studies have emerged to improve the
universality of GNNs, they rarely consider the label reuse and the correlation
of their proposed metrics and models. In this paper, we first design a new
metric, named Neighborhood Homophily (\textit{NH}), to measure the label
complexity or purity in the neighborhood of nodes. Furthermore, we incorporate
this metric into the classical graph convolutional network (GCN) architecture
and propose \textbf{N}eighborhood \textbf{H}omophily-\textbf{G}uided
\textbf{G}raph \textbf{C}onvolutional \textbf{N}etwork (\textbf{NHGCN}). In
this framework, nodes are grouped by estimated \textit{NH} values to achieve
intra-group weight sharing during message propagation and aggregation. Then the
generated node predictions are used to estimate and update new \textit{NH}
values. The two processes of metric estimation and model inference are
alternately optimized to achieve better node classification. Extensive
experiments on both homophilous and heterophilous benchmarks demonstrate that
\textbf{NHGCN} achieves state-of-the-art overall performance on semi-supervised
node classification for the universality problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gossiped and Quantized Online Multi-Kernel Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomas Ortega, Hamid Jafarkhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In instances of online kernel learning where little prior information is
available and centralized learning is unfeasible, past research has shown that
distributed and online multi-kernel learning provides sub-linear regret as long
as every pair of nodes in the network can communicate (i.e., the communications
network is a complete graph). In addition, to manage the communication load,
which is often a performance bottleneck, communications between nodes can be
quantized. This letter expands on these results to non-fully connected graphs,
which is often the case in wireless sensor networks. To address this challenge,
we propose a gossip algorithm and provide a proof that it achieves sub-linear
regret. Experiments with real datasets confirm our findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implementation of the Critical Wave Groups Method with Computational
  Fluid Dynamics and Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin M. Silva, Kevin J. Maki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and efficient prediction of extreme ship responses continues to be a
challenging problem in ship hydrodynamics. Probabilistic frameworks in
conjunction with computationally efficient numerical hydrodynamic tools have
been developed that allow researchers and designers to better understand
extremes. However, the ability of these hydrodynamic tools to represent the
physics quantitatively during extreme events is limited. Previous research
successfully implemented the critical wave groups (CWG) probabilistic method
with computational fluid dynamics (CFD). Although the CWG method allows for
less simulation time than a Monte Carlo approach, the large quantity of
simulations required is cost prohibitive. The objective of the present paper is
to reduce the computational cost of implementing CWG with CFD, through the
construction of long short-term memory (LSTM) neural networks. After training
the models with a limited quantity of simulations, the models can provide a
larger quantity of predictions to calculate the probability. The new framework
is demonstrated with a 2-D midship section of the Office of Naval Research
Tumblehome (ONRT) hull in Sea State 7 and beam seas at zero speed. The new
framework is able to produce predictions that are representative of a purely
CFD-driven CWG framework, with two orders of magnitude of computational cost
savings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimus-CC: Efficient Large NLP Model Training with 3D Parallelism Aware
  Communication Compression <span class="chip">ASPLOS'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeyong Song, Jinkyu Yim, Jaewon Jung, Hongsun Jang, Hyung-Jin Kim, Youngsok Kim, Jinho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In training of modern large natural language processing (NLP) models, it has
become a common practice to split models using 3D parallelism to multiple GPUs.
Such technique, however, suffers from a high overhead of inter-node
communication. Compressing the communication is one way to mitigate the
overhead by reducing the inter-node traffic volume; however, the existing
compression techniques have critical limitations to be applied for NLP models
with 3D parallelism in that 1) only the data parallelism traffic is targeted,
and 2) the existing compression schemes already harm the model quality too
much.
  In this paper, we present Optimus-CC, a fast and scalable distributed
training framework for large NLP models with aggressive communication
compression. Optimus-CC differs from existing communication compression
frameworks in the following ways: First, we compress pipeline parallel
(inter-stage) traffic. In specific, we compress the inter-stage backpropagation
and the embedding synchronization in addition to the existing data-parallel
traffic compression methods. Second, we propose techniques to avoid the model
quality drop that comes from the compression. We further provide mathematical
and empirical analyses to show that our techniques can successfully suppress
the compression error. Lastly, we analyze the pipeline and opt to selectively
compress those traffic lying on the critical path. This further helps reduce
the compression error. We demonstrate our solution on a GPU cluster, and
achieve superior speedup from the baseline state-of-the-art solutions for
distributed training without sacrificing the model quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ASPLOS'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Stability Analysis of Fine-Tuning a <span class="highlight-title">Pre-Train</span>ed Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Fu, Anthony Man-Cho So, Nigel Collier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning a pre-trained model (such as BERT, ALBERT, RoBERTa, T5, GPT,
etc.) has proven to be one of the most promising paradigms in recent NLP
research. However, numerous recent works indicate that fine-tuning suffers from
the instability problem, i.e., tuning the same model under the same setting
results in significantly different performance. Many recent works have proposed
different methods to solve this problem, but there is no theoretical
understanding of why and how these methods work. In this paper, we propose a
novel theoretical stability analysis of fine-tuning that focuses on two
commonly used settings, namely, full fine-tuning and head tuning. We define the
stability under each setting and prove the corresponding stability bounds. The
theoretical bounds explain why and how several existing methods can stabilize
the fine-tuning procedure. In addition to being able to explain most of the
observed empirical discoveries, our proposed theoretical analysis framework can
also help in the design of effective and provable methods. Based on our theory,
we propose three novel strategies to stabilize the fine-tuning procedure,
namely, Maximal Margin Regularizer (MMR), Multi-Head Loss (MHLoss), and Self
Unsupervised Re-Training (SURT). We extensively evaluate our proposed
approaches on 11 widely used real-world benchmark datasets, as well as hundreds
of synthetic classification datasets. The experiment results show that our
proposed methods significantly stabilize the fine-tuning procedure and also
corroborate our theoretical analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Agnostic Sample Reweighting for Out-of-Distribution Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Zhou, Yong Lin, Renjie Pi, Weizhong Zhang, Renzhe Xu, Peng Cui, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributionally robust optimization (DRO) and invariant risk minimization
(IRM) are two popular methods proposed to improve out-of-distribution (OOD)
generalization performance of machine learning models. While effective for
small models, it has been observed that these methods can be vulnerable to
overfitting with large overparameterized models. This work proposes a
principled method, \textbf{M}odel \textbf{A}gnostic sam\textbf{PL}e
r\textbf{E}weighting (\textbf{MAPLE}), to effectively address OOD problem,
especially in overparameterized scenarios. Our key idea is to find an effective
reweighting of the training samples so that the standard empirical risk
minimization training of a large model on the weighted training data leads to
superior OOD generalization performance. The overfitting issue is addressed by
considering a bilevel formulation to search for the sample reweighting, in
which the generalization complexity depends on the search space of sample
weights instead of the model size. We present theoretical analysis in linear
case to prove the insensitivity of MAPLE to model size, and empirically verify
its superiority in surpassing state-of-the-art methods by a large margin. Code
is available at \url{https://github.com/x-zho14/MAPLE}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SMART: <span class="highlight-title">Self-supervised</span> Multi-task <span class="highlight-title">pretrAin</span>ing with contRol <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanchao Sun, Shuang Ma, Ratnesh Madaan, Rogerio Bonatti, Furong Huang, Ashish Kapoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised pretraining has been extensively studied in language and
vision domains, where a unified model can be easily adapted to various
downstream tasks by pretraining representations without explicit labels. When
it comes to sequential decision-making tasks, however, it is difficult to
properly design such a pretraining approach that can cope with both
high-dimensional perceptual information and the complexity of sequential
control over long interaction horizons. The challenge becomes combinatorially
more complex if we want to pretrain representations amenable to a large variety
of tasks. To tackle this problem, in this work, we formulate a general
pretraining-finetuning pipeline for sequential decision making, under which we
propose a generic pretraining framework \textit{Self-supervised Multi-task
pretrAining with contRol Transformer (SMART)}. By systematically investigating
pretraining regimes, we carefully design a Control Transformer (CT) coupled
with a novel control-centric pretraining objective in a self-supervised manner.
SMART encourages the representation to capture the common essential information
relevant to short-term control and long-term control, which is transferrable
across tasks. We show by extensive experiments in DeepMind Control Suite that
SMART significantly improves the learning efficiency among seen and unseen
downstream tasks and domains under different learning scenarios including
Imitation Learning (IL) and Reinforcement Learning (RL). Benefiting from the
proposed control-centric objective, SMART is resilient to distribution shift
between pretraining and finetuning, and even works well with low-quality
pretraining datasets that are randomly collected.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixed Effects Random Forests for Personalised Predictions of Clinical
  Depression Severity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert A. Lewis, Asma Ghandeharioun, Szymon Fedor, Paola Pedrelli, Rosalind Picard, David Mischoulon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work demonstrates how mixed effects random forests enable accurate
predictions of depression severity using multimodal physiological and digital
activity data collected from an 8-week study involving 31 patients with major
depressive disorder. We show that mixed effects random forests outperform
standard random forests and personal average baselines when predicting clinical
Hamilton Depression Rating Scale scores (HDRS_17). Compared to the latter
baseline, accuracy is significantly improved for each patient by an average of
0.199-0.276 in terms of mean absolute error (p<0.05). This is noteworthy as
these simple baselines frequently outperform machine learning methods in mental
health prediction tasks. We suggest that this improved performance results from
the ability of the mixed effects random forest to personalise model parameters
to individuals in the dataset. However, we find that these improvements pertain
exclusively to scenarios where labelled patient data are available to the model
at training time. Investigating methods that improve accuracy when generalising
to new patients is left as important future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Slice-and-Forge: Making Better Use of Caches for Graph Convolutional
  Network Accelerators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09813v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09813v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingi Yoo, Jaeyong Song, Hyeyoon Lee, Jounghoo Lee, Namhyung Kim, Youngsok Kim, Jinho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph convolutional networks (GCNs) are becoming increasingly popular as they
can process a wide variety of data formats that prior deep neural networks
cannot easily support. One key challenge in designing hardware accelerators for
GCNs is the vast size and randomness in their data access patterns which
greatly reduces the effectiveness of the limited on-chip cache. Aimed at
improving the effectiveness of the cache by mitigating the irregular data
accesses, prior studies often employ the vertex tiling techniques used in
traditional graph processing applications. While being effective at enhancing
the cache efficiency, those approaches are often sensitive to the tiling
configurations where the optimal setting heavily depends on target input
datasets. Furthermore, the existing solutions require manual tuning through
trial-and-error or rely on sub-optimal analytical models.
  In this paper, we propose Slice-and-Forge (SnF), an efficient hardware
accelerator for GCNs which greatly improves the effectiveness of the limited
on-chip cache. SnF chooses a tiling strategy named feature slicing that splits
the features into vertical slices and processes them in the outermost loop of
the execution. This particular choice results in a repetition of the identical
computational patterns over irregular graph data over multiple rounds. Taking
advantage of such repetitions, SnF dynamically tunes its tile size. Our
experimental results reveal that SnF can achieve 1.73x higher performance in
geomean compared to prior work on multi-engine settings, and 1.46x higher
performance in geomean on small scale settings, without the need for off-line
analyses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at PACT'22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-view Kernel PCA for Time series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arun Pandey, Hannes De Meulemeester, Bart De Moor, Johan A. K. Suykens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a kernel principal component analysis model for
multi-variate time series forecasting, where the training and prediction
schemes are derived from the multi-view formulation of Restricted Kernel
Machines. The training problem is simply an eigenvalue decomposition of the
summation of two kernel matrices corresponding to the views of the input and
output data. When a linear kernel is used for the output view, it is shown that
the forecasting equation takes the form of kernel ridge regression. When that
kernel is non-linear, a pre-image problem has to be solved to forecast a point
in the input space. We evaluate the model on several standard time series
datasets, perform ablation studies, benchmark with closely related models and
discuss its results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Dynamic Regret and Constraint Violations in Constrained Online Convex
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Vaze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A constrained version of the online convex optimization (OCO) problem is
considered. With slotted time, for each slot, first an action is chosen.
Subsequently the loss function and the constraint violation penalty evaluated
at the chosen action point is revealed. For each slot, both the loss function
as well as the function defining the constraint set is assumed to be smooth and
strongly convex. In addition, once an action is chosen, local information about
a feasible set within a small neighborhood of the current action is also
revealed. An algorithm is allowed to compute at most one gradient at its point
of choice given the described feedback to choose the next action. The goal of
an algorithm is to simultaneously minimize the dynamic regret (loss incurred
compared to the oracle's loss) and the constraint violation penalty (penalty
accrued compared to the oracle's penalty). We propose an algorithm that follows
projected gradient descent over a suitably chosen set around the current
action. We show that both the dynamic regret and the constraint violation is
order-wise bounded by the {\it path-length}, the sum of the distances between
the consecutive optimal actions. Moreover, we show that the derived bounds are
the best possible.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Heterogeneous Domain Adaptation for IoT Intrusion Detection: A Geometric
  Graph Alignment Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiashu Wu, Hao Dai, Yang Wang, Kejiang Ye, Chengzhong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data scarcity hinders the usability of data-dependent algorithms when
tackling IoT intrusion detection (IID). To address this, we utilise the data
rich network intrusion detection (NID) domain to facilitate more accurate
intrusion detection for IID domains. In this paper, a Geometric Graph Alignment
(GGA) approach is leveraged to mask the geometric heterogeneities between
domains for better intrusion knowledge transfer. Specifically, each intrusion
domain is formulated as a graph where vertices and edges represent intrusion
categories and category-wise interrelationships, respectively. The overall
shape is preserved via a confused discriminator incapable to identify adjacency
matrices between different intrusion domain graphs. A rotation avoidance
mechanism and a centre point matching mechanism is used to avoid graph
misalignment due to rotation and symmetry, respectively. Besides, category-wise
semantic knowledge is transferred to act as vertex-level alignment. To exploit
the target data, a pseudo-label election mechanism that jointly considers
network prediction, geometric property and neighbourhood information is used to
produce fine-grained pseudo-label assignment. Upon aligning the intrusion
graphs geometrically from different granularities, the transferred intrusion
knowledge can boost IID performance. Comprehensive experiments on several
intrusion datasets demonstrate state-of-the-art performance of the GGA approach
and validate the usefulness of GGA constituting components.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Internet of Things Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LDMIC: Learning-based Distributed Multi-view Image Coding <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinjie Zhang, Jiawei Shao, Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-view image compression plays a critical role in 3D-related
applications. Existing methods adopt a predictive coding architecture, which
requires joint encoding to compress the corresponding disparity as well as
residual information. This demands collaboration among cameras and enforces the
epipolar geometric constraint between different views, which makes it
challenging to deploy these methods in distributed camera systems with randomly
overlapping fields of view. Meanwhile, distributed source coding theory
indicates that efficient data compression of correlated sources can be achieved
by independent encoding and joint decoding, which motivates us to design a
learning-based distributed multi-view image coding (LDMIC) framework. With
independent encoders, LDMIC introduces a simple yet effective joint context
transfer module based on the cross-attention mechanism at the decoder to
effectively capture the global inter-view correlations, which is insensitive to
the geometric relationships between images. Experimental results show that
LDMIC significantly outperforms both traditional and learning-based MIC methods
while enjoying fast encoding speed. Code will be released at
https://github.com/Xinjie-Q/LDMIC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantification of Damage Using Indirect Structural Health Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Achyuth Madabhushi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structural health monitoring is important to make sure bridges do not fail.
Since direct monitoring can be complicated and expensive, indirect methods have
been a focus on research. Indirect monitoring can be much cheaper and easier to
conduct, however there are challenges with getting accurate results. This work
focuses on damage quantification by using accelerometers. Tests were conducted
on a model bridge and car with four accelerometers attached to to the vehicle.
Different weights were placed on the bridge to simulate different levels of
damage, and 31 tests were run for 20 different damage levels. The acceleration
data collected was normalized and a Fast-Fourier Transform (FFT) was performed
on that data. Both the normalized acceleration data and the normalized FFT data
were inputted into a Non-Linear Principal Component Analysis (separately) and
three principal components were extracted for each data set. Support Vector
Regression (SVR) and Gaussian Process Regression (GPR) were used as the
supervised machine learning methods to develop models. Multiple models were
created so that the best one could be selected, and the models were compared by
looking at their Mean Squared Errors (MSE). This methodology should be applied
in the field to measure how effective it can be in real world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentiable bit-rate estimation for neural-based video codec
  enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Said, Manish Kumar Singh, Reza Pourreza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks (NN) can improve standard video compression by pre- and
post-processing the encoded video. For optimal NN training, the standard codec
needs to be replaced with a codec proxy that can provide derivatives of
estimated bit-rate and distortion, which are used for gradient
back-propagation. Since entropy coding of standard codecs is designed to take
into account non-linear dependencies between transform coefficients, bit-rates
cannot be well approximated with simple per-coefficient estimators. This paper
presents a new approach for bit-rate estimation that is similar to the type
employed in training end-to-end neural codecs, and able to efficiently take
into account those statistical dependencies. It is defined from a mathematical
model that provides closed-form formulas for the estimates and their gradients,
reducing the computational complexity. Experimental results demonstrate the
method's accuracy in estimating HEVC/H.265 codec bit-rates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Truveta Mapper: A Zero-shot Ontology Alignment Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mariyam Amir, Murchana Baruah, Mahsa Eslamialishah, Sina Ehsani, Alireza Bahramali, Sadra Naddaf-Sh, Saman Zarandioon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a new perspective is suggested for unsupervised Ontology
Matching (OM) or Ontology Alignment (OA) by treating it as a translation task.
Ontologies are represented as graphs, and the translation is performed from a
node in the source ontology graph to a path in the target ontology graph. The
proposed framework, Truveta Mapper (TM), leverages a multi-task
sequence-to-sequence transformer model to perform alignment across multiple
ontologies in a zero-shot, unified and end-to-end manner. Multi-tasking enables
the model to implicitly learn the relationship between different ontologies via
transfer-learning without requiring any explicit cross-ontology manually
labeled data. This also enables the formulated framework to outperform existing
solutions for both runtime latency and alignment quality. The model is
pre-trained and fine-tuned only on publicly available text corpus and
inner-ontologies data. The proposed solution outperforms state-of-the-art
approaches, Edit-Similarity, LogMap, AML, BERTMap, and the recently presented
new OM frameworks in Ontology Alignment Evaluation Initiative (OAEI22), offers
log-linear complexity in contrast to quadratic in the existing end-to-end
methods, and overall makes the OM task efficient and more straightforward
without much post-processing involving mapping extension or mapping repair.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constrained Reinforcement Learning for Dexterous Manipulation <span class="chip">IJCAI
  2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhineet Jain, Jack Kolb, Harish Ravichandar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing learning approaches to dexterous manipulation use demonstrations or
interactions with the environment to train black-box neural networks that
provide little control over how the robot learns the skills or how it would
perform post training. These approaches pose significant challenges when
implemented on physical platforms given that, during initial stages of
training, the robot's behavior could be erratic and potentially harmful to its
own hardware, the environment, or any humans in the vicinity. A potential way
to address these limitations is to add constraints during learning that
restrict and guide the robot's behavior during training as well as roll outs.
Inspired by the success of constrained approaches in other domains, we
investigate the effects of adding position-based constraints to a 24-DOF robot
hand learning to perform object relocation using Constrained Policy
Optimization. We find that a simple geometric constraint can ensure the robot
learns to move towards the object sooner than without constraints. Further,
training with this constraint requires a similar number of samples as its
unconstrained counterpart to master the skill. These findings shed light on how
simple constraints can help robots achieve sensible and safe behavior quickly
and ease concerns surrounding hardware deployment. We also investigate the
effects of the strictness of these constraints and report findings that provide
insights into how different degrees of strictness affect learning outcomes. Our
code is available at
https://github.com/GT-STAR-Lab/constrained-rl-dexterous-manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the International Workshop on Safe Reinforcement Learning
  at the 31st International Joint Conference on Artificial Intelligence (IJCAI
  2022). 6 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Model Selection for Time-series Anomaly Detection <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01078v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01078v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mononito Goswami, Cristian Challu, Laurent Callot, Lenon Minorics, Andrey Kan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection in time-series has a wide range of practical applications.
While numerous anomaly detection methods have been proposed in the literature,
a recent survey concluded that no single method is the most accurate across
various datasets. To make matters worse, anomaly labels are scarce and rarely
available in practice. The practical problem of selecting the most accurate
model for a given dataset without labels has received little attention in the
literature. This paper answers this question i.e. Given an unlabeled dataset
and a set of candidate anomaly detectors, how can we select the most accurate
model? To this end, we identify three classes of surrogate (unsupervised)
metrics, namely, prediction error, model centrality, and performance on
injected synthetic anomalies, and show that some metrics are highly correlated
with standard supervised anomaly detection performance metrics such as the
$F_1$ score, but to varying degrees. We formulate metric combination with
multiple imperfect surrogate metrics as a robust rank aggregation problem. We
then provide theoretical justification behind the proposed approach.
Large-scale experiments on multiple real-world datasets demonstrate that our
proposed unsupervised approach is as effective as selecting the most accurate
model based on partially labeled data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at International Conference on Learning Representations
  (ICLR) 2023 with a notable-top-25% recommendation. Reviewer, AC and author
  discussion available at https://openreview.net/forum?id=gOZ_pKANaPW</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Tradeoff between Energy, Precision, and Accuracy in Federated
  Quantized Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.07911v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.07911v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minsu Kim, Walid Saad, Mohammad Mozaffari, Merouane Debbah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying federated learning (FL) over wireless networks with
resource-constrained devices requires balancing between accuracy, energy
efficiency, and precision. Prior art on FL often requires devices to train deep
neural networks (DNNs) using a 32-bit precision level for data representation
to improve accuracy. However, such algorithms are impractical for
resource-constrained devices since DNNs could require execution of millions of
operations. Thus, training DNNs with a high precision level incurs a high
energy cost for FL. In this paper, a quantized FL framework, that represents
data with a finite level of precision in both local training and uplink
transmission, is proposed. Here, the finite level of precision is captured
through the use of quantized neural networks (QNNs) that quantize weights and
activations in fixed-precision format. In the considered FL model, each device
trains its QNN and transmits a quantized training result to the base station.
Energy models for the local training and the transmission with the quantization
are rigorously derived. An energy minimization problem is formulated with
respect to the level of precision while ensuring convergence. To solve the
problem, we first analytically derive the FL convergence rate and use a line
search method. Simulation results show that our FL framework can reduce energy
consumption by up to 53% compared to a standard FL model. The results also shed
light on the tradeoff between precision, energy, and accuracy in FL over
wireless networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is submitted to IEEE International Conference on
  Communications 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Green, Quantized Federated Learning over Wireless Networks: An
  Energy-Efficient Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09387v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09387v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minsu Kim, Walid Saad, Mohammad Mozaffari, Merouane Debbah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a green-quantized FL framework, which represents data with a
finite precision level in both local training and uplink transmission, is
proposed. Here, the finite precision level is captured through the use of
quantized neural networks (QNNs) that quantize weights and activations in
fixed-precision format. In the considered FL model, each device trains its QNN
and transmits a quantized training result to the base station. Energy models
for the local training and the transmission with quantization are rigorously
derived. To minimize the energy consumption and the number of communication
rounds simultaneously, a multi-objective optimization problem is formulated
with respect to the number of local iterations, the number of selected devices,
and the precision levels for both local training and transmission while
ensuring convergence under a target accuracy constraint. To solve this problem,
the convergence rate of the proposed FL system is analytically derived with
respect to the system control variables. Then, the Pareto boundary of the
problem is characterized to provide efficient solutions using the normal
boundary inspection method. Design insights on balancing the tradeoff between
the two objectives while achieving a target accuracy are drawn from using the
Nash bargaining solution and analyzing the derived convergence rate. Simulation
results show that the proposed FL framework can reduce energy consumption until
convergence by up to 70\% compared to a baseline FL algorithm that represents
data with full precision without damaging the convergence rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Wireless Communications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatial Attention Kinetic Networks with E(n)-Equivariance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08893v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08893v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanqing Wang, John D. Chodera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks that are equivariant to rotations, translations, reflections,
and permutations on n-dimensional geometric space have shown promise in
physical modeling for tasks such as accurately but inexpensively modeling
complex potential energy surfaces to guiding the sampling of complex dynamical
systems or forecasting their time evolution. Current state-of-the-art methods
employ spherical harmonics to encode higher-order interactions among particles,
which are computationally expensive. In this paper, we propose a simple
alternative functional form that uses neurally parametrized linear combinations
of edge vectors to achieve equivariance while still universally approximating
node environments. Incorporating this insight, we design spatial attention
kinetic networks with E(n)-equivariance, or SAKE, which are competitive in
many-body system modeling tasks while being significantly faster.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SIAN: Style-Guided Instance-Adaptive Normalization for Multi-Organ
  Histopathology Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.02412v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.02412v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotian Wang, Min Xian, Aleksandar Vakanski, Bryar Shareef
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing deep neural networks for histopathology image synthesis cannot
generate image styles that align with different organs, and cannot produce
accurate boundaries of clustered nuclei. To address these issues, we propose a
style-guided instance-adaptive normalization (SIAN) approach to synthesize
realistic color distributions and textures for histopathology images from
different organs. SIAN contains four phases, semantization, stylization,
instantiation, and modulation. The first two phases synthesize image semantics
and styles by using semantic maps and learned image style vectors. The
instantiation module integrates geometrical and topological information and
generates accurate nuclei boundaries. We validate the proposed approach on a
multiple-organ dataset, Extensive experimental results demonstrate that the
proposed method generates more realistic histopathology images than four
state-of-the-art approaches for five organs. By incorporating synthetic images
from the proposed approach to model training, an instance segmentation network
can achieve state-of-the-art performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Learning Based Hypothesis Test for Harmful Covariate Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02742v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02742v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Ginsberg, Zhongyuan Liang, Rahul G. Krishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to quickly and accurately identify covariate shift at test time
is a critical and often overlooked component of safe machine learning systems
deployed in high-risk domains. While methods exist for detecting when
predictions should not be made on out-of-distribution test examples,
identifying distributional level differences between training and test time can
help determine when a model should be removed from the deployment setting and
retrained. In this work, we define harmful covariate shift (HCS) as a change in
distribution that may weaken the generalization of a predictive model. To
detect HCS, we use the discordance between an ensemble of classifiers trained
to agree on training data and disagree on test data. We derive a loss function
for training this ensemble and show that the disagreement rate and entropy
represent powerful discriminative statistics for HCS. Empirically, we
demonstrate the ability of our method to detect harmful covariate shift with
statistical certainty on a variety of high-dimensional datasets. Across
numerous domains and modalities, we show state-of-the-art performance compared
to existing methods, particularly when the number of observed test samples is
small.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain generalization in deep learning-based mass detection in
  mammography: A large-scale multi-center study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.11620v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.11620v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lidia Garrucho, Kaisar Kushibar, Socayna Jouide, Oliver Diaz, Laura Igual, Karim Lekadir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer-aided detection systems based on deep learning have shown great
potential in breast cancer detection. However, the lack of domain
generalization of artificial neural networks is an important obstacle to their
deployment in changing clinical environments. In this work, we explore the
domain generalization of deep learning methods for mass detection in digital
mammography and analyze in-depth the sources of domain shift in a large-scale
multi-center setting. To this end, we compare the performance of eight
state-of-the-art detection methods, including Transformer-based models, trained
in a single domain and tested in five unseen domains. Moreover, a single-source
mass detection training pipeline is designed to improve the domain
generalization without requiring images from the new domain. The results show
that our workflow generalizes better than state-of-the-art transfer
learning-based approaches in four out of five domains while reducing the domain
shift caused by the different acquisition protocols and scanner manufacturers.
Subsequently, an extensive analysis is performed to identify the covariate
shifts with bigger effects on the detection performance, such as due to
differences in patient age, breast density, mass size, and mass malignancy.
Ultimately, this comprehensive study provides key insights and best practices
for future research on domain generalization in deep learning-based breast
cancer detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-grained Early Frequency Attention for Deep Speaker Representation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2009.01822v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2009.01822v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirhossein Hajavi, Ali Etemad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning techniques have considerably improved speech processing in
recent years. Speaker representations extracted by deep learning models are
being used in a wide range of tasks such as speaker recognition and speech
emotion recognition. Attention mechanisms have started to play an important
role in improving deep learning models in the field of speech processing.
Nonetheless, despite the fact that important speaker-related information can be
embedded in individual frequency-bins of the input spectral representations,
current attention models are unable to attend to fine-grained information items
in spectral representations. In this paper we propose Fine-grained Early
Frequency Attention (FEFA) for speaker representation learning. Our model is a
simple and lightweight model that can be integrated into various CNN pipelines
and is capable of focusing on information items as small as frequency-bins. We
evaluate the proposed model on three tasks of speaker recognition, speech
emotion recognition, and spoken digit recognition. We use Three widely used
public datasets, namely VoxCeleb, IEMOCAP, and Free Spoken Digit Dataset for
our experiments. We attach FEFA to several prominent deep learning models and
evaluate its impact on the final performance. We also compare our work with
other related works in the area. Our experiments show that by adding FEFA to
different CNN architectures, performance is consistently improved by
substantial margins, and the models equipped with FEFA outperform all the other
attentive models. We also test our model against different levels of added
noise showing improvements in robustness and less sensitivity compared to the
backbone networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ESTAS: Effective and Stable Trojan Attacks in <span class="highlight-title">Self-supervised</span> Encoders
  with One Target Unlabelled Sample 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10908v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10908v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Xue, Qian Lou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emerging self-supervised learning (SSL) has become a popular image
representation encoding method to obviate the reliance on labeled data and
learn rich representations from large-scale, ubiquitous unlabelled data. Then
one can train a downstream classifier on top of the pre-trained SSL image
encoder with few or no labeled downstream data. Although extensive works show
that SSL has achieved remarkable and competitive performance on different
downstream tasks, its security concerns, e.g, Trojan attacks in SSL encoders,
are still not well-studied. In this work, we present a novel Trojan Attack
method, denoted by ESTAS, that can enable an effective and stable attack in SSL
encoders with only one target unlabeled sample. In particular, we propose
consistent trigger poisoning and cascade optimization in ESTAS to improve
attack efficacy and model accuracy, and eliminate the expensive target-class
data sample extraction from large-scale disordered unlabelled data. Our
substantial experiments on multiple datasets show that ESTAS stably achieves >
99% attacks success rate (ASR) with one target-class sample. Compared to prior
works, ESTAS attains > 30% ASR increase and > 8.3% accuracy improvement on
average.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-Supervised</span> Learning Through Efference Copies <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.09224v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.09224v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Franz Scherr, Qinghai Guo, Timoleon Moraitis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) methods aim to exploit the abundance of
unlabelled data for machine learning (ML), however the underlying principles
are often method-specific. An SSL framework derived from biological first
principles of embodied learning could unify the various SSL methods, help
elucidate learning in the brain, and possibly improve ML. SSL commonly
transforms each training datapoint into a pair of views, uses the knowledge of
this pairing as a positive (i.e. non-contrastive) self-supervisory sign, and
potentially opposes it to unrelated, (i.e. contrastive) negative examples.
Here, we show that this type of self-supervision is an incomplete
implementation of a concept from neuroscience, the Efference Copy (EC).
Specifically, the brain also transforms the environment through efference, i.e.
motor commands, however it sends to itself an EC of the full commands, i.e.
more than a mere SSL sign. In addition, its action representations are likely
egocentric. From such a principled foundation we formally recover and extend
SSL methods such as SimCLR, BYOL, and ReLIC under a common theoretical
framework, i.e. Self-supervision Through Efference Copies (S-TEC). Empirically,
S-TEC restructures meaningfully the within- and between-class representations.
This manifests as improvement in recent strong SSL baselines in image
classification, segmentation, object detection, and in audio. These results
hypothesize a testable positive influence from the brain's motor outputs onto
its sensory representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Entropy-Based Model for Hierarchical Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.14681v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.14681v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir R. Asadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning is the dominant approach to artificial intelligence, through
which computers learn from data and experience. In the framework of supervised
learning, a necessity for a computer to learn from data accurately and
efficiently is to be provided with auxiliary information about the data
distribution and target function through the learning model. This notion of
auxiliary information relates to the concept of regularization in statistical
learning theory. A common feature among real-world datasets is that data
domains are multiscale and target functions are well-behaved and smooth. This
paper proposes an entropy-based learning model that exploits this data
structure and discusses its statistical and computational benefits. The
hierarchical learning model is inspired by human beings' logical and
progressive easy-to-hard learning mechanism and has interpretable levels. The
model apportions computational resources according to the complexity of data
instances and target functions. This property can have multiple benefits,
including higher inference speed and computational savings in training a model
for many users or when training is interrupted. We provide a statistical
analysis of the learning mechanism using multiscale entropies and show that it
can yield significantly stronger guarantees than uniform convergence bounds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VaiPhy: a Variational Inference Based Algorithm for Phylogeny <span class="chip">NeurIPS-22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.01121v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.01121v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hazal Koptagel, Oskar Kviman, Harald Melin, Negar Safinianaini, Jens Lagergren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Phylogenetics is a classical methodology in computational biology that today
has become highly relevant for medical investigation of single-cell data, e.g.,
in the context of cancer development. The exponential size of the tree space
is, unfortunately, a substantial obstacle for Bayesian phylogenetic inference
using Markov chain Monte Carlo based methods since these rely on local
operations. And although more recent variational inference (VI) based methods
offer speed improvements, they rely on expensive auto-differentiation
operations for learning the variational parameters. We propose VaiPhy, a
remarkably fast VI based algorithm for approximate posterior inference in an
augmented tree space. VaiPhy produces marginal log-likelihood estimates on par
with the state-of-the-art methods on real data and is considerably faster since
it does not require auto-differentiation. Instead, VaiPhy combines coordinate
ascent update equations with two novel sampling schemes: (i) SLANTIS, a
proposal distribution for tree topologies in the augmented tree space, and (ii)
the JC sampler, to the best of our knowledge, the first-ever scheme for
sampling branch lengths directly from the popular Jukes-Cantor model. We
compare VaiPhy in terms of density estimation and runtime. Additionally, we
evaluate the reproducibility of the baselines. We provide our code on GitHub:
\url{https://github.com/Lagergren-Lab/VaiPhy}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS-22 conference paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ To Trust or Not To Trust Prediction Scores for Membership Inference
  Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.09076v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.09076v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Hintersdorf, Lukas Struppek, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Membership inference attacks (MIAs) aim to determine whether a specific
sample was used to train a predictive model. Knowing this may indeed lead to a
privacy breach. Most MIAs, however, make use of the model's prediction scores -
the probability of each output given some input - following the intuition that
the trained model tends to behave differently on its training data. We argue
that this is a fallacy for many modern deep network architectures.
Consequently, MIAs will miserably fail since overconfidence leads to high
false-positive rates not only on known domains but also on out-of-distribution
data and implicitly acts as a defense against MIAs. Specifically, using
generative adversarial networks, we are able to produce a potentially infinite
number of samples falsely classified as part of the training data. In other
words, the threat of MIAs is overestimated, and less information is leaked than
previously assumed. Moreover, there is actually a trade-off between the
overconfidence of models and their susceptibility to MIAs: the more classifiers
know when they do not know, making low confidence predictions, the more they
reveal the training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures, 10 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pex: Memory-efficient Microcontroller Deep Learning through Partial
  Execution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.17246v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.17246v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edgar Liberis, Nicholas D. Lane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embedded and IoT devices, largely powered by microcontroller units (MCUs),
could be made more intelligent by leveraging on-device deep learning. One of
the main challenges of neural network inference on an MCU is the extremely
limited amount of read-write on-chip memory (SRAM, < 512 kB). SRAM is consumed
by the neural network layer (operator) input and output buffers, which,
traditionally, must be in memory (materialised) for an operator to execute. We
discuss a novel execution paradigm for microcontroller deep learning, which
modifies the execution of neural networks to avoid materialising full buffers
in memory, drastically reducing SRAM usage with no computation overhead. This
is achieved by exploiting the properties of operators, which can
consume/produce a fraction of their input/output at a time. We describe a
partial execution compiler, Pex, which produces memory-efficient execution
schedules automatically by identifying subgraphs of operators whose execution
can be split along the feature ("channel") dimension. Memory usage is reduced
further by targeting memory bottlenecks with structured pruning, leading to the
co-design of the network architecture and its execution schedule. Our
evaluation of image and audio classification models: (a) establishes
state-of-the-art performance in low SRAM usage regimes for considered tasks
with up to +2.9% accuracy increase; (b) finds that a 4x memory reduction is
possible by applying partial execution alone, or up to 10.5x when using the
compiler-pruning co-design, while maintaining the classification accuracy
compared to prior work; (c) uses the recovered SRAM to process higher
resolution inputs instead, increasing accuracy by up to +3.9% on Visual Wake
Words.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Simulation Software Demonstration for Quantum Multi-Drone
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15375v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15375v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanyoung Park, Jae Pyoung Kim, Won Joon Yun, Soyi Jung, Joongheon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum computing (QC) has received a lot of attention according to its light
training parameter numbers and computational speeds by qubits. Moreover,
various researchers have tried to enable quantum machine learning (QML) using
QC, where there are also multifarious efforts to use QC to implement quantum
multi-agent reinforcement learning (QMARL). Existing classical multi-agent
reinforcement learning (MARL) using neural network features non-stationarity
and uncertain properties due to its large number of parameters. Therefore, this
paper presents a visual simulation software framework for a novel QMARL
algorithm to control autonomous multi-drone systems to take advantage of QC.
Our proposed QMARL framework accomplishes reasonable reward convergence and
service quality performance with fewer trainable parameters than the classical
MARL. Furthermore, QMARL shows more stable training results than existing MARL
algorithms. Lastly, our proposed visual simulation software allows us to
analyze the agents' training process and results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We need to revise and resubmit this paper to new version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Implicit k-Space for Binning-free Non-Cartesian Cardiac MR
  Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08479v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08479v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Huang, Hongwei Li, Jiazhen Pan, Gastao Cruz, Daniel Rueckert, Kerstin Hammernik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a novel image reconstruction framework that directly
learns a neural implicit representation in k-space for ECG-triggered
non-Cartesian Cardiac Magnetic Resonance Imaging (CMR). While existing methods
bin acquired data from neighboring time points to reconstruct one phase of the
cardiac motion, our framework allows for a continuous, binning-free, and
subject-specific k-space representation.We assign a unique coordinate that
consists of time, coil index, and frequency domain location to each sampled
k-space point. We then learn the subject-specific mapping from these unique
coordinates to k-space intensities using a multi-layer perceptron with
frequency domain regularization. During inference, we obtain a complete k-space
for Cartesian coordinates and an arbitrary temporal resolution. A simple
inverse Fourier transform recovers the image, eliminating the need for density
compensation and costly non-uniform Fourier transforms for non-Cartesian data.
This novel imaging framework was tested on 42 radially sampled datasets from 6
subjects. The proposed method outperforms other techniques qualitatively and
quantitatively using data from four and one heartbeat(s) and 30 cardiac phases.
Our results for one heartbeat reconstruction of 50 cardiac phases show improved
artifact removal and spatio-temporal resolution, leveraging the potential for
real-time CMR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Integrating Reward Maximization and Population Estimation: Sequential
  Decision-Making for Internal Revenue Service Audit Selection <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.11910v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.11910v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Henderson, Ben Chugg, Brandon Anderson, Kristen Altenburger, Alex Turk, John Guyton, Jacob Goldin, Daniel E. Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new setting, optimize-and-estimate structured bandits. Here, a
policy must select a batch of arms, each characterized by its own context, that
would allow it to both maximize reward and maintain an accurate (ideally
unbiased) population estimate of the reward. This setting is inherent to many
public and private sector applications and often requires handling delayed
feedback, small data, and distribution shifts. We demonstrate its importance on
real data from the United States Internal Revenue Service (IRS). The IRS
performs yearly audits of the tax base. Two of its most important objectives
are to identify suspected misreporting and to estimate the "tax gap" -- the
global difference between the amount paid and true amount owed. Based on a
unique collaboration with the IRS, we cast these two processes as a unified
optimize-and-estimate structured bandit. We analyze optimize-and-estimate
approaches to the IRS problem and propose a novel mechanism for unbiased
population estimation that achieves rewards comparable to baseline approaches.
This approach has the potential to improve audit efficacy, while maintaining
policy-relevant estimates of the tax gap. This has important social
consequences given that the current tax gap is estimated at nearly half a
trillion dollars. We suggest that this problem setting is fertile ground for
further research and we highlight its interesting challenges. The results of
this and related research are currently being incorporated into the continual
improvement of the IRS audit selection methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Thirty-Seventh AAAI Conference On Artificial
  Intelligence (AAAI), 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ExcelFormer: A Neural Network Surpassing GBDTs on Tabular Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.02819v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.02819v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintai Chen, Jiahuan Yan, Danny Ziyi Chen, Jian Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Though deep neural networks have gained enormous successes in various fields
(e.g., computer vision) with supervised learning, they have so far been still
trailing after the performances of GBDTs on tabular data. Delving into this
task, we determine that a judicious handling of feature interactions and
feature representation is crucial to the effectiveness of neural networks on
tabular data. We develop a novel neural network called ExcelFormer, which
alternates in turn between two attention modules that shrewdly manipulate
feature interactions and feature representation updates, respectively. A
bespoke training methodology is jointly introduced to facilitate model
performances. Specifically, by initializing parameters with minuscule values,
these attention modules are attenuated when the training begins, and the
effects of feature interactions and representation updates grow progressively
up to optimum levels under the guidance of our proposed specific regularization
schemes Feat-Mix and Hidden-Mix as the training proceeds. Experiments on 28
public tabular datasets show that our ExcelFormer approach is superior to
extensively-tuned GBDTs, which is an unprecedented progress of deep neural
networks on supervised tabular learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaze-based Object Detection in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.15651v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.15651v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Weber, Wolfgang Fuhl, Andreas Zell, Enkelejda Kasneci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In human-robot collaboration, one challenging task is to teach a robot new
yet unknown objects enabling it to interact with them. Thereby, gaze can
contain valuable information. We investigate if it is possible to detect
objects (object or no object) merely from gaze data and determine their
bounding box parameters. For this purpose, we explore different sizes of
temporal windows, which serve as a basis for the computation of heatmaps, i.e.,
the spatial distribution of the gaze data. Additionally, we analyze different
grid sizes of these heatmaps, and demonstrate the functionality in a proof of
concept using different machine learning techniques. Our method is
characterized by its speed and resource efficiency compared to conventional
object detectors. In order to generate the required data, we conducted a study
with five subjects who could move freely and thus, turn towards arbitrary
objects. This way, we chose a scenario for our data collection that is as
realistic as possible. Since the subjects move while facing objects, the
heatmaps also contain gaze data trajectories, complicating the detection and
parameter regression. We make our data set publicly available to the research
community for download.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Multi-view Clustering via Ensembles: Towards Scalability,
  Superiority, and Simplicity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11572v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11572v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Huang, Chang-Dong Wang, Jian-Huang Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant progress, there remain three limitations to the previous
multi-view clustering algorithms. First, they often suffer from high
computational complexity, restricting their feasibility for large-scale
datasets. Second, they typically fuse multi-view information via one-stage
fusion, neglecting the possibilities in multi-stage fusions. Third,
dataset-specific hyperparameter-tuning is frequently required, further
undermining their practicability. In light of this, we propose a fast
multi-view clustering via ensembles (FastMICE) approach. Particularly, the
concept of random view groups is presented to capture the versatile view-wise
relationships, through which the hybrid early-late fusion strategy is designed
to enable efficient multi-stage fusions. With multiple views extended to many
view groups, three levels of diversity (w.r.t. features, anchors, and
neighbors, respectively) are jointly leveraged for constructing the
view-sharing bipartite graphs in the early-stage fusion. Then, a set of
diversified base clusterings for different view groups are obtained via fast
graph partitioning, which are further formulated into a unified bipartite graph
for final clustering in the late-stage fusion. Notably, FastMICE has almost
linear time and space complexity, and is free of dataset-specific tuning.
Experiments on 22 multi-view datasets demonstrate its advantages in scalability
(for extremely large datasets), superiority (in clustering performance), and
simplicity (to be applied) over the state-of-the-art. Code available:
https://github.com/huangdonghere/FastMICE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in IEEE Transactions on Knowledge and Data Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fed<span class="highlight-title">Prompt</span>: Communication-Efficient and Privacy Preserving <span class="highlight-title">Prompt</span> Tuning
  in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.12268v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.12268v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haodong Zhao, Wei Du, Fangqi Li, Peixuan Li, Gongshen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) has enabled global model training on decentralized
data in a privacy-preserving way by aggregating model updates. However, for
many natural language processing (NLP) tasks that utilize pre-trained language
models (PLMs) with large numbers of parameters, there are considerable
communication costs associated with FL. Recently, prompt tuning, which tunes
some soft prompts without modifying PLMs, has achieved excellent performance as
a new learning paradigm. Therefore we want to combine the two methods and
explore the effect of prompt tuning under FL. In this paper, we propose
"FedPrompt" to study prompt tuning in a model split aggregation way using FL,
and prove that split aggregation greatly reduces the communication cost, only
0.01% of the PLMs' parameters, with little decrease on accuracy both on IID and
Non-IID data distribution. This improves the efficiency of FL method while also
protecting the data privacy in prompt tuning. In addition, like PLMs, prompts
are uploaded and downloaded between public platforms and personal users, so we
try to figure out whether there is still a backdoor threat using only soft
prompts in FL scenarios. We further conduct backdoor attacks by data poisoning
on FedPrompt. Our experiments show that normal backdoor attack can not achieve
a high attack success rate, proving the robustness of FedPrompt. We hope this
work can promote the application of prompt in FL and raise the awareness of the
possible security threats.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring Fairness Under Unawareness of Sensitive Attributes: A
  Quantification-Based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.08549v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.08549v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Fabris, Andrea Esuli, Alejandro Moreo, Fabrizio Sebastiani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Algorithms and models are increasingly deployed to inform decisions about
people, inevitably affecting their lives. As a consequence, those in charge of
developing these models must carefully evaluate their impact on different
groups of people and favour group fairness, that is, ensure that groups
determined by sensitive demographic attributes, such as race or sex, are not
treated unjustly. To achieve this goal, the availability (awareness) of these
demographic attributes to those evaluating the impact of these models is
fundamental. Unfortunately, collecting and storing these attributes is often in
conflict with industry practices and legislation on data minimisation and
privacy. For this reason, it can be hard to measure the group fairness of
trained models, even from within the companies developing them. In this work,
we tackle the problem of measuring group fairness under unawareness of
sensitive attributes, by using techniques from quantification, a supervised
learning task concerned with directly providing group-level prevalence
estimates (rather than individual-level class labels). We show that
quantification approaches are particularly suited to tackle the
fairness-under-unawareness problem, as they are robust to inevitable
distribution shifts while at the same time decoupling the (desirable) objective
of measuring group fairness from the (undesirable) side effect of allowing the
inference of sensitive attributes of individuals. More in detail, we show that
fairness under unawareness can be cast as a quantification problem and solved
with proven methods from the quantification literature. We show that these
methods outperform previous approaches to measure demographic parity in five
experimental protocols, corresponding to important challenges that complicate
the estimation of classifier fairness under unawareness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can large language models reason about medical questions? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08143v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08143v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Liévin, Christoffer Egeberg Hother, Ole Winther
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although large language models (LLMs) often produce impressive outputs, it
remains unclear how they perform in real-world scenarios requiring strong
reasoning skills and expert domain knowledge. We set out to investigate whether
GPT-3.5 (Codex and InstructGPT) can be applied to answer and reason about
difficult real-world-based questions. We utilize two multiple-choice medical
exam questions (USMLE and MedMCQA) and a medical reading comprehension dataset
(PubMedQA). We investigate multiple prompting scenarios: Chain-of-Thought (CoT,
think step-by-step), zero- and few-shot (prepending the question with
question-answer exemplars) and retrieval augmentation (injecting Wikipedia
passages into the prompt). For a subset of the USMLE questions, a medical
expert reviewed and annotated the model's CoT. We found that InstructGPT can
often read, reason and recall expert knowledge. Failure are primarily due to
lack of knowledge and reasoning errors and trivial guessing heuristics are
observed, e.g.\ too often predicting labels A and D on USMLE. Sampling and
combining many completions overcome some of these limitations. Using 100
samples, Codex 5-shot CoT not only gives close to well-calibrated predictive
probability but also achieves human-level performances on the three datasets.
USMLE: 60.2%, MedMCQA: 62.7% and PubMedQA: 78.2%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 6 figures, to be submitted. v1: results using InstructGPT,
  v2: added the Codex experiments, v3: added the missing test MedMCQA results
  for Codex 5-shot CoT and using k=100 samples</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HTMOT : Hierarchical Topic Modelling Over Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.03104v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.03104v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Judicael Poumay, Ashwin Ittoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the years, topic models have provided an efficient way of extracting
insights from text. However, while many models have been proposed, none are
able to model topic temporality and hierarchy jointly. Modelling time provide
more precise topics by separating lexically close but temporally distinct
topics while modelling hierarchy provides a more detailed view of the content
of a document corpus. In this study, we therefore propose a novel method,
HTMOT, to perform Hierarchical Topic Modelling Over Time. We train HTMOT using
a new implementation of Gibbs sampling, which is more efficient. Specifically,
we show that only applying time modelling to deep sub-topics provides a way to
extract specific stories or events while high level topics extract larger
themes in the corpus. Our results show that our training procedure is fast and
can extract accurate high-level topics and temporally precise sub-topics. We
measured our model's performance using the Word Intrusion task and outlined
some limitations of this evaluation method, especially for hierarchical models.
As a case study, we focused on the various developments in the space industry
in 2020.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Planning in a Compact Latent Action Space <span class="chip">ICLR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.10291v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.10291v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyao Jiang, Tianjun Zhang, Michael Janner, Yueying Li, Tim Rocktäschel, Edward Grefenstette, Yuandong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Planning-based reinforcement learning has shown strong performance in tasks
in discrete and low-dimensional continuous action spaces. However, planning
usually brings significant computational overhead for decision-making, and
scaling such methods to high-dimensional action spaces remains challenging. To
advance efficient planning for high-dimensional continuous control, we propose
Trajectory Autoencoding Planner (TAP), which learns low-dimensional latent
action codes with a state-conditional VQ-VAE. The decoder of the VQ-VAE thus
serves as a novel dynamics model that takes latent actions and current state as
input and reconstructs long-horizon trajectories. During inference time, given
a starting state, TAP searches over discrete latent actions to find
trajectories that have both high probability under the training distribution
and high predicted cumulative reward. Empirical evaluation in the offline RL
setting demonstrates low decision latency which is indifferent to the growing
raw action dimensionality. For Adroit robotic hand manipulation tasks with
high-dimensional continuous action space, TAP surpasses existing model-based
methods by a large margin and also beats strong model-free actor-critic
baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2023. Code available at
  https://github.com/ZhengyaoJiang/latentplan</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08664v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08664v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingting Yuan, Liang Mi, Weijun Wang, Haipeng Dai, Xiaoming Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quality of the video stream is key to neural network-based video
analytics. However, low-quality video is inevitably collected by existing
surveillance systems because of poor quality cameras or over-compressed/pruned
video streaming protocols, e.g., as a result of upstream bandwidth limit. To
address this issue, existing studies use quality enhancers (e.g., neural
super-resolution) to improve the quality of videos (e.g., resolution) and
eventually ensure inference accuracy. Nevertheless, directly applying quality
enhancers does not work in practice because it will introduce unacceptable
latency. In this paper, we present AccDecoder, a novel accelerated decoder for
real-time and neural-enhanced video analytics. AccDecoder can select a few
frames adaptively via Deep Reinforcement Learning (DRL) to enhance the quality
by neural super-resolution and then up-scale the unselected frames that
reference them, which leads to 6-21% accuracy improvement. AccDecoder provides
efficient inference capability via filtering important frames using DRL for
DNN-based inference and reusing the results for the other frames via extracting
the reference relationship among frames and blocks, which results in a latency
reduction of 20-80% than baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2023 IEEE INFOCOM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robustness through Data Augmentation Loss Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.11205v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.11205v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianjian Huang, Shaunak Halbe, Chinnadhurai Sankar, Pooyan Amini, Satwik Kottur, Alborz Geramifard, Meisam Razaviyayn, Ahmad Beirami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep learning through empirical risk minimization (ERM) has succeeded
at achieving human-level performance at a variety of complex tasks, ERM is not
robust to distribution shifts or adversarial attacks. Synthetic data
augmentation followed by empirical risk minimization (DA-ERM) is a simple and
widely used solution to improve robustness in ERM. In addition, consistency
regularization can be applied to further improve the robustness of the model by
forcing the representation of the original sample and the augmented one to be
similar. However, existing consistency regularization methods are not
applicable to covariant data augmentation, where the label in the augmented
sample is dependent on the augmentation function. For example, dialog state
covaries with named entity when we augment data with a new named entity. In
this paper, we propose data augmented loss invariant regularization (DAIR), a
simple form of consistency regularization that is applied directly at the loss
level rather than intermediate features, making it widely applicable to both
invariant and covariant data augmentation regardless of network architecture,
problem setup, and task. We apply DAIR to real-world learning problems
involving covariant data augmentation: robust neural task-oriented dialog state
tracking and robust visual question answering. We also apply DAIR to tasks
involving invariant data augmentation: robust regression, robust classification
against adversarial attacks, and robust ImageNet classification under
distribution shift. Our experiments show that DAIR consistently outperforms ERM
and DA-ERM with little marginal computational cost and sets new
state-of-the-art results in several benchmarks involving covariant data
augmentation. Our code of all experiments is available at:
https://github.com/optimization-for-data-driven-science/DAIR.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Slate Recommendation with Reinforcement Learning <span class="chip">WSDM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08632v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08632v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romain Deffayet, Thibaut Thonet, Jean-Michel Renders, Maarten de Rijke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has employed reinforcement learning (RL) algorithms to
optimize long-term user engagement in recommender systems, thereby avoiding
common pitfalls such as user boredom and filter bubbles. They capture the
sequential and interactive nature of recommendations, and thus offer a
principled way to deal with long-term rewards and avoid myopic behaviors.
However, RL approaches are intractable in the slate recommendation scenario -
where a list of items is recommended at each interaction turn - due to the
combinatorial action space. In that setting, an action corresponds to a slate
that may contain any combination of items.
  While previous work has proposed well-chosen decompositions of actions so as
to ensure tractability, these rely on restrictive and sometimes unrealistic
assumptions. Instead, in this work we propose to encode slates in a continuous,
low-dimensional latent space learned by a variational auto-encoder. Then, the
RL agent selects continuous actions in this latent space, which are ultimately
decoded into the corresponding slates. By doing so, we are able to (i) relax
assumptions required by previous work, and (ii) improve the quality of the
action selection by modeling full slates instead of independent items, in
particular by enabling diversity. Our experiments performed on a wide array of
simulated environments confirm the effectiveness of our generative modeling of
slates over baselines in practical scenarios where the restrictive assumptions
underlying the baselines are lifted. Our findings suggest that representation
learning using generative models is a promising direction towards generalizable
RL-based slate recommendation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WSDM 2023, 9 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A DNN Optimizer that Improves over AdaBelief by Suppression of the
  Adaptive Stepsize Range 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13273v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13273v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoqiang Zhang, Kenta Niwa, W. Bastiaan Kleijn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We make contributions towards improving adaptive-optimizer performance. Our
improvements are based on suppression of the range of adaptive stepsizes in the
AdaBelief optimizer. Firstly, we show that the particular placement of the
parameter epsilon within the update expressions of AdaBelief reduces the range
of the adaptive stepsizes, making AdaBelief closer to SGD with momentum.
Secondly, we extend AdaBelief by further suppressing the range of the adaptive
stepsizes. To achieve the above goal, we perform mutual layerwise vector
projections between the gradient g_t and its first momentum m_t before using
them to estimate the second momentum. The new optimization method is referred
to as Aida. Thirdly, extensive experimental results show that Aida outperforms
nine optimizers when training transformers and LSTMs for NLP, and VGG and
ResNet for image classification over CIAF10 and CIFAR100 while matching the
best performance of the nine methods when training WGAN-GP models for image
generation tasks. Furthermore, Aida produces higher validation accuracies than
AdaBelief for training ResNet18 over ImageNet. Code is available <a
href="https://github.com/guoqiang-x-zhang/AidaOptimizer">at this URL</a>
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Planckian Jitter: countering the color-crippling effects of color jitter
  on <span class="highlight-title">self-supervised</span> training <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.07993v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.07993v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Zini, Alex Gomez-Villa, Marco Buzzelli, Bartłomiej Twardowski, Andrew D. Bagdanov, Joost van de Weijer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several recent works on self-supervised learning are trained by mapping
different augmentations of the same image to the same feature representation.
The data augmentations used are of crucial importance to the quality of learned
feature representations. In this paper, we analyze how the color jitter
traditionally used in data augmentation negatively impacts the quality of the
color features in learned feature representations. To address this problem, we
propose a more realistic, physics-based color data augmentation - which we call
Planckian Jitter - that creates realistic variations in chromaticity and
produces a model robust to illumination changes that can be commonly observed
in real life, while maintaining the ability to discriminate image content based
on color information. Experiments confirm that such a representation is
complementary to the representations learned with the currently-used color
jitter augmentation and that a simple concatenation leads to significant
performance gains on a wide range of downstream datasets. In addition, we
present a color sensitivity analysis that documents the impact of different
training methods on model neurons and shows that the performance of the learned
features is robust with respect to illuminant variations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Eleventh International Conference on Learning
  Representations (ICLR 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Foresight -- Generative <span class="highlight-title">Pretrain</span>ed <span class="highlight-title">Transformer</span> (<span class="highlight-title">GPT</span>) for Modelling of
  Patient Timelines using EHRs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08072v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08072v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeljko Kraljevic, Dan Bean, Anthony Shek, Rebecca Bendayan, Harry Hemingway, Joshua Au Yeung, Alexander Deng, Alfie Baston, Jack Ross, Esther Idowu, James T Teo, Richard J Dobson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: Electronic Health Records hold detailed longitudinal information
about each patient's health status and general clinical history, a large
portion of which is stored within the unstructured text. Existing approaches
focus mostly on structured data and a subset of single-domain outcomes. We
explore how temporal modelling of patients from free text and structured data,
using deep generative transformers can be used to forecast a wide range of
future disorders, substances, procedures or findings. Methods: We present
Foresight, a novel transformer-based pipeline that uses named entity
recognition and linking tools to convert document text into structured, coded
concepts, followed by providing probabilistic forecasts for future medical
events such as disorders, substances, procedures and findings. We processed the
entire free-text portion from three different hospital datasets totalling
811336 patients covering both physical and mental health. Findings: On tests in
two UK hospitals (King's College Hospital, South London and Maudsley) and the
US MIMIC-III dataset precision@10 0.68, 0.76 and 0.88 was achieved for
forecasting the next disorder in a patient timeline, while precision@10 of
0.80, 0.81 and 0.91 was achieved for forecasting the next biomedical concept.
Foresight was also validated on 34 synthetic patient timelines by five
clinicians and achieved relevancy of 97% for the top forecasted candidate
disorder. As a generative model, it can forecast follow-on biomedical concepts
for as many steps as required. Interpretation: Foresight is a general-purpose
model for biomedical concept modelling that can be used for real-world risk
forecasting, virtual trials and clinical research to study the progression of
disorders, simulate interventions and counterfactuals, and educational
purposes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Incorporating functional summary information in Bayesian neural networks
  using a Dirichlet process likelihood approach <span class="chip">AISTATS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01234v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01234v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishnu Raj, Tianyu Cui, Markus Heinonen, Pekka Marttinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian neural networks (BNNs) can account for both aleatoric and epistemic
uncertainty. However, in BNNs the priors are often specified over the weights
which rarely reflects true prior knowledge in large and complex neural network
architectures. We present a simple approach to incorporate prior knowledge in
BNNs based on external summary information about the predicted classification
probabilities for a given dataset. The available summary information is
incorporated as augmented data and modeled with a Dirichlet process, and we
derive the corresponding \emph{Summary Evidence Lower BOund}. The approach is
founded on Bayesian principles, and all hyperparameters have a proper
probabilistic interpretation. We show how the method can inform the model about
task difficulty and class imbalance. Extensive experiments show that, with
negligible computational overhead, our method parallels and in many cases
outperforms popular alternatives in accuracy, uncertainty calibration, and
robustness against corruptions with both balanced and imbalanced data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in AISTATS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mind the Gap -- Modelling Difference Between Censored and Uncensored
  Electric Vehicle Charging Demand 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06418v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06418v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frederik Boe Hüttel, Filipe Rodrigues, Francisco Câmara Pereira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electric vehicle charging demand models, with charging records as input, will
inherently be biased toward the supply of available chargers, as the data do
not include demand lost from occupied stations and competitors. This lost
demand implies that the records only observe a fraction of the total demand,
i.e. the observations are censored, and actual demand is likely higher than
what the data reflect. Machine learning models often neglect to account for
this censored demand when forecasting the charging demand, which limits models'
applications for future expansions and supply management. We address this gap
by modelling the charging demand with probabilistic censorship-aware graph
neural networks, which learn the latent demand distribution in both the spatial
and temporal dimensions. We use GPS trajectories from cars in Copenhagen,
Denmark, to study how censoring occurs and much demand is lost due to occupied
charging and competing services. We find that censorship varies throughout the
city and over time, encouraging spatial and temporal modelling. We find that in
some regions of Copenhagen, censorship occurs 61% of the time. Our results show
censorship-aware models provide better prediction and uncertainty estimation in
actual future demand than censorship-unaware models. Our results suggest that
future models based on charging records should account for the censoring to
expand the application areas of machine learning models in this supply
management and infrastructure expansion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model-Agnostic Confidence Intervals for Feature Importance: A Fast and
  Powerful Approach Using Minipatch Ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.02088v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.02088v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luqin Gan, Lili Zheng, Genevera I. Allen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To promote new scientific discoveries from complex data sets, feature
importance inference has been a long-standing statistical problem. Instead of
testing for parameters that are only interpretable for specific models, there
has been increasing interest in model-agnostic methods, often in the form of
feature occlusion or leave-one-covariate-out (LOCO) inference. Existing
approaches often make distributional assumptions, which can be difficult to
verify in practice, or require model refitting and data splitting, which are
computationally intensive and lead to losses in power. In this work, we develop
a novel, mostly model-agnostic and distribution-free inference framework for
feature importance that is computationally efficient and statistically
powerful. Our approach is fast as we avoid model refitting by leveraging a form
of random observation and feature subsampling called minipatch ensembles; this
approach also improves statistical power by avoiding data splitting. Our
framework can be applied on tabular data and with any machine learning
algorithm, together with minipatch ensembles, for regression and classification
tasks. Despite the dependencies induced by using minipatch ensembles, we show
that our approach provides asymptotic coverage for the feature importance score
of any model under mild assumptions. Finally, our same procedure can also be
leveraged to provide valid confidence intervals for predictions, hence
providing fast, simultaneous quantification of the uncertainty of both
predictions and feature importance. We validate our intervals on a series of
synthetic and real data examples, including non-linear settings, showing that
our approach detects the correct important features and exhibits many
computational and statistical advantages over existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Agent Patrolling with Battery Constraints through Deep
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08230v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08230v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhao Tong, Aaron Harwood, Maria A. Rodriguez, Richard O. Sinnott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous vehicles are suited for continuous area patrolling problems.
However, finding an optimal patrolling strategy can be challenging for many
reasons. Firstly, patrolling environments are often complex and can include
unknown environmental factors. Secondly, autonomous vehicles can have failures
or hardware constraints, such as limited battery life. Importantly, patrolling
large areas often requires multiple agents that need to collectively coordinate
their actions. In this work, we consider these limitations and propose an
approach based on model-free, deep multi-agent reinforcement learning. In this
approach, the agents are trained to automatically recharge themselves when
required, to support continuous collective patrolling. A distributed
homogeneous multi-agent architecture is proposed, where all patrolling agents
execute identical policies locally based on their local observations and shared
information. This architecture provides a fault-tolerant and robust patrolling
system that can tolerate agent failures and allow supplementary agents to be
added to replace failed agents or to increase the overall patrol performance.
The solution is validated through simulation experiments from multiple
perspectives, including the overall patrol performance, the efficiency of
battery recharging strategies, and the overall fault tolerance and robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Framework for Evaluating the Impact of Food Security Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09320v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09320v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachid Belmeskine, Abed Benaichouche
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study proposes an approach for predicting the impacts of scenarios on
food security and demonstrates its application in a case study. The approach
involves two main steps: (1) scenario definition, in which the end user
specifies the assumptions and impacts of the scenario using a scenario
template, and (2) scenario evaluation, in which a Vector Autoregression (VAR)
model is used in combination with Monte Carlo simulation to generate
predictions for the impacts of the scenario based on the defined assumptions
and impacts. The case study is based on a proprietary time series food security
database created using data from the Food and Agriculture Organization of the
United Nations (FAOSTAT), the World Bank, and the United States Department of
Agriculture (USDA). The database contains a wide range of data on various
indicators of food security, such as production, trade, consumption, prices,
availability, access, and nutritional value. The results show that the proposed
approach can be used to predict the potential impacts of scenarios on food
security and that the proprietary time series food security database can be
used to support this approach. The study provides specific insights on how this
approach can inform decision-making processes related to food security such as
food prices and availability in the case study region.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D UX-Net: A Large Kernel Volumetric ConvNet Modernizing Hierarchical
  <span class="highlight-title">Transformer</span> for Medical Image Segmentation <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.15076v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.15076v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ho Hin Lee, Shunxing Bao, Yuankai Huo, Bennett A. Landman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent 3D medical ViTs (e.g., SwinUNETR) achieve the state-of-the-art
performances on several 3D volumetric data benchmarks, including 3D medical
image segmentation. Hierarchical transformers (e.g., Swin Transformers)
reintroduced several ConvNet priors and further enhanced the practical
viability of adapting volumetric segmentation in 3D medical datasets. The
effectiveness of hybrid approaches is largely credited to the large receptive
field for non-local self-attention and the large number of model parameters. In
this work, we propose a lightweight volumetric ConvNet, termed 3D UX-Net, which
adapts the hierarchical transformer using ConvNet modules for robust volumetric
segmentation. Specifically, we revisit volumetric depth-wise convolutions with
large kernel size (e.g. starting from $7\times7\times7$) to enable the larger
global receptive fields, inspired by Swin Transformer. We further substitute
the multi-layer perceptron (MLP) in Swin Transformer blocks with pointwise
depth convolutions and enhance model performances with fewer normalization and
activation layers, thus reducing the number of model parameters. 3D UX-Net
competes favorably with current SOTA transformers (e.g. SwinUNETR) using three
challenging public datasets on volumetric brain and abdominal imaging: 1)
MICCAI Challenge 2021 FLARE, 2) MICCAI Challenge 2021 FeTA, and 3) MICCAI
Challenge 2022 AMOS. 3D UX-Net consistently outperforms SwinUNETR with
improvement from 0.929 to 0.938 Dice (FLARE2021) and 0.867 to 0.874 Dice
(Feta2021). We further evaluate the transfer learning capability of 3D UX-Net
with AMOS2022 and demonstrates another improvement of $2.27\%$ Dice (from 0.880
to 0.900). The source code with our proposed model are available at
https://github.com/MASILab/3DUX-Net.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Asteroid Detection in Microlensing <span class="highlight-title">Survey</span>s with Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.02239v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.02239v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Preeti Cowan, Ian A. Bond, Napoleon H. Reyes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Asteroids are an indelible part of most astronomical surveys though only a
few surveys are dedicated to their detection. Over the years, high cadence
microlensing surveys have amassed several terabytes of data while scanning
primarily the Galactic Bulge and Magellanic Clouds for microlensing events and
thus provide a treasure trove of opportunities for scientific data mining. In
particular, numerous asteroids have been observed by visual inspection of
selected images. This paper presents novel deep learning-based solutions for
the recovery and discovery of asteroids in the microlensing data gathered by
the MOA project. Asteroid tracklets can be clearly seen by combining all the
observations on a given night and these tracklets inform the structure of the
dataset. Known asteroids were identified within these composite images and used
for creating the labelled datasets required for supervised learning. Several
custom CNN models were developed to identify images with asteroid tracklets.
Model ensembling was then employed to reduce the variance in the predictions as
well as to improve the generalisation error, achieving a recall of 97.67%.
Furthermore, the YOLOv4 object detector was trained to localize asteroid
tracklets, achieving a mean Average Precision (mAP) of 90.97%. These trained
networks will be applied to 16 years of MOA archival data to find both known
and unknown asteroids that have been observed by the survey over the years. The
methodologies developed can be adapted for use by other surveys for asteroid
recovery and discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 17 figures, to be published in Astronomy and Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Recommendation with Additive Personalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Li, Guodong Long, Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With rising concerns about privacy, developing recommendation systems in a
federated setting become a new paradigm to develop next-generation Internet
service architecture. However, existing approaches are usually derived from a
distributed recommendation framework with an additional mechanism for privacy
protection, thus most of them fail to fully exploit personalization in the new
context of federated recommendation settings. In this paper, we propose a novel
approach called Federated Recommendation with Additive Personalization (FedRAP)
to enhance recommendation by learning user embedding and the user's personal
view of item embeddings. Specifically, the proposed additive personalization is
to add a personalized item embedding to a sparse global item embedding
aggregated from all users. Moreover, a curriculum learning mechanism has been
applied for additive personalization on item embeddings by gradually increasing
regularization weights to mitigate the performance degradation caused by large
variances among client-specific item embeddings. A unified formulation has been
proposed with a sparse regularization of global item embeddings for reducing
communication overhead. Experimental results on four real-world recommendation
datasets demonstrate the effectiveness of FedRAP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAIN: RegulArization on Input and Network for Black-Box Domain
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.10531v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.10531v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qucheng Peng, Zhengming Ding, Lingjuan Lyu, Lichao Sun, Chen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Source-Free domain adaptation transits the source-trained model towards
target domain without exposing the source data, trying to dispel these concerns
about data privacy and security. However, this paradigm is still at risk of
data leakage due to adversarial attacks on the source model. Hence, the
Black-Box setting only allows to use the outputs of source model, but still
suffers from overfitting on the source domain more severely due to source
model's unseen weights. In this paper, we propose a novel approach named RAIN
(RegulArization on Input and Network) for Black-Box domain adaptation from both
input-level and network-level regularization. For the input-level, we design a
new data augmentation technique as Phase MixUp, which highlights task-relevant
objects in the interpolations, thus enhancing input-level regularization and
class consistency for target models. For network-level, we develop a Subnetwork
Distillation mechanism to transfer knowledge from the target subnetwork to the
full target network via knowledge distillation, which thus alleviates
overfitting on the source domain by learning diverse target representations.
Extensive experiments show that our method achieves state-of-the-art
performance on several cross-domain benchmarks under both single- and
multi-source black-box domain adaptation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Superficial White Matter Analysis: An Efficient Point-cloud-based Deep
  Learning Framework with Supervised Contrastive Learning for Consistent
  Tractography Parcellation across Populations and dMRI Acquisitions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08975v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08975v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tengfei Xue, Fan Zhang, Chaoyi Zhang, Yuqian Chen, Yang Song, Alexandra J. Golby, Nikos Makris, Yogesh Rathi, Weidong Cai, Lauren J. O'Donnell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion MRI tractography is an advanced imaging technique that enables in
vivo mapping of the brain's white matter connections. White matter parcellation
classifies tractography streamlines into clusters or anatomically meaningful
tracts. It enables quantification and visualization of whole-brain
tractography. Currently, most parcellation methods focus on the deep white
matter (DWM), whereas fewer methods address the superficial white matter (SWM)
due to its complexity. We propose a novel two-stage deep-learning-based
framework, Superficial White Matter Analysis (SupWMA), that performs an
efficient and consistent parcellation of 198 SWM clusters from whole-brain
tractography. A point-cloud-based network is adapted to our SWM parcellation
task, and supervised contrastive learning enables more discriminative
representations between plausible streamlines and outliers for SWM. We train
our model on a large-scale tractography dataset including streamline samples
from labeled long- and medium-range (over 40 mm) SWM clusters and anatomically
implausible streamline samples, and we perform testing on six independently
acquired datasets of different ages and health conditions (including neonates
and patients with space-occupying brain tumors). Compared to several
state-of-the-art methods, SupWMA obtains highly consistent and accurate SWM
parcellation results on all datasets, showing good generalization across the
lifespan in health and disease. In addition, the computational speed of SupWMA
is much faster than other methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Medical Image Analysis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tempo: Accelerating <span class="highlight-title">Transformer</span>-Based Model Training through Memory
  Footprint Reduction <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.10246v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.10246v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muralidhar Andoorveedu, Zhanda Zhu, Bojian Zheng, Gennady Pekhimenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training deep learning models can be computationally expensive. Prior works
have shown that increasing the batch size can potentially lead to better
overall throughput. However, the batch size is frequently limited by the
accelerator memory capacity due to the activations/feature maps stored for the
training backward pass, as larger batch sizes require larger feature maps to be
stored. Transformer-based models, which have recently seen a surge in
popularity due to their good performance and applicability to a variety of
tasks, have a similar problem. To remedy this issue, we propose Tempo, a new
approach to efficiently use accelerator (e.g., GPU) memory resources for
training Transformer-based models. Our approach provides drop-in replacements
for the GELU, LayerNorm, and Attention layers, reducing the memory usage and
ultimately leading to more efficient training. We implement Tempo and evaluate
the throughput, memory usage, and accuracy/loss on the BERT Large pre-training
task. We demonstrate that Tempo enables up to 2x higher batch sizes and 16%
higher training throughput over the state-of-the-art baseline. We also evaluate
Tempo on GPT2 and RoBERTa models, showing 19% and 26% speedup over the
baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2022. Fixed some minor typos and added some small
  clarifications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Counter: Stochastic Feature-based Learning for Diverse
  Counterfactual Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.13446v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.13446v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vy Vo, Trung Le, Van Nguyen, He Zhao, Edwin Bonilla, Gholamreza Haffari, Dinh Phung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpretable machine learning seeks to understand the reasoning process of
complex black-box systems that are long notorious for lack of explainability.
One flourishing approach is through counterfactual explanations, which provide
suggestions on what a user can do to alter an outcome. Not only must a
counterfactual example counter the original prediction from the black-box
classifier but it should also satisfy various constraints for practical
applications. Diversity is one of the critical constraints that however remains
less discussed. While diverse counterfactuals are ideal, it is computationally
challenging to simultaneously address some other constraints. Furthermore,
there is a growing privacy concern over the released counterfactual data. To
this end, we propose a feature-based learning framework that effectively
handles the counterfactual constraints and contributes itself to the limited
pool of private explanation models. We demonstrate the flexibility and
effectiveness of our method in generating diverse counterfactuals of
actionability and plausibility. Our counterfactual engine is more efficient
than counterparts of the same capacity while yielding the lowest
re-identification risks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Fair Clustering: A Novel Fairness Attack and Defense Framework <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01953v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01953v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anshuman Chhabra, Peizhao Li, Prasant Mohapatra, Hongfu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clustering algorithms are widely used in many societal resource allocation
applications, such as loan approvals and candidate recruitment, among others,
and hence, biased or unfair model outputs can adversely impact individuals that
rely on these applications. To this end, many fair clustering approaches have
been recently proposed to counteract this issue. Due to the potential for
significant harm, it is essential to ensure that fair clustering algorithms
provide consistently fair outputs even under adversarial influence. However,
fair clustering algorithms have not been studied from an adversarial attack
perspective. In contrast to previous research, we seek to bridge this gap and
conduct a robustness analysis against fair clustering by proposing a novel
black-box fairness attack. Through comprehensive experiments, we find that
state-of-the-art models are highly susceptible to our attack as it can reduce
their fairness performance significantly. Finally, we propose Consensus Fair
Clustering (CFC), the first robust fair clustering approach that transforms
consensus clustering into a fair graph partitioning problem, and iteratively
learns to generate fair cluster outputs. Experimentally, we observe that CFC is
highly robust to the proposed attack and is thus a truly robust fair clustering
alternative.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 11th International Conference on Learning
  Representations (ICLR 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature-based Image Matching for Identifying Individual Kākā 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06678v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06678v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fintan O'Sullivan, Kirita-Rose Escott, Rachael C. Shaw, Andrew Lensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report investigates an unsupervised, feature-based image matching
pipeline for the novel application of identifying individual k\=ak\=a. Applied
with a similarity network for clustering, this addresses a weakness of current
supervised approaches to identifying individual birds which struggle to handle
the introduction of new individuals to the population. Our approach uses object
localisation to locate k\=ak\=a within images and then extracts local features
that are invariant to rotation and scale. These features are matched between
images with nearest neighbour matching techniques and mismatch removal to
produce a similarity score for image match comparison. The results show that
matches obtained via the image matching pipeline achieve high accuracy of true
matches. We conclude that feature-based image matching could be used with a
similarity network to provide a viable alternative to existing supervised
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, honour's report from Victoria University of Wellington</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Agnostic Data-Driven Inverse Text Normalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08506v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08506v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Szu-Jui Chen, Debjyoti Paul, Yutong Pang, Peng Su, Xuedong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of automatic speech recognition (ASR) models, converting
the spoken form text (from ASR) to the written form is in urgent need. This
inverse text normalization (ITN) problem attracts the attention of researchers
from various fields. Recently, several works show that data-driven ITN methods
can output high-quality written form text. Due to the scarcity of labeled
spoken-written datasets, the studies on non-English data-driven ITN are quite
limited. In this work, we propose a language-agnostic data-driven ITN framework
to fill this gap. Specifically, we leverage the data augmentation in
conjunction with neural machine translated data for low resource languages.
Moreover, we design an evaluation method for language agnostic ITN model when
only English data is available. Our empirical evaluation shows this language
agnostic modeling approach is effective for low resource languages while
preserving the performance for high resource languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Effects of Computational Parameter Changes to Image
  Recognition Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.00471v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.00471v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Louloudakis, Perry Gibson, José Cano, Ajitha Rajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image recognition tasks typically use deep learning and require enormous
processing power, thus relying on hardware accelerators like GPUs and FPGAs for
fast, timely processing. Failure in real-time image recognition tasks can occur
due to incorrect mapping on hardware accelerators, which may lead to timing
uncertainty and incorrect behavior. Owing to the increased use of image
recognition tasks in safety-critical applications like autonomous driving and
medical imaging, it is imperative to assess their robustness to changes in the
computational environment as parameters like deep learning frameworks, compiler
optimizations for code generation, and hardware devices are not regulated with
varying impact on model performance and correctness. In this paper we conduct
robustness analysis of four popular image recognition models (MobileNetV2,
ResNet101V2, DenseNet121 and InceptionV3) with the ImageNet dataset, assessing
the impact of the following parameters in the model's computational
environment: (1) deep learning frameworks; (2) compiler optimizations; and (3)
hardware devices. We report sensitivity of model performance in terms of output
label and inference time for changes in each of these environment parameters.
We find that output label predictions for all four models are sensitive to
choice of deep learning framework (by up to 57%) and insensitive to other
parameters. On the other hand, model inference time was affected by all
environment parameters with changes in hardware device having the most effect.
The extent of effect was not uniform across models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Side Eye: Characterizing the Limits of POV Acoustic Eavesdropping from
  Smartphone Cameras with Rolling Shutters and Movable Lenses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Long, Pirouz Naghavi, Blas Kojusner, Kevin Butler, Sara Rampazzi, Kevin Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our research discovers how the rolling shutter and movable lens structures
widely found in smartphone cameras modulate structure-borne sounds onto camera
images, creating a point-of-view (POV) optical-acoustic side channel for
acoustic eavesdropping. The movement of smartphone camera hardware leaks
acoustic information because images unwittingly modulate ambient sound as
imperceptible distortions. Our experiments find that the side channel is
further amplified by intrinsic behaviors of Complementary
metal-oxide-semiconductor (CMOS) rolling shutters and movable lenses such as in
Optical Image Stabilization (OIS) and Auto Focus (AF). Our paper characterizes
the limits of acoustic information leakage caused by structure-borne sound that
perturbs the POV of smartphone cameras. In contrast with traditional
optical-acoustic eavesdropping on vibrating objects, this side channel requires
no line of sight and no object within the camera's field of view (images of a
ceiling suffice). Our experiments test the limits of this side channel with a
novel signal processing pipeline that extracts and recognizes the leaked
acoustic information. Our evaluation with 10 smartphones on a spoken digit
dataset reports 80.66%, 91.28%, and 99.67% accuracies on recognizing 10 spoken
digits, 20 speakers, and 2 genders respectively. We further systematically
discuss the possible defense strategies and implementations. By modeling,
measuring, and demonstrating the limits of acoustic eavesdropping from
smartphone camera image streams, our contributions explain the physics-based
causality and possible ways to reduce the threat on current and future devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LDMIC: Learning-based Distributed Multi-view Image Coding <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinjie Zhang, Jiawei Shao, Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-view image compression plays a critical role in 3D-related
applications. Existing methods adopt a predictive coding architecture, which
requires joint encoding to compress the corresponding disparity as well as
residual information. This demands collaboration among cameras and enforces the
epipolar geometric constraint between different views, which makes it
challenging to deploy these methods in distributed camera systems with randomly
overlapping fields of view. Meanwhile, distributed source coding theory
indicates that efficient data compression of correlated sources can be achieved
by independent encoding and joint decoding, which motivates us to design a
learning-based distributed multi-view image coding (LDMIC) framework. With
independent encoders, LDMIC introduces a simple yet effective joint context
transfer module based on the cross-attention mechanism at the decoder to
effectively capture the global inter-view correlations, which is insensitive to
the geometric relationships between images. Experimental results show that
LDMIC significantly outperforms both traditional and learning-based MIC methods
while enjoying fast encoding speed. Code will be released at
https://github.com/Xinjie-Q/LDMIC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentiable bit-rate estimation for neural-based video codec
  enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Said, Manish Kumar Singh, Reza Pourreza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks (NN) can improve standard video compression by pre- and
post-processing the encoded video. For optimal NN training, the standard codec
needs to be replaced with a codec proxy that can provide derivatives of
estimated bit-rate and distortion, which are used for gradient
back-propagation. Since entropy coding of standard codecs is designed to take
into account non-linear dependencies between transform coefficients, bit-rates
cannot be well approximated with simple per-coefficient estimators. This paper
presents a new approach for bit-rate estimation that is similar to the type
employed in training end-to-end neural codecs, and able to efficiently take
into account those statistical dependencies. It is defined from a mathematical
model that provides closed-form formulas for the estimates and their gradients,
reducing the computational complexity. Experimental results demonstrate the
method's accuracy in estimating HEVC/H.265 codec bit-rates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SONIA: an immersive customizable virtual reality system for the
  education and exploration of brain networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Owen Hellum, Christopher Steele, Yiming Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While mastery of neuroanatomy is important for the investigation of the
brain, there is an increasing interest in exploring the neural pathways to
better understand the roles of neural circuitry in brain functions. To tackle
the limitations of traditional 2D-display-based neuronavigation software in
intuitively visualizing complex 3D anatomies, several virtual reality (VR) and
augmented reality (AR) solutions have been proposed to facilitate
neuroanatomical education. However, with the increasing knowledge on brain
connectivity and the functioning of the sub-systems, there is still a lack of
similar software solutions for the education and exploration of these topics,
which demand more elaborate visualization and interaction strategies. To
address this gap, we designed the immerSive custOmizable Neuro learnIng plAform
(SONIA), a novel user-friendly VR software system with a multi-scale
interaction paradigm that allows flexible customization of learning materials.
With both quantitative and qualitative evaluations through user studies, the
proposed system is shown to have high usability, attractive visual design, and
good educational value. As the first immersive system that integrates
customizable design and detailed narratives of the brain sub-systems for the
education of neuroanatomy and brain connectivity, SONIA showcases new potential
directions and provides valuable insights regarding medical learning and
exploration in VR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08664v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08664v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingting Yuan, Liang Mi, Weijun Wang, Haipeng Dai, Xiaoming Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quality of the video stream is key to neural network-based video
analytics. However, low-quality video is inevitably collected by existing
surveillance systems because of poor quality cameras or over-compressed/pruned
video streaming protocols, e.g., as a result of upstream bandwidth limit. To
address this issue, existing studies use quality enhancers (e.g., neural
super-resolution) to improve the quality of videos (e.g., resolution) and
eventually ensure inference accuracy. Nevertheless, directly applying quality
enhancers does not work in practice because it will introduce unacceptable
latency. In this paper, we present AccDecoder, a novel accelerated decoder for
real-time and neural-enhanced video analytics. AccDecoder can select a few
frames adaptively via Deep Reinforcement Learning (DRL) to enhance the quality
by neural super-resolution and then up-scale the unselected frames that
reference them, which leads to 6-21% accuracy improvement. AccDecoder provides
efficient inference capability via filtering important frames using DRL for
DNN-based inference and reusing the results for the other frames via extracting
the reference relationship among frames and blocks, which results in a latency
reduction of 20-80% than baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2023 IEEE INFOCOM</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-01-23T00:00:00Z">2023-01-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Language Model Training through Cross-Lingual and Progressive
  Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malte Ostendorff, Georg Rehm
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most Transformer language models are primarily pretrained on English text,
limiting their use for other languages. As the model sizes grow, the
performance gap between English and other languages with fewer compute and data
resources increases even further. Consequently, more resource-efficient
training methods are needed to bridge the gap for languages with fewer
resources available. To address this problem, we introduce a cross-lingual and
progressive transfer learning approach, called CLP-Transfer, that transfers
models from a source language, for which pretrained models are publicly
available, like English, to a new target language. As opposed to prior work,
which focused on the cross-lingual transfer between two languages, we extend
the transfer to the model size. Given a pretrained model in a source language,
we aim for a same-sized model in a target language. Instead of training a model
from scratch, we exploit a smaller model that is in the target language but
requires much fewer resources. Both small and source models are then used to
initialize the token embeddings of the larger model based on the overlapping
vocabulary of the source and target language. All remaining weights are reused
from the model in the source language. This approach outperforms the sole
cross-lingual transfer and can save up to 80% of the training steps compared to
the random initialization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Energy Worker Profiler from Technologies to Skills to Realize Energy
  Efficiency in Manufacturing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silvia Fareri, Riccardo Apreda, Valentina Mulas, Ruben Alonso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the manufacturing sector has been responsible for nearly 55
percent of total energy consumption, inducing a major impact on the global
ecosystem. Although stricter regulations, restrictions on heavy manufacturing
and technological advances are increasing its sustainability, zero-emission and
fuel-efficient manufacturing is still considered a utopian target. In
parallel,companies that have invested in digital innovation now need to align
their internal competencies to maximize their return on investment. Moreover, a
primary feature of Industry 4.0 is the digitization of production processes,
which offers the opportunity to optimize energy consumption. However, given the
speed with which innovation manifests itself, tools capable of measuring the
impact that technology is having on digital and green professions and skills
are still being designed. In light of the above, in this article we present the
Worker Profiler, a software designed to map the skills currently possessed by
workers, identifying misalignment with those they should ideally possess to
meet the renewed demands that digital innovation and environmental preservation
impose. The creation of the Worker Profiler consists of two steps: first, the
authors inferred the key technologies and skills for the area of interest,
isolating those with markedly increasing patent trends and identifying green
and digital enabling skills and occupations. Thus, the software was designed
and implemented at the user-interface level. The output of the self-assessment
is the definition of the missing digital and green skills and the job roles
closest to the starting one in terms of current skills; both the results enable
the definition of a customized retraining strategy. The tool has shown evidence
of being user-friendly, effective in identifying skills gaps and easily
adaptable to other contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning Mental Health Dialogue System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lennart Brocki, George C. Dyer, Anna Gładka, Neo Christopher Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mental health counseling remains a major challenge in modern society due to
cost, stigma, fear, and unavailability. We posit that generative artificial
intelligence (AI) models designed for mental health counseling could help
improve outcomes by lowering barriers to access. To this end, we have developed
a deep learning (DL) dialogue system called Serena. The system consists of a
core generative model and post-processing algorithms. The core generative model
is a 2.7 billion parameter Seq2Seq Transformer fine-tuned on thousands of
transcripts of person-centered-therapy (PCT) sessions. The series of
post-processing algorithms detects contradictions, improves coherency, and
removes repetitive answers. Serena is implemented and deployed on
\url{https://serena.chat}, which currently offers limited free services. While
the dialogue system is capable of responding in a qualitatively empathetic and
engaging manner, occasionally it displays hallucination and long-term
incoherence. Overall, we demonstrate that a deep learning mental health
dialogue system has the potential to provide a low-cost and effective
complement to traditional human counselors with less barriers to access.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SMDDH: Singleton Mention detection using Deep Learning in Hindi Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kusum Lata, Pardeep Singh, Kamlesh Dutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mention detection is an important component of coreference resolution system,
where mentions such as name, nominal, and pronominals are identified. These
mentions can be purely coreferential mentions or singleton mentions
(non-coreferential mentions). Coreferential mentions are those mentions in a
text that refer to the same entities in a real world. Whereas, singleton
mentions are mentioned only once in the text and do not participate in the
coreference as they are not mentioned again in the following text. Filtering of
these singleton mentions can substantially improve the performance of a
coreference resolution process. This paper proposes a singleton mention
detection module based on a fully connected network and a Convolutional neural
network for Hindi text. This model utilizes a few hand-crafted features and
context information, and word embedding for words. The coreference annotated
Hindi dataset comprising of 3.6K sentences, and 78K tokens are used for the
task. In terms of Precision, Recall, and F-measure, the experimental findings
obtained are excellent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large-scale fine-grained semantic indexing of biomedical literature
  based on weakly-supervised deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasios Nentidis, Thomas Chatzopoulos, Anastasia Krithara, Grigorios Tsoumakas, Georgios Paliouras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic indexing of biomedical literature is usually done at the level of
MeSH descriptors, representing topics of interest for the biomedical community.
Several related but distinct biomedical concepts are often grouped together in
a single coarse-grained descriptor and are treated as a single topic for
semantic indexing. This study proposes a new method for the automated
refinement of subject annotations at the level of concepts, investigating deep
learning approaches. Lacking labelled data for this task, our method relies on
weak supervision based on concept occurrence in the abstract of an article. The
proposed approach is evaluated on an extended large-scale retrospective
scenario, taking advantage of concepts that eventually become MeSH descriptors,
for which annotations become available in MEDLINE/PubMed. The results suggest
that concept occurrence is a strong heuristic for automated subject annotation
refinement and can be further enhanced when combined with dictionary-based
heuristics. In addition, such heuristics can be useful as weak supervision for
developing deep learning models that can achieve further improvement in some
cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages, 5 figures, 9 tables, 1 algorithm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sensemaking About Contraceptive Methods Across Online Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        LeAnn McDowall, Maria Antoniak, David Mimno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Selecting a birth control method is a complex healthcare decision. While
birth control methods provide important benefits, they can also cause
unpredictable side effects and be stigmatized, leading many people to seek
additional information online, where they can find reviews, advice, hypotheses,
and experiences of other birth control users. However, the relationships
between their healthcare concerns, sensemaking activities, and online settings
are not well understood. We gather texts about birth control shared on Twitter,
Reddit, and WebMD -- platforms with different affordances, moderation, and
audiences -- to study where and how birth control is discussed online. Using a
combination of topic modeling and hand annotation, we identify and characterize
the dominant sensemaking practices across these platforms, and we create
lexicons to draw comparisons across birth control methods and side effects. We
use these to measure variations from survey reports of side effect experiences
and method usage. Our findings characterize how online platforms are used to
make sense of difficult healthcare choices and highlight unmet needs of birth
control users.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StockEmotions: Discover Investor Emotions for Financial Sentiment
  Analysis and Multivariate Time Series <span class="chip">AAAI-23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean Lee, Hoyoul Luis Youn, Josiah Poon, Soyeon Caren Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been growing interest in applying NLP techniques in the financial
domain, however, resources are extremely limited. This paper introduces
StockEmotions, a new dataset for detecting emotions in the stock market that
consists of 10,000 English comments collected from StockTwits, a financial
social media platform. Inspired by behavioral finance, it proposes 12
fine-grained emotion classes that span the roller coaster of investor emotion.
Unlike existing financial sentiment datasets, StockEmotions presents granular
features such as investor sentiment classes, fine-grained emotions, emojis, and
time series data. To demonstrate the usability of the dataset, we perform a
dataset analysis and conduct experimental downstream tasks. For financial
sentiment/emotion classification tasks, DistilBERT outperforms other baselines,
and for multivariate time series forecasting, a Temporal Attention LSTM model
combining price index, text, and emotion features achieves the best performance
than using a single feature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint for the AAAI-23 Bridge Program (AI for Financial Services)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Encoders for Streaming Sequence Tagging <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Kaushal, Aditya Gupta, Shyam Upadhyay, Manaal Faruqui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A naive application of state-of-the-art bidirectional encoders for streaming
sequence tagging would require encoding each token from scratch for each new
token in an incremental streaming input (like transcribed speech). The lack of
re-usability of previous computation leads to a higher number of Floating Point
Operations (or FLOPs) and higher number of unnecessary label flips. Increased
FLOPs consequently lead to higher wall-clock time and increased label flipping
leads to poorer streaming performance. In this work, we present a Hybrid
Encoder with Adaptive Restart (HEAR) that addresses these issues while
maintaining the performance of bidirectional encoders over the offline (or
complete) inputs while improving performance on streaming (or incomplete)
inputs. HEAR has a Hybrid unidirectional-bidirectional encoder architecture to
perform sequence tagging, along with an Adaptive Restart Module (ARM) to
selectively guide the restart of bidirectional portion of the encoder. Across
four sequence tagging tasks, HEAR offers FLOP savings in streaming settings
upto 71.1% and also outperforms bidirectional encoders for streaming
predictions by upto +10% streaming exact match.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic-aware Contrastive Learning for Electroencephalography-to-Text
  Generation with Curriculum Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiachong Feng, Xiaocheng Feng, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electroencephalography-to-Text generation (EEG-to-Text), which aims to
directly generate natural text from EEG signals has drawn increasing attention
in recent years due to the enormous potential for Brain-computer interfaces
(BCIs). However, the remarkable discrepancy between the subject-dependent EEG
representation and the semantic-dependent text representation poses a great
challenge to this task. To mitigate this challenge, we devise a Curriculum
Semantic-aware Contrastive Learning strategy (C-SCL), which effectively
re-calibrates the subject-dependent EEG representation to the
semantic-dependent EEG representation, thus reducing the discrepancy.
Specifically, our C-SCL pulls semantically similar EEG representations together
while pushing apart dissimilar ones. Besides, in order to introduce more
meaningful contrastive pairs, we carefully employ curriculum learning to not
only craft meaningful contrastive pairs but also make the learning
progressively. We conduct extensive experiments on the ZuCo benchmark and our
method combined with diverse models and architectures shows stable improvements
across three types of metrics while achieving the new state-of-the-art. Further
investigation proves not only its superiority in both the single-subject and
low-resource settings but also its robust generalizability in the zero-shot
setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Topic Ontologies for Arguments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yamen Ajjour, Johannes Kiesel, Benno Stein, Martin Potthast
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many computational argumentation tasks, like stance classification, are
topic-dependent: the effectiveness of approaches to these tasks significantly
depends on whether the approaches were trained on arguments from the same
topics as those they are tested on. So, which are these topics that researchers
train approaches on? This paper contributes the first comprehensive survey of
topic coverage, assessing 45 argument corpora. For the assessment, we take the
first step towards building an argument topic ontology, consulting three
diverse authoritative sources: the World Economic Forum, the Wikipedia list of
controversial topics, and Debatepedia. Comparing the topic sets between the
authoritative sources and corpora, our analysis shows that the corpora
topics-which are mostly those frequently discussed in public online fora - are
covered well by the sources. However, other topics from the sources are less
extensively covered by the corpora of today, revealing interesting future
directions for corpus construction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PRIMEQA: The Prime Repository for State-of-the-Art MultilingualQuestion
  Answering Research and Development 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avirup Sil, Jaydeep Sen, Bhavani Iyer, Martin Franz, Kshitij Fadnis, Mihaela Bornea, Sara Rosenthal, Scott McCarley, Rong Zhang, Vishwajeet Kumar, Yulong Li, Md Arafat Sultan, Riyaz Bhat, Radu Florian, Salim Roukos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of Question Answering (QA) has made remarkable progress in recent
years, thanks to the advent of large pre-trained language models, newer
realistic benchmark datasets with leaderboards, and novel algorithms for key
components such as retrievers and readers. In this paper, we introduce PRIMEQA:
a one-stop and open-source QA repository with an aim to democratize QA
re-search and facilitate easy replication of state-of-the-art (SOTA) QA
methods. PRIMEQA supports core QA functionalities like retrieval and reading
comprehension as well as auxiliary capabilities such as question generation.It
has been designed as an end-to-end toolkit for various use cases: building
front-end applications, replicating SOTA methods on pub-lic benchmarks, and
expanding pre-existing methods. PRIMEQA is available at :
https://github.com/primeqa.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Noisy Parallel Data Alignment <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruoyu Xie, Antonios Anastasopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An ongoing challenge in current natural language processing is how its major
advancements tend to disproportionately favor resource-rich languages, leaving
a significant number of under-resourced languages behind. Due to the lack of
resources required to train and evaluate models, most modern language
technologies are either nonexistent or unreliable to process endangered, local,
and non-standardized languages. Optical character recognition (OCR) is often
used to convert endangered language documents into machine-readable data.
However, such OCR output is typically noisy, and most word alignment models are
not built to work under such noisy conditions. In this work, we study the
existing word-level alignment models under noisy settings and aim to make them
more robust to noisy data. Our noise simulation and structural biasing method,
tested on multiple language pairs, manages to reduce the alignment error rate
on a state-of-the-art neural-based alignment model up to 59.6%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in EACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selective Explanations: Leveraging Human Input to Align Explainable AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivian Lai, Yiming Zhang, Chacha Chen, Q. Vera Liao, Chenhao Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While a vast collection of explainable AI (XAI) algorithms have been
developed in recent years, they are often criticized for significant gaps with
how humans produce and consume explanations. As a result, current XAI
techniques are often found to be hard to use and lack effectiveness. In this
work, we attempt to close these gaps by making AI explanations selective -- a
fundamental property of human explanations -- by selectively presenting a
subset from a large set of model reasons based on what aligns with the
recipient's preferences. We propose a general framework for generating
selective explanations by leveraging human input on a small sample. This
framework opens up a rich design space that accounts for different selectivity
goals, types of input, and more. As a showcase, we use a decision-support task
to explore selective explanations based on what the decision-maker would
consider relevant to the decision task. We conducted two experimental studies
to examine three out of a broader possible set of paradigms based on our
proposed framework: in Study 1, we ask the participants to provide their own
input to generate selective explanations, with either open-ended or
critique-based input. In Study 2, we show participants selective explanations
based on input from a panel of similar users (annotators). Our experiments
demonstrate the promise of selective explanations in reducing over-reliance on
AI and improving decision outcomes and subjective perceptions of the AI, but
also paint a nuanced picture that attributes some of these positive effects to
the opportunity to provide one's own input to augment AI explanations. Overall,
our work proposes a novel XAI framework inspired by human communication
behaviors and demonstrates its potentials to encourage future work to better
align AI explanations with human production and consumption of explanations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 25 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lexi: <span class="highlight-title">Self-Supervised</span> Learning of the UI Language <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pratyay Banerjee, Shweti Mahajan, Kushal Arora, Chitta Baral, Oriana Riva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans can learn to operate the user interface (UI) of an application by
reading an instruction manual or how-to guide. Along with text, these resources
include visual content such as UI screenshots and images of application icons
referenced in the text. We explore how to leverage this data to learn generic
visio-linguistic representations of UI screens and their components. These
representations are useful in many real applications, such as accessibility,
voice navigation, and task automation. Prior UI representation models rely on
UI metadata (UI trees and accessibility labels), which is often missing,
incompletely defined, or not accessible. We avoid such a dependency, and
propose Lexi, a pre-trained vision and language model designed to handle the
unique features of UI screens, including their text richness and context
sensitivity. To train Lexi we curate the UICaption dataset consisting of 114k
UI images paired with descriptions of their functionality. We evaluate Lexi on
four tasks: UI action entailment, instruction-based UI image retrieval,
grounding referring expressions, and UI entity recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP (Findings) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Augmenting <span class="highlight-title">Pre-train</span>ed Language Models with QA-Memory for Open-Domain
  Question Answering <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.04581v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.04581v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhu Chen, Pat Verga, Michiel de Jong, John Wieting, William Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval augmented language models have recently become the standard for
knowledge intensive tasks. Rather than relying purely on latent semantics
within the parameters of large neural models, these methods enlist a
semi-parametric memory to encode an index of knowledge for the model to
retrieve over. Most prior work has employed text passages as the unit of
knowledge, which has high coverage at the cost of interpretability,
controllability, and efficiency. The opposite properties arise in other methods
which have instead relied on knowledge base (KB) facts. At the same time, more
recent work has demonstrated the effectiveness of storing and retrieving from
an index of Q-A pairs derived from text \citep{lewis2021paq}. This approach
yields a high coverage knowledge representation that maintains KB-like
properties due to its representations being more atomic units of information.
In this work we push this line of research further by proposing a
question-answer augmented encoder-decoder model and accompanying pretraining
strategy. This yields an end-to-end system that not only outperforms prior QA
retrieval methods on single-hop QA tasks but also enables compositional
reasoning, as demonstrated by strong performance on two multi-hop QA datasets.
Together, these methods improve the ability to interpret and control the model
while narrowing the performance gap with passage retrieval systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models are few(1)-shot Table Reasoners <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.06710v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.06710v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent literature has shown that large language models (LLMs) are generally
excellent few-shot reasoners to solve text reasoning tasks. However, the
capability of LLMs on table reasoning tasks is yet to be explored. In this
paper, we aim at understanding how well LLMs can perform table-related tasks
with few-shot in-context learning. Specifically, we evaluated LLMs on popular
table QA and fact verification datasets like WikiTableQuestion, FetaQA,
TabFact, and FEVEROUS and found that LLMs are competent at complex reasoning
over table structures, though these models are not pre-trained on any table
corpus. When combined with `chain of thoughts' prompting, LLMs can achieve very
strong performance with only a 1-shot demonstration, even on par with some SoTA
models. We show that LLMs are even more competent at generating comprehensive
long-form answers on FetaQA than tuned T5-large. We further manually studied
the reasoning chains elicited from LLMs and found that these reasoning chains
are highly consistent with the underlying semantic form. We believe that LLMs
can serve as a simple yet generic baseline for future research. The code and
data are released in https://github.com/wenhuchen/TableCoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Findings of EACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linear Connectivity Reveals Generalization Strategies <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.12411v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.12411v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeevesh Juneja, Rachit Bansal, Kyunghyun Cho, João Sedoc, Naomi Saphra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is widely accepted in the mode connectivity literature that when two
neural networks are trained similarly on the same data, they are connected by a
path through parameter space over which test set accuracy is maintained. Under
some circumstances, including transfer learning from pretrained models, these
paths are presumed to be linear. In contrast to existing results, we find that
among text classifiers (trained on MNLI, QQP, and CoLA), some pairs of
finetuned models have large barriers of increasing loss on the linear paths
between them. On each task, we find distinct clusters of models which are
linearly connected on the test loss surface, but are disconnected from models
outside the cluster -- models that occupy separate basins on the surface. By
measuring performance on specially-crafted diagnostic datasets, we find that
these clusters correspond to different generalization strategies: one cluster
behaves like a bag of words model under domain shift, while another cluster
uses syntactic heuristics. Our work demonstrates how the geometry of the loss
surface can guide models towards different heuristic functions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Publushed as a conference paper at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demonstrate-Search-Predict: Composing retrieval and language models for
  knowledge-intensive NLP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.14024v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.14024v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, Matei Zaharia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented in-context learning has emerged as a powerful approach
for addressing knowledge-intensive tasks using frozen language models (LM) and
retrieval models (RM). Existing work has combined these in simple
"retrieve-then-read" pipelines in which the RM retrieves passages that are
inserted into the LM prompt. To begin to fully realize the potential of frozen
LMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that
relies on passing natural language texts in sophisticated pipelines between an
LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware
demonstrations, search for relevant passages, and generate grounded
predictions, systematically breaking down problems into small transformations
that the LM and RM can handle more reliably. We have written novel DSP programs
for answering questions in open-domain, multi-hop, and conversational settings,
establishing in early evaluations new state-of-the-art in-context learning
results and delivering 37-120%, 8-39%, and 80-290% relative gains against the
vanilla LM (GPT-3.5), a standard retrieve-then-read pipeline, and a
contemporaneous self-ask pipeline, respectively. We release DSP at
https://github.com/stanfordnlp/dsp
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cascading Biases: Investigating the Effect of Heuristic Annotation
  Strategies on Data and Models <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.13439v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.13439v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaitanya Malaviya, Sudeep Bhatia, Mark Yatskar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cognitive psychologists have documented that humans use cognitive heuristics,
or mental shortcuts, to make quick decisions while expending less effort. While
performing annotation work on crowdsourcing platforms, we hypothesize that such
heuristic use among annotators cascades on to data quality and model
robustness. In this work, we study cognitive heuristic use in the context of
annotating multiple-choice reading comprehension datasets. We propose tracking
annotator heuristic traces, where we tangibly measure low-effort annotation
strategies that could indicate usage of various cognitive heuristics. We find
evidence that annotators might be using multiple such heuristics, based on
correlations with a battery of psychological tests. Importantly, heuristic use
among annotators determines data quality along several dimensions: (1) known
biased models, such as partial input models, more easily solve examples
authored by annotators that rate highly on heuristic use, (2) models trained on
annotators scoring highly on heuristic use don't generalize as well, and (3)
heuristic-seeking annotators tend to create qualitatively less challenging
examples. Our findings suggest that tracking heuristic usage among annotators
can potentially help with collecting challenging datasets and diagnosing model
biases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visualize Before You Write: Imagination-Guided Open-Ended Text
  Generation <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.03765v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.03765v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanrong Zhu, An Yan, Yujie Lu, Wenda Xu, Xin Eric Wang, Miguel Eckstein, William Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-to-image synthesis make it possible to visualize
machine imaginations for a given context. On the other hand, when generating
text, human writers are gifted at creative visualization, which enhances their
writings by forming imaginations as blueprints before putting down the stories
in words. Inspired by such a cognitive process, we ask the natural question of
whether we can endow machines with the same ability to utilize visual
information and construct a general picture of the context to guide text
generation. In this work, we propose iNLG that uses machine-generated images to
guide language models in open-ended text generation. The experiments and
analyses demonstrate the effectiveness of iNLG on open-ended text generation
tasks, including text completion, story generation, and concept-to-text
generation in both few-shot and full-data scenarios. Both automatic metrics and
human evaluations verify that the text snippets generated by our iNLG are
coherent and informative while displaying minor degeneration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Defeat of the Winograd Schema Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.02387v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.02387v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vid Kocijan, Ernest Davis, Thomas Lukasiewicz, Gary Marcus, Leora Morgenstern
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Winograd Schema Challenge - a set of twin sentences involving pronoun
reference disambiguation that seem to require the use of commonsense knowledge
- was proposed by Hector Levesque in 2011. By 2019, a number of AI systems,
based on large pre-trained transformer-based language models and fine-tuned on
these kinds of problems, achieved better than 90% accuracy. In this paper, we
review the history of the Winograd Schema Challenge and discuss the lasting
contributions of the flurry of research that has taken place on the WSC in the
last decade. We discuss the significance of various datasets developed for WSC,
and the research community's deeper understanding of the role of surrogate
tasks in assessing the intelligence of an AI system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReInform: Selecting paths with reinforcement learning for contextualized
  link prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10688v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10688v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marina Speranskaya, Sameh Methias, Benjamin Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose to use reinforcement learning to inform transformer-based
contextualized link prediction models by providing paths that are most useful
for predicting the correct answer. This is in contrast to previous approaches,
that either used reinforcement learning (RL) to directly search for the answer,
or based their prediction on limited or randomly selected context. Our
experiments on WN18RR and FB15k-237 show that contextualized link prediction
models consistently outperform RL-based answer search, and that additional
improvements (of up to 13.5% MRR) can be gained by combining RL with a link
prediction model. The PyTorch implementation of the RL agent is available at
https://github.com/marina-sp/reinform
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Talking About Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.03551v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.03551v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Murray Shanahan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thanks to rapid progress in artificial intelligence, we have entered an era
when technology and philosophy intersect in interesting ways. Sitting squarely
at the centre of this intersection are large language models (LLMs). The more
adept LLMs become at mimicking human language, the more vulnerable we become to
anthropomorphism, to seeing the systems in which they are embedded as more
human-like than they really are. This trend is amplified by the natural
tendency to use philosophically loaded terms, such as "knows", "believes", and
"thinks", when describing these systems. To mitigate this trend, this paper
advocates the practice of repeatedly stepping back to remind ourselves of how
LLMs, and the systems of which they form a part, actually work. The hope is
that increased scientific precision will encourage more philosophical nuance in
the discourse around artificial intelligence, both within the field and in the
public sphere.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geometry-Aware Supertagging with Heterogeneous Dynamic Convolutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12235v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12235v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantinos Kogkalidis, Michael Moortgat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The syntactic categories of categorial grammar formalisms are structured
units made of smaller, indivisible primitives, bound together by the underlying
grammar's category formation rules. In the trending approach of constructive
supertagging, neural models are increasingly made aware of the internal
category structure, which in turn enables them to more reliably predict rare
and out-of-vocabulary categories, with significant implications for grammars
previously deemed too complex to find practical use. In this work, we revisit
constructive supertagging from a graph-theoretic perspective, and propose a
framework based on heterogeneous dynamic graph convolutions aimed at exploiting
the distinctive structure of a supertagger's output space. We test our approach
on a number of categorial grammar datasets spanning different languages and
grammar formalisms, achieving substantial improvements over previous state of
the art scores. Code will be made available at
https://github.com/konstantinosKokos/dynamic-graph-supertagging
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages plus references, unpublished preprint v2: fixed small typos,
  added appendix with a visualization of the decoding process; v3: improved
  presentation, improved the decoding figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Keyword Embeddings for Query Suggestion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08006v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08006v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jorge Gabín, M. Eduardo Ares, Javier Parapar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, search engine users commonly rely on query suggestions to improve
their initial inputs. Current systems are very good at recommending lexical
adaptations or spelling corrections to users' queries. However, they often
struggle to suggest semantically related keywords given a user's query. The
construction of a detailed query is crucial in some tasks, such as legal
retrieval or academic search. In these scenarios, keyword suggestion methods
are critical to guide the user during the query formulation. This paper
proposes two novel models for the keyword suggestion task trained on scientific
literature. Our techniques adapt the architecture of Word2Vec and FastText to
generate keyword embeddings by leveraging documents' keyword co-occurrence.
Along with these models, we also present a specially tailored negative sampling
approach that exploits how keywords appear in academic publications. We devise
a ranking-based evaluation methodology following both known-item and ad-hoc
search scenarios. Finally, we evaluate our proposals against the
state-of-the-art word and sentence embedding models showing considerable
improvements over the baselines for the tasks.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InfiniCity: Infinite-Scale City Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chieh Hubert Lin, Hsin-Ying Lee, Willi Menapace, Menglei Chai, Aliaksandr Siarohin, Ming-Hsuan Yang, Sergey Tulyakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Toward infinite-scale 3D city synthesis, we propose a novel framework,
InfiniCity, which constructs and renders an unconstrainedly large and
3D-grounded environment from random noises. InfiniCity decomposes the seemingly
impractical task into three feasible modules, taking advantage of both 2D and
3D data. First, an infinite-pixel image synthesis module generates
arbitrary-scale 2D maps from the bird's-eye view. Next, an octree-based voxel
completion module lifts the generated 2D map to 3D octrees. Finally, a
voxel-based neural rendering module texturizes the voxels and renders 2D
images. InfiniCity can thus synthesize arbitrary-scale and traversable 3D city
environments, and allow flexible and interactive editing from users. We
quantitatively and qualitatively demonstrate the efficacy of the proposed
framework. Project page: https://hubert0527.github.io/infinicity/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HexPlane: A Fast Representation for Dynamic Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ang Cao, Justin Johnson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling and re-rendering dynamic 3D scenes is a challenging task in 3D
vision. Prior approaches build on NeRF and rely on implicit representations.
This is slow since it requires many MLP evaluations, constraining real-world
applications. We show that dynamic 3D scenes can be explicitly represented by
six planes of learned features, leading to an elegant solution we call
HexPlane. A HexPlane computes features for points in spacetime by fusing
vectors extracted from each plane, which is highly efficient. Pairing a
HexPlane with a tiny MLP to regress output colors and training via volume
rendering gives impressive results for novel view synthesis on dynamic scenes,
matching the image quality of prior work but reducing training time by more
than $100\times$. Extensive ablations confirm our HexPlane design and show that
it is robust to different feature fusion mechanisms, coordinate systems, and
decoding mechanisms. HexPlanes are a simple and effective solution for
representing 4D volumes, and we hope they can broadly contribute to modeling
spacetime for dynamic 3D scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://caoang327.github.io/HexPlane</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LEGO-Net: Learning Regular Rearrangements of Objects in Rooms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuhong Anna Wei, Sijie Ding, Jeong Joon Park, Rahul Sajnani, Adrien Poulenard, Srinath Sridhar, Leonidas Guibas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans universally dislike the task of cleaning up a messy room. If machines
were to help us with this task, they must understand human criteria for regular
arrangements, such as several types of symmetry, co-linearity or
co-circularity, spacing uniformity in linear or circular patterns, and further
inter-object relationships that relate to style and functionality. Previous
approaches for this task relied on human input to explicitly specify goal
state, or synthesized scenes from scratch -- but such methods do not address
the rearrangement of existing messy scenes without providing a goal state. In
this paper, we present LEGO-Net, a data-driven transformer-based iterative
method for learning regular rearrangement of objects in messy rooms. LEGO-Net
is partly inspired by diffusion models -- it starts with an initial messy state
and iteratively "de-noises'' the position and orientation of objects to a
regular state while reducing the distance traveled. Given randomly perturbed
object positions and orientations in an existing dataset of
professionally-arranged scenes, our method is trained to recover a regular
re-arrangement. Results demonstrate that our method is able to reliably
rearrange room scenes and outperform other methods. We additionally propose a
metric for evaluating regularity in room arrangements using number-theoretic
machinery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://ivl.cs.brown.edu/projects/lego-net</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Maximum Mean Discrepancy Kernels for Predictive and Prognostic Modeling
  of Whole Slide Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piotr Keller, Muhammad Dawood, Fayyaz ul Amir Afsar Minhas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How similar are two images? In computational pathology, where Whole Slide
Images (WSIs) of digitally scanned tissue samples from patients can be
multi-gigapixels in size, determination of degree of similarity between two
WSIs is a challenging task with a number of practical applications. In this
work, we explore a novel strategy based on kernelized Maximum Mean Discrepancy
(MMD) analysis for determination of pairwise similarity between WSIs. The
proposed approach works by calculating MMD between two WSIs using kernels over
deep features of image patches. This allows representation of an entire dataset
of WSIs as a kernel matrix for WSI level clustering, weakly-supervised
prediction of TP-53 mutation status in breast cancer patients from their
routine WSIs as well as survival analysis with state of the art prediction
performance. We believe that this work will open up further avenues for
application of WSI-level kernels for predictive and prognostic tasks in
computational pathology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>* Joint first authorship Accepted: IEEE - ISBI 2023 International
  Symposium on Biomedical Imaging</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tracking the industrial growth of modern China with high-resolution
  panchromatic imagery: A sequential convolutional approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Brewer, Zhonghui Lv, Dan Runfola
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to insufficient or difficult to obtain data on development in
inaccessible regions, remote sensing data is an important tool for interested
stakeholders to collect information on economic growth. To date, no studies
have utilized deep learning to estimate industrial growth at the level of
individual sites. In this study, we harness high-resolution panchromatic
imagery to estimate development over time at 419 industrial sites in the
People's Republic of China using a multi-tier computer vision framework. We
present two methods for approximating development: (1) structural area coverage
estimated through a Mask R-CNN segmentation algorithm, and (2) imputing
development directly with visible & infrared radiance from the Visible Infrared
Imaging Radiometer Suite (VIIRS). Labels generated from these methods are
comparatively evaluated and tested. On a dataset of 2,078 50 cm resolution
images spanning 19 years, the results indicate that two dimensions of
industrial development can be estimated using high-resolution daytime imagery,
including (a) the total square meters of industrial development (average error
of 0.021 $\textrm{km}^2$), and (b) the radiance of lights (average error of 9.8
$\mathrm{\frac{nW}{cm^{2}sr}}$). Trend analysis of the techniques reveal
estimates from a Mask R-CNN-labeled CNN-LSTM track ground truth measurements
most closely. The Mask R-CNN estimates positive growth at every site from the
oldest image to the most recent, with an average change of 4,084
$\textrm{m}^2$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fully <span class="highlight-title">transformer</span>-based biomarker prediction from colorectal cancer
  histology: a large-scale multicentric study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sophia J. Wagner, Daniel Reisenbüchler, Nicholas P. West, Jan Moritz Niehues, Gregory Patrick Veldhuizen, Philip Quirke, Heike I. Grabsch, Piet A. van den Brandt, Gordon G. A. Hutchins, Susan D. Richman, Tanwei Yuan, Rupert Langer, Josien Christina Anna Jenniskens, Kelly Offermans, Wolfram Mueller, Richard Gray, Stephen B. Gruber, Joel K. Greenson, Gad Rennert, Joseph D. Bonner, Daniel Schmolze, Jacqueline A. James, Maurice B. Loughrey, Manuel Salto-Tellez, Hermann Brenner, Michael Hoffmeister, Daniel Truhn, Julia A. Schnabel, Melanie Boxberg, Tingying Peng, Jakob Nikolas Kather
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: Deep learning (DL) can extract predictive and prognostic
biomarkers from routine pathology slides in colorectal cancer. For example, a
DL test for the diagnosis of microsatellite instability (MSI) in CRC has been
approved in 2022. Current approaches rely on convolutional neural networks
(CNNs). Transformer networks are outperforming CNNs and are replacing them in
many applications, but have not been used for biomarker prediction in cancer at
a large scale. In addition, most DL approaches have been trained on small
patient cohorts, which limits their clinical utility. Methods: In this study,
we developed a new fully transformer-based pipeline for end-to-end biomarker
prediction from pathology slides. We combine a pre-trained transformer encoder
and a transformer network for patch aggregation, capable of yielding single and
multi-target prediction at patient level. We train our pipeline on over 9,000
patients from 10 colorectal cancer cohorts. Results: A fully transformer-based
approach massively improves the performance, generalizability, data efficiency,
and interpretability as compared with current state-of-the-art algorithms.
After training on a large multicenter cohort, we achieve a sensitivity of 0.97
with a negative predictive value of 0.99 for MSI prediction on surgical
resection specimens. We demonstrate for the first time that resection
specimen-only training reaches clinical-grade performance on endoscopic biopsy
tissue, solving a long-standing diagnostic problem. Interpretation: A fully
transformer-based end-to-end pipeline trained on thousands of pathology slides
yields clinical-grade performance for biomarker prediction on surgical
resections and biopsies. Our new methods are freely available under an open
source license.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adapting the Hypersphere Loss Function from Anomaly Detection to Anomaly
  Segmentation <span class="chip">ICIP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joao P. C. Bertoldo, Santiago Velasco-Forero, Jesus Angulo, Etienne Decencière
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an incremental improvement to Fully Convolutional Data Description
(FCDD), an adaptation of the one-class classification approach from anomaly
detection to image anomaly segmentation (a.k.a. anomaly localization). We
analyze its original loss function and propose a substitute that better
resembles its predecessor, the Hypersphere Classifier (HSC). Both are compared
on the MVTec Anomaly Detection Dataset (MVTec-AD) -- training images are
flawless objects/textures and the goal is to segment unseen defects -- showing
that consistent improvement is achieved by better designing the pixel-wise
supervision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the 2023 IEEE International Conference on Image
  Processing (ICIP 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zorro: the masked multimodal <span class="highlight-title">transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrià Recasens, Jason Lin, Joāo Carreira, Drew Jaegle, Luyu Wang, Jean-baptiste Alayrac, Pauline Luc, Antoine Miech, Lucas Smaira, Ross Hemsley, Andrew Zisserman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention-based models are appealing for multimodal processing because inputs
from multiple modalities can be concatenated and fed to a single backbone
network - thus requiring very little fusion engineering. The resulting
representations are however fully entangled throughout the network, which may
not always be desirable: in learning, contrastive audio-visual self-supervised
learning requires independent audio and visual features to operate, otherwise
learning collapses; in inference, evaluation of audio-visual models should be
possible on benchmarks having just audio or just video. In this paper, we
introduce Zorro, a technique that uses masks to control how inputs from each
modality are routed inside Transformers, keeping some parts of the
representation modality-pure. We apply this technique to three popular
transformer-based architectures (ViT, Swin and HiP) and show that with
contrastive pre-training Zorro achieves state-of-the-art results on most
relevant benchmarks for multimodal tasks (AudioSet and VGGSound). Furthermore,
the resulting models are able to perform unimodal inference on both video and
audio benchmarks such as Kinetics-400 or ESC-50.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to View: Decision <span class="highlight-title">Transformer</span>s for Active Object Detection <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Ding, Nathalie Majcherczyk, Mohit Deshpande, Xuewei Qi, Ding Zhao, Rajasimman Madhivanan, Arnie Sen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active perception describes a broad class of techniques that couple planning
and perception systems to move the robot in a way to give the robot more
information about the environment. In most robotic systems, perception is
typically independent of motion planning. For example, traditional object
detection is passive: it operates only on the images it receives. However, we
have a chance to improve the results if we allow planning to consume detection
signals and move the robot to collect views that maximize the quality of the
results. In this paper, we use reinforcement learning (RL) methods to control
the robot in order to obtain images that maximize the detection quality.
Specifically, we propose using a Decision Transformer with online fine-tuning,
which first optimizes the policy with a pre-collected expert dataset and then
improves the learned policy by exploring better solutions in the environment.
We evaluate the performance of proposed method on an interactive dataset
collected from an indoor scenario simulator. Experimental results demonstrate
that our method outperforms all baselines, including expert policy and pure
offline RL methods. We also provide exhaustive analyses of the reward
distribution and observation space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Presentation Attack Detection for ID Cards on Remote
  Verification Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Gonzalez, Juan Tapia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, an updated two-stage, end-to-end Presentation Attack Detection
method for remote biometric verification systems of ID cards, based on
MobileNetV2, is presented. Several presentation attack species such as printed,
display, composite (based on cropped and spliced areas), plastic (PVC), and
synthetic ID card images using different capture sources are used. This
proposal was developed using a database consisting of 190.000 real case Chilean
ID card images with the support of a third-party company. Also, a new framework
called PyPAD, used to estimate multi-class metrics compliant with the ISO/IEC
30107-3 standard was developed, and will be made available for research
purposes. Our method is trained on two convolutional neural networks
separately, reaching BPCER\textsubscript{100} scores on ID cards attacks of
1.69\% and 2.36\% respectively. The two-stage method using both models together
can reach a BPCER\textsubscript{100} score of 0.92\%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepFEL: Deep Fastfood Ensemble Learning for Histopathology Image
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nima Hatami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational pathology tasks have some unique characterises such as
multi-gigapixel images, tedious and frequently uncertain annotations, and
unavailability of large number of cases [13]. To address some of these issues,
we present Deep Fastfood Ensembles - a simple, fast and yet effective method
for combining deep features pooled from popular CNN models pre-trained on
totally different source domains (e.g., natural image objects) and projected
onto diverse dimensions using random projections, the so-called Fastfood [11].
The final ensemble output is obtained by a consensus of simple individual
classifiers, each of which is trained on a different collection of random basis
vectors. This offers extremely fast and yet effective solution, especially when
training times and domain labels are of the essence. We demonstrate the
effectiveness of the proposed deep fastfood ensemble learning as compared to
the state-of-the-art methods for three different tasks in histopathology image
analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2104.00669</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimising Event-Driven Spiking Neural Network with Regularisation and
  Cutoff 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dengyu Wu, Gaojie Jin, Han Yu, Xinping Yi, Xiaowei Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking neural networks (SNNs), a variant of artificial neural networks
(ANNs) with the benefit of energy efficiency, have achieved the accuracy close
to its ANN counterparts, on benchmark datasets such as CIFAR10/100 and
ImageNet. However, comparing with frame-based input (e.g., images), event-based
inputs from e.g., Dynamic Vision Sensor (DVS) can make a better use of SNNs
thanks to the SNNs' asynchronous working mechanism. In this paper, we
strengthen the marriage between SNNs and event-based inputs with a proposal to
consider anytime optimal inference SNNs, or AOI-SNNs, which can terminate
anytime during the inference to achieve optimal inference result. Two novel
optimisation techniques are presented to achieve AOI-SNNs: a regularisation and
a cutoff. The regularisation enables the training and construction of SNNs with
optimised performance, and the cutoff technique optimises the inference of SNNs
on event-driven inputs. We conduct an extensive set of experiments on multiple
benchmark event-based datasets, including CIFAR10-DVS, N-Caltech101 and DVS128
Gesture. The experimental results demonstrate that our techniques are superior
to the state-of-the-art with respect to the accuracy and latency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale
  Text-to-Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, Timo Aila
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image synthesis has recently seen significant progress thanks to
large pretrained language models, large-scale training data, and the
introduction of scalable model families such as diffusion and autoregressive
models. However, the best-performing models require iterative evaluation to
generate a single sample. In contrast, generative adversarial networks (GANs)
only need a single forward pass. They are thus much faster, but they currently
remain far behind the state-of-the-art in large-scale text-to-image synthesis.
This paper aims to identify the necessary steps to regain competitiveness. Our
proposed model, StyleGAN-T, addresses the specific requirements of large-scale
text-to-image synthesis, such as large capacity, stable training on diverse
datasets, strong text alignment, and controllable variation vs. text alignment
tradeoff. StyleGAN-T significantly improves over previous GANs and outperforms
distilled diffusion models - the previous state-of-the-art in fast
text-to-image synthesis - in terms of sample quality and speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://sites.google.com/view/stylegan-t/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OvarNet: Towards Open-vocabulary Object Attribute Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keyan Chen, Xiaolong Jiang, Yao Hu, Xu Tang, Yan Gao, Jianqi Chen, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider the problem of simultaneously detecting objects
and inferring their visual attributes in an image, even for those with no
manual annotations provided at the training stage, resembling an
open-vocabulary scenario. To achieve this goal, we make the following
contributions: (i) we start with a naive two-stage approach for open-vocabulary
object detection and attribute classification, termed CLIP-Attr. The candidate
objects are first proposed with an offline RPN and later classified for
semantic category and attributes; (ii) we combine all available datasets and
train with a federated strategy to finetune the CLIP model, aligning the visual
representation with attributes, additionally, we investigate the efficacy of
leveraging freely available online image-caption pairs under weakly supervised
learning; (iii) in pursuit of efficiency, we train a Faster-RCNN type model
end-to-end with knowledge distillation, that performs class-agnostic object
proposals and classification on semantic categories and attributes with
classifiers generated from a text encoder; Finally, (iv) we conduct extensive
experiments on VAW, MS-COCO, LSA, and OVAD datasets, and show that recognition
of semantic category and attributes is complementary for visual scene
understanding, i.e., jointly training object detection and attributes
prediction largely outperform existing approaches that treat the two tasks
independently, demonstrating strong generalization ability to novel attributes
and categories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Triplet Contrastive Learning for Unsupervised Vehicle Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Shen, Xiaoyu Du, Liyan Zhang, Jinhui Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Part feature learning is a critical technology for finegrained semantic
understanding in vehicle re-identification. However, recent unsupervised
re-identification works exhibit serious gradient collapse issues when directly
modeling the part features and global features. To address this problem, in
this paper, we propose a novel Triplet Contrastive Learning framework (TCL)
which leverages cluster features to bridge the part features and global
features. Specifically, TCL devises three memory banks to store the features
according to their attributes and proposes a proxy contrastive loss (PCL) to
make contrastive learning between adjacent memory banks, thus presenting the
associations between the part and global features as a transition of the
partcluster and cluster-global associations. Since the cluster memory bank
deals with all the instance features, it can summarize them into a
discriminative feature representation. To deeply exploit the instance
information, TCL proposes two additional loss functions. For the inter-class
instance, a hybrid contrastive loss (HCL) re-defines the sample correlations by
approaching the positive cluster features and leaving the all negative instance
features. For the intra-class instances, a weighted regularization cluster
contrastive loss (WRCCL) refines the pseudo labels by penalizing the mislabeled
images according to the instance similarity. Extensive experiments show that
TCL outperforms many state-of-the-art unsupervised vehicle re-identification
approaches. The code will be available at https://github.com/muzishen/TCL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/muzishen/TCL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contracting Skeletal Kinematic Embeddings for Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Flaborea, Guido Maria D'Amely di Melendugno, Stefano D'arrigo, Marco Aurelio Sterpa, Alessio Sampieri, Fabio Galasso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting the anomaly of human behavior is paramount to timely recognizing
endangering situations, such as street fights or elderly falls. However,
anomaly detection is complex, since anomalous events are rare and because it is
an open set recognition task, i.e., what is anomalous at inference has not been
observed at training. We propose COSKAD, a novel model which encodes skeletal
human motion by an efficient graph convolutional network and learns to COntract
SKeletal kinematic embeddings onto a latent hypersphere of minimum volume for
Anomaly Detection. We propose and analyze three latent space designs for
COSKAD: the commonly-adopted Euclidean, and the new spherical-radial and
hyperbolic volumes. All three variants outperform the state-of-the-art,
including video-based techniques, on the ShangaiTechCampus, the Avenue, and on
the most recent UBnormal dataset, for which we contribute novel skeleton
annotations and the selection of human-related videos. The source code and
dataset will be released upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Patter Recognition Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Study on the identification limits of craniofacial superimposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Óscar Ibáñez, Enrique Bermejo, Andrea Valsecchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Craniofacial Superimposition involves the superimposition of an image of a
skull with a number of ante-mortem face images of an individual and the
analysis of their morphological correspondence. Despite being used for one
century, it is not yet a mature and fully accepted technique due to the absence
of solid scientific approaches, significant reliability studies, and
international standards. In this paper we present a comprehensive
experimentation on the limitations of Craniofacial Superimposition as a
forensic identification technique. The study involves different experiments
over more than 1 Million comparisons performed by a landmark-based automatic
3D/2D superimposition method. The total sample analyzed consists of 320
subjects and 29 craniofacial landmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures. To be submitted to Scientific Reports</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HRVQA: A Visual Question Answering Benchmark for High-Resolution Aerial
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Li, George Vosselman, Michael Ying Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual question answering (VQA) is an important and challenging multimodal
task in computer vision. Recently, a few efforts have been made to bring VQA
task to aerial images, due to its potential real-world applications in disaster
monitoring, urban planning, and digital earth product generation. However, not
only the huge variation in the appearance, scale and orientation of the
concepts in aerial images, but also the scarcity of the well-annotated datasets
restricts the development of VQA in this domain. In this paper, we introduce a
new dataset, HRVQA, which provides collected 53512 aerial images of 1024*1024
pixels and semi-automatically generated 1070240 QA pairs. To benchmark the
understanding capability of VQA models for aerial images, we evaluate the
relevant methods on HRVQA. Moreover, we propose a novel model, GFTransformer,
with gated attention modules and a mutual fusion module. The experiments show
that the proposed dataset is quite challenging, especially the specific
attribute related questions. Our method achieves superior performance in
comparison to the previous state-of-the-art approaches. The dataset and the
source code will be released at https://hrvqa.nl/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple Recipe for Competitive Low-compute Self supervised Vision
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quentin Duval, Ishan Misra, Nicolas Ballas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised methods in vision have been mostly focused on large
architectures as they seem to suffer from a significant performance drop for
smaller architectures. In this paper, we propose a simple self-supervised
distillation technique that can train high performance low-compute neural
networks. Our main insight is that existing joint-embedding based SSL methods
can be repurposed for knowledge distillation from a large self-supervised
teacher to a small student model. Thus, we call our method Replace one Branch
(RoB) as it simply replaces one branch of the joint-embedding training with a
large teacher model. RoB is widely applicable to a number of architectures such
as small ResNets, MobileNets and ViT, and pretrained models such as DINO, SwAV
or iBOT. When pretraining on the ImageNet dataset, RoB yields models that
compete with supervised knowledge distillation. When applied to MSN, RoB
produces students with strong semi-supervised capabilities. Finally, our best
ViT-Tiny models improve over prior SSL state-of-the-art on ImageNet by $2.3\%$
and are on par or better than a supervised distilled DeiT on five downstream
transfer tasks (iNaturalist, CIFAR, Clevr/Count, Clevr/Dist and Places). We
hope RoB enables practical self-supervision at smaller scale.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast and robust single particle reconstruction in 3D fluorescence
  microscopy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thibaut Eloy, Etienne Baudrier, Marine Laporte, Virginie Hamel, Paul Guichard, Denis Fortun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Single particle reconstruction has recently emerged in 3D fluorescence
microscopy as a powerful technique to improve the axial resolution and the
degree of fluorescent labeling. It is based on the reconstruction of an average
volume of a biological particle from the acquisition multiple views with
unknown poses. Current methods are limited either by template bias, restriction
to 2D data, high computational cost or a lack of robustness to low fluorescent
labeling. In this work, we propose a single particle reconstruction method
dedicated to convolutional models in 3D fluorescence microscopy that overcome
these issues. We address the joint reconstruction and estimation of the poses
of the particles, which translates into a challenging non-convex optimization
problem. Our approach is based on a multilevel reformulation of this problem,
and the development of efficient optimization techniques at each level. We
demonstrate on synthetic data that our method outperforms the standard
approaches in terms of resolution and reconstruction error, while achieving a
low computational cost. We also perform successful reconstruction on real
datasets of centrioles to show the potential of our method in concrete
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-domain stain normalization for digital pathology: A
  cycle-consistent adversarial network for whole slide images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin J. Hetz, Tabea-Clara Bucher, Titus J. Brinker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The variation in histologic staining between different medical centers is one
of the most profound challenges in the field of computer-aided diagnosis. The
appearance disparity of pathological whole slide images causes algorithms to
become less reliable, which in turn impedes the wide-spread applicability of
downstream tasks like cancer diagnosis. Furthermore, different stainings lead
to biases in the training which in case of domain shifts negatively affect the
test performance. Therefore, in this paper we propose MultiStain-CycleGAN, a
multi-domain approach to stain normalization based on CycleGAN. Our
modifications to CycleGAN allow us to normalize images of different origins
without retraining or using different models. We perform an extensive
evaluation of our method using various metrics and compare it to commonly used
methods that are multi-domain capable. First, we evaluate how well our method
fools a domain classifier that tries to assign a medical center to an image.
Then, we test our normalization on the tumor classification performance of a
downstream classifier. Furthermore, we evaluate the image quality of the
normalized images using the Structural similarity index and the ability to
reduce the domain shift using the Fr\'echet inception distance. We show that
our method proves to be multi-domain capable, provides the highest image
quality among the compared methods, and can most reliably fool the domain
classifier while keeping the tumor classifier performance high. By reducing the
domain influence, biases in the data can be removed on the one hand and the
origin of the whole slide image can be disguised on the other, thus enhancing
patient data privacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 11 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RainDiffusion:When Unsupervised Learning Meets Diffusion Models for
  Real-world Image Deraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingqiang Wei, Yiyang Shen, Yongzhen Wang, Haoran Xie, Fu Lee Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What will happen when unsupervised learning meets diffusion models for
real-world image deraining? To answer it, we propose RainDiffusion, the first
unsupervised image deraining paradigm based on diffusion models. Beyond the
traditional unsupervised wisdom of image deraining, RainDiffusion introduces
stable training of unpaired real-world data instead of weakly adversarial
training. RainDiffusion consists of two cooperative branches: Non-diffusive
Translation Branch (NTB) and Diffusive Translation Branch (DTB). NTB exploits a
cycle-consistent architecture to bypass the difficulty in unpaired training of
standard diffusion models by generating initial clean/rainy image pairs. DTB
leverages two conditional diffusion modules to progressively refine the desired
output with initial image pairs and diffusive generative prior, to obtain a
better generalization ability of deraining and rain generation. Rain-Diffusion
is a non adversarial training paradigm, serving as a new standard bar for
real-world image deraining. Extensive experiments confirm the superiority of
our RainDiffusion over un/semi-supervised methods and show its competitive
advantages over fully-supervised ones.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Crowd3D: Towards Hundreds of People Reconstruction from a Single Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wen, Jing Huang, Huili Cui, Haozhe Lin, YuKun Lai, Lu Fang, Kun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-based multi-person reconstruction in wide-field large scenes is
critical for crowd analysis and security alert. However, existing methods
cannot deal with large scenes containing hundreds of people, which encounter
the challenges of large number of people, large variations in human scale, and
complex spatial distribution. In this paper, we propose Crowd3D, the first
framework to reconstruct the 3D poses, shapes and locations of hundreds of
people with global consistency from a single large-scene image. The core of our
approach is to convert the problem of complex crowd localization into pixel
localization with the help of our newly defined concept, Human-scene Virtual
Interaction Point (HVIP). To reconstruct the crowd with global consistency, we
propose a progressive reconstruction network based on HVIP by pre-estimating a
scene-level camera and a ground plane. To deal with a large number of persons
and various human sizes, we also design an adaptive human-centric cropping
scheme. Besides, we contribute a benchmark dataset, LargeCrowd, for crowd
reconstruction in a large scene. Experimental results demonstrate the
effectiveness of the proposed method. The code and datasets will be made
public.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages (not including reference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computer Vision for a Camel-Vehicle Collision Mitigation System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khalid Alnujaidi, Ghadah Alhabib
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the population grows and more land is being used for urbanization,
ecosystems are disrupted by our roads and cars. This expansion of
infrastructure cuts through wildlife territories, leading to many instances of
Wildlife-Vehicle Collision (WVC). These instances of WVC are a global issue
that is having a global socio-economic impact, resulting in billions of dollars
in property damage and, at times, fatalities for vehicle occupants. In Saudi
Arabia, this issue is similar, with instances of Camel-Vehicle Collision (CVC)
being particularly deadly due to the large size of camels, which results in a
25% fatality rate [4]. The focus of this work is to test different object
detection models on the task of detecting camels on the road. The Deep Learning
(DL) object detection models used in the experiments are: CenterNet,
EfficientDet, Faster R-CNN, and SSD. Results of the experiments show that
CenterNet performed the best in terms of accuracy and was the most efficient in
training. In the future, the plan is to expand on this work by developing a
system to make countryside roads safer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Employing similarity to highlight differences: On the impact of
  anatomical assumptions in chest X-ray registration methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Astrid Berg, Eva Vandersmissen, Maria Wimmer, David Major, Theresa Neubauer, Dimitrios Lenis, Jeroen Cant, Annemiek Snoeckx, Katja Bühler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To facilitate both the detection and the interpretation of findings in chest
X-rays, comparison with a previous image of the same patient is very valuable
to radiologists. Today, the most common approach for deep learning methods to
automatically inspect chest X-rays disregards the patient history and
classifies only single images as normal or abnormal. Nevertheless, several
methods for assisting in the task of comparison through image registration have
been proposed in the past. However, as we illustrate, they tend to miss
specific types of pathological changes like cardiomegaly and effusion. Due to
assumptions on fixed anatomical structures or their measurements of
registration quality they tend to produce unnaturally deformed warp fields
impacting visualization of the difference image between moving and fixed
images. To overcome these limitations, we are the first to use a new paradigm
based on individual rib pair segmentation for anatomy penalized registration,
which proves a natural way to limit folding of the warp field, especially
beneficial for image pairs with large pathological changes. We show that it is
possible to develop a deep learning powered solution that can visualize what
other methods overlook on a large data set of paired public images, starting
from less than 25 fully labeled and 50 partly labeled training images,
employing sequential instance memory segmentation with hole dropout, weak
labeling, coarse-to-fine refinement and Gaussian mixture model histogram
matching. We statistically evaluate the benefits of our method over the SOTA
and highlight the limits of currently used metrics for registration of chest
X-rays.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning-Based Assessment of Cerebral Microbleeds in COVID-19 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neus Rodeja Ferrer, Malini Vendela Sagar, Kiril Vadimovic Klein, Christina Kruuse, Mads Nielsen, Mostafa Mehdipour Ghazi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cerebral Microbleeds (CMBs), typically captured as hypointensities from
susceptibility-weighted imaging (SWI), are particularly important for the study
of dementia, cerebrovascular disease, and normal aging. Recent studies on
COVID-19 have shown an increase in CMBs of coronavirus cases. Automatic
detection of CMBs is challenging due to the small size and amount of CMBs
making the classes highly imbalanced, lack of publicly available annotated
data, and similarity with CMB mimics such as calcifications, irons, and veins.
Hence, the existing deep learning methods are mostly trained on very limited
research data and fail to generalize to unseen data with high variability and
cannot be used in clinical setups. To this end, we propose an efficient 3D deep
learning framework that is actively trained on multi-domain data. Two public
datasets assigned for normal aging, stroke, and Alzheimer's disease analysis as
well as an in-house dataset for COVID-19 assessment are used to train and
evaluate the models. The obtained results show that the proposed method is
robust to low-resolution images and achieves 78% recall and 80% precision on
the entire test set with an average false positive of 1.6 per scan.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Symposium on Biomedical Imaging (ISBI) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Foundation Models for Earth Monitoring: Generalizable Deep
  Learning Models for Natural Hazard Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Jakubik, Michal Muszynski, Michael Vössing, Niklas Kühl, Thomas Brunschwiler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Climate change results in an increased probability of extreme weather events
that put societies and businesses at risk on a global scale. Therefore, near
real-time mapping of natural hazards is an emerging priority for the support of
natural disaster relief, risk management, and informing governmental policy
decisions. Recent methods to achieve near real-time mapping increasingly
leverage deep learning (DL). However, DL-based approaches are designed for one
specific task in a single geographic region based on specific frequency bands
of satellite data. Therefore, DL models used to map specific natural hazards
struggle with their generalization to other types of natural hazards in unseen
regions. In this work, we propose a methodology to significantly improve the
generalizability of DL natural hazards mappers based on pre-training on a
suitable pre-task. Without access to any data from the target domain, we
demonstrate this improved generalizability across four U-Net architectures for
the segmentation of unseen natural hazards. Importantly, our method is
invariant to geographic differences and differences in the type of frequency
bands of satellite data. By leveraging characteristics of unlabeled images from
the target domain that are publicly available, our approach is able to further
improve the generalization behavior without fine-tuning. Thereby, our approach
supports the development of foundation models for earth monitoring with the
objective of directly segmenting unseen natural hazards across novel geographic
regions given different sources of satellite imagery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-Based Framework for Understanding Car Following Behaviors of Drivers
  in A Naturalistic Driving Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armstrong Aboah, Abdul Rashid Mussah, Yaw Adu-Gyamfi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The most common type of accident on the road is a rear-end crash. These
crashes have a significant negative impact on traffic flow and are frequently
fatal. To gain a more practical understanding of these scenarios, it is
necessary to accurately model car following behaviors that result in rear-end
crashes. Numerous studies have been carried out to model drivers' car-following
behaviors; however, the majority of these studies have relied on simulated
data, which may not accurately represent real-world incidents. Furthermore,
most studies are restricted to modeling the ego vehicle's acceleration, which
is insufficient to explain the behavior of the ego vehicle. As a result, the
current study attempts to address these issues by developing an artificial
intelligence framework for extracting features relevant to understanding driver
behavior in a naturalistic environment. Furthermore, the study modeled the
acceleration of both the ego vehicle and the leading vehicle using extracted
information from NDS videos. According to the study's findings, young people
are more likely to be aggressive drivers than elderly people. In addition, when
modeling the ego vehicle's acceleration, it was discovered that the relative
velocity between the ego vehicle and the leading vehicle was more important
than the distance between the two vehicles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-Supervised</span> Image Representation Learning: Transcending Masking with
  Paired Image Overlay 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinheng Li, Han Ding, Shaofei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning has become a popular approach in recent years for
its ability to learn meaningful representations without the need for data
annotation. This paper proposes a novel image augmentation technique,
overlaying images, which has not been widely applied in self-supervised
learning. This method is designed to provide better guidance for the model to
understand underlying information, resulting in more useful representations.
The proposed method is evaluated using contrastive learning, a widely used
self-supervised learning method that has shown solid performance in downstream
tasks. The results demonstrate the effectiveness of the proposed augmentation
technique in improving the performance of self-supervised models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classification of Luminal Subtypes in Full Mammogram Images Using
  Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adarsh Bhandary Panambur, Prathmesh Madhu, Andreas Maier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic identification of patients with luminal and non-luminal subtypes
during a routine mammography screening can support clinicians in streamlining
breast cancer therapy planning. Recent machine learning techniques have shown
promising results in molecular subtype classification in mammography; however,
they are highly dependent on pixel-level annotations, handcrafted, and radiomic
features. In this work, we provide initial insights into the luminal subtype
classification in full mammogram images trained using only image-level labels.
Transfer learning is applied from a breast abnormality classification task, to
finetune a ResNet-18-based luminal versus non-luminal subtype classification
task. We present and compare our results on the publicly available CMMD dataset
and show that our approach significantly outperforms the baseline classifier by
achieving a mean AUC score of 0.6688 and a mean F1 score of 0.6693 on the test
dataset. The improvement over baseline is statistically significant, with a
p-value of p<0.0001.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE ISBI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PCBDet: An Efficient Deep Neural Network Object Detection Architecture
  for Automatic PCB Component Detection on the Edge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Li, Steven Palayew, Francis Li, Saad Abbasi, Saeejith Nair, Alexander Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There can be numerous electronic components on a given PCB, making the task
of visual inspection to detect defects very time-consuming and prone to error,
especially at scale. There has thus been significant interest in automatic PCB
component detection, particularly leveraging deep learning. However, deep
neural networks typically require high computational resources, possibly
limiting their feasibility in real-world use cases in manufacturing, which
often involve high-volume and high-throughput detection with constrained edge
computing resource availability. As a result of an exploration of efficient
deep neural network architectures for this use case, we introduce PCBDet, an
attention condenser network design that provides state-of-the-art inference
throughput while achieving superior PCB component detection performance
compared to other state-of-the-art efficient architecture designs. Experimental
results show that PCBDet can achieve up to 2$\times$ inference speed-up on an
ARM Cortex A72 processor when compared to an EfficientNet-based design while
achieving $\sim$2-4\% higher mAP on the FICS-PCB benchmark dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FInC Flow: Fast and Invertible $k \times k$ Convolutions for Normalizing
  Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Kallappa, Sandeep Nagar, Girish Varma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Invertible convolutions have been an essential element for building
expressive normalizing flow-based generative models since their introduction in
Glow. Several attempts have been made to design invertible $k \times k$
convolutions that are efficient in training and sampling passes. Though these
attempts have improved the expressivity and sampling efficiency, they severely
lagged behind Glow which used only $1 \times 1$ convolutions in terms of
sampling time. Also, many of the approaches mask a large number of parameters
of the underlying convolution, resulting in lower expressivity on a fixed
run-time budget. We propose a $k \times k$ convolutional layer and Deep
Normalizing Flow architecture which i.) has a fast parallel inversion algorithm
with running time O$(n k^2)$ ($n$ is height and width of the input image and k
is kernel size), ii.) masks the minimal amount of learnable parameters in a
layer. iii.) gives better forward pass and sampling times comparable to other
$k \times k$ convolution-based models on real-world benchmarks. We provide an
implementation of the proposed parallel algorithm for sampling using our
invertible convolutions on GPUs. Benchmarks on CIFAR-10, ImageNet, and CelebA
datasets show comparable performance to previous works regarding bits per
dimension while significantly improving the sampling time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted: VISAPP'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Training Under Limited Resources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahdi Zolnouri, Dounia Lakhmiri, Christophe Tribes, Eyyüb Sari, Sébastien Le Digabel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training time budget and size of the dataset are among the factors affecting
the performance of a Deep Neural Network (DNN). This paper shows that Neural
Architecture Search (NAS), Hyper Parameters Optimization (HPO), and Data
Augmentation help DNNs perform much better while these two factors are limited.
However, searching for an optimal architecture and the best hyperparameter
values besides a good combination of data augmentation techniques under low
resources requires many experiments. We present our approach to achieving such
a goal in three steps: reducing training epoch time by compressing the model
while maintaining the performance compared to the original model, preventing
model overfitting when the dataset is small, and performing the hyperparameter
tuning. We used NOMAD, which is a blackbox optimization software based on a
derivative-free algorithm to do NAS and HPO. Our work achieved an accuracy of
86.0 % on a tiny subset of Mini-ImageNet at the ICLR 2021 Hardware Aware
Efficient Training (HAET) Challenge and won second place in the competition.
The competition results can be found at haet2021.github.io/challenge and our
source code can be found at github.com/DouniaLakhmiri/ICLR\_HAET2021.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time Simultaneous Localization and Mapping with LiDAR intensity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqiang Du, Giovanni Beltrame
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel real-time LiDAR intensity image-based simultaneous
localization and mapping method , which addresses the geometry degeneracy
problem in unstructured environments. Traditional LiDAR-based front-end
odometry mostly relies on geometric features such as points, lines and planes.
A lack of these features in the environment can lead to the failure of the
entire odometry system. To avoid this problem, we extract feature points from
the LiDAR-generated point cloud that match features identified in LiDAR
intensity images. We then use the extracted feature points to perform scan
registration and estimate the robot ego-movement. For the back-end, we jointly
optimize the distance between the corresponding feature points, and the point
to plane distance for planes identified in the map. In addition, we use the
features extracted from intensity images to detect loop closure candidates from
previous scans and perform pose graph optimization. Our experiments show that
our method can run in real time with high accuracy and works well with
illumination changes, low-texture, and unstructured environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combined Use of Federated Learning and Image Encryption for
  Privacy-Preserving Image Classification with Vision <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teru Nagamori, Hitoshi Kiya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, privacy-preserving methods for deep learning have become an
urgent problem. Accordingly, we propose the combined use of federated learning
(FL) and encrypted images for privacy-preserving image classification under the
use of the vision transformer (ViT). The proposed method allows us not only to
train models over multiple participants without directly sharing their raw data
but to also protect the privacy of test (query) images for the first time. In
addition, it can also maintain the same accuracy as normally trained models. In
an experiment, the proposed method was demonstrated to well work without any
performance degradation on the CIFAR-10 and CIFAR-100 datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Linearize Deep Neural Networks for Secure and Efficient
  Private Inference <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Souvik Kundu, Shunlin Lu, Yuke Zhang, Jacqueline Liu, Peter A. Beerel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The large number of ReLU non-linearity operations in existing deep neural
networks makes them ill-suited for latency-efficient private inference (PI).
Existing techniques to reduce ReLU operations often involve manual effort and
sacrifice significant accuracy. In this paper, we first present a novel measure
of non-linearity layers' ReLU sensitivity, enabling mitigation of the
time-consuming manual efforts in identifying the same. Based on this
sensitivity, we then present SENet, a three-stage training method that for a
given ReLU budget, automatically assigns per-layer ReLU counts, decides the
ReLU locations for each layer's activation map, and trains a model with
significantly fewer ReLUs to potentially yield latency and communication
efficient PI. Experimental evaluations with multiple models on various datasets
show SENet's superior performance both in terms of reduced ReLUs and improved
classification accuracy compared to existing alternatives. In particular, SENet
can yield models that require up to ~2x fewer ReLUs while yielding similar
accuracy. For a similar ReLU budget SENet can yield models with ~2.32% improved
classification accuracy, evaluated on CIFAR-100.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 10 figures, 11 tables. Accepted as a conference paper at
  ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CircNet: Meshing 3D Point Clouds with Circumcenter Detection <span class="chip">ICLR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huan Lei, Ruitao Leng, Liang Zheng, Hongdong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing 3D point clouds into triangle meshes is a key problem in
computational geometry and surface reconstruction. Point cloud triangulation
solves this problem by providing edge information to the input points. Since no
vertex interpolation is involved, it is beneficial to preserve sharp details on
the surface. Taking advantage of learning-based techniques in triangulation,
existing methods enumerate the complete combinations of candidate triangles,
which is both complex and inefficient. In this paper, we leverage the duality
between a triangle and its circumcenter, and introduce a deep neural network
that detects the circumcenters to achieve point cloud triangulation.
Specifically, we introduce multiple anchor priors to divide the neighborhood
space of each point. The neural network then learns to predict the presences
and locations of circumcenters under the guidance of those anchors. We extract
the triangles dual to the detected circumcenters to form a primitive mesh, from
which an edge-manifold mesh is produced via simple post-processing. Unlike
existing learning-based triangulation methods, the proposed method bypasses an
exhaustive enumeration of triangle combinations and local surface
parameterization. We validate the efficiency, generalization, and robustness of
our method on prominent datasets of both watertight and open surfaces. The code
and trained models are provided at https://github.com/Ruitao-L/CircNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to ICLR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Active 3D Object Detection from a Generalization Perspective <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yadan Luo, Zhuoxiao Chen, Zijian Wang, Xin Yu, Zi Huang, Mahsa Baktashmotlagh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To alleviate the high annotation cost in LiDAR-based 3D object detection,
active learning is a promising solution that learns to select only a small
portion of unlabeled data to annotate, without compromising model performance.
Our empirical study, however, suggests that mainstream uncertainty-based and
diversity-based active learning policies are not effective when applied in the
3D detection task, as they fail to balance the trade-off between point cloud
informativeness and box-level annotation costs. To overcome this limitation, we
jointly investigate three novel criteria in our framework Crb for point cloud
acquisition - label conciseness}, feature representativeness and geometric
balance, which hierarchically filters out the point clouds of redundant 3D
bounding box labels, latent features and geometric characteristics (e.g., point
cloud density) from the unlabeled sample pool and greedily selects informative
ones with fewer objects to annotate. Our theoretical analysis demonstrates that
the proposed criteria align the marginal distributions of the selected subset
and the prior distributions of the unseen test set, and minimizes the upper
bound of the generalization error. To validate the effectiveness and
applicability of \textsc{Crb}, we conduct extensive experiments on the two
benchmark 3D object detection datasets of KITTI and Waymo and examine both
one-stage (\textit{i.e.}, \textsc{Second}) and two-stage 3D detectors (i.e.,
Pv-rcnn). Experiments evidence that the proposed approach outperforms existing
active learning strategies and achieves fully supervised performance requiring
$1\%$ and $8\%$ annotations of bounding boxes and point clouds, respectively.
Source code: https://github.com/Luoyadan/CRB-active-3Ddet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Minimally Invasive Live Tissue High-fidelity Thermophysical Modeling
  using Real-time Thermography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamza El-Kebir, Junren Ran, Yongseok Lee, Leonardo P. Chamorro, Martin Ostoja-Starzewski, Richard Berlin, Gabriela M. Aguiluz Cornejo, Enrico Benedetti, Pier C. Giulianotti, Joseph Bentsman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel thermodynamic parameter estimation framework for
energy-based surgery on live tissue, with direct applications to tissue
characterization during electrosurgery. This framework addresses the problem of
estimating tissue-specific thermodynamics in real-time, which would enable
accurate prediction of thermal damage impact to the tissue and damage-conscious
planning of electrosurgical procedures. Our approach provides basic
thermodynamic information such as thermal diffusivity, and also allows for
obtaining the thermal relaxation time and a model of the heat source, yielding
in real-time a controlled hyperbolic thermodynamics model. The latter accounts
for the finite thermal propagation time necessary for modeling of the
electrosurgical action, in which the probe motion speed often surpasses the
speed of thermal propagation in the tissue operated on. Our approach relies
solely on thermographer feedback and a knowledge of the power level and
position of the electrosurgical pencil, imposing only very minor adjustments to
normal electrosurgery to obtain a high-fidelity model of the tissue-probe
interaction. Our method is minimally invasive and can be performed in situ. We
apply our method first to simulated data based on porcine muscle tissue to
verify its accuracy and then to in vivo liver tissue, and compare the results
with those from the literature. This comparison shows that parameterizing the
Maxwell--Cattaneo model through the framework proposed yields a noticeably
higher fidelity real-time adaptable representation of the thermodynamic tissue
response to the electrosurgical impact than currently available. A discussion
on the differences between the live and the dead tissue thermodynamics is also
provided.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the IEEE Transactions on Biomedical
  Engineering. Research reported in this publication was supported by the
  National Institute of Biomedical Imaging and Bioengineering of the National
  Institutes of Health under award number R01EB029766</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-tail Detection with Effective Class-Margins <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jang Hyun Cho, Philipp Krähenbühl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale object detection and instance segmentation face a severe data
imbalance. The finer-grained object classes become, the less frequent they
appear in our datasets. However, at test-time, we expect a detector that
performs well for all classes and not just the most frequent ones. In this
paper, we provide a theoretical understanding of the long-trail detection
problem. We show how the commonly used mean average precision evaluation metric
on an unknown test set is bound by a margin-based binary classification error
on a long-tailed object detection training set. We optimize margin-based binary
classification error with a novel surrogate objective called \textbf{Effective
Class-Margin Loss} (ECM). The ECM loss is simple, theoretically well-motivated,
and outperforms other heuristic counterparts on LVIS v1 benchmark over a wide
range of architecture and detectors. Code is available at
\url{https://github.com/janghyuncho/ECM-Loss}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022 Oral. Code is available at
  https://github.com/janghyuncho/ECM-Loss</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Illumination Variation Correction Using Image Synthesis For Unsupervised
  Domain Adaptive Person Re-Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Guo, Amy R. Reibman, Edward J. Delp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised domain adaptive (UDA) person re-identification (re-ID) aims to
learn identity information from labeled images in source domains and apply it
to unlabeled images in a target domain. One major issue with many unsupervised
re-identification methods is that they do not perform well relative to large
domain variations such as illumination, viewpoint, and occlusions. In this
paper, we propose a Synthesis Model Bank (SMB) to deal with illumination
variation in unsupervised person re-ID. The proposed SMB consists of several
convolutional neural networks (CNN) for feature extraction and Mahalanobis
matrices for distance metrics. They are trained using synthetic data with
different illumination conditions such that their synergistic effect makes the
SMB robust against illumination variation. To better quantify the illumination
intensity and improve the quality of synthetic images, we introduce a new 3D
virtual-human dataset for GAN-based image synthesis. From our experiments, the
proposed SMB outperforms other synthesis methods on several re-ID benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Performance of Object Detection using the Mechanisms of Visual
  Recognition in Humans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Ghasemi, Fatemeh Mottaghian, Akram Bayat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object recognition systems are usually trained and evaluated on high
resolution images. However, in real world applications, it is common that the
images have low resolutions or have small sizes. In this study, we first track
the performance of the state-of-the-art deep object recognition network,
Faster- RCNN, as a function of image resolution. The results reveals negative
effects of low resolution images on recognition performance. They also show
that different spatial frequencies convey different information about the
objects in recognition process. It means multi-resolution recognition system
can provides better insight into optimal selection of features that results in
better recognition of objects. This is similar to the mechanisms of the human
visual systems that are able to implement multi-scale representation of a
visual scene simultaneously. Then, we propose a multi-resolution object
recognition framework rather than a single-resolution network. The proposed
framework is evaluated on the PASCAL VOC2007 database. The experimental results
show the performance of our adapted multi-resolution Faster-RCNN framework
outperforms the single-resolution Faster-RCNN on input images with various
resolutions with an increase in the mean Average Precision (mAP) of 9.14%
across all resolutions and 1.2% on the full-spectrum images. Furthermore, the
proposed model yields robustness of the performance over a wide range of
spatial frequencies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GyroFlow+: Gyroscope-Guided Unsupervised Deep Homography and Optical
  Flow Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haipeng Li, Kunming Luo, Bing Zeng, Shuaicheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing homography and optical flow methods are erroneous in challenging
scenes, such as fog, rain, night, and snow because the basic assumptions such
as brightness and gradient constancy are broken. To address this issue, we
present an unsupervised learning approach that fuses gyroscope into homography
and optical flow learning. Specifically, we first convert gyroscope readings
into motion fields named gyro field. Second, we design a self-guided fusion
module (SGF) to fuse the background motion extracted from the gyro field with
the optical flow and guide the network to focus on motion details. Meanwhile,
we propose a homography decoder module (HD) to combine gyro field and
intermediate results of SGF to produce the homography. To the best of our
knowledge, this is the first deep learning framework that fuses gyroscope data
and image content for both deep homography and optical flow learning. To
validate our method, we propose a new dataset that covers regular and
challenging scenes. Experiments show that our method outperforms the
state-of-the-art methods in both regular and challenging scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages. arXiv admin note: substantial text overlap with
  arXiv:2103.13725</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive <span class="highlight-title">pretrain</span>ing for semantic segmentation is robust to noisy
  positive pairs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.13756v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.13756v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Gerard, Josephine Sullivan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain-specific variants of contrastive learning can construct positive pairs
from two distinct in-domain images, while traditional methods just augment the
same image twice. For example, we can form a positive pair from two satellite
images showing the same location at different times. Ideally, this teaches the
model to ignore changes caused by seasons, weather conditions or image
acquisition artifacts. However, unlike in traditional contrastive methods, this
can result in undesired positive pairs, since we form them without human
supervision. For example, a positive pair might consist of one image before a
disaster and one after. This could teach the model to ignore the differences
between intact and damaged buildings, which might be what we want to detect in
the downstream task. Similar to false negative pairs, this could impede model
performance. Crucially, in this setting only parts of the images differ in
relevant ways, while other parts remain similar. Surprisingly, we find that
downstream semantic segmentation is either robust to such badly matched pairs
or even benefits from them. The experiments are conducted on the remote sensing
dataset xBD, and a synthetic segmentation dataset for which we have full
control over the pairing conditions. As a result, practitioners can use these
domain-specific contrastive methods without having to filter their positive
pairs beforehand, or might even be encouraged to purposefully include such
pairs in their pretraining dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible. Compared to the previous version, large-scale changes
  were made to make the paper easier to understand for people less familiar
  with contrastive learning and to make it easier to follow certain arguments.
  10 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SoundSpaces 2.0: A Simulation Platform for Visual-Acoustic Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08312v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08312v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changan Chen, Carl Schissler, Sanchit Garg, Philip Kobernik, Alexander Clegg, Paul Calamia, Dhruv Batra, Philip W Robinson, Kristen Grauman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SoundSpaces 2.0, a platform for on-the-fly geometry-based audio
rendering for 3D environments. Given a 3D mesh of a real-world environment,
SoundSpaces can generate highly realistic acoustics for arbitrary sounds
captured from arbitrary microphone locations. Together with existing 3D visual
assets, it supports an array of audio-visual research tasks, such as
audio-visual navigation, mapping, source localization and separation, and
acoustic matching. Compared to existing resources, SoundSpaces 2.0 has the
advantages of allowing continuous spatial sampling, generalization to novel
environments, and configurable microphone and material properties. To our
knowledge, this is the first geometry-based acoustic simulation that offers
high fidelity and realism while also being fast enough to use for embodied
learning. We showcase the simulator's properties and benchmark its performance
against real-world audio measurements. In addition, we demonstrate two
downstream tasks -- embodied navigation and far-field automatic speech
recognition -- and highlight sim2real performance for the latter. SoundSpaces
2.0 is publicly available to facilitate wider research for perceptual systems
that can both see and hear.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version. Website: https://soundspaces.org. Project page:
  https://vision.cs.utexas.edu/projects/soundspaces2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Novel-View Acoustic Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08730v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08730v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changan Chen, Alexander Richard, Roman Shapovalov, Vamsi Krishna Ithapu, Natalia Neverova, Kristen Grauman, Andrea Vedaldi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the novel-view acoustic synthesis (NVAS) task: given the sight
and sound observed at a source viewpoint, can we synthesize the sound of that
scene from an unseen target viewpoint? We propose a neural rendering approach:
Visually-Guided Acoustic Synthesis (ViGAS) network that learns to synthesize
the sound of an arbitrary point in space by analyzing the input audio-visual
cues. To benchmark this task, we collect two first-of-their-kind large-scale
multi-view audio-visual datasets, one synthetic and one real. We show that our
model successfully reasons about the spatial cues and synthesizes faithful
audio on both datasets. To our knowledge, this work represents the very first
formulation, dataset, and approach to solve the novel-view acoustic synthesis
task, which has exciting potential applications ranging from AR/VR to art and
design. Unlocked by this work, we believe that the future of novel-view
synthesis is in multi-modal learning from videos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://vision.cs.utexas.edu/projects/nvas</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Gradient Inversion Attacks Make Federated Learning Unsafe? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.06924v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.06924v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Hatamizadeh, Hongxu Yin, Pavlo Molchanov, Andriy Myronenko, Wenqi Li, Prerna Dogra, Andrew Feng, Mona G. Flores, Jan Kautz, Daguang Xu, Holger R. Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) allows the collaborative training of AI models
without needing to share raw data. This capability makes it especially
interesting for healthcare applications where patient and data privacy is of
utmost concern. However, recent works on the inversion of deep neural networks
from model gradients raised concerns about the security of FL in preventing the
leakage of training data. In this work, we show that these attacks presented in
the literature are impractical in FL use-cases where the clients' training
involves updating the Batch Normalization (BN) statistics and provide a new
baseline attack that works for such scenarios. Furthermore, we present new ways
to measure and visualize potential data leakage in FL. Our work is a step
towards establishing reproducible methods of measuring data leakage in FL and
could help determine the optimal tradeoffs between privacy-preserving
techniques, such as differential privacy, and model accuracy based on
quantifiable metrics.
  Code is available at
https://nvidia.github.io/NVFlare/research/quantifying-data-leakage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Revised version; Accepted to IEEE Transactions on Medical Imaging;
  Improved and reformatted version of
  https://www.researchsquare.com/article/rs-1147182/v2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainable Image Quality Assessments in Teledermatological Photography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.04699v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.04699v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raluca Jalaboi, Ole Winther, Alfiia Galimzianova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image quality is a crucial factor in the effectiveness and efficiency of
teledermatological consultations. However, up to 50% of images sent by patients
have quality issues, thus increasing the time to diagnosis and treatment. An
automated, easily deployable, explainable method for assessing image quality is
necessary to improve the current teledermatological consultation flow. We
introduce ImageQX, a convolutional neural network for image quality assessment
with a learning mechanism for identifying the most common poor image quality
explanations: bad framing, bad lighting, blur, low resolution, and distance
issues. ImageQX was trained on 26,635 photographs and validated on 9,874
photographs, each annotated with image quality labels and poor image quality
explanations by up to 12 board-certified dermatologists. The photographic
images were taken between 2017 and 2019 using a mobile skin disease tracking
application accessible worldwide. Our method achieves expert-level performance
for both image quality assessment and poor image quality explanation. For image
quality assessment, ImageQX obtains a macro F1-score of 0.73 +- 0.01, which
places it within standard deviation of the pairwise inter-rater F1-score of
0.77 +- 0.07. For poor image quality explanations, our method obtains F1-scores
of between 0.37 +- 0.01 and 0.70 +- 0.01, similar to the inter-rater pairwise
F1-score of between 0.24 +- 0.15 and 0.83 +- 0.06. Moreover, with a size of
only 15 MB, ImageQX is easily deployable on mobile devices. With an image
quality detection performance similar to that of dermatologists, incorporating
ImageQX into the teledermatology flow can enable a better, faster flow for
remote consultations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Telemedicine and eHealth Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text Detection Forgot About Document OCR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07903v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07903v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krzysztof Olejniczak, Milan Šulc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detection and recognition of text from scans and other images, commonly
denoted as Optical Character Recognition (OCR), is a widely used form of
automated document processing with a number of methods available. Yet OCR
systems still do not achieve 100% accuracy, requiring human corrections in
applications where correct readout is essential. Advances in machine learning
enabled even more challenging scenarios of text detection and recognition
"in-the-wild" - such as detecting text on objects from photographs of complex
scenes. While the state-of-the-art methods for in-the-wild text recognition are
typically evaluated on complex scenes, their performance in the domain of
documents is typically not published, and a comprehensive comparison with
methods for document OCR is missing. This paper compares several methods
designed for in-the-wild text recognition and for document text recognition,
and provides their evaluation on the domain of structured documents. The
results suggest that state-of-the-art methods originally proposed for
in-the-wild text detection also achieve competitive results on document text
detection, outperforming available OCR methods. We argue that the application
of document OCR should not be omitted in evaluation of text detection and
recognition methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 26th Computer Vision Winter Workshop (CVWW), 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data standardization for robust lip sync 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.06198v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.06198v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lip sync is a fundamental audio-visual task. However, existing lip sync
methods fall short of being robust to the incredible diversity of videos taken
in the wild, and the majority of the diversity is caused by compound
distracting factors that could degrade existing lip sync methods. To address
these issues, this paper proposes a data standardization pipeline that can
produce standardized expressive images while preserving lip motion information
from the input and reducing the effects of compound distracting factors. Based
on recent advances in 3D face reconstruction, we first create a model that can
consistently disentangle expressions, with lip motion information embedded.
Then, to reduce the effects of compound distracting factors on synthesized
images, we synthesize images with only expressions from the input,
intentionally setting all other attributes at predefined values independent of
the input. Using synthesized images, existing lip sync methods improve their
data efficiency and robustness, and they achieve competitive performance for
the active speaker detection task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Concept-level Debugging of Part-Prototype Networks <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.15769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.15769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Bontempelli, Stefano Teso, Katya Tentori, Fausto Giunchiglia, Andrea Passerini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Part-prototype Networks (ProtoPNets) are concept-based classifiers designed
to achieve the same performance as black-box models without compromising
transparency. ProtoPNets compute predictions based on similarity to
class-specific part-prototypes learned to recognize parts of training examples,
making it easy to faithfully determine what examples are responsible for any
target prediction and why. However, like other models, they are prone to
picking up confounders and shortcuts from the data, thus suffering from
compromised prediction accuracy and limited generalization. We propose
ProtoPDebug, an effective concept-level debugger for ProtoPNets in which a
human supervisor, guided by the model's explanations, supplies feedback in the
form of what part-prototypes must be forgotten or kept, and the model is
fine-tuned to align with this supervision. Our experimental evaluation shows
that ProtoPDebug outperforms state-of-the-art debuggers for a fraction of the
annotation cost. An online experiment with laypeople confirms the simplicity of
the feedback requested to the users and the effectiveness of the collected
feedback for learning confounder-free part-prototypes. ProtoPDebug is a
promising tool for trustworthy interactive learning in critical applications,
as suggested by a preliminary evaluation on a medical decision making task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Side-Tuning for Document Classification <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07502v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07502v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Pio Zingaro, Giuseppe Lisanti, Maurizio Gabbrielli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose to exploit the side-tuning framework for multimodal
document classification. Side-tuning is a methodology for network adaptation
recently introduced to solve some of the problems related to previous
approaches. Thanks to this technique it is actually possible to overcome model
rigidity and catastrophic forgetting of transfer learning by fine-tuning. The
proposed solution uses off-the-shelf deep learning architectures leveraging the
side-tuning framework to combine a base model with a tandem of two side
networks. We show that side-tuning can be successfully employed also when
different data sources are considered, e.g. text and images in document
classification. The experimental results show that this approach pushes further
the limit for document classification accuracy with respect to the state of the
art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2020 25th International Conference on Pattern Recognition (ICPR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interactive Object Segmentation in 3D Point Clouds <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.07183v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.07183v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theodora Kontogianni, Ekin Celikkan, Siyu Tang, Konrad Schindler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an interactive approach for 3D instance segmentation, where users
can iteratively collaborate with a deep learning model to segment objects in a
3D point cloud directly. Current methods for 3D instance segmentation are
generally trained in a fully-supervised fashion, which requires large amounts
of costly training labels, and does not generalize well to classes unseen
during training. Few works have attempted to obtain 3D segmentation masks using
human interactions. Existing methods rely on user feedback in the 2D image
domain. As a consequence, users are required to constantly switch between 2D
images and 3D representations, and custom architectures are employed to combine
multiple input modalities. Therefore, integration with existing standard 3D
models is not straightforward. The core idea of this work is to enable users to
interact directly with 3D point clouds by clicking on desired 3D objects of
interest~(or their background) to interactively segment the scene in an
open-world setting. Specifically, our method does not require training data
from any target domain, and can adapt to new environments where no appropriate
training sets are available. Our system continuously adjusts the object
segmentation based on the user feedback and achieves accurate dense 3D
segmentation masks with minimal human effort (few clicks per object). Besides
its potential for efficient labeling of large-scale and varied 3D datasets, our
approach, where the user directly interacts with the 3D environment, enables
new applications in AR/VR and human-robot interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2023 IEEE International Conference on Robotics and Automation (ICRA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthesis of Compositional Animations from Textual Descriptions <span class="chip">ICCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.14675v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.14675v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anindita Ghosh, Noshaba Cheema, Cennet Oguz, Christian Theobalt, Philipp Slusallek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  "How can we animate 3D-characters from a movie script or move robots by
simply telling them what we would like them to do?" "How unstructured and
complex can we make a sentence and still generate plausible movements from it?"
These are questions that need to be answered in the long-run, as the field is
still in its infancy. Inspired by these problems, we present a new technique
for generating compositional actions, which handles complex input sentences.
Our output is a 3D pose sequence depicting the actions in the input sentence.
We propose a hierarchical two-stream sequential model to explore a finer
joint-level mapping between natural language sentences and 3D pose sequences
corresponding to the given motion. We learn two manifold representations of the
motion -- one each for the upper body and the lower body movements. Our model
can generate plausible pose sequences for short sentences describing single
actions as well as long compositional sentences describing multiple sequential
and superimposed actions. We evaluate our proposed model on the publicly
available KIT Motion-Language Dataset containing 3D pose data with
human-annotated sentences. Experimental results show that our model advances
the state-of-the-art on text-based motion synthesis in objective evaluations by
a margin of 50%. Qualitative evaluations based on a user study indicate that
our synthesized motions are perceived to be the closest to the ground-truth
motion captures for both short and compositional sentences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures, 3 tables. Proceedings of the IEEE/CVF
  International Conference on Computer Vision (ICCV), 2021, pp. 1396-1406</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ mvHOTA: A multi-view higher order tracking accuracy metric to measure
  spatial and temporal associations in multi-point detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.09372v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.09372v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lalith Sharan, Halvar Kelm, Gabriele Romano, Matthias Karck, Raffaele De Simone, Sandy Engelhardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-point tracking is a challenging task that involves detecting points in
the scene and tracking them across a sequence of frames. Computing
detection-based measures like the F-measure on a frame-by-frame basis is not
sufficient to assess the overall performance, as it does not interpret
performance in the temporal domain. The main evaluation metric available comes
from Multi-object tracking (MOT) methods to benchmark performance on datasets
such as KITTI with the recently proposed higher order tracking accuracy (HOTA)
metric, which is capable of providing a better description of the performance
over metrics such as MOTA, DetA, and IDF1. While the HOTA metric takes into
account temporal associations, it does not provide a tailored means to analyse
the spatial associations of a dataset in a multi-camera setup. Moreover, there
are differences in evaluating the detection task for points when compared to
objects (point distances vs. bounding box overlap). Therefore in this work, we
propose a multi-view higher order tracking metric (mvHOTA) to determine the
accuracy of multi-point (multi-instance and multi-class) tracking methods,
while taking into account temporal and spatial associations.mvHOTA can be
interpreted as the geometric mean of detection, temporal, and spatial
associations, thereby providing equal weighting to each of the factors. We
demonstrate the use of this metric to evaluate the tracking performance on an
endoscopic point detection dataset from a previously organised surgical data
science challenge. Furthermore, we compare with other adjusted MOT metrics for
this use-case, discuss the properties of mvHOTA, and show how the proposed
multi-view Association and the Occlusion index (OI) facilitate analysis of
methods with respect to handling of occlusions. The code is available at
https://github.com/Cardio-AI/mvhota.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Image Compression with a Diffusion-Based Decoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.05489v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.05489v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noor Fathima Ghouse, Jens Petersen, Auke Wiggers, Tianlin Xu, Guillaume Sautière
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion probabilistic models have recently achieved remarkable success in
generating high quality image and video data. In this work, we build on this
class of generative models and introduce a method for lossy compression of high
resolution images. The resulting codec, which we call DIffuson-based Residual
Augmentation Codec (DIRAC),is the first neural codec to allow smooth traversal
of the rate-distortion-perception tradeoff at test time, while obtaining
competitive performance with GAN-based methods in perceptual quality.
Furthermore, while sampling from diffusion probabilistic models is notoriously
expensive, we show that in the compression setting the number of steps can be
drastically reduced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v1: 26 pages, 13 figures v2: corrected typo in first author name in
  arxiv metadata</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking data-driven point spread function modeling with a
  differentiable optical model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.04908v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.04908v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Liaudat, Jean-Luc Starck, Martin Kilbinger, Pierre-Antoine Frugier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In astronomy, upcoming space telescopes with wide-field optical instruments
have a spatially varying point spread function (PSF). Specific scientific goals
require a high-fidelity estimation of the PSF at target positions where no
direct measurement of the PSF is provided. Even though observations of the PSF
are available at some positions of the field of view (FOV), they are
undersampled, noisy, and integrated into wavelength in the instrument's
passband. PSF modeling represents a challenging ill-posed problem, as it
requires building a model from degraded observations that can infer a
super-resolved PSF at any wavelength and position in the FOV. Our model, coined
WaveDiff, proposes a paradigm shift in the data-driven modeling of the point
spread function field of telescopes. We change the data-driven modeling space
from the pixels to the wavefront by adding a differentiable optical forward
model into the modeling framework. This change allows the transfer of
complexity from the instrumental response into the forward model. The proposed
model relies on stochastic gradient descent to estimate its parameters. Our
framework paves the way to building powerful, physically motivated models that
do not require special calibration data. This paper demonstrates the WaveDiff
model in a simplified setting of a space telescope. The proposed framework
represents a performance breakthrough with respect to the existing
state-of-the-art data-driven approach. The pixel reconstruction errors decrease
6-fold at observation resolution and 44-fold for a 3x super-resolution. The
ellipticity errors are reduced at least 20 times, and the size error is reduced
more than 250 times. By only using noisy broad-band in-focus observations, we
successfully capture the PSF chromatic variations due to diffraction. Code
available at https://github.com/tobias-liaudat/wf-psf.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted. Without appendix: 42 pages, 10 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Colorization of Structured Mobile Web Pages <span class="chip">WACV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.11541v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.11541v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kotaro Kikuchi, Naoto Inoue, Mayu Otani, Edgar Simo-Serra, Kota Yamaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Color is a critical design factor for web pages, affecting important factors
such as viewer emotions and the overall trust and satisfaction of a website.
Effective coloring requires design knowledge and expertise, but if this process
could be automated through data-driven modeling, efficient exploration and
alternative workflows would be possible. However, this direction remains
underexplored due to the lack of a formalization of the web page colorization
problem, datasets, and evaluation protocols. In this work, we propose a new
dataset consisting of e-commerce mobile web pages in a tractable format, which
are created by simplifying the pages and extracting canonical color styles with
a common web browser. The web page colorization problem is then formalized as a
task of estimating plausible color styles for a given web page content with a
given hierarchical structure of the elements. We present several
Transformer-based methods that are adapted to this task by prepending
structural message passing to capture hierarchical relationships between
elements. Experimental results, including a quantitative evaluation designed
for this task, demonstrate the advantages of our methods over statistical and
image colorization methods. The code is available at
https://github.com/CyberAgentAILab/webcolor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WACV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Dataset</span> Structural Index: Leveraging a machine's perspective towards
  visual data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.04070v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.04070v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dishant Parikh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With advances in vision and perception architectures, we have realized that
working with data is equally crucial, if not more, than the algorithms. Till
today, we have trained machines based on our knowledge and perspective of the
world. The entire concept of Dataset Structural Index(DSI) revolves around
understanding a machine`s perspective of the dataset. With DSI, I show two meta
values with which we can get more information over a visual dataset and use it
to optimize data, create better architectures, and have an ability to guess
which model would work best. These two values are the Variety contribution
ratio and Similarity matrix. In the paper, I show many applications of DSI, one
of which is how the same level of accuracy can be achieved with the same model
architectures trained over less amount of data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Facial Thermal and Blood Perfusion Patterns of Human Emotions:
  Proof-of-Concept 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07650v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07650v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor H. Aristizabal-Tique, Marcela Henao-Perez, Diana Carolina Lopez-Medina, Renato Zambrano-Cruz, Gloria Diaz-Londoñod
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, a preliminary study of proof-of-concept was conducted to
evaluate the performance of the thermographic and blood perfusion data when
emotions of positive and negative valence are applied, where the blood
perfusion data are obtained from the thermographic data. The images were
obtained for baseline, positive, and negative valence according to the protocol
of the Geneva Affective Picture Database. Absolute and percentage differences
of average values of the data between the valences and the baseline were
calculated for different regions of interest (forehead, periorbital eyes,
cheeks, nose and upper lips). For negative valence, a decrease in temperature
and blood perfusion was observed in the regions of interest, and the effect was
greater on the left side than on the right side. In positive valence, the
temperature and blood perfusion increased in some cases, showing a complex
pattern. The temperature and perfusion of the nose was reduced for both
valences, which is indicative of the arousal dimension. The blood perfusion
images were found to be greater contrast; the percentage differences in the
blood perfusion images are greater than those obtained in thermographic images.
Moreover, the blood perfusion images, and vasomotor answer are consistent,
therefore, they can be a better biomarker than thermographic analysis in
identifying emotions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Retrospective k-space Subsampling schemes For Deep MRI Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08365v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08365v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Yiasemis, Clara I. Sánchez, Jan-Jakob Sonke, Jonas Teuwen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  $\textbf{Purpose:}$ The MRI $k$-space acquisition is time consuming.
Traditional techniques aim to acquire accelerated data, which in conjunction
with recent DL methods, aid in producing high-fidelity images in truncated
times. Conventionally, subsampling the $k$-space is performed by utilizing
Cartesian-rectilinear trajectories, which even with the use of DL, provide
imprecise reconstructions, though, a plethora of non-rectilinear or
non-Cartesian trajectories can be implemented in modern MRI scanners. This work
investigates the effect of the $k$-space subsampling scheme on the quality of
reconstructed accelerated MRI measurements produced by trained DL models.
  $\textbf{Methods:}$ The RecurrentVarNet was used as the DL-based
MRI-reconstruction architecture. Cartesian fully-sampled multi-coil $k$-space
measurements from three datasets with different accelerations were
retrospectively subsampled using eight distinct subsampling schemes (four
Cartesian-rectilinear, two Cartesian non-rectilinear, two non-Cartesian).
Experiments were conducted in two frameworks: Scheme-specific, where a distinct
model was trained and evaluated for each dataset-subsampling scheme pair, and
multi-scheme, where for each dataset a single model was trained on data
randomly subsampled by any of the eight schemes and evaluated on data
subsampled by all schemes.
  $\textbf{Results:}$ In the scheme-specific setting RecurrentVarNets trained
and evaluated on non-rectilinearly subsampled data demonstrated superior
performance especially for high accelerations, whilst in the multi-scheme
setting, reconstruction performance on rectilinearly subsampled data improved
when compared to the scheme-specific experiments.
  $\textbf{Conclusion:}$ Training DL-based MRI reconstruction algorithms on
non-rectilinearly subsampled measurements can produce more faithful
reconstructions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 13 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Provably Convergent Plug & Play Linearized ADMM, applied to Deblurring
  Spatially Varying Kernels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.10605v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.10605v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles Laroche, Andrés Almansa, Eva Coupeté, Matias Tassano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Plug & Play methods combine proximal algorithms with denoiser priors to solve
inverse problems. These methods rely on the computability of the proximal
operator of the data fidelity term. In this paper, we propose a Plug & Play
framework based on linearized ADMM that allows us to bypass the computation of
intractable proximal operators. We demonstrate the convergence of the algorithm
and provide results on restoration tasks such as super-resolution and
deblurring with non-uniform blur.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intelligent Painter: Picture Composition With Resampling Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.17106v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.17106v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wing-Fung Ku, Wan-Chi Siu, Xi Cheng, H. Anthony Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Have you ever thought that you can be an intelligent painter? This means that
you can paint a picture with a few expected objects in mind, or with a
desirable scene. This is different from normal inpainting approaches for which
the location of specific objects cannot be determined. In this paper, we
present an intelligent painter that generate a person's imaginary scene in one
go, given explicit hints. We propose a resampling strategy for Denoising
Diffusion Probabilistic Model (DDPM) to intelligently compose unconditional
harmonized pictures according to the input subjects at specific locations. By
exploiting the diffusion property, we resample efficiently to produce realistic
pictures. Experimental results show that our resampling method favors the
semantic meaning of the generated output efficiently and generates less blurry
output. Quantitative analysis of image quality assessment shows that our method
produces higher perceptual quality images compared with the state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Source-Free Progressive Graph Learning for Open-Set Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.06174v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.06174v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yadan Luo, Zijian Wang, Zhuoxiao Chen, Zi Huang, Mahsa Baktashmotlagh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-set domain adaptation (OSDA) has gained considerable attention in many
visual recognition tasks. However, most existing OSDA approaches are limited
due to three main reasons, including: (1) the lack of essential theoretical
analysis of generalization bound, (2) the reliance on the coexistence of source
and target data during adaptation, and (3) failing to accurately estimate the
uncertainty of model predictions. We propose a Progressive Graph Learning (PGL)
framework that decomposes the target hypothesis space into the shared and
unknown subspaces, and then progressively pseudo-labels the most confident
known samples from the target domain for hypothesis adaptation. Moreover, we
tackle a more realistic source-free open-set domain adaptation (SF-OSDA)
setting that makes no assumption about the coexistence of source and target
domains, and introduce a balanced pseudo-labeling (BP-L) strategy in a
two-stage framework, namely SF-PGL. Different from PGL that applies a
class-agnostic constant threshold for all target samples for pseudo-labeling,
the SF-PGL model uniformly selects the most confident target instances from
each category at a fixed ratio. The confidence thresholds in each class are
regarded as the 'uncertainty' of learning the semantic information, which are
then used to weigh the classification loss in the adaptation step. We conducted
unsupervised and semi-supervised OSDA and SF-OSDA experiments on the benchmark
image classification and action recognition datasets. Additionally, we find
that balanced pseudo-labeling plays a significant role in improving
calibration, which makes the trained model less prone to over-confident or
under-confident predictions on the target data. Source code is available at
https://github.com/Luoyadan/SF-PGL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2006.12087</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Injecting the BM25 Score as Text Improves <span class="highlight-title">BERT</span>-Based Re-rankers <span class="chip">ECIR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arian Askari, Amin Abolghasemi, Gabriella Pasi, Wessel Kraaij, Suzan Verberne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we propose a novel approach for combining first-stage lexical
retrieval models and Transformer-based re-rankers: we inject the relevance
score of the lexical model as a token in the middle of the input of the
cross-encoder re-ranker. It was shown in prior work that interpolation between
the relevance score of lexical and BERT-based re-rankers may not consistently
result in higher effectiveness. Our idea is motivated by the finding that BERT
models can capture numeric information. We compare several representations of
the BM25 score and inject them as text in the input of four different
cross-encoders. We additionally analyze the effect for different query types,
and investigate the effectiveness of our method for capturing exact matching
relevance. Evaluation on the MSMARCO Passage collection and the TREC DL
collections shows that the proposed method significantly improves over all
cross-encoder re-rankers as well as the common interpolation methods. We show
that the improvement is consistent for all query types. We also find an
improvement in exact matching capabilities over both BM25 and the
cross-encoders. Our findings indicate that cross-encoder re-rankers can
efficiently be improved without additional computational burden and extra steps
in the pipeline by explicitly adding the output of the first-stage ranker to
the model input, and this effect is robust for different models and query
types.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PRIMEQA: The Prime Repository for State-of-the-Art MultilingualQuestion
  Answering Research and Development 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avirup Sil, Jaydeep Sen, Bhavani Iyer, Martin Franz, Kshitij Fadnis, Mihaela Bornea, Sara Rosenthal, Scott McCarley, Rong Zhang, Vishwajeet Kumar, Yulong Li, Md Arafat Sultan, Riyaz Bhat, Radu Florian, Salim Roukos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of Question Answering (QA) has made remarkable progress in recent
years, thanks to the advent of large pre-trained language models, newer
realistic benchmark datasets with leaderboards, and novel algorithms for key
components such as retrievers and readers. In this paper, we introduce PRIMEQA:
a one-stop and open-source QA repository with an aim to democratize QA
re-search and facilitate easy replication of state-of-the-art (SOTA) QA
methods. PRIMEQA supports core QA functionalities like retrieval and reading
comprehension as well as auxiliary capabilities such as question generation.It
has been designed as an end-to-end toolkit for various use cases: building
front-end applications, replicating SOTA methods on pub-lic benchmarks, and
expanding pre-existing methods. PRIMEQA is available at :
https://github.com/primeqa.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demonstrate-Search-Predict: Composing retrieval and language models for
  knowledge-intensive NLP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.14024v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.14024v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, Matei Zaharia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented in-context learning has emerged as a powerful approach
for addressing knowledge-intensive tasks using frozen language models (LM) and
retrieval models (RM). Existing work has combined these in simple
"retrieve-then-read" pipelines in which the RM retrieves passages that are
inserted into the LM prompt. To begin to fully realize the potential of frozen
LMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that
relies on passing natural language texts in sophisticated pipelines between an
LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware
demonstrations, search for relevant passages, and generate grounded
predictions, systematically breaking down problems into small transformations
that the LM and RM can handle more reliably. We have written novel DSP programs
for answering questions in open-domain, multi-hop, and conversational settings,
establishing in early evaluations new state-of-the-art in-context learning
results and delivering 37-120%, 8-39%, and 80-290% relative gains against the
vanilla LM (GPT-3.5), a standard retrieve-then-read pipeline, and a
contemporaneous self-ask pipeline, respectively. We release DSP at
https://github.com/stanfordnlp/dsp
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Keyword Embeddings for Query Suggestion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08006v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08006v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jorge Gabín, M. Eduardo Ares, Javier Parapar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, search engine users commonly rely on query suggestions to improve
their initial inputs. Current systems are very good at recommending lexical
adaptations or spelling corrections to users' queries. However, they often
struggle to suggest semantically related keywords given a user's query. The
construction of a detailed query is crucial in some tasks, such as legal
retrieval or academic search. In these scenarios, keyword suggestion methods
are critical to guide the user during the query formulation. This paper
proposes two novel models for the keyword suggestion task trained on scientific
literature. Our techniques adapt the architecture of Word2Vec and FastText to
generate keyword embeddings by leveraging documents' keyword co-occurrence.
Along with these models, we also present a specially tailored negative sampling
approach that exploits how keywords appear in academic publications. We devise
a ranking-based evaluation methodology following both known-item and ad-hoc
search scenarios. Finally, we evaluate our proposals against the
state-of-the-art word and sentence embedding models showing considerable
improvements over the baselines for the tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A computational framework for physics-informed symbolic regression with
  straightforward integration of domain knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.06257v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.06257v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liron Simon Keren, Alex Liberzon, Teddy Lazebnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discovering a meaningful symbolic expression that explains experimental data
is a fundamental challenge in many scientific fields. We present a novel,
open-source computational framework called Scientist-Machine Equation Detector
(SciMED), which integrates scientific discipline wisdom in a
scientist-in-the-loop approach, with state-of-the-art symbolic regression (SR)
methods. SciMED combines a wrapper selection method, that is based on a genetic
algorithm, with automatic machine learning and two levels of SR methods. We
test SciMED on five configurations of a settling sphere, with and without
aerodynamic non-linear drag force, and with excessive noise in the
measurements. We show that SciMED is sufficiently robust to discover the
correct physically meaningful symbolic expressions from the data, and
demonstrate how the integration of domain knowledge enhances its performance.
Our results indicate better performance on these tasks than the
state-of-the-art SR software packages , even in cases where no knowledge is
integrated. Moreover, we demonstrate how SciMED can alert the user about
possible missing features, unlike the majority of current SR systems.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InfiniCity: Infinite-Scale City Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chieh Hubert Lin, Hsin-Ying Lee, Willi Menapace, Menglei Chai, Aliaksandr Siarohin, Ming-Hsuan Yang, Sergey Tulyakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Toward infinite-scale 3D city synthesis, we propose a novel framework,
InfiniCity, which constructs and renders an unconstrainedly large and
3D-grounded environment from random noises. InfiniCity decomposes the seemingly
impractical task into three feasible modules, taking advantage of both 2D and
3D data. First, an infinite-pixel image synthesis module generates
arbitrary-scale 2D maps from the bird's-eye view. Next, an octree-based voxel
completion module lifts the generated 2D map to 3D octrees. Finally, a
voxel-based neural rendering module texturizes the voxels and renders 2D
images. InfiniCity can thus synthesize arbitrary-scale and traversable 3D city
environments, and allow flexible and interactive editing from users. We
quantitatively and qualitatively demonstrate the efficacy of the proposed
framework. Project page: https://hubert0527.github.io/infinicity/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prediction-Powered Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasios N. Angelopoulos, Stephen Bates, Clara Fannjiang, Michael I. Jordan, Tijana Zrnic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce prediction-powered inference $\unicode{x2013}$ a framework for
performing valid statistical inference when an experimental data set is
supplemented with predictions from a machine-learning system such as AlphaFold.
Our framework yields provably valid conclusions without making any assumptions
on the machine-learning algorithm that supplies the predictions. Higher
accuracy of the predictions translates to smaller confidence intervals,
permitting more powerful inference. Prediction-powered inference yields simple
algorithms for computing valid confidence intervals for statistical objects
such as means, quantiles, and linear and logistic regression coefficients. We
demonstrate the benefits of prediction-powered inference with data sets from
proteomics, genomics, electronic voting, remote sensing, census analysis, and
ecology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at
  https://github.com/aangelopoulos/prediction-powered-inference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature construction using explanations of individual predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boštjan Vouk, Matej Guid, Marko Robnik-Šikonja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature construction can contribute to comprehensibility and performance of
machine learning models. Unfortunately, it usually requires exhaustive search
in the attribute space or time-consuming human involvement to generate
meaningful features. We propose a novel heuristic approach for reducing the
search space based on aggregation of instance-based explanations of predictive
models. The proposed Explainable Feature Construction (EFC) methodology
identifies groups of co-occurring attributes exposed by popular explanation
methods, such as IME and SHAP. We empirically show that reducing the search to
these groups significantly reduces the time of feature construction using
logical, relational, Cartesian, numerical, and threshold num-of-N and X-of-N
constructive operators. An analysis on 10 transparent synthetic datasets shows
that EFC effectively identifies informative groups of attributes and constructs
relevant features. Using 30 real-world classification datasets, we show
significant improvements in classification accuracy for several classifiers and
demonstrate the feasibility of the proposed feature construction even for large
datasets. Finally, EFC generated interpretable features on a real-world problem
from the financial industry, which were confirmed by a domain expert.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>54 pages, 10 figures, 22 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Impossibility of Parallelizing Boosting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amin Karbasi, Kasper Green Larsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The aim of boosting is to convert a sequence of weak learners into a strong
learner. At their heart, these methods are fully sequential. In this paper, we
investigate the possibility of parallelizing boosting. Our main contribution is
a strong negative result, implying that significant parallelization of boosting
requires an exponential blow-up in the total computing resources needed for
training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Maximum Mean Discrepancy Kernels for Predictive and Prognostic Modeling
  of Whole Slide Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piotr Keller, Muhammad Dawood, Fayyaz ul Amir Afsar Minhas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How similar are two images? In computational pathology, where Whole Slide
Images (WSIs) of digitally scanned tissue samples from patients can be
multi-gigapixels in size, determination of degree of similarity between two
WSIs is a challenging task with a number of practical applications. In this
work, we explore a novel strategy based on kernelized Maximum Mean Discrepancy
(MMD) analysis for determination of pairwise similarity between WSIs. The
proposed approach works by calculating MMD between two WSIs using kernels over
deep features of image patches. This allows representation of an entire dataset
of WSIs as a kernel matrix for WSI level clustering, weakly-supervised
prediction of TP-53 mutation status in breast cancer patients from their
routine WSIs as well as survival analysis with state of the art prediction
performance. We believe that this work will open up further avenues for
application of WSI-level kernels for predictive and prognostic tasks in
computational pathology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>* Joint first authorship Accepted: IEEE - ISBI 2023 International
  Symposium on Biomedical Imaging</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explaining Deep Learning Hidden Neuron Activations using Concept
  Induction <span class="chip">IJCAI-23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhilekha Dalal, Md Kamruzzaman Sarker, Adrita Barua, Pascal Hitzler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the current key challenges in Explainable AI is in correctly
interpreting activations of hidden neurons. It seems evident that accurate
interpretations thereof would provide insights into the question what a deep
learning system has internally \emph{detected} as relevant on the input, thus
lifting some of the black box character of deep learning systems.
  The state of the art on this front indicates that hidden node activations
appear to be interpretable in a way that makes sense to humans, at least in
some cases. Yet, systematic automated methods that would be able to first
hypothesize an interpretation of hidden neuron activations, and then verify it,
are mostly missing.
  In this paper, we provide such a method and demonstrate that it provides
meaningful interpretations. It is based on using large-scale background
knowledge -- a class hierarchy of approx. 2 million classes curated from the
Wikipedia Concept Hierarchy -- together with a symbolic reasoning approach
called \emph{concept induction} based on description logics that was originally
developed for applications in the Semantic Web field.
  Our results show that we can automatically attach meaningful labels from the
background knowledge to individual neurons in the dense layer of a
Convolutional Neural Network through a hypothesis and verification process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IJCAI-23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedExP: Speeding up Federated Averaging Via Extrapolation <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Divyansh Jhunjhunwala, Shiqiang Wang, Gauri Joshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Averaging (FedAvg) remains the most popular algorithm for Federated
Learning (FL) optimization due to its simple implementation, stateless nature,
and privacy guarantees combined with secure aggregation. Recent work has sought
to generalize the vanilla averaging in FedAvg to a generalized gradient descent
step by treating client updates as pseudo-gradients and using a server step
size. While the use of a server step size has been shown to provide performance
improvement theoretically, the practical benefit of the server step size has
not been seen in most existing works. In this work, we present FedExP, a method
to adaptively determine the server step size in FL based on dynamically varying
pseudo-gradients throughout the FL process. We begin by considering the
overparameterized convex regime, where we reveal an interesting similarity
between FedAvg and the Projection Onto Convex Sets (POCS) algorithm. We then
show how FedExP can be motivated as a novel extension to the extrapolation
mechanism that is used to speed up POCS. Our theoretical analysis later also
discusses the implications of FedExP in underparameterized and non-convex
settings. Experimental results show that FedExP consistently converges faster
than FedAvg and competing baselines on a range of realistic FL datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Huber-Robust Confidence Sequences <span class="chip">AISTATS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjian Wang, Aaditya Ramdas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Confidence sequences are confidence intervals that can be sequentially
tracked, and are valid at arbitrary data-dependent stopping times. This paper
presents confidence sequences for a univariate mean of an unknown distribution
with a known upper bound on the p-th central moment (p > 1), but allowing for
(at most) {\epsilon} fraction of arbitrary distribution corruption, as in
Huber's contamination model. We do this by designing new robust exponential
supermartingales, and show that the resulting confidence sequences attain the
optimal width achieved in the nonsequential setting. Perhaps surprisingly, the
constant margin between our sequential result and the lower bound is smaller
than even fixed-time robust confidence intervals based on the trimmed mean, for
example. Since confidence sequences are a common tool used within A/B/n testing
and bandits, these results open the door to sequential experimentation that is
robust to outliers and adversarial corruptions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26th International Conference on Artificial Intelligence and
  Statistics (AISTATS 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpArX: Sparse Argumentative Explanations for Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09559v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09559v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamed Ayoobi, Nico Potyka, Francesca Toni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks (NNs) have various applications in AI, but explaining their
decision process remains challenging. Existing approaches often focus on
explaining how changing individual inputs affects NNs' outputs. However, an
explanation that is consistent with the input-output behaviour of an NN is not
necessarily faithful to the actual mechanics thereof. In this paper, we exploit
relationships between multi-layer perceptrons (MLPs) and quantitative
argumentation frameworks (QAFs) to create argumentative explanations for the
mechanics of MLPs. Our SpArX method first sparsifies the MLP while maintaining
as much of the original mechanics as possible. It then translates the sparse
MLP into an equivalent QAF to shed light on the underlying decision process of
the MLP, producing global and/or local explanations. We demonstrate
experimentally that SpArX can give more faithful explanations than existing
approaches, while simultaneously providing deeper insights into the actual
reasoning process of MLPs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning Meets Sparse Regularization: A Signal Processing
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Parhi, Robert D. Nowak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has been widely successful in practice and most
state-of-the-art machine learning methods are based on neural networks.
Lacking, however, is a rigorous mathematical theory that adequately explains
the amazing performance of deep neural networks. In this article, we present a
relatively new mathematical framework that provides the beginning of a deeper
understanding of deep learning. This framework precisely characterizes the
functional properties of neural networks that are trained to fit to data. The
key mathematical tools which support this framework include transform-domain
sparse regularization, the Radon transform of computed tomography, and
approximation theory, which are all techniques deeply rooted in signal
processing. This framework explains the effect of weight decay regularization
in neural network training, the use of skip connections and low-rank weight
matrices in network architectures, the role of sparsity in neural networks, and
explains why neural networks can perform well in high-dimensional problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Entoptic Field Camera as Metaphor-Driven Research-through-Design
  with AI Technologies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesse Josua Benjamin, Heidi Biggs, Arne Berger, Julija Rukanskaitė, Michael Heidt, Nick Merrill, James Pierce, Joseph Lindley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) technologies are widely deployed in smartphone
photography; and prompt-based image synthesis models have rapidly become
commonplace. In this paper, we describe a Research-through-Design (RtD) project
which explores this shift in the means and modes of image production via the
creation and use of the Entoptic Field Camera. Entoptic phenomena usually refer
to perceptions of floaters or bright blue dots stemming from the physiological
interplay of the eye and brain. We use the term entoptic as a metaphor to
investigate how the material interplay of data and models in AI technologies
shapes human experiences of reality. Through our case study using first-person
design and a field study, we offer implications for critical, reflective,
more-than-human and ludic design to engage AI technologies; the
conceptualisation of an RtD research space which contributes to AI literacy
discourses; and outline a research trajectory concerning materiality and design
affordances of AI technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in Proceedings of the 2023 CHI Conference on Human
  Factors in Computing Systems (CHI '23), April 23--28, 2023, Hamburg, Germany</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepFEL: Deep Fastfood Ensemble Learning for Histopathology Image
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nima Hatami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational pathology tasks have some unique characterises such as
multi-gigapixel images, tedious and frequently uncertain annotations, and
unavailability of large number of cases [13]. To address some of these issues,
we present Deep Fastfood Ensembles - a simple, fast and yet effective method
for combining deep features pooled from popular CNN models pre-trained on
totally different source domains (e.g., natural image objects) and projected
onto diverse dimensions using random projections, the so-called Fastfood [11].
The final ensemble output is obtained by a consensus of simple individual
classifiers, each of which is trained on a different collection of random basis
vectors. This offers extremely fast and yet effective solution, especially when
training times and domain labels are of the essence. We demonstrate the
effectiveness of the proposed deep fastfood ensemble learning as compared to
the state-of-the-art methods for three different tasks in histopathology image
analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2104.00669</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WDC Products: A Multi-Dimensional Entity Matching Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ralph Peeters, Reng Chiz Der, Christian Bizer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The difficulty of an entity matching task depends on a combination of
multiple factors such as the amount of corner-case pairs, the fraction of
entities in the test set that have not been seen during training, and the size
of the development set. Current entity matching benchmarks usually represent
single points in the space along such dimensions or they provide for the
evaluation of matching methods along a single dimension, for instance the
amount of training data. This paper presents WDC Products, an entity matching
benchmark which provides for the systematic evaluation of matching systems
along combinations of three dimensions while relying on real-word data. The
three dimensions are (i) amount of corner-cases (ii) generalization to unseen
entities, and (iii) development set size. Generalization to unseen entities is
a dimension not covered by any of the existing benchmarks yet but is crucial
for evaluating the robustness of entity matching systems. WDC Products is based
on heterogeneous product data from thousands of e-shops which mark-up products
offers using schema.org annotations. Instead of learning how to match entity
pairs, entity matching can also be formulated as a multi-class classification
task that requires the matcher to recognize individual entities. WDC Products
is the first benchmark that provides a pair-wise and a multi-class formulation
of the same tasks and thus allows to directly compare the two alternatives. We
evaluate WDC Products using several state-of-the-art matching systems,
including Ditto, HierGAT, and R-SupCon. The evaluation shows that all matching
systems struggle with unseen entities to varying degrees. It also shows that
some systems are more training data efficient than others.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A New Approach to Learning Linear Dynamical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ainesh Bakshi, Allen Liu, Ankur Moitra, Morris Yau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linear dynamical systems are the foundational statistical model upon which
control theory is built. Both the celebrated Kalman filter and the linear
quadratic regulator require knowledge of the system dynamics to provide
analytic guarantees. Naturally, learning the dynamics of a linear dynamical
system from linear measurements has been intensively studied since Rudolph
Kalman's pioneering work in the 1960's. Towards these ends, we provide the
first polynomial time algorithm for learning a linear dynamical system from a
polynomial length trajectory up to polynomial error in the system parameters
under essentially minimal assumptions: observability, controllability, and
marginal stability. Our algorithm is built on a method of moments estimator to
directly estimate Markov parameters from which the dynamics can be extracted.
Furthermore, we provide statistical lower bounds when our observability and
controllability assumptions are violated.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sampling-based Nyström Approximation and Kernel Quadrature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satoshi Hayakawa, Harald Oberhauser, Terry Lyons
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analyze the Nystr\"om approximation of a positive definite kernel
associated with a probability measure. We first prove an improved error bound
for the conventional Nystr\"om approximation with i.i.d. sampling and
singular-value decomposition in the continuous regime; the proof techniques are
borrowed from statistical learning theory. We further introduce a refined
selection of subspaces in Nystr\"om approximation with theoretical guarantees
that is applicable to non-i.i.d. landmark points. Finally, we discuss their
application to convex kernel quadrature and give novel theoretical guarantees
as well as numerical observations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale
  Text-to-Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, Timo Aila
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image synthesis has recently seen significant progress thanks to
large pretrained language models, large-scale training data, and the
introduction of scalable model families such as diffusion and autoregressive
models. However, the best-performing models require iterative evaluation to
generate a single sample. In contrast, generative adversarial networks (GANs)
only need a single forward pass. They are thus much faster, but they currently
remain far behind the state-of-the-art in large-scale text-to-image synthesis.
This paper aims to identify the necessary steps to regain competitiveness. Our
proposed model, StyleGAN-T, addresses the specific requirements of large-scale
text-to-image synthesis, such as large capacity, stable training on diverse
datasets, strong text alignment, and controllable variation vs. text alignment
tradeoff. StyleGAN-T significantly improves over previous GANs and outperforms
distilled diffusion models - the previous state-of-the-art in fast
text-to-image synthesis - in terms of sample quality and speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://sites.google.com/view/stylegan-t/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Convergence of the Gradient Descent Method with Stochastic
  Fixed-point Rounding Errors under the Polyak-Lojasiewicz Inequality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Xia, Michiel E. Hochstenbach, Stefano Massei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When training neural networks with low-precision computation, rounding errors
often cause stagnation or are detrimental to the convergence of the optimizers;
in this paper we study the influence of rounding errors on the convergence of
the gradient descent method for problems satisfying the Polyak-Lojasiewicz
inequality. Within this context, we show that, in contrast, biased stochastic
rounding errors may be beneficial since choosing a proper rounding strategy
eliminates the vanishing gradient problem and forces the rounding bias in a
descent direction. Furthermore, we obtain a bound on the convergence rate that
is stricter than the one achieved by unbiased stochastic rounding. The
theoretical analysis is validated by comparing the performances of various
rounding strategies when optimizing several examples using low-precision
fixed-point number formats.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BayBFed: Bayesian Backdoor Defense for Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kavita Kumari, Phillip Rieger, Hossein Fereidooni, Murtuza Jadliwala, Ahmad-Reza Sadeghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) allows participants to jointly train a machine
learning model without sharing their private data with others. However, FL is
vulnerable to poisoning attacks such as backdoor attacks. Consequently, a
variety of defenses have recently been proposed, which have primarily utilized
intermediary states of the global model (i.e., logits) or distance of the local
models (i.e., L2-norm) from the global model to detect malicious backdoors.
However, as these approaches directly operate on client updates, their
effectiveness depends on factors such as clients' data distribution or the
adversary's attack strategies. In this paper, we introduce a novel and more
generic backdoor defense framework, called BayBFed, which proposes to utilize
probability distributions over client updates to detect malicious updates in
FL: it computes a probabilistic measure over the clients' updates to keep track
of any adjustments made in the updates, and uses a novel detection algorithm
that can leverage this probabilistic measure to efficiently detect and filter
out malicious updates. Thus, it overcomes the shortcomings of previous
approaches that arise due to the direct usage of client updates; as our
probabilistic measure will include all aspects of the local client training
strategies. BayBFed utilizes two Bayesian Non-Parametric extensions: (i) a
Hierarchical Beta-Bernoulli process to draw a probabilistic measure given the
clients' updates, and (ii) an adaptation of the Chinese Restaurant Process
(CRP), referred by us as CRP-Jensen, which leverages this probabilistic measure
to detect and filter out malicious updates. We extensively evaluate our defense
approach on five benchmark datasets: CIFAR10, Reddit, IoT intrusion detection,
MNIST, and FMNIST, and show that it can effectively detect and eliminate
malicious updates in FL without deteriorating the benign performance of the
global model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Characterizing Polarization in Social Networks using the Signed
  Relational Latent Distance Model <span class="chip">AISTATS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Nakis, Abdulkadir Çelikkanat, Louis Boucherie, Christian Djurhuus, Felix Burmester, Daniel Mathias Holmelund, Monika Frolcová, Morten Mørup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph representation learning has become a prominent tool for the
characterization and understanding of the structure of networks in general and
social networks in particular. Typically, these representation learning
approaches embed the networks into a low-dimensional space in which the role of
each individual can be characterized in terms of their latent position. A major
current concern in social networks is the emergence of polarization and filter
bubbles promoting a mindset of "us-versus-them" that may be defined by extreme
positions believed to ultimately lead to political violence and the erosion of
democracy. Such polarized networks are typically characterized in terms of
signed links reflecting likes and dislikes. We propose the latent Signed
relational Latent dIstance Model (SLIM) utilizing for the first time the
Skellam distribution as a likelihood function for signed networks and extend
the modeling to the characterization of distinct extreme positions by
constraining the embedding space to polytopes. On four real social signed
networks of polarization, we demonstrate that the model extracts
low-dimensional characterizations that well predict friendships and animosity
while providing interpretable visualizations defined by extreme positions when
endowing the model with an embedding space restricted to polytopes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint version - Accepted for the proceedings of the 26th
  International Conference on Artificial Intelligence and Statistics (AISTATS)
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking the Expressive Power of GNNs via Graph Biconnectivity <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohang Zhang, Shengjie Luo, Liwei Wang, Di He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing expressive Graph Neural Networks (GNNs) is a central topic in
learning graph-structured data. While numerous approaches have been proposed to
improve GNNs in terms of the Weisfeiler-Lehman (WL) test, generally there is
still a lack of deep understanding of what additional power they can
systematically and provably gain. In this paper, we take a fundamentally
different perspective to study the expressive power of GNNs beyond the WL test.
Specifically, we introduce a novel class of expressivity metrics via graph
biconnectivity and highlight their importance in both theory and practice. As
biconnectivity can be easily calculated using simple algorithms that have
linear computational costs, it is natural to expect that popular GNNs can learn
it easily as well. However, after a thorough review of prior GNN architectures,
we surprisingly find that most of them are not expressive for any of these
metrics. The only exception is the ESAN framework (Bevilacqua et al., 2022),
for which we give a theoretical justification of its power. We proceed to
introduce a principled and more efficient approach, called the Generalized
Distance Weisfeiler-Lehman (GD-WL), which is provably expressive for all
biconnectivity metrics. Practically, we show GD-WL can be implemented by a
Transformer-like architecture that preserves expressiveness and enjoys full
parallelizability. A set of experiments on both synthetic and real datasets
demonstrates that our approach can consistently outperform prior GNN
architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023 notable top-5%; 58 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Sufficient Dimension Reduction Through High-Dimensional Sparse
  Sliced Inverse Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenquan Cui, Yue Zhao, Jianjun Xu, Haoyang Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning has become a popular tool in the big data era nowadays. It
trains a centralized model based on data from different clients while keeping
data decentralized. In this paper, we propose a federated sparse sliced inverse
regression algorithm for the first time. Our method can simultaneously estimate
the central dimension reduction subspace and perform variable selection in a
federated setting. We transform this federated high-dimensional sparse sliced
inverse regression problem into a convex optimization problem by constructing
the covariance matrix safely and losslessly. We then use a linearized
alternating direction method of multipliers algorithm to estimate the central
subspace. We also give approaches of Bayesian information criterion and
hold-out validation to ascertain the dimension of the central subspace and the
hyper-parameter of the algorithm. We establish an upper bound of the
statistical error rate of our estimator under the heterogeneous setting. We
demonstrate the effectiveness of our method through simulations and real world
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECGAN: <span class="highlight-title">Self-supervised</span> generative adversarial network for
  electrocardiography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Simone, Davide Bacciu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality synthetic data can support the development of effective
predictive models for biomedical tasks, especially in rare diseases or when
subject to compelling privacy constraints. These limitations, for instance,
negatively impact open access to electrocardiography datasets about
arrhythmias. This work introduces a self-supervised approach to the generation
of synthetic electrocardiography time series which is shown to promote
morphological plausibility. Our model (ECGAN) allows conditioning the
generative process for specific rhythm abnormalities, enhancing synchronization
and diversity across samples with respect to literature models. A dedicated
sample quality assessment framework is also defined, leveraging arrhythmia
classifiers. The empirical results highlight a substantial improvement against
state-of-the-art generative models for sequences and audio synthesis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speeding Up BatchBALD: A k-BALD Family of Approximations for Active
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Kirsch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active learning is a powerful method for training machine learning models
with limited labeled data. One commonly used technique for active learning is
BatchBALD, which uses Bayesian neural networks to find the most informative
points to label in a pool set. However, BatchBALD can be very slow to compute,
especially for larger datasets. In this paper, we propose a new approximation,
k-BALD, which uses k-wise mutual information terms to approximate BatchBALD,
making it much less expensive to compute. Results on the MNIST dataset show
that k-BALD is significantly faster than BatchBALD while maintaining similar
performance. Additionally, we also propose a dynamic approach for choosing k
based on the quality of the approximation, making it more efficient for larger
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, workshop preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ordinal Regression for Difficulty Estimation of StepMania Levels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Billy Joe Franks, Benjamin Dinkelmann, Sophie Fellenz, Marius Kloft
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  StepMania is a popular open-source clone of a rhythm-based video game. As is
common in popular games, there is a large number of community-designed levels.
It is often difficult for players and level authors to determine the difficulty
level of such community contributions. In this work, we formalize and analyze
the difficulty prediction task on StepMania levels as an ordinal regression
(OR) task. We standardize a more extensive and diverse selection of this data
resulting in five data sets, two of which are extensions of previous work. We
evaluate many competitive OR and non-OR models, demonstrating that neural
network-based models significantly outperform the state of the art and that
StepMania-level data makes for an excellent test bed for deep OR models. We
conclude with a user experiment showing our trained models' superiority over
human labeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An iterative multi-fidelity approach for model order reduction of
  multi-dimensional input parametric PDE systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manisha Chetry, Domenico Borzacchiello, Lucas Lestandi, Luisa Rocha Da Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a parametric sampling strategy for the reduction of large-scale
PDE systems with multidimensional input parametric spaces by leveraging models
of different fidelity. The design of this methodology allows a user to
adaptively sample points ad hoc from a discrete training set with no prior
requirement of error estimators. It is achieved by exploiting low-fidelity
models throughout the parametric space to sample points using an efficient
sampling strategy, and at the sampled parametric points, high-fidelity models
are evaluated to recover the reduced basis functions. The low-fidelity models
are then adapted with the reduced order models ( ROMs) built by projection onto
the subspace spanned by the recovered basis functions. The process continues
until the low-fidelity model can represent the high-fidelity model adequately
for all the parameters in the parametric space. Since the proposed methodology
leverages the use of low-fidelity models to assimilate the solution database,
it significantly reduces the computational cost in the offline stage. The
highlight of this article is to present the construction of the initial
low-fidelity model, and a sampling strategy based on the discrete empirical
interpolation method (DEIM). We test this approach on a 2D steady-state heat
conduction problem for two different input parameters and make a qualitative
comparison with the classical greedy reduced basis method (RBM), and further
test on a 9-dimensional parametric non-coercive elliptic problem and analyze
the computational performance based on different tuning of greedy selection of
points.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modality-Agnostic Variational Compression of Implicit Neural
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Richard Schwarz, Jihoon Tack, Yee Whye Teh, Jaeho Lee, Jinwoo Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a modality-agnostic neural data compression algorithm based on a
functional view of data and parameterised as an Implicit Neural Representation
(INR). Bridging the gap between latent coding and sparsity, we obtain compact
latent representations which are non-linearly mapped to a soft gating mechanism
capable of specialising a shared INR base network to each data item through
subnetwork selection. After obtaining a dataset of such compact latent
representations, we directly optimise the rate/distortion trade-off in this
modality-agnostic space using non-linear transform coding. We term this method
Variational Compression of Implicit Neural Representation (VC-INR) and show
both improved performance given the same representational capacity pre
quantisation while also outperforming previous quantisation schemes used for
other INR-based techniques. Our experiments demonstrate strong results over a
large set of diverse data modalities using the same algorithm without any
modality-specific inductive biases. We show results on images, climate data, 3D
shapes and scenes as well as audio and video, introducing VC-INR as the first
INR-based method to outperform codecs as well-known and diverse as JPEG 2000,
MP3 and AVC/HEVC on their respective modalities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DIFFormer: Scalable (Graph) <span class="highlight-title">Transformer</span>s Induced by Energy Constrained
  Diffusion <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qitian Wu, Chenxiao Yang, Wentao Zhao, Yixuan He, David Wipf, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world data generation often involves complex inter-dependencies among
instances, violating the IID-data hypothesis of standard learning paradigms and
posing a challenge for uncovering the geometric structures for learning desired
instance representations. To this end, we introduce an energy constrained
diffusion model which encodes a batch of instances from a dataset into
evolutionary states that progressively incorporate other instances' information
by their interactions. The diffusion process is constrained by descent criteria
w.r.t.~a principled energy function that characterizes the global consistency
of instance representations over latent structures. We provide rigorous theory
that implies closed-form optimal estimates for the pairwise diffusion strength
among arbitrary instance pairs, which gives rise to a new class of neural
encoders, dubbed as DIFFormer (diffusion-based Transformers), with two
instantiations: a simple version with linear complexity for prohibitive
instance numbers, and an advanced version for learning complex structures.
Experiments highlight the wide applicability of our model as a general-purpose
encoder backbone with superior performance in various tasks, such as node
classification on large graphs, semi-supervised image/text classification, and
spatial-temporal dynamics prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by International Conference on Learning Representations
  (ICLR 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Non-deterministic Human Behaviors in Discrete Food Choices <span class="chip">ICDM</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Starnes, Anton Dereventsov, E. Susanne Blazek, Folasade Phillips
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We establish a non-deterministic model that predicts a user's food
preferences from their demographic information. Our simulator is based on
NHANES dataset and domain expert knowledge in the form of established
behavioral studies. Our model can be used to generate an arbitrary amount of
synthetic datapoints that are similar in distribution to the original dataset
and align with behavioral science expectations. Such a simulator can be used in
a variety of machine learning tasks and especially in applications requiring
human behavior prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures, published in 2022 IEEE International Conference
  on Data Mining Workshops (ICDMW)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple Recipe for Competitive Low-compute Self supervised Vision
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quentin Duval, Ishan Misra, Nicolas Ballas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised methods in vision have been mostly focused on large
architectures as they seem to suffer from a significant performance drop for
smaller architectures. In this paper, we propose a simple self-supervised
distillation technique that can train high performance low-compute neural
networks. Our main insight is that existing joint-embedding based SSL methods
can be repurposed for knowledge distillation from a large self-supervised
teacher to a small student model. Thus, we call our method Replace one Branch
(RoB) as it simply replaces one branch of the joint-embedding training with a
large teacher model. RoB is widely applicable to a number of architectures such
as small ResNets, MobileNets and ViT, and pretrained models such as DINO, SwAV
or iBOT. When pretraining on the ImageNet dataset, RoB yields models that
compete with supervised knowledge distillation. When applied to MSN, RoB
produces students with strong semi-supervised capabilities. Finally, our best
ViT-Tiny models improve over prior SSL state-of-the-art on ImageNet by $2.3\%$
and are on par or better than a supervised distilled DeiT on five downstream
transfer tasks (iNaturalist, CIFAR, Clevr/Count, Clevr/Dist and Places). We
hope RoB enables practical self-supervision at smaller scale.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explaining the effects of non-convergent sampling in the training of
  Energy-Based Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elisabeth Agoritsas, Giovanni Catania, Aurélien Decelle, Beatriz Seoane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we quantify the impact of using non-convergent Markov chains
to train Energy-Based models (EBMs). In particular, we show analytically that
EBMs trained with non-persistent short runs to estimate the gradient can
perfectly reproduce a set of empirical statistics of the data, not at the level
of the equilibrium measure, but through a precise dynamical process. Our
results provide a first-principles explanation for the observations of recent
works proposing the strategy of using short runs starting from random initial
conditions as an efficient way to generate high-quality samples in EBMs, and
lay the groundwork for using EBMs as diffusion models. After explaining this
effect in generic EBMs, we analyze two solvable models in which the effect of
the non-convergent sampling in the trained parameters can be described in
detail. Finally, we test these predictions numerically on the Boltzmann
machine.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LSTM and CNN application for core-collapse supernova search in
  gravitational wave real data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto Iess, Elena Cuoco, Filip Morawski, Constantina Nicolaou, Ofer Lahav
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  $Context.$ Core-collapse supernovae (CCSNe) are expected to emit
gravitational wave signals that could be detected by current and future
generation interferometers within the Milky Way and nearby galaxies. The
stochastic nature of the signal arising from CCSNe requires alternative
detection methods to matched filtering. $Aims.$ We aim to show the potential of
machine learning (ML) for multi-label classification of different CCSNe
simulated signals and noise transients using real data. We compared the
performance of 1D and 2D convolutional neural networks (CNNs) on single and
multiple detector data. For the first time, we tested multi-label
classification also with long short-term memory (LSTM) networks. $Methods.$ We
applied a search and classification procedure for CCSNe signals, using an event
trigger generator, the Wavelet Detection Filter (WDF), coupled with ML. We used
time series and time-frequency representations of the data as inputs to the ML
models. To compute classification accuracies, we simultaneously injected, at
detectable distance of 1\,kpc, CCSN waveforms, obtained from recent
hydrodynamical simulations of neutrino-driven core-collapse, onto
interferometer noise from the O2 LIGO and Virgo science run. $Results.$ We
compared the performance of the three models on single detector data. We then
merged the output of the models for single detector classification of noise and
astrophysical transients, obtaining overall accuracies for LIGO ($\sim99\%$)
and ($\sim80\%$) for Virgo. We extended our analysis to the multi-detector case
using triggers coincident among the three ITFs and achieved an accuracy of
$\sim98\%$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 13 figures. Accepted by A&A journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Structural Approach to the Design of Domain Specific Neural Network
  Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gerrit Nolte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This is a master's thesis concerning the theoretical ideas of geometric deep
learning. Geometric deep learning aims to provide a structured characterization
of neural network architectures, specifically focused on the ideas of
invariance and equivariance of data with respect to given transformations.
  This thesis aims to provide a theoretical evaluation of geometric deep
learning, compiling theoretical results that characterize the properties of
invariant neural networks with respect to learning performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>94 pages and 16 Figures Upload of my Master's thesis. Not peer
  reviewed and potentially contains errors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive <span class="highlight-title">Survey</span> on Heart Sound Analysis in the Deep Learning Era 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhao Ren, Yi Chang, Thanh Tam Nguyen, Yang Tan, Kun Qian, Björn W. Schuller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Heart sound auscultation has been demonstrated to be beneficial in clinical
usage for early screening of cardiovascular diseases. Due to the high
requirement of well-trained professionals for auscultation, automatic
auscultation benefiting from signal processing and machine learning can help
auxiliary diagnosis and reduce the burdens of training professional clinicians.
Nevertheless, classic machine learning is limited to performance improvement in
the era of big data. Deep learning has achieved better performance than classic
machine learning in many research fields, as it employs more complex model
architectures with stronger capability of extracting effective representations.
Deep learning has been successfully applied to heart sound analysis in the past
years. As most review works about heart sound analysis were given before 2017,
the present survey is the first to work on a comprehensive overview to
summarise papers on heart sound analysis with deep learning in the past six
years 2017--2022. We introduce both classic machine learning and deep learning
for comparison, and further offer insights about the advances and future
research directions in deep learning for heart sound analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SMDDH: Singleton Mention detection using Deep Learning in Hindi Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kusum Lata, Pardeep Singh, Kamlesh Dutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mention detection is an important component of coreference resolution system,
where mentions such as name, nominal, and pronominals are identified. These
mentions can be purely coreferential mentions or singleton mentions
(non-coreferential mentions). Coreferential mentions are those mentions in a
text that refer to the same entities in a real world. Whereas, singleton
mentions are mentioned only once in the text and do not participate in the
coreference as they are not mentioned again in the following text. Filtering of
these singleton mentions can substantially improve the performance of a
coreference resolution process. This paper proposes a singleton mention
detection module based on a fully connected network and a Convolutional neural
network for Hindi text. This model utilizes a few hand-crafted features and
context information, and word embedding for words. The coreference annotated
Hindi dataset comprising of 3.6K sentences, and 78K tokens are used for the
task. In terms of Precision, Recall, and F-measure, the experimental findings
obtained are excellent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Fair Federated Learning: Adaptive Federated Adam 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Ju, Tianru Zhang, Salman Toor, Andreas Hellander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a distributed and privacy-preserving approach to train
a statistical model collaboratively from decentralized data of different
parties. However, when datasets of participants are not independent and
identically distributed (non-IID), models trained by naive federated algorithms
may be biased towards certain participants, and model performance across
participants is non-uniform. This is known as the fairness problem in federated
learning. In this paper, we formulate fairness-controlled federated learning as
a dynamical multi-objective optimization problem to ensure fair performance
across all participants. To solve the problem efficiently, we study the
convergence and bias of Adam as the server optimizer in federated learning, and
propose Adaptive Federated Adam (AdaFedAdam) to accelerate fair federated
learning with alleviated bias. We validated the effectiveness, Pareto
optimality and robustness of AdaFedAdam in numerical experiments and show that
AdaFedAdam outperforms existing algorithms, providing better convergence and
fairness properties of the federated scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large-scale fine-grained semantic indexing of biomedical literature
  based on weakly-supervised deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasios Nentidis, Thomas Chatzopoulos, Anastasia Krithara, Grigorios Tsoumakas, Georgios Paliouras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic indexing of biomedical literature is usually done at the level of
MeSH descriptors, representing topics of interest for the biomedical community.
Several related but distinct biomedical concepts are often grouped together in
a single coarse-grained descriptor and are treated as a single topic for
semantic indexing. This study proposes a new method for the automated
refinement of subject annotations at the level of concepts, investigating deep
learning approaches. Lacking labelled data for this task, our method relies on
weak supervision based on concept occurrence in the abstract of an article. The
proposed approach is evaluated on an extended large-scale retrospective
scenario, taking advantage of concepts that eventually become MeSH descriptors,
for which annotations become available in MEDLINE/PubMed. The results suggest
that concept occurrence is a strong heuristic for automated subject annotation
refinement and can be further enhanced when combined with dictionary-based
heuristics. In addition, such heuristics can be useful as weak supervision for
developing deep learning models that can achieve further improvement in some
cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages, 5 figures, 9 tables, 1 algorithm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning-Based Assessment of Cerebral Microbleeds in COVID-19 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neus Rodeja Ferrer, Malini Vendela Sagar, Kiril Vadimovic Klein, Christina Kruuse, Mads Nielsen, Mostafa Mehdipour Ghazi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cerebral Microbleeds (CMBs), typically captured as hypointensities from
susceptibility-weighted imaging (SWI), are particularly important for the study
of dementia, cerebrovascular disease, and normal aging. Recent studies on
COVID-19 have shown an increase in CMBs of coronavirus cases. Automatic
detection of CMBs is challenging due to the small size and amount of CMBs
making the classes highly imbalanced, lack of publicly available annotated
data, and similarity with CMB mimics such as calcifications, irons, and veins.
Hence, the existing deep learning methods are mostly trained on very limited
research data and fail to generalize to unseen data with high variability and
cannot be used in clinical setups. To this end, we propose an efficient 3D deep
learning framework that is actively trained on multi-domain data. Two public
datasets assigned for normal aging, stroke, and Alzheimer's disease analysis as
well as an in-house dataset for COVID-19 assessment are used to train and
evaluate the models. The obtained results show that the proposed method is
robust to low-resolution images and achieves 78% recall and 80% precision on
the entire test set with an average false positive of 1.6 per scan.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Symposium on Biomedical Imaging (ISBI) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Framework for Evaluating the Impact of Food Security Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachid Belmeskine, Abed Benaichouche
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study proposes an approach for predicting the impacts of scenarios on
food security and demonstrates its application in a case study. The approach
involves two main steps: (1) scenario definition, in which the end user
specifies the assumptions and impacts of the scenario using a scenario
template, and (2) scenario evaluation, in which a Vector Autoregression (VAR)
model is used in combination with Monte Carlo simulation to generate
predictions for the impacts of the scenario based on the defined assumptions
and impacts. The case study is based on a proprietary time series food security
database created using data from the Food and Agriculture Organization of the
United Nations (FAOSTAT), the World Bank, and the United States Department of
Agriculture (USDA). The database contains a wide range of data on various
indicators of food security, such as production, trade, consumption, prices,
availability, access, and nutritional value. The results show that the proposed
approach can be used to predict the potential impacts of scenarios on food
security and that the proprietary time series food security database can be
used to support this approach. The study provides specific insights on how this
approach can inform decision-making processes related to food security such as
food prices and availability in the case study region.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Foundation Models for Earth Monitoring: Generalizable Deep
  Learning Models for Natural Hazard Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Jakubik, Michal Muszynski, Michael Vössing, Niklas Kühl, Thomas Brunschwiler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Climate change results in an increased probability of extreme weather events
that put societies and businesses at risk on a global scale. Therefore, near
real-time mapping of natural hazards is an emerging priority for the support of
natural disaster relief, risk management, and informing governmental policy
decisions. Recent methods to achieve near real-time mapping increasingly
leverage deep learning (DL). However, DL-based approaches are designed for one
specific task in a single geographic region based on specific frequency bands
of satellite data. Therefore, DL models used to map specific natural hazards
struggle with their generalization to other types of natural hazards in unseen
regions. In this work, we propose a methodology to significantly improve the
generalizability of DL natural hazards mappers based on pre-training on a
suitable pre-task. Without access to any data from the target domain, we
demonstrate this improved generalizability across four U-Net architectures for
the segmentation of unseen natural hazards. Importantly, our method is
invariant to geographic differences and differences in the type of frequency
bands of satellite data. By leveraging characteristics of unlabeled images from
the target domain that are publicly available, our approach is able to further
improve the generalization behavior without fine-tuning. Thereby, our approach
supports the development of foundation models for earth monitoring with the
objective of directly segmenting unseen natural hazards across novel geographic
regions given different sources of satellite imagery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Actionable Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayed Erfan Arefin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Actionable Knowledge Discovery (AKD) is a crucial aspect of data mining that
is gaining popularity and being applied in a wide range of domains. This is
because AKD can extract valuable insights and information, also known as
knowledge, from large datasets. The goal of this paper is to examine different
research studies that focus on various domains and have different objectives.
The paper will review and discuss the methods used in these studies in detail.
AKD is a process of identifying and extracting actionable insights from data,
which can be used to make informed decisions and improve business outcomes. It
is a powerful tool for uncovering patterns and trends in data that can be used
for various applications such as customer relationship management, marketing,
and fraud detection. The research studies reviewed in this paper will explore
different techniques and approaches for AKD in different domains, such as
healthcare, finance, and telecommunications. The paper will provide a thorough
analysis of the current state of AKD in the field and will review the main
methods used by various research studies. Additionally, the paper will evaluate
the advantages and disadvantages of each method and will discuss any novel or
new solutions presented in the field. Overall, this paper aims to provide a
comprehensive overview of the methods and techniques used in AKD and the impact
they have on different domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enabling Hard Constraints in Differentiable Neural Network and
  Accelerator Co-Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deokki Hong, Kanghyun Choi, Hye Yoon Lee, Joonsang Yu, Noseong Park, Youngsok Kim, Jinho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Co-exploration of an optimal neural architecture and its hardware accelerator
is an approach of rising interest which addresses the computational cost
problem, especially in low-profile systems. The large co-exploration space is
often handled by adopting the idea of differentiable neural architecture
search. However, despite the superior search efficiency of the differentiable
co-exploration, it faces a critical challenge of not being able to
systematically satisfy hard constraints such as frame rate. To handle the hard
constraint problem of differentiable co-exploration, we propose HDX, which
searches for hard-constrained solutions without compromising the global design
objectives. By manipulating the gradients in the interest of the given hard
constraint, high-quality solutions satisfying the constraint can be obtained.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>publisehd at DAC'22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Expressive Power of Geometric Graph Neural Networks <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaitanya K. Joshi, Cristian Bodnar, Simon V. Mathis, Taco Cohen, Pietro Liò
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The expressive power of Graph Neural Networks (GNNs) has been studied
extensively through the Weisfeiler-Leman (WL) graph isomorphism test. However,
standard GNNs and the WL framework are inapplicable for geometric graphs
embedded in Euclidean space, such as biomolecules, materials, and other
physical systems. In this work, we propose a geometric version of the WL test
(GWL) for discriminating geometric graphs while respecting the underlying
physical symmetries: permutations, rotation, reflection, and translation. We
use GWL to characterise the expressive power of geometric GNNs that are
invariant or equivariant to physical symmetries in terms of distinguishing
geometric graphs. GWL unpacks how key design choices influence geometric GNN
expressivity: (1) Invariant layers have limited expressivity as they cannot
distinguish one-hop identical geometric graphs; (2) Equivariant layers
distinguish a larger class of graphs by propagating geometric information
beyond local neighbourhoods; (3) Higher order tensors and scalarisation enable
maximally powerful geometric GNNs; and (4) GWL's discrimination-based
perspective is equivalent to universal approximation. Synthetic experiments
supplementing our results are available at
https://github.com/chaitjo/geometric-gnn-dojo
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022 Workshop on Symmetry and Geometry in Neural
  Representations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Practical Adversarial Attacks Against AI-Driven Power Allocation in a
  Distributed MIMO Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ömer Faruk Tuna, Fehmi Emre Kadan, Leyli Karaçay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In distributed multiple-input multiple-output (D-MIMO) networks, power
control is crucial to optimize the spectral efficiencies of users and max-min
fairness (MMF) power control is a commonly used strategy as it satisfies
uniform quality-of-service to all users. The optimal solution of MMF power
control requires high complexity operations and hence deep neural network based
artificial intelligence (AI) solutions are proposed to decrease the complexity.
Although quite accurate models can be achieved by using AI, these models have
some intrinsic vulnerabilities against adversarial attacks where carefully
crafted perturbations are applied to the input of the AI model. In this work,
we show that threats against the target AI model which might be originated from
malicious users or radio units can substantially decrease the network
performance by applying a successful adversarial sample, even in the most
constrained circumstances. We also demonstrate that the risk associated with
these kinds of adversarial attacks is higher than the conventional attack
threats. Detailed simulations reveal the effectiveness of adversarial attacks
and the necessity of smart defense techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 10 figures, accepted for presentation in International
  Conference on Communications (ICC) 2023 in Communication and Information
  System Security Symposium</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Tale of Two Latent Flows: Learning Latent Space Normalizing Flow with
  Short-run Langevin Flow for Approximate Inference <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianwen Xie, Yaxuan Zhu, Yifei Xu, Dingcheng Li, Ping Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a normalizing flow in the latent space of a top-down generator
model, in which the normalizing flow model plays the role of the informative
prior model of the generator. We propose to jointly learn the latent space
normalizing flow prior model and the top-down generator model by a Markov chain
Monte Carlo (MCMC)-based maximum likelihood algorithm, where a short-run
Langevin sampling from the intractable posterior distribution is performed to
infer the latent variables for each observed example, so that the parameters of
the normalizing flow prior and the generator can be updated with the inferred
latent variables. We show that, under the scenario of non-convergent short-run
MCMC, the finite step Langevin dynamics is a flow-like approximate inference
model and the learning objective actually follows the perturbation of the
maximum likelihood estimation (MLE). We further point out that the learning
framework seeks to (i) match the latent space normalizing flow and the
aggregated posterior produced by the short-run Langevin flow, and (ii) bias the
model from MLE such that the short-run Langevin flow inference is close to the
true posterior. Empirical results of extensive experiments validate the
effectiveness of the proposed latent space normalizing flow model in the tasks
of image generation, image reconstruction, anomaly detection, supervised image
inpainting and unsupervised image recovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI)
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-Supervised</span> Image Representation Learning: Transcending Masking with
  Paired Image Overlay 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinheng Li, Han Ding, Shaofei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning has become a popular approach in recent years for
its ability to learn meaningful representations without the need for data
annotation. This paper proposes a novel image augmentation technique,
overlaying images, which has not been widely applied in self-supervised
learning. This method is designed to provide better guidance for the model to
understand underlying information, resulting in more useful representations.
The proposed method is evaluated using contrastive learning, a widely used
self-supervised learning method that has shown solid performance in downstream
tasks. The results demonstrate the effectiveness of the proposed augmentation
technique in improving the performance of self-supervised models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classification of Luminal Subtypes in Full Mammogram Images Using
  Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adarsh Bhandary Panambur, Prathmesh Madhu, Andreas Maier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic identification of patients with luminal and non-luminal subtypes
during a routine mammography screening can support clinicians in streamlining
breast cancer therapy planning. Recent machine learning techniques have shown
promising results in molecular subtype classification in mammography; however,
they are highly dependent on pixel-level annotations, handcrafted, and radiomic
features. In this work, we provide initial insights into the luminal subtype
classification in full mammogram images trained using only image-level labels.
Transfer learning is applied from a breast abnormality classification task, to
finetune a ResNet-18-based luminal versus non-luminal subtype classification
task. We present and compare our results on the publicly available CMMD dataset
and show that our approach significantly outperforms the baseline classifier by
achieving a mean AUC score of 0.6688 and a mean F1 score of 0.6693 on the test
dataset. The improvement over baseline is statistically significant, with a
p-value of p<0.0001.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE ISBI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StockEmotions: Discover Investor Emotions for Financial Sentiment
  Analysis and Multivariate Time Series <span class="chip">AAAI-23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean Lee, Hoyoul Luis Youn, Josiah Poon, Soyeon Caren Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been growing interest in applying NLP techniques in the financial
domain, however, resources are extremely limited. This paper introduces
StockEmotions, a new dataset for detecting emotions in the stock market that
consists of 10,000 English comments collected from StockTwits, a financial
social media platform. Inspired by behavioral finance, it proposes 12
fine-grained emotion classes that span the roller coaster of investor emotion.
Unlike existing financial sentiment datasets, StockEmotions presents granular
features such as investor sentiment classes, fine-grained emotions, emojis, and
time series data. To demonstrate the usability of the dataset, we perform a
dataset analysis and conduct experimental downstream tasks. For financial
sentiment/emotion classification tasks, DistilBERT outperforms other baselines,
and for multivariate time series forecasting, a Temporal Attention LSTM model
combining price index, text, and emotion features achieves the best performance
than using a single feature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint for the AAAI-23 Bridge Program (AI for Financial Services)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M22: A Communication-Efficient Algorithm for Federated Learning Inspired
  by Rate-Distortion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangyi Liu, Stefano Rini, Sadaf Salehkalaibar, Jun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In federated learning (FL), the communication constraint between the remote
learners and the Parameter Server (PS) is a crucial bottleneck. For this
reason, model updates must be compressed so as to minimize the loss in accuracy
resulting from the communication constraint. This paper proposes ``\emph{${\bf
M}$-magnitude weighted $L_{\bf 2}$ distortion + $\bf 2$ degrees of freedom''}
(M22) algorithm, a rate-distortion inspired approach to gradient compression
for federated training of deep neural networks (DNNs). In particular, we
propose a family of distortion measures between the original gradient and the
reconstruction we referred to as ``$M$-magnitude weighted $L_2$'' distortion,
and we assume that gradient updates follow an i.i.d. distribution --
generalized normal or Weibull, which have two degrees of freedom. In both the
distortion measure and the gradient, there is one free parameter for each that
can be fitted as a function of the iteration number. Given a choice of gradient
distribution and distortion measure, we design the quantizer minimizing the
expected distortion in gradient reconstruction. To measure the gradient
compression performance under a communication constraint, we define the
\emph{per-bit accuracy} as the optimal improvement in accuracy that one bit of
communication brings to the centralized model over the training period. Using
this performance measure, we systematically benchmark the choice of gradient
distribution and distortion measure. We provide substantial insights on the
role of these choices and argue that significant performance improvements can
be attained using such a rate-distortion inspired compressor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2202.02812</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FInC Flow: Fast and Invertible $k \times k$ Convolutions for Normalizing
  Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Kallappa, Sandeep Nagar, Girish Varma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Invertible convolutions have been an essential element for building
expressive normalizing flow-based generative models since their introduction in
Glow. Several attempts have been made to design invertible $k \times k$
convolutions that are efficient in training and sampling passes. Though these
attempts have improved the expressivity and sampling efficiency, they severely
lagged behind Glow which used only $1 \times 1$ convolutions in terms of
sampling time. Also, many of the approaches mask a large number of parameters
of the underlying convolution, resulting in lower expressivity on a fixed
run-time budget. We propose a $k \times k$ convolutional layer and Deep
Normalizing Flow architecture which i.) has a fast parallel inversion algorithm
with running time O$(n k^2)$ ($n$ is height and width of the input image and k
is kernel size), ii.) masks the minimal amount of learnable parameters in a
layer. iii.) gives better forward pass and sampling times comparable to other
$k \times k$ convolution-based models on real-world benchmarks. We provide an
implementation of the proposed parallel algorithm for sampling using our
invertible convolutions on GPUs. Benchmarks on CIFAR-10, ImageNet, and CelebA
datasets show comparable performance to previous works regarding bits per
dimension while significantly improving the sampling time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted: VISAPP'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Training Under Limited Resources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahdi Zolnouri, Dounia Lakhmiri, Christophe Tribes, Eyyüb Sari, Sébastien Le Digabel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training time budget and size of the dataset are among the factors affecting
the performance of a Deep Neural Network (DNN). This paper shows that Neural
Architecture Search (NAS), Hyper Parameters Optimization (HPO), and Data
Augmentation help DNNs perform much better while these two factors are limited.
However, searching for an optimal architecture and the best hyperparameter
values besides a good combination of data augmentation techniques under low
resources requires many experiments. We present our approach to achieving such
a goal in three steps: reducing training epoch time by compressing the model
while maintaining the performance compared to the original model, preventing
model overfitting when the dataset is small, and performing the hyperparameter
tuning. We used NOMAD, which is a blackbox optimization software based on a
derivative-free algorithm to do NAS and HPO. Our work achieved an accuracy of
86.0 % on a tiny subset of Mini-ImageNet at the ICLR 2021 Hardware Aware
Efficient Training (HAET) Challenge and won second place in the competition.
The competition results can be found at haet2021.github.io/challenge and our
source code can be found at github.com/DouniaLakhmiri/ICLR\_HAET2021.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MEMO : Accelerating <span class="highlight-title">Transformer</span>s with Memoization on Big Memory Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Feng, Hyeran Jeon, Filip Blagojevic, Cyril Guyot, Qing Li, Dong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers gain popularity because of their superior prediction accuracy
and inference throughput. However, the transformer is computation-intensive,
causing a long inference time. The existing work to accelerate transformer
inferences has limitations because of the changes to transformer architectures
or the need for specialized hardware. In this paper, we identify the
opportunities of using memoization to accelerate the attention mechanism in
transformers without the above limitation. Built upon a unique observation that
there is a rich similarity in attention computation across inference sequences,
we build an attention database upon the emerging big memory system. We
introduce the embedding technique to find semantically similar inputs to
identify computation similarity. We also introduce a series of techniques such
as memory mapping and selective memoization to avoid memory copy and
unnecessary overhead. We enable 21% performance improvement on average (up to
68%) with the TB-scale attention database and with ignorable loss in inference
accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combined Use of Federated Learning and Image Encryption for
  Privacy-Preserving Image Classification with Vision <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teru Nagamori, Hitoshi Kiya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, privacy-preserving methods for deep learning have become an
urgent problem. Accordingly, we propose the combined use of federated learning
(FL) and encrypted images for privacy-preserving image classification under the
use of the vision transformer (ViT). The proposed method allows us not only to
train models over multiple participants without directly sharing their raw data
but to also protect the privacy of test (query) images for the first time. In
addition, it can also maintain the same accuracy as normally trained models. In
an experiment, the proposed method was demonstrated to well work without any
performance degradation on the CIFAR-10 and CIFAR-100 datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Linearize Deep Neural Networks for Secure and Efficient
  Private Inference <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Souvik Kundu, Shunlin Lu, Yuke Zhang, Jacqueline Liu, Peter A. Beerel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The large number of ReLU non-linearity operations in existing deep neural
networks makes them ill-suited for latency-efficient private inference (PI).
Existing techniques to reduce ReLU operations often involve manual effort and
sacrifice significant accuracy. In this paper, we first present a novel measure
of non-linearity layers' ReLU sensitivity, enabling mitigation of the
time-consuming manual efforts in identifying the same. Based on this
sensitivity, we then present SENet, a three-stage training method that for a
given ReLU budget, automatically assigns per-layer ReLU counts, decides the
ReLU locations for each layer's activation map, and trains a model with
significantly fewer ReLUs to potentially yield latency and communication
efficient PI. Experimental evaluations with multiple models on various datasets
show SENet's superior performance both in terms of reduced ReLUs and improved
classification accuracy compared to existing alternatives. In particular, SENet
can yield models that require up to ~2x fewer ReLUs while yielding similar
accuracy. For a similar ReLU budget SENet can yield models with ~2.32% improved
classification accuracy, evaluated on CIFAR-100.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 10 figures, 11 tables. Accepted as a conference paper at
  ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Congested Bandits: Optimal Routing via Short-term Resets <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranjal Awasthi, Kush Bhatia, Sreenivas Gollapudi, Kostas Kollias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For traffic routing platforms, the choice of which route to recommend to a
user depends on the congestion on these routes -- indeed, an individual's
utility depends on the number of people using the recommended route at that
instance. Motivated by this, we introduce the problem of Congested Bandits
where each arm's reward is allowed to depend on the number of times it was
played in the past $\Delta$ timesteps. This dependence on past history of
actions leads to a dynamical system where an algorithm's present choices also
affect its future pay-offs, and requires an algorithm to plan for this. We
study the congestion aware formulation in the multi-armed bandit (MAB) setup
and in the contextual bandit setup with linear rewards. For the multi-armed
setup, we propose a UCB style algorithm and show that its policy regret scales
as $\tilde{O}(\sqrt{K \Delta T})$. For the linear contextual bandit setup, our
algorithm, based on an iterative least squares planner, achieves policy regret
$\tilde{O}(\sqrt{dT} + \Delta)$. From an experimental standpoint, we
corroborate the no-regret properties of our algorithms via a simulation study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICML 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards NeuroAI: Introducing Neuronal Diversity into Artificial Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng-Lei Fan, Yingxin Li, Hanchuan Peng, Tieyong Zeng, Fei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Throughout history, the development of artificial intelligence, particularly
artificial neural networks, has been open to and constantly inspired by the
increasingly deepened understanding of the brain, such as the inspiration of
neocognitron, which is the pioneering work of convolutional neural networks.
Per the motives of the emerging field: NeuroAI, a great amount of neuroscience
knowledge can help catalyze the next generation of AI by endowing a network
with more powerful capabilities. As we know, the human brain has numerous
morphologically and functionally different neurons, while artificial neural
networks are almost exclusively built on a single neuron type. In the human
brain, neuronal diversity is an enabling factor for all kinds of biological
intelligent behaviors. Since an artificial network is a miniature of the human
brain, introducing neuronal diversity should be valuable in terms of addressing
those essential problems of artificial networks such as efficiency,
interpretability, and memory. In this Primer, we first discuss the
preliminaries of biological neuronal diversity and the characteristics of
information transmission and processing in a biological neuron. Then, we review
studies of designing new neurons for artificial networks. Next, we discuss what
gains can neuronal diversity bring into artificial networks and exemplary
applications in several important fields. Lastly, we discuss the challenges and
future directions of neuronal diversity to explore the potential of NeuroAI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Reservoir Dynamics with Temporal Self-Modulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Sakemi, Sou Nobukawa, Toshitaka Matsuki, Takashi Morie, Kazuyuki Aihara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reservoir computing (RC) can efficiently process time-series data by
transferring the input signal to randomly connected recurrent neural networks
(RNNs), which are referred to as a reservoir. The high-dimensional
representation of time-series data in the reservoir significantly simplifies
subsequent learning tasks. Although this simple architecture allows fast
learning and facile physical implementation, the learning performance is
inferior to that of other state-of-the-art RNN models. In this paper, to
improve the learning ability of RC, we propose self-modulated RC (SM-RC), which
extends RC by adding a self-modulation mechanism. The self-modulation mechanism
is realized with two gating variables: an input gate and a reservoir gate. The
input gate modulates the input signal, and the reservoir gate modulates the
dynamical properties of the reservoir. We demonstrated that SM-RC can perform
attention tasks where input information is retained or discarded depending on
the input signal. We also found that a chaotic state emerged as a result of
learning in SM-RC. This indicates that self-modulation mechanisms provide RC
with qualitatively different information-processing capabilities. Furthermore,
SM-RC outperformed RC in NARMA and Lorentz model tasks. In particular, SM-RC
achieved a higher prediction accuracy than RC with a reservoir 10 times larger
in the Lorentz model tasks. Because the SM-RC architecture only requires two
additional gates, it is physically implementable as RC, providing a new
direction for realizing edge AI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimising complexity of CNN models for resource constrained devices:
  QRS detection case study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahsan Habib, Chandan Karmakar, John Yearwood
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional DL models are complex and resource hungry and thus, care needs to
be taken in designing Internet of (medical) things (IoT, or IoMT) applications
balancing efficiency-complexity trade-off. Recent IoT solutions tend to avoid
using deep-learning methods due to such complexities, and rather classical
filter-based methods are commonly used. We hypothesize that a shallow CNN model
can offer satisfactory level of performance in combination by leveraging other
essential solution-components, such as post-processing that is suitable for
resource constrained environment. In an IoMT application context, QRS-detection
and R-peak localisation from ECG signal as a case study, the complexities of
CNN models and post-processing were varied to identify a set of combinations
suitable for a range of target resource-limited environments. To the best of
our knowledge, finding a deploy-able configuration, by incrementally increasing
the CNN model complexity, as required to match the target's resource capacity,
and leveraging the strength of post-processing, is the first of its kind. The
results show that a shallow 2-layer CNN with a suitable post-processing can
achieve $>$90\% F1-score, and the scores continue to improving for 8-32 layer
CNNs, which can be used to profile target constraint environment. The outcome
shows that it is possible to design an optimal DL solution with known target
performance characteristics and resource (computing capacity, and memory)
constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GP-NAS-ensemble: a model for NAS Performance Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunlong Chen, Liu Yang, Yitian Chen, Kunjin Chen, Yidan Xu, Lujun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is of great significance to estimate the performance of a given model
architecture without training in the application of Neural Architecture Search
(NAS) as it may take a lot of time to evaluate the performance of an
architecture. In this paper, a novel NAS framework called GP-NAS-ensemble is
proposed to predict the performance of a neural network architecture with a
small training dataset. We make several improvements on the GP-NAS model to
make it share the advantage of ensemble learning methods. Our method ranks
second in the CVPR2022 second lightweight NAS challenge performance prediction
track.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Earthquake Magnitude and b value prediction model using Extreme Learning
  Machine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gunbir Singh Baveja, Jaspreet Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Earthquake prediction has been a challenging research area for many decades,
where the future occurrence of this highly uncertain calamity is predicted. In
this paper, several parametric and non-parametric features were calculated,
where the non-parametric features were calculated using the parametric
features. $8$ seismic features were calculated using Gutenberg-Richter law, the
total recurrence, and the seismic energy release. Additionally, criterions such
as Maximum Relevance and Maximum Redundancy were applied to choose the
pertinent features. These features along with others were used as input for an
Extreme Learning Machine (ELM) Regression Model. Magnitude and time data of $5$
decades from the Assam-Guwahati region were used to create this model for
magnitude prediction. The Testing Accuracy and Testing Speed were computed
taking the Root Mean Squared Error (RMSE) as the parameter for evaluating the
mode. As confirmed by the results, ELM shows better scalability with much
faster training and testing speed (up to a thousand times faster) than
traditional Support Vector Machines. The testing RMSE came out to be around
$0.097$. To further test the model's robustness -- magnitude-time data from
California was used to calculate the seismic indicators which were then fed
into an ELM and then tested on the Assam-Guwahati region. The model proves to
be robust and can be implemented in early warning systems as it continues to be
a major part of Disaster Response and management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 13 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Modular Machine Learning Solution Development: Benefits and
  Trade-offs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samiyuru Menik, Lakshmish Ramaswamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning technologies have demonstrated immense capabilities in
various domains. They play a key role in the success of modern businesses.
However, adoption of machine learning technologies has a lot of untouched
potential. Cost of developing custom machine learning solutions that solve
unique business problems is a major inhibitor to far-reaching adoption of
machine learning technologies. We recognize that the monolithic nature
prevalent in today's machine learning applications stands in the way of
efficient and cost effective customized machine learning solution development.
In this work we explore the benefits of modular machine learning solutions and
discuss how modular machine learning solutions can overcome some of the major
solution engineering limitations of monolithic machine learning solutions. We
analyze the trade-offs between modular and monolithic machine learning
solutions through three deep learning problems; one text based and the two
image based. Our experimental results show that modular machine learning
solutions have a promising potential to reap the solution engineering
advantages of modularity while gaining performance and data advantages in a way
the monolithic machine learning solutions do not permit.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Topological Understanding of Neural Networks, a <span class="highlight-title">survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tushar Pandey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We look at the internal structure of neural networks which is usually treated
as a black box. The easiest and the most comprehensible thing to do is to look
at a binary classification and try to understand the approach a neural network
takes. We review the significance of different activation functions, types of
network architectures associated to them, and some empirical data. We find some
interesting observations and a possibility to build upon the ideas to verify
the process for real datasets. We suggest some possible experiments to look
forward to in three different directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Literature Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DODEM: DOuble DEfense Mechanism Against Adversarial Attacks Towards
  Secure Industrial Internet of Things Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Onat Gungor, Tajana Rosing, Baris Aksanli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Industrial Internet of Things (I-IoT) is a collaboration of devices, sensors,
and networking equipment to monitor and collect data from industrial
operations. Machine learning (ML) methods use this data to make high-level
decisions with minimal human intervention. Data-driven predictive maintenance
(PDM) is a crucial ML-based I-IoT application to find an optimal maintenance
schedule for industrial assets. The performance of these ML methods can
seriously be threatened by adversarial attacks where an adversary crafts
perturbed data and sends it to the ML model to deteriorate its prediction
performance. The models should be able to stay robust against these attacks
where robustness is measured by how much perturbation in input data affects
model performance. Hence, there is a need for effective defense mechanisms that
can protect these models against adversarial attacks. In this work, we propose
a double defense mechanism to detect and mitigate adversarial attacks in I-IoT
environments. We first detect if there is an adversarial attack on a given
sample using novelty detection algorithms. Then, based on the outcome of our
algorithm, marking an instance as attack or normal, we select adversarial
retraining or standard training to provide a secondary defense layer. If there
is an attack, adversarial retraining provides a more robust model, while we
apply standard training for regular samples. Since we may not know if an attack
will take place, our adaptive mechanism allows us to consider irregular changes
in data. The results show that our double defense strategy is highly efficient
where we can improve model robustness by up to 64.6% and 52% compared to
standard and adversarial retraining, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Topological Structure is Predictive of Deep Neural Network Success in
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Griffin, Trevor Karn, Benjamin Apple
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning has become a fundamental tool in modern science, yet its
limitations are still not fully understood. Using a simple children's game, we
show that the topological structure of the underlying training data can have a
dramatic effect on the ability of a deep neural network (DNN) classifier to
learn to classify data. We then take insights obtained from this toy model and
apply them to two physical data sets (one from particle physics and one from
acoustics), which are known to be amenable to classification by DNN's. We show
that the simplicity in their topological structure explains the majority of the
DNN's ability to operate on these data sets by showing that fully interpretable
topological classifiers are able to perform nearly as well as their DNN
counterparts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Backdoor Attacks in Peer-to-Peer Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gokberk Yar, Cristina Nita-Rotaru, Alina Oprea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study backdoor attacks in peer-to-peer federated learning systems on
different graph topologies and datasets. We show that only 5% attacker nodes
are sufficient to perform a backdoor attack with 42% attack success without
decreasing the accuracy on clean data by more than 2%. We also demonstrate that
the attack can be amplified by the attacker crashing a small number of nodes.
We evaluate defenses proposed in the context of centralized federated learning
and show they are ineffective in peer-to-peer settings. Finally, we propose a
defense that mitigates the attacks by applying different clipping norms to the
model updates received from peers and local model trained by a node.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-term stable Electromyography classification using Canonical
  Correlation Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elisa Donati, Simone Benatti, Enea Ceolini, Giacomo Indiveri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discrimination of hand gestures based on the decoding of surface
electromyography (sEMG) signals is a well-establish approach for controlling
prosthetic devices and for Human-Machine Interfaces (HMI). However, despite the
promising results achieved by this approach in well-controlled experimental
conditions, its deployment in long-term real-world application scenarios is
still hindered by several challenges. One of the most critical challenges is
maintaining high EMG data classification performance across multiple days
without retraining the decoding system. The drop in performance is mostly due
to the high EMG variability caused by electrodes shift, muscle artifacts,
fatigue, user adaptation, or skin-electrode interfacing issues. Here we propose
a novel statistical method based on canonical correlation analysis (CCA) that
stabilizes EMG classification performance across multiple days for long-term
control of prosthetic devices. We show how CCA can dramatically decrease the
performance drop of standard classifiers observed across days, by maximizing
the correlation among multiple-day acquisition data sets. Our results show how
the performance of a classifier trained on EMG data acquired only of the first
day of the experiment maintains 90% relative accuracy across multiple days,
compensating for the EMG data variability that occurs over long-term periods,
using the CCA transformation on data obtained from a small number of gestures.
This approach eliminates the need for large data sets and multiple or periodic
training sessions, which currently hamper the usability of conventional pattern
recognition based approaches
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-tail Detection with Effective Class-Margins <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jang Hyun Cho, Philipp Krähenbühl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale object detection and instance segmentation face a severe data
imbalance. The finer-grained object classes become, the less frequent they
appear in our datasets. However, at test-time, we expect a detector that
performs well for all classes and not just the most frequent ones. In this
paper, we provide a theoretical understanding of the long-trail detection
problem. We show how the commonly used mean average precision evaluation metric
on an unknown test set is bound by a margin-based binary classification error
on a long-tailed object detection training set. We optimize margin-based binary
classification error with a novel surrogate objective called \textbf{Effective
Class-Margin Loss} (ECM). The ECM loss is simple, theoretically well-motivated,
and outperforms other heuristic counterparts on LVIS v1 benchmark over a wide
range of architecture and detectors. Code is available at
\url{https://github.com/janghyuncho/ECM-Loss}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022 Oral. Code is available at
  https://github.com/janghyuncho/ECM-Loss</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PRIMEQA: The Prime Repository for State-of-the-Art MultilingualQuestion
  Answering Research and Development 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avirup Sil, Jaydeep Sen, Bhavani Iyer, Martin Franz, Kshitij Fadnis, Mihaela Bornea, Sara Rosenthal, Scott McCarley, Rong Zhang, Vishwajeet Kumar, Yulong Li, Md Arafat Sultan, Riyaz Bhat, Radu Florian, Salim Roukos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of Question Answering (QA) has made remarkable progress in recent
years, thanks to the advent of large pre-trained language models, newer
realistic benchmark datasets with leaderboards, and novel algorithms for key
components such as retrievers and readers. In this paper, we introduce PRIMEQA:
a one-stop and open-source QA repository with an aim to democratize QA
re-search and facilitate easy replication of state-of-the-art (SOTA) QA
methods. PRIMEQA supports core QA functionalities like retrieval and reading
comprehension as well as auxiliary capabilities such as question generation.It
has been designed as an end-to-end toolkit for various use cases: building
front-end applications, replicating SOTA methods on pub-lic benchmarks, and
expanding pre-existing methods. PRIMEQA is available at :
https://github.com/primeqa.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On The Convergence Of Policy Iteration-Based Reinforcement Learning With
  Monte Carlo Policy Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Winnicki, R. Srikant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A common technique in reinforcement learning is to evaluate the value
function from Monte Carlo simulations of a given policy, and use the estimated
value function to obtain a new policy which is greedy with respect to the
estimated value function. A well-known longstanding open problem in this
context is to prove the convergence of such a scheme when the value function of
a policy is estimated from data collected from a single sample path obtained
from implementing the policy (see page 99 of [Sutton and Barto, 2018], page 8
of [Tsitsiklis, 2002]). We present a solution to the open problem by showing
that a first-visit version of such a policy iteration scheme indeed converges
to the optimal policy provided that the policy improvement step uses lookahead
[Silver et al., 2016, Mnih et al., 2016, Silver et al., 2017b] rather than a
simple greedy policy improvement. We provide results both for the original open
problem in the tabular setting and also present extensions to the function
approximation setting, where we show that the policy resulting from the
algorithm performs close to the optimal policy within a function approximation
error.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two-Stage Learning For the Flexible Job Shop Scheduling Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenbo Chen, Reem Khir, Pascal Van Hentenryck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Flexible Job-shop Scheduling Problem (FJSP) is an important combinatorial
optimization problem that arises in manufacturing and service settings. FJSP is
composed of two subproblems, an assignment problem that assigns tasks to
machines, and a scheduling problem that determines the starting times of tasks
on their chosen machines. Solving FJSP instances of realistic size and
composition is an ongoing challenge even under simplified, deterministic
assumptions. Motivated by the inevitable randomness and uncertainties in supply
chains, manufacturing, and service operations, this paper investigates the
potential of using a deep learning framework to generate fast and accurate
approximations for FJSP. In particular, this paper proposes a two-stage
learning framework 2SLFJSP that explicitly models the hierarchical nature of
FJSP decisions, uses a confidence-aware branching scheme to generate
appropriate instances for the scheduling stage from the assignment predictions
and leverages a novel symmetry-breaking formulation to improve learnability.
2SL-FJSP is evaluated on instances from the FJSP benchmark library. Results
show that 2SL-FJSP can generate high-quality solutions in milliseconds,
outperforming a state-of-the-art reinforcement learning approach recently
proposed in the literature, and other heuristics commonly used in practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Illumination Variation Correction Using Image Synthesis For Unsupervised
  Domain Adaptive Person Re-Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Guo, Amy R. Reibman, Edward J. Delp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised domain adaptive (UDA) person re-identification (re-ID) aims to
learn identity information from labeled images in source domains and apply it
to unlabeled images in a target domain. One major issue with many unsupervised
re-identification methods is that they do not perform well relative to large
domain variations such as illumination, viewpoint, and occlusions. In this
paper, we propose a Synthesis Model Bank (SMB) to deal with illumination
variation in unsupervised person re-ID. The proposed SMB consists of several
convolutional neural networks (CNN) for feature extraction and Mahalanobis
matrices for distance metrics. They are trained using synthetic data with
different illumination conditions such that their synergistic effect makes the
SMB robust against illumination variation. To better quantify the illumination
intensity and improve the quality of synthetic images, we introduce a new 3D
virtual-human dataset for GAN-based image synthesis. From our experiments, the
proposed SMB outperforms other synthesis methods on several re-ID benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing the Noise in <span class="highlight-title">Self-Supervised</span> Learning: from Importance
  Sampling to Noise-Contrastive Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Chehab, Alexandre Gramfort, Aapo Hyvarinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning is an increasingly popular approach to unsupervised
learning, achieving state-of-the-art results. A prevalent approach consists in
contrasting data points and noise points within a classification task: this
requires a good noise distribution which is notoriously hard to specify. While
a comprehensive theory is missing, it is widely assumed that the optimal noise
distribution should in practice be made equal to the data distribution, as in
Generative Adversarial Networks (GANs). We here empirically and theoretically
challenge this assumption. We turn to Noise-Contrastive Estimation (NCE) which
grounds this self-supervised task as an estimation problem of an energy-based
model of the data. This ties the optimality of the noise distribution to the
sample efficiency of the estimator, which is rigorously defined as its
asymptotic variance, or mean-squared error. In the special case where the
normalization constant only is unknown, we show that NCE recovers a family of
Importance Sampling estimators for which the optimal noise is indeed equal to
the data distribution. However, in the general case where the energy is also
unknown, we prove that the optimal noise density is the data density multiplied
by a correction term based on the Fisher score. In particular, the optimal
noise distribution is different from the data distribution, and is even from a
different family. Nevertheless, we soberly conclude that the optimal noise may
be hard to sample from, and the gain in efficiency can be modest compared to
choosing the noise distribution equal to the data's.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2203.01110</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Neural Networks for Decentralized Multi-Agent Perimeter Defense 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elijah S. Lee, Lifeng Zhou, Alejandro Ribeiro, Vijay Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we study the problem of decentralized multi-agent perimeter
defense that asks for computing actions for defenders with local perceptions
and communications to maximize the capture of intruders. One major challenge
for practical implementations is to make perimeter defense strategies scalable
for large-scale problem instances. To this end, we leverage graph neural
networks (GNNs) to develop an imitation learning framework that learns a
mapping from defenders' local perceptions and their communication graph to
their actions. The proposed GNN-based learning network is trained by imitating
a centralized expert algorithm such that the learned actions are close to that
generated by the expert algorithm. We demonstrate that our proposed network
performs closer to the expert algorithm and is superior to other baseline
algorithms by capturing more intruders. Our GNN-based network is trained at a
small scale and can be generalized to large-scale cases. We run perimeter
defense games in scenarios with different team sizes and configurations to
demonstrate the performance of the learned network.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 10 figures. Published in Frontiers in Control Engineering
  2023. arXiv admin note: substantial text overlap with arXiv:2211.01757</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Noisy Parallel Data Alignment <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruoyu Xie, Antonios Anastasopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An ongoing challenge in current natural language processing is how its major
advancements tend to disproportionately favor resource-rich languages, leaving
a significant number of under-resourced languages behind. Due to the lack of
resources required to train and evaluate models, most modern language
technologies are either nonexistent or unreliable to process endangered, local,
and non-standardized languages. Optical character recognition (OCR) is often
used to convert endangered language documents into machine-readable data.
However, such OCR output is typically noisy, and most word alignment models are
not built to work under such noisy conditions. In this work, we study the
existing word-level alignment models under noisy settings and aim to make them
more robust to noisy data. Our noise simulation and structural biasing method,
tested on multiple language pairs, manages to reduce the alignment error rate
on a state-of-the-art neural-based alignment model up to 59.6%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in EACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum Heavy-tailed Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulian Wu, Chaowen Guan, Vaneet Aggarwal, Di Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study multi-armed bandits (MAB) and stochastic linear
bandits (SLB) with heavy-tailed rewards and quantum reward oracle. Unlike the
previous work on quantum bandits that assumes bounded/sub-Gaussian
distributions for rewards, here we investigate the quantum bandits problem
under a weaker assumption that the distributions of rewards only have bounded
$(1+v)$-th moment for some $v\in (0,1]$. In order to achieve regret
improvements for heavy-tailed bandits, we first propose a new quantum mean
estimator for heavy-tailed distributions, which is based on the Quantum Monte
Carlo Mean Estimator and achieves a quadratic improvement of estimation error
compared to the classical one. Based on our quantum mean estimator, we focus on
quantum heavy-tailed MAB and SLB and propose quantum algorithms based on the
Upper Confidence Bound (UCB) framework for both problems with
$\Tilde{O}(T^{\frac{1-v}{1+v}})$ regrets, polynomially improving the dependence
in terms of $T$ as compared to classical (near) optimal regrets of
$\Tilde{O}(T^{\frac{1}{1+v}})$, where $T$ is the number of rounds. Finally,
experiments also support our theoretical results and show the effectiveness of
our proposed methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Online learning; Quantum machine learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flexible conditional density estimation for time series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustavo Grivol, Rafael Izbicki, Alex A. Okuno, Rafael B. Stern
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces FlexCodeTS, a new conditional density estimator for
time series. FlexCodeTS is a flexible nonparametric conditional density
estimator, which can be based on an arbitrary regression method. It is shown
that FlexCodeTS inherits the rate of convergence of the chosen regression
method. Hence, FlexCodeTS can adapt its convergence by employing the regression
method that best fits the structure of data. From an empirical perspective,
FlexCodeTS is compared to NNKCDE and GARCH in both simulated and real data.
FlexCodeTS is shown to generally obtain the best performance among the selected
methods according to either the CDE loss or the pinball loss.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selective Explanations: Leveraging Human Input to Align Explainable AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivian Lai, Yiming Zhang, Chacha Chen, Q. Vera Liao, Chenhao Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While a vast collection of explainable AI (XAI) algorithms have been
developed in recent years, they are often criticized for significant gaps with
how humans produce and consume explanations. As a result, current XAI
techniques are often found to be hard to use and lack effectiveness. In this
work, we attempt to close these gaps by making AI explanations selective -- a
fundamental property of human explanations -- by selectively presenting a
subset from a large set of model reasons based on what aligns with the
recipient's preferences. We propose a general framework for generating
selective explanations by leveraging human input on a small sample. This
framework opens up a rich design space that accounts for different selectivity
goals, types of input, and more. As a showcase, we use a decision-support task
to explore selective explanations based on what the decision-maker would
consider relevant to the decision task. We conducted two experimental studies
to examine three out of a broader possible set of paradigms based on our
proposed framework: in Study 1, we ask the participants to provide their own
input to generate selective explanations, with either open-ended or
critique-based input. In Study 2, we show participants selective explanations
based on input from a panel of similar users (annotators). Our experiments
demonstrate the promise of selective explanations in reducing over-reliance on
AI and improving decision outcomes and subjective perceptions of the AI, but
also paint a nuanced picture that attributes some of these positive effects to
the opportunity to provide one's own input to augment AI explanations. Overall,
our work proposes a novel XAI framework inspired by human communication
behaviors and demonstrates its potentials to encourage future work to better
align AI explanations with human production and consumption of explanations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 25 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tracking the industrial growth of modern China with high-resolution
  panchromatic imagery: A sequential convolutional approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Brewer, Zhonghui Lv, Dan Runfola
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to insufficient or difficult to obtain data on development in
inaccessible regions, remote sensing data is an important tool for interested
stakeholders to collect information on economic growth. To date, no studies
have utilized deep learning to estimate industrial growth at the level of
individual sites. In this study, we harness high-resolution panchromatic
imagery to estimate development over time at 419 industrial sites in the
People's Republic of China using a multi-tier computer vision framework. We
present two methods for approximating development: (1) structural area coverage
estimated through a Mask R-CNN segmentation algorithm, and (2) imputing
development directly with visible & infrared radiance from the Visible Infrared
Imaging Radiometer Suite (VIIRS). Labels generated from these methods are
comparatively evaluated and tested. On a dataset of 2,078 50 cm resolution
images spanning 19 years, the results indicate that two dimensions of
industrial development can be estimated using high-resolution daytime imagery,
including (a) the total square meters of industrial development (average error
of 0.021 $\textrm{km}^2$), and (b) the radiance of lights (average error of 9.8
$\mathrm{\frac{nW}{cm^{2}sr}}$). Trend analysis of the techniques reveal
estimates from a Mask R-CNN-labeled CNN-LSTM track ground truth measurements
most closely. The Mask R-CNN estimates positive growth at every site from the
oldest image to the most recent, with an average change of 4,084
$\textrm{m}^2$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Augmenting <span class="highlight-title">Pre-train</span>ed Language Models with QA-Memory for Open-Domain
  Question Answering <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.04581v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.04581v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhu Chen, Pat Verga, Michiel de Jong, John Wieting, William Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval augmented language models have recently become the standard for
knowledge intensive tasks. Rather than relying purely on latent semantics
within the parameters of large neural models, these methods enlist a
semi-parametric memory to encode an index of knowledge for the model to
retrieve over. Most prior work has employed text passages as the unit of
knowledge, which has high coverage at the cost of interpretability,
controllability, and efficiency. The opposite properties arise in other methods
which have instead relied on knowledge base (KB) facts. At the same time, more
recent work has demonstrated the effectiveness of storing and retrieving from
an index of Q-A pairs derived from text \citep{lewis2021paq}. This approach
yields a high coverage knowledge representation that maintains KB-like
properties due to its representations being more atomic units of information.
In this work we push this line of research further by proposing a
question-answer augmented encoder-decoder model and accompanying pretraining
strategy. This yields an end-to-end system that not only outperforms prior QA
retrieval methods on single-hop QA tasks but also enables compositional
reasoning, as demonstrated by strong performance on two multi-hop QA datasets.
Together, these methods improve the ability to interpret and control the model
while narrowing the performance gap with passage retrieval systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linear Connectivity Reveals Generalization Strategies <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.12411v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.12411v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeevesh Juneja, Rachit Bansal, Kyunghyun Cho, João Sedoc, Naomi Saphra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is widely accepted in the mode connectivity literature that when two
neural networks are trained similarly on the same data, they are connected by a
path through parameter space over which test set accuracy is maintained. Under
some circumstances, including transfer learning from pretrained models, these
paths are presumed to be linear. In contrast to existing results, we find that
among text classifiers (trained on MNLI, QQP, and CoLA), some pairs of
finetuned models have large barriers of increasing loss on the linear paths
between them. On each task, we find distinct clusters of models which are
linearly connected on the test loss surface, but are disconnected from models
outside the cluster -- models that occupy separate basins on the surface. By
measuring performance on specially-crafted diagnostic datasets, we find that
these clusters correspond to different generalization strategies: one cluster
behaves like a bag of words model under domain shift, while another cluster
uses syntactic heuristics. Our work demonstrates how the geometry of the loss
surface can guide models towards different heuristic functions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Publushed as a conference paper at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computationally-efficient initialisation of GPs: The generalised
  variogram method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.05394v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.05394v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felipe Tobar, Elsa Cazelles, Taco de Wolff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a computationally-efficient strategy to find the hyperparameters
of a Gaussian process (GP) avoiding the computation of the likelihood function.
The found hyperparameters can then be used directly for regression or passed as
initial conditions to maximum-likelihood (ML) training. Motivated by the fact
that training a GP via ML is equivalent (on average) to minimising the
KL-divergence between the true and learnt model, we set to explore different
metrics/divergences among GPs that are computationally inexpensive and provide
estimates close to those of ML. In particular, we identify the GP
hyperparameters by projecting the empirical covariance or (Fourier) power
spectrum onto a parametric family, thus proposing and studying various measures
of discrepancy operating on the temporal or frequency domains. Our contribution
extends the Variogram method developed by the geostatistics literature and,
accordingly, it is referred to as the Generalised Variogram method (GVM). In
addition to the theoretical presentation of GVM, we provide experimental
validation in terms of accuracy, consistency with ML and computational
complexity for different kernels using synthetic and real-world data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Kepler to Newton: Explainable AI for Science <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.12210v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.12210v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zelong Li, Jianchao Ji, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Observation--Hypothesis--Prediction--Experimentation loop paradigm for
scientific research has been practiced by researchers for years towards
scientific discoveries. However, with data explosion in both mega-scale and
milli-scale scientific research, it has been sometimes very difficult to
manually analyze the data and propose new hypotheses to drive the cycle for
scientific discovery. In this paper, we discuss the role of Explainable AI in
scientific discovery process by demonstrating an Explainable AI-based paradigm
for science discovery. The key is to use Explainable AI to help derive data or
model interpretations, hypotheses, as well as scientific discoveries or
insights. We show how computational and data-intensive methodology -- together
with experimental and theoretical methodology -- can be seamlessly integrated
for scientific research. To demonstrate the AI-based science discovery process,
and to pay our respect to some of the greatest minds in human history, we show
how Kepler's laws of planetary motion and Newton's law of universal gravitation
can be rediscovered by (Explainable) AI based on Tycho Brahe's astronomical
observation data, whose works were leading the scientific revolution in the
16-17th century. This work also highlights the important role of Explainable AI
(as compared to Blackbox AI) in science discovery to help humans prevent or
better prepare for the possible technological singularity that may happen in
the future, since science is not only about the know how, but also the know
why. Presentation of the work is available at
https://slideslive.com/38986142/from-kepler-to-newton-explainable-ai-for-science-discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML-AI4Science 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Short Blocklength Wiretap Channel Codes via Deep Learning: Design and
  Performance Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.03477v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.03477v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vidhi Rana, Remi A. Chou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We design short blocklength codes for the Gaussian wiretap channel under
information-theoretic security guarantees. Our approach consists in decoupling
the reliability and secrecy constraints in our code design. Specifically, we
handle the reliability constraint via an autoencoder, and handle the secrecy
constraint with hash functions. For blocklengths smaller than or equal to 128,
we evaluate through simulations the probability of error at the legitimate
receiver and the leakage at the eavesdropper for our code construction. This
leakage is defined as the mutual information between the confidential message
and the eavesdropper's channel observations, and is empirically measured via a
neural network-based mutual information estimator. Our simulation results
provide examples of codes with positive secrecy rates that outperform the best
known achievable secrecy rates obtained non-constructively for the Gaussian
wiretap channel. Additionally, we show that our code design is suitable for the
compound and arbitrarily varying Gaussian wiretap channels, for which the
channel statistics are not perfectly known but only known to belong to a
pre-specified uncertainty set. These models not only capture uncertainty
related to channel statistics estimation, but also scenarios where the
eavesdropper jams the legitimate transmission or influences its own channel
statistics by changing its location.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predictive Model for Gross Community Production Rate of Coral Reefs
  using Ensemble Learning Methodologies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.04003v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.04003v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Umanandini S, Rishivardhan M, Aouthithiye Barathwaj SR Y, Jasline Augusta J, Shrirang Sapate, Reenasree S, Vigneash M
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coral reefs play a vital role in maintaining the ecological balance of the
marine ecosystem. Various marine organisms depend on coral reefs for their
existence and their natural processes. Coral reefs provide the necessary
habitat for reproduction and growth for various exotic species of the marine
ecosystem. In this article, we discuss the most important parameters which
influence the lifecycle of coral and coral reefs such as ocean acidification,
deoxygenation and other physical parameters such as flow rate and surface area.
Ocean acidification depends on the amount of dissolved Carbon dioxide (CO2).
This is due to the release of H+ ions upon the reaction of the dissolved CO2
gases with the calcium carbonate compounds in the ocean. Deoxygenation is
another problem that leads to hypoxia which is characterized by a lesser amount
of dissolved oxygen in water than the required amount for the existence of
marine organisms. In this article, we highlight the importance of physical
parameters such as flow rate which influence gas exchange, heat dissipation,
bleaching sensitivity, nutrient supply, feeding, waste and sediment removal,
growth and reproduction. In this paper, we also bring out these important
parameters and propose an ensemble machine learning-based model for analyzing
these parameters and provide better rates that can help us to understand and
suitably improve the ocean composition which in turn can eminently improve the
sustainability of the marine ecosystem, mainly the coral reefs
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sustaining Fairness via Incremental Learning <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.12212v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.12212v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Somnath Basu Roy Chowdhury, Snigdha Chaturvedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning systems are often deployed for making critical decisions
like credit lending, hiring, etc. While making decisions, such systems often
encode the user's demographic information (like gender, age) in their
intermediate representations. This can lead to decisions that are biased
towards specific demographics. Prior work has focused on debiasing intermediate
representations to ensure fair decisions. However, these approaches fail to
remain fair with changes in the task or demographic distribution. To ensure
fairness in the wild, it is important for a system to adapt to such changes as
it accesses new data in an incremental fashion. In this work, we propose to
address this issue by introducing the problem of learning fair representations
in an incremental learning setting. To this end, we present Fairness-aware
Incremental Representation Learning (FaIRL), a representation learning system
that can sustain fairness while incrementally learning new tasks. FaIRL is able
to achieve fairness and learn new tasks by controlling the rate-distortion
function of the learned representations. Our empirical evaluations show that
FaIRL is able to make fair decisions while achieving high performance on the
target task, outperforming several baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Gradient Inversion Attacks Make Federated Learning Unsafe? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.06924v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.06924v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Hatamizadeh, Hongxu Yin, Pavlo Molchanov, Andriy Myronenko, Wenqi Li, Prerna Dogra, Andrew Feng, Mona G. Flores, Jan Kautz, Daguang Xu, Holger R. Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) allows the collaborative training of AI models
without needing to share raw data. This capability makes it especially
interesting for healthcare applications where patient and data privacy is of
utmost concern. However, recent works on the inversion of deep neural networks
from model gradients raised concerns about the security of FL in preventing the
leakage of training data. In this work, we show that these attacks presented in
the literature are impractical in FL use-cases where the clients' training
involves updating the Batch Normalization (BN) statistics and provide a new
baseline attack that works for such scenarios. Furthermore, we present new ways
to measure and visualize potential data leakage in FL. Our work is a step
towards establishing reproducible methods of measuring data leakage in FL and
could help determine the optimal tradeoffs between privacy-preserving
techniques, such as differential privacy, and model accuracy based on
quantifiable metrics.
  Code is available at
https://nvidia.github.io/NVFlare/research/quantifying-data-leakage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Revised version; Accepted to IEEE Transactions on Medical Imaging;
  Improved and reformatted version of
  https://www.researchsquare.com/article/rs-1147182/v2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainable Image Quality Assessments in Teledermatological Photography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.04699v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.04699v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raluca Jalaboi, Ole Winther, Alfiia Galimzianova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image quality is a crucial factor in the effectiveness and efficiency of
teledermatological consultations. However, up to 50% of images sent by patients
have quality issues, thus increasing the time to diagnosis and treatment. An
automated, easily deployable, explainable method for assessing image quality is
necessary to improve the current teledermatological consultation flow. We
introduce ImageQX, a convolutional neural network for image quality assessment
with a learning mechanism for identifying the most common poor image quality
explanations: bad framing, bad lighting, blur, low resolution, and distance
issues. ImageQX was trained on 26,635 photographs and validated on 9,874
photographs, each annotated with image quality labels and poor image quality
explanations by up to 12 board-certified dermatologists. The photographic
images were taken between 2017 and 2019 using a mobile skin disease tracking
application accessible worldwide. Our method achieves expert-level performance
for both image quality assessment and poor image quality explanation. For image
quality assessment, ImageQX obtains a macro F1-score of 0.73 +- 0.01, which
places it within standard deviation of the pairwise inter-rater F1-score of
0.77 +- 0.07. For poor image quality explanations, our method obtains F1-scores
of between 0.37 +- 0.01 and 0.70 +- 0.01, similar to the inter-rater pairwise
F1-score of between 0.24 +- 0.15 and 0.83 +- 0.06. Moreover, with a size of
only 15 MB, ImageQX is easily deployable on mobile devices. With an image
quality detection performance similar to that of dermatologists, incorporating
ImageQX into the teledermatology flow can enable a better, faster flow for
remote consultations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Telemedicine and eHealth Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convergence and Implicit Regularization Properties of Gradient Descent
  for Deep Residual Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.07261v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.07261v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rama Cont, Alain Rossier, RenYuan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We prove linear convergence of gradient descent to a global optimum for the
training of deep residual networks with constant layer width and smooth
activation function. We show that if the trained weights, as a function of the
layer index, admit a scaling limit as the depth increases, then the limit has
finite $p-$variation with $p=2$. Proofs are based on non-asymptotic estimates
for the loss function and for norms of the network weights along the gradient
descent path. We illustrate the relevance of our theoretical results to
practical settings using detailed numerical experiments on supervised learning
problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning from Long-Tailed Noisy Data with Sample Selection and Balanced
  Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10906v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10906v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lefan Zhang, Zhang-Hao Tian, Wujun Zhou, Wei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of deep learning depends on large-scale and well-curated training
data, while data in real-world applications are commonly long-tailed and noisy.
Many methods have been proposed to deal with long-tailed data or noisy data,
while a few methods are developed to tackle long-tailed noisy data. To solve
this, we propose a robust method for learning from long-tailed noisy data with
sample selection and balanced loss. Specifically, we separate the noisy
training data into clean labeled set and unlabeled set with sample selection,
and train the deep neural network in a semi-supervised manner with a balanced
loss based on model bias. Extensive experiments on benchmarks demonstrate that
our method outperforms existing state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autoencoding Hyperbolic Representation for Adversarial Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.12825v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.12825v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Qu, Dongmian Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the recent advance of geometric deep learning, neural networks have been
extensively used for data in non-Euclidean domains. In particular, hyperbolic
neural networks have proved successful in processing hierarchical information
of data. However, many hyperbolic neural networks are numerically unstable
during training, which precludes using complex architectures. This crucial
problem makes it difficult to build hyperbolic generative models for real and
complex data. In this work, we propose a hyperbolic generative network in which
we design novel architecture and layers to improve stability in training. Our
proposed network contains three parts: first, a hyperbolic autoencoder (AE)
that produces hyperbolic embedding for input data; second, a hyperbolic
generative adversarial network (GAN) for generating the hyperbolic latent
embedding of the AE from simple noise; third, a generator that inherits the
decoder from the AE and the generator from the GAN. We call this network the
hyperbolic AE-GAN, or HAEGAN for short. The architecture of HAEGAN fosters
expressive representation in the hyperbolic space, and the specific design of
layers ensures numerical stability. Experiments show that HAEGAN is able to
generate complex data with state-of-the-art structure-related performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Concept-level Debugging of Part-Prototype Networks <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.15769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.15769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Bontempelli, Stefano Teso, Katya Tentori, Fausto Giunchiglia, Andrea Passerini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Part-prototype Networks (ProtoPNets) are concept-based classifiers designed
to achieve the same performance as black-box models without compromising
transparency. ProtoPNets compute predictions based on similarity to
class-specific part-prototypes learned to recognize parts of training examples,
making it easy to faithfully determine what examples are responsible for any
target prediction and why. However, like other models, they are prone to
picking up confounders and shortcuts from the data, thus suffering from
compromised prediction accuracy and limited generalization. We propose
ProtoPDebug, an effective concept-level debugger for ProtoPNets in which a
human supervisor, guided by the model's explanations, supplies feedback in the
form of what part-prototypes must be forgotten or kept, and the model is
fine-tuned to align with this supervision. Our experimental evaluation shows
that ProtoPDebug outperforms state-of-the-art debuggers for a fraction of the
annotation cost. An online experiment with laypeople confirms the simplicity of
the feedback requested to the users and the effectiveness of the collected
feedback for learning confounder-free part-prototypes. ProtoPDebug is a
promising tool for trustworthy interactive learning in critical applications,
as suggested by a preliminary evaluation on a medical decision making task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Side-Tuning for Document Classification <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07502v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07502v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Pio Zingaro, Giuseppe Lisanti, Maurizio Gabbrielli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose to exploit the side-tuning framework for multimodal
document classification. Side-tuning is a methodology for network adaptation
recently introduced to solve some of the problems related to previous
approaches. Thanks to this technique it is actually possible to overcome model
rigidity and catastrophic forgetting of transfer learning by fine-tuning. The
proposed solution uses off-the-shelf deep learning architectures leveraging the
side-tuning framework to combine a base model with a tandem of two side
networks. We show that side-tuning can be successfully employed also when
different data sources are considered, e.g. text and images in document
classification. The experimental results show that this approach pushes further
the limit for document classification accuracy with respect to the state of the
art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2020 25th International Conference on Pattern Recognition (ICPR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Particle algorithms for maximum likelihood training of latent variable
  models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.12965v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.12965v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Kuntz, Jen Ning Lim, Adam M. Johansen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  (Neal and Hinton, 1998) recast maximum likelihood estimation of any given
latent variable model as the minimization of a free energy functional $F$, and
the EM algorithm as coordinate descent applied to $F$. Here, we explore
alternative ways to optimize the functional. In particular, we identify various
gradient flows associated with $F$ and show that their limits coincide with
$F$'s stationary points. By discretizing the flows, we obtain practical
particle-based algorithms for maximum likelihood estimation in broad classes of
latent variable models. The novel algorithms scale to high-dimensional settings
and perform well in numerical experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainable deep learning for insights in El Niño and river flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.02596v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.02596v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yumin Liu, Kate Duffy, Jennifer G. Dy, Auroop R. Ganguly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The El Ni\~no Southern Oscillation (ENSO) is a semi-periodic fluctuation in
sea surface temperature (SST) over the tropical central and eastern Pacific
Ocean that influences interannual variability in regional hydrology across the
world through long-range dependence or teleconnections. Recent research has
demonstrated the value of Deep Learning (DL) methods for improving ENSO
prediction as well as Complex Networks (CN) for understanding teleconnections.
However, gaps in predictive understanding of ENSO-driven river flows include
the black box nature of DL, the use of simple ENSO indices to describe a
complex phenomenon and translating DL-based ENSO predictions to river flow
predictions. Here we show that eXplainable DL (XDL) methods, based on saliency
maps, can extract interpretable predictive information contained in global SST
and discover SST information regions and dependence structures relevant for
river flows which, in tandem with climate network constructions, enable
improved predictive understanding. Our results reveal additional information
content in global SST beyond ENSO indices, develop understanding of how SSTs
influence river flows, and generate improved river flow prediction, including
uncertainty estimation. Observations, reanalysis data, and earth system model
simulations are used to demonstrate the value of the XDL-CN based methods for
future interannual and decadal scale climate projections.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dealing with Unknown Variances in Best-Arm Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.00974v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.00974v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Jourdan, Rémy Degenne, Emilie Kaufmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of identifying the best arm among a collection of items having
Gaussian rewards distribution is well understood when the variances are known.
Despite its practical relevance for many applications, few works studied it for
unknown variances. In this paper we introduce and analyze two approaches to
deal with unknown variances, either by plugging in the empirical variance or by
adapting the transportation costs. In order to calibrate our two stopping
rules, we derive new time-uniform concentration inequalities, which are of
independent interest. Then, we illustrate the theoretical and empirical
performances of our two sampling rule wrappers on Track-and-Stop and on a Top
Two algorithm. Moreover, by quantifying the impact on the sample complexity of
not knowing the variances, we reveal that it is rather small.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>73 pages, 5 figures, 3 tables. To be published in the 34th
  International Conference on Algorithmic Learning Theory, Singapore, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tailoring to the Tails: Risk Measures for Fine-Grained Tail Sensitivity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.03066v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.03066v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Fröhlich, Robert C. Williamson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expected risk minimization (ERM) is at the core of many machine learning
systems. This means that the risk inherent in a loss distribution is summarized
using a single number - its average. In this paper, we propose a general
approach to construct risk measures which exhibit a desired tail sensitivity
and may replace the expectation operator in ERM. Our method relies on the
specification of a reference distribution with a desired tail behaviour, which
is in a one-to-one correspondence to a coherent upper probability. Any risk
measure, which is compatible with this upper probability, displays a tail
sensitivity which is finely tuned to the reference distribution. As a concrete
example, we focus on divergence risk measures based on f-divergence ambiguity
sets, which are a widespread tool used to foster distributional robustness of
machine learning systems. For instance, we show how ambiguity sets based on the
Kullback-Leibler divergence are intricately tied to the class of subexponential
random variables. We elaborate the connection of divergence risk measures and
rearrangement invariant Banach norms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Made multiple minor edits</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Estimating average causal effects from patient trajectories <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.01228v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.01228v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Frauen, Tobias Hatt, Valentyn Melnychuk, Stefan Feuerriegel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In medical practice, treatments are selected based on the expected causal
effects on patient outcomes. Here, the gold standard for estimating causal
effects are randomized controlled trials; however, such trials are costly and
sometimes even unethical. Instead, medical practice is increasingly interested
in estimating causal effects among patient (sub)groups from electronic health
records, that is, observational data. In this paper, we aim at estimating the
average causal effect (ACE) from observational data (patient trajectories) that
are collected over time. For this, we propose DeepACE: an end-to-end deep
learning model. DeepACE leverages the iterative G-computation formula to adjust
for the bias induced by time-varying confounders. Moreover, we develop a novel
sequential targeting procedure which ensures that DeepACE has favorable
theoretical properties, i.e., is doubly robust and asymptotically efficient. To
the best of our knowledge, this is the first work that proposes an end-to-end
deep learning model tailored for estimating time-varying ACEs. We compare
DeepACE in an extensive number of experiments, confirming that it achieves
state-of-the-art performance. We further provide a case study for patients
suffering from low back pain to demonstrate that DeepACE generates important
and meaningful findings for clinical practice. Our work enables practitioners
to develop effective treatment recommendations based on population effects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantifying the Impact of Label Noise on Federated Learning <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.07816v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.07816v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuqi Ke, Chao Huang, Xin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a distributed machine learning paradigm where
clients collaboratively train a model using their local (human-generated)
datasets. While existing studies focus on FL algorithm development to tackle
data heterogeneity across clients, the important issue of data quality (e.g.,
label noise) in FL is overlooked. This paper aims to fill this gap by providing
a quantitative study on the impact of label noise on FL. We derive an upper
bound for the generalization error that is linear in the clients' label noise
level. Then we conduct experiments on MNIST and CIFAR-10 datasets using various
FL algorithms. Our empirical results show that the global model accuracy
linearly decreases as the noise level increases, which is consistent with our
theoretical analysis. We further find that label noise slows down the
convergence of FL training, and the global model tends to overfit when the
noise level is high.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The AAAI 2023 Workshop on Representation Learning for
  Responsible Human-Centric AI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prediction Errors for Penalized Regressions based on Generalized
  Approximate Message Passing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.12832v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.12832v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayaka Sakata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We discuss the prediction accuracy of assumed statistical models in terms of
prediction errors for the generalized linear model and penalized maximum
likelihood methods. We derive the forms of estimators for the prediction
errors, such as $C_p$ criterion, information criteria, and leave-one-out cross
validation (LOOCV) error, using the generalized approximate message passing
(GAMP) algorithm and replica method. These estimators coincide with each other
when the number of model parameters is sufficiently small; however, there is a
discrepancy between them in particular in the parameter region where the number
of model parameters is larger than the data dimension. In this paper, we review
the prediction errors and corresponding estimators, and discuss their
differences. In the framework of GAMP, we show that the information criteria
can be expressed by using the variance of the estimates. Further, we
demonstrate how to approach LOOCV error from the information criteria by
utilizing the expression provided by GAMP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>73 pages, 13 figures, accepted Journal of Physics A</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The European AI Liability Directives -- Critique of a Half-Hearted
  Approach and Lessons for the Future 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.13960v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.13960v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Hacker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As ChatGPT et al. conquer the world, the optimal liability framework for AI
systems remains an unsolved problem across the globe. In a much-anticipated
move, the European Commission advanced two proposals outlining the European
approach to AI liability in September 2022: a novel AI Liability Directive and
a revision of the Product Liability Directive. They constitute the final
cornerstone of EU AI regulation. Crucially, the liability proposals and the EU
AI Act are inherently intertwined: the latter does not contain any individual
rights of affected persons, and the former lack specific, substantive rules on
AI development and deployment. Taken together, these acts may well trigger a
Brussels Effect in AI regulation, with significant consequences for the US and
beyond.
  This paper makes three novel contributions. First, it examines in detail the
Commission proposals and shows that, while making steps in the right direction,
they ultimately represent a half-hearted approach: if enacted as foreseen, AI
liability in the EU will primarily rest on disclosure of evidence mechanisms
and a set of narrowly defined presumptions concerning fault, defectiveness and
causality. Hence, second, the article suggests amendments, which are collected
in an Annex at the end of the paper. Third, based on an analysis of the key
risks AI poses, the final part of the paper maps out a road for the future of
AI liability and regulation, in the EU and beyond. This includes: a
comprehensive framework for AI liability; provisions to support innovation; an
extension to non-discrimination/algorithmic fairness, as well as explainable
AI; and sustainability. I propose to jump-start sustainable AI regulation via
sustainability impact assessments in the AI Act and sustainable design defects
in the liability regime. In this way, the law may help spur not only fair AI
and XAI, but potentially also sustainable AI (SAI).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under peer-review; contains 3 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Be More Active! Understanding the Differences between Mean and Sampled
  Representations of Variational Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.12679v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.12679v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisa Bonheme, Marek Grzes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability of Variational Autoencoders to learn disentangled representations
has made them appealing for practical applications. However, their mean
representations, which are generally used for downstream tasks, have recently
been shown to be more correlated than their sampled counterpart, on which
disentanglement is usually measured. In this paper, we refine this observation
through the lens of selective posterior collapse, which states that only a
subset of the learned representations, the active variables, is encoding useful
information while the rest (the passive variables) is discarded. We first
extend the existing definition to multiple data examples and show that active
variables are equally disentangled in mean and sampled representations. Based
on this extension and the pre-trained models from disentanglement lib, we then
isolate the passive variables and show that they are responsible for the
discrepancies between mean and sampled representations. Specifically, passive
variables exhibit high correlation scores with other variables in mean
representations while being fully uncorrelated in sampled ones. We thus
conclude that despite what their higher correlation might suggest, mean
representations are still good candidates for downstream tasks applications.
However, it may be beneficial to remove their passive variables, especially
when used with models sensitive to correlated features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>the main paper of 20 pages plus an appendix; 29 pages in total</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthesis of Compositional Animations from Textual Descriptions <span class="chip">ICCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.14675v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.14675v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anindita Ghosh, Noshaba Cheema, Cennet Oguz, Christian Theobalt, Philipp Slusallek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  "How can we animate 3D-characters from a movie script or move robots by
simply telling them what we would like them to do?" "How unstructured and
complex can we make a sentence and still generate plausible movements from it?"
These are questions that need to be answered in the long-run, as the field is
still in its infancy. Inspired by these problems, we present a new technique
for generating compositional actions, which handles complex input sentences.
Our output is a 3D pose sequence depicting the actions in the input sentence.
We propose a hierarchical two-stream sequential model to explore a finer
joint-level mapping between natural language sentences and 3D pose sequences
corresponding to the given motion. We learn two manifold representations of the
motion -- one each for the upper body and the lower body movements. Our model
can generate plausible pose sequences for short sentences describing single
actions as well as long compositional sentences describing multiple sequential
and superimposed actions. We evaluate our proposed model on the publicly
available KIT Motion-Language Dataset containing 3D pose data with
human-annotated sentences. Experimental results show that our model advances
the state-of-the-art on text-based motion synthesis in objective evaluations by
a margin of 50%. Qualitative evaluations based on a user study indicate that
our synthesized motions are perceived to be the closest to the ground-truth
motion captures for both short and compositional sentences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures, 3 tables. Proceedings of the IEEE/CVF
  International Conference on Computer Vision (ICCV), 2021, pp. 1396-1406</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Talking About Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.03551v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.03551v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Murray Shanahan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thanks to rapid progress in artificial intelligence, we have entered an era
when technology and philosophy intersect in interesting ways. Sitting squarely
at the centre of this intersection are large language models (LLMs). The more
adept LLMs become at mimicking human language, the more vulnerable we become to
anthropomorphism, to seeing the systems in which they are embedded as more
human-like than they really are. This trend is amplified by the natural
tendency to use philosophically loaded terms, such as "knows", "believes", and
"thinks", when describing these systems. To mitigate this trend, this paper
advocates the practice of repeatedly stepping back to remind ourselves of how
LLMs, and the systems of which they form a part, actually work. The hope is
that increased scientific precision will encourage more philosophical nuance in
the discourse around artificial intelligence, both within the field and in the
public sphere.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geometry-Aware Supertagging with Heterogeneous Dynamic Convolutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12235v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12235v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantinos Kogkalidis, Michael Moortgat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The syntactic categories of categorial grammar formalisms are structured
units made of smaller, indivisible primitives, bound together by the underlying
grammar's category formation rules. In the trending approach of constructive
supertagging, neural models are increasingly made aware of the internal
category structure, which in turn enables them to more reliably predict rare
and out-of-vocabulary categories, with significant implications for grammars
previously deemed too complex to find practical use. In this work, we revisit
constructive supertagging from a graph-theoretic perspective, and propose a
framework based on heterogeneous dynamic graph convolutions aimed at exploiting
the distinctive structure of a supertagger's output space. We test our approach
on a number of categorial grammar datasets spanning different languages and
grammar formalisms, achieving substantial improvements over previous state of
the art scores. Code will be made available at
https://github.com/konstantinosKokos/dynamic-graph-supertagging
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages plus references, unpublished preprint v2: fixed small typos,
  added appendix with a visualization of the decoding process; v3: improved
  presentation, improved the decoding figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continuous-time identification of dynamic state-space models by deep
  subspace encoding <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.09405v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.09405v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gerben I. Beintema, Maarten Schoukens, Roland Tóth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continuous-time (CT) modeling has proven to provide improved sample
efficiency and interpretability in learning the dynamical behavior of physical
systems compared to discrete-time (DT) models. However, even with numerous
recent developments, the CT nonlinear state-space (NL-SS) model identification
problem remains to be solved in full, considering common experimental aspects
such as the presence of external inputs, measurement noise, latent states, and
general robustness. This paper presents a novel estimation method that
addresses all these aspects and that can obtain state-of-the-art results on
multiple benchmarks with compact fully connected neural networks capturing the
CT dynamics. The proposed estimation method called the subspace encoder
approach (SUBNET) ascertains these results by efficiently approximating the
complete simulation loss by evaluating short simulations on subsections of the
data, by using an encoder function to estimate the initial state for each
subsection and a novel state-derivative normalization to ensure stability and
good numerical conditioning of the training process. We prove that the use of
subsections increases cost function smoothness together with the necessary
requirements for the existence of the encoder function and we show that the
proposed state-derivative normalization is essential for reliable estimation of
CT NL-SS models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pseudo-Hamiltonian Neural Networks with State-Dependent External Forces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.02660v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.02660v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sølve Eidnes, Alexander J. Stasik, Camilla Sterud, Eivind Bøhn, Signe Riemer-Sørensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hybrid machine learning based on Hamiltonian formulations has recently been
successfully demonstrated for simple mechanical systems, both energy conserving
and not energy conserving. We introduce a pseudo-Hamiltonian formulation that
is a generalization of the Hamiltonian formulation via the port-Hamiltonian
formulation, and show that pseudo-Hamiltonian neural network models can be used
to learn external forces acting on a system. We argue that this property is
particularly useful when the external forces are state dependent, in which case
it is the pseudo-Hamiltonian structure that facilitates the separation of
internal and external forces. Numerical results are provided for a forced and
damped mass-spring system and a tank system of higher complexity, and a
symmetric fourth-order integration scheme is introduced for improved training
on sparse and noisy data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 13 figures; v4: slight title change, expanded on
  methodology for more clarity, updated plots</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beating the Best: Improving on AlphaFold2 at Protein Structure
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07568v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07568v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abbi Abdel-Rehim, Oghenejokpeme Orhobor, Hang Lou, Hao Ni, Ross D. King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of Protein Structure Prediction (PSP) problem is to predict a
protein's 3D structure (confirmation) from its amino acid sequence. The problem
has been a 'holy grail' of science since the Noble prize-winning work of
Anfinsen demonstrated that protein conformation was determined by sequence. A
recent and important step towards this goal was the development of AlphaFold2,
currently the best PSP method. AlphaFold2 is probably the highest profile
application of AI to science. Both AlphaFold2 and RoseTTAFold (another
impressive PSP method) have been published and placed in the public domain
(code & models). Stacking is a form of ensemble machine learning ML in which
multiple baseline models are first learnt, then a meta-model is learnt using
the outputs of the baseline level model to form a model that outperforms the
base models. Stacking has been successful in many applications. We developed
the ARStack PSP method by stacking AlphaFold2 and RoseTTAFold. ARStack
significantly outperforms AlphaFold2. We rigorously demonstrate this using two
sets of non-homologous proteins, and a test set of protein structures published
after that of AlphaFold2 and RoseTTAFold. As more high quality prediction
methods are published it is likely that ensemble methods will increasingly
outperform any single method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Pattern Extraction Multi-Task Learning for Multi-Step
  Conversion Estimations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.02494v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.02494v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuewen Tao, Mingming Ha, Xiaobo Guo, Qiongxu Ma, Hongwei Cheng, Wenfang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task learning (MTL) has been successfully used in many real-world
applications, which aims to simultaneously solve multiple tasks with a single
model. The general idea of multi-task learning is designing kinds of global
parameter sharing mechanism and task-specific feature extractor to improve the
performance of all tasks. However, challenge still remains in balancing the
trade-off of various tasks since model performance is sensitive to the
relationships between them. Less correlated or even conflict tasks will
deteriorate the performance by introducing unhelpful or negative information.
Therefore, it is important to efficiently exploit and learn fine-grained
feature representation corresponding to each task. In this paper, we propose an
Adaptive Pattern Extraction Multi-task (APEM) framework, which is adaptive and
flexible for large-scale industrial application. APEM is able to fully utilize
the feature information by learning the interactions between the input feature
fields and extracted corresponding tasks-specific information. We first
introduce a DeepAuto Group Transformer module to automatically and efficiently
enhance the feature expressivity with a modified set attention mechanism and a
Squeeze-and-Excitation operation. Second, explicit Pattern Selector is
introduced to further enable selectively feature representation learning by
adaptive task-indicator vectors. Empirical evaluations show that APEM
outperforms the state-of-the-art MTL methods on public and real-world financial
services datasets. More importantly, we explore the online performance of APEM
in a real industrial-level recommendation scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Max-Quantile Grouped Infinite-Arm Bandits <span class="chip">ALT 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01295v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01295v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Lau, Yan Hao Ling, Mayank Shrivastava, Jonathan Scarlett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider a bandit problem in which there are a number of
groups each consisting of infinitely many arms. Whenever a new arm is requested
from a given group, its mean reward is drawn from an unknown reservoir
distribution (different for each group), and the uncertainty in the arm's mean
reward can only be reduced via subsequent pulls of the arm. The goal is to
identify the infinite-arm group whose reservoir distribution has the highest
$(1-\alpha)$-quantile (e.g., median if $\alpha = \frac{1}{2}$), using as few
total arm pulls as possible. We introduce a two-step algorithm that first
requests a fixed number of arms from each group and then runs a finite-arm
grouped max-quantile bandit algorithm. We characterize both the
instance-dependent and worst-case regret, and provide a matching lower bound
for the latter, while discussing various strengths, weaknesses, algorithmic
improvements, and potential lower bounds associated with our instance-dependent
upper bounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ALT 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Latent State Space Models for Time-Series Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.12749v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.12749v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linqi Zhou, Michael Poli, Winnie Xu, Stefano Massaroli, Stefano Ermon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Methods based on ordinary differential equations (ODEs) are widely used to
build generative models of time-series. In addition to high computational
overhead due to explicitly computing hidden states recurrence, existing
ODE-based models fall short in learning sequence data with sharp transitions -
common in many real-world systems - due to numerical challenges during
optimization. In this work, we propose LS4, a generative model for sequences
with latent variables evolving according to a state space ODE to increase
modeling capacity. Inspired by recent deep state space models (S4), we achieve
speedups by leveraging a convolutional representation of LS4 which bypasses the
explicit evaluation of hidden states. We show that LS4 significantly
outperforms previous continuous-time generative models in terms of marginal
distribution, classification, and prediction scores on real-world datasets in
the Monash Forecasting Repository, and is capable of modeling highly stochastic
data with sharp temporal transitions. LS4 sets state-of-the-art for
continuous-time latent generative models, with significant improvement of mean
squared error and tighter variational lower bounds on irregularly-sampled
datasets, while also being x100 faster than other baselines on long sequences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Spectral Clustering Using Spectrum-Preserving Node Aggregation <span class="chip">ICPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.12328v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.12328v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spectral clustering is one of the most popular clustering methods. However,
the high computational cost due to the involved eigen-decomposition procedure
can immediately hinder its applications in large-scale tasks. In this paper we
use spectrum-preserving node reduction to accelerate eigen-decomposition and
generate concise representations of data sets. Specifically, we create a small
number of pseudonodes based on spectral similarity. Then, standard spectral
clustering algorithm is performed on the smaller node set. Finally, each data
point in the original data set is assigned to the cluster as its representative
pseudo-node. The proposed framework run in nearly-linear time. Meanwhile, the
clustering accuracy can be significantly improved by mining concise
representations. The experimental results show dramatically improved clustering
performance when compared with state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Dataset</span> Structural Index: Leveraging a machine's perspective towards
  visual data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.04070v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.04070v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dishant Parikh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With advances in vision and perception architectures, we have realized that
working with data is equally crucial, if not more, than the algorithms. Till
today, we have trained machines based on our knowledge and perspective of the
world. The entire concept of Dataset Structural Index(DSI) revolves around
understanding a machine`s perspective of the dataset. With DSI, I show two meta
values with which we can get more information over a visual dataset and use it
to optimize data, create better architectures, and have an ability to guess
which model would work best. These two values are the Variety contribution
ratio and Similarity matrix. In the paper, I show many applications of DSI, one
of which is how the same level of accuracy can be achieved with the same model
architectures trained over less amount of data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Explainable-AI approach for Diagnosis of COVID-19 using MALDI-ToF
  Mass Spectrometry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.14099v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.14099v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Venkata Devesh Reddy Seethi, Zane LaCasse, Prajkta Chivte, Joshua Bland, Shrihari S. Kadkol, Elizabeth R. Gaillard, Pratool Bharti, Hamed Alhoori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The severe acute respiratory syndrome coronavirus type-2 (SARS-CoV-2) caused
a global pandemic and imposed immense effects on the global economy. Accurate,
cost-effective, and quick tests have proven substantial in identifying infected
people and mitigating the spread. Recently, multiple alternative platforms for
testing coronavirus disease 2019 (COVID-19) have been published that show high
agreement with current gold standard real-time polymerase chain reaction
(RT-PCR) results. These new methods do away with nasopharyngeal (NP) swabs,
eliminate the need for complicated reagents, and reduce the burden on RT-PCR
test reagent supply. In the present work, we have designed an artificial
intelligence-based (AI) testing method to provide confidence in the results.
Current AI applications to COVID-19 studies often lack a biological foundation
in the decision-making process, and our AI approach is one of the earliest to
leverage explainable-AI (X-AI) algorithms for COVID-19 diagnosis using mass
spectrometry. Here, we have employed X-AI to explain the decision-making
process on a local (per-sample) and global (all samples) basis underscored by
biologically relevant features. We evaluated our technique with data extracted
from human gargle samples and achieved a testing accuracy of 94.44%. Such
techniques would strengthen the relationship between AI and clinical
diagnostics by providing biomedical researchers and healthcare workers with
trustworthy and, most importantly, explainable test results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning-Based Data Storage [Vision] (Technical Report) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.05778v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.05778v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Lian, Xiaofei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural network (DNN) and its variants have been extensively used for a
wide spectrum of real applications such as image classification, face/speech
recognition, fraud detection, and so on. In addition to many important machine
learning tasks, as artificial networks emulating the way brain cells function,
DNNs also show the capability of storing non-linear relationships between input
and output data, which exhibits the potential of storing data via DNNs. We
envision a new paradigm of data storage, "DNN-as-a-Database", where data are
encoded in well-trained machine learning models. Compared with conventional
data storage that directly records data in raw formats, learning-based
structures (e.g., DNN) can implicitly encode data pairs of inputs and outputs
and compute/materialize actual output data of different resolutions only if
input data are provided. This new paradigm can greatly enhance the data
security by allowing flexible data privacy settings on different levels,
achieve low space consumption and fast computation with the acceleration of new
hardware (e.g., Diffractive Neural Network and AI chips), and can be
generalized to distributed DNN-based storage/computing. In this paper, we
propose this novel concept of learning-based data storage, which utilizes a
learning structure called learning-based memory unit (LMU), to store, organize,
and retrieve data. As a case study, we use DNNs as the engine in the LMU, and
study the data capacity and accuracy of the DNN-based data storage. Our
preliminary experimental results show the feasibility of the learning-based
data storage by achieving high (100%) accuracy of the DNN storage. We explore
and design effective solutions to utilize the DNN-based data storage to manage
and query relational tables. We discuss how to generalize our solutions to
other data types (e.g., graphs) and environments such as distributed DNN
storage/computing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analyzing Data-Centric Properties for Graph Contrastive Learning <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.02810v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.02810v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Puja Trivedi, Ekdeep Singh Lubana, Mark Heimann, Danai Koutra, Jayaraman J. Thiagarajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent analyses of self-supervised learning (SSL) find the following
data-centric properties to be critical for learning good representations:
invariance to task-irrelevant semantics, separability of classes in some latent
space, and recoverability of labels from augmented samples. However, given
their discrete, non-Euclidean nature, graph datasets and graph SSL methods are
unlikely to satisfy these properties. This raises the question: how do graph
SSL methods, such as contrastive learning (CL), work well? To systematically
probe this question, we perform a generalization analysis for CL when using
generic graph augmentations (GGAs), with a focus on data-centric properties.
Our analysis yields formal insights into the limitations of GGAs and the
necessity of task-relevant augmentations. As we empirically show, GGAs do not
induce task-relevant invariances on common benchmark datasets, leading to only
marginal gains over naive, untrained baselines. Our theory motivates a
synthetic data generation process that enables control over task-relevant
information and boasts pre-defined optimal augmentations. This flexible
benchmark helps us identify yet unrecognized limitations in advanced
augmentation techniques (e.g., automated methods). Overall, our work rigorously
contextualizes, both empirically and theoretically, the effects of data-centric
properties on augmentation strategies and learning paradigms for graph SSL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Learning Meets Multi-objective Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2006.11489v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2006.11489v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeou Hu, Kiarash Shaloudegi, Guojun Zhang, Yaoliang Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning has emerged as a promising, massively distributed way to
train a joint deep model over large amounts of edge devices while keeping
private user data strictly on device. In this work, motivated from ensuring
fairness among users and robustness against malicious adversaries, we formulate
federated learning as multi-objective optimization and propose a new algorithm
FedMGDA+ that is guaranteed to converge to Pareto stationary solutions.
FedMGDA+ is simple to implement, has fewer hyperparameters to tune, and
refrains from sacrificing the performance of any participating user. We
establish the convergence properties of FedMGDA+ and point out its connections
to existing approaches. Extensive experiments on a variety of datasets confirm
that FedMGDA+ compares favorably against state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE Transactions on Network Science and Engineering 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AtMan: Understanding <span class="highlight-title">Transformer</span> Predictions Through Memory Efficient
  Attention Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08110v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08110v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mayukh Deb, Björn Deiseroth, Samuel Weinbach, Patrick Schramowski, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative transformer models have become increasingly complex, with large
numbers of parameters and the ability to process multiple input modalities.
Current methods for explaining their predictions are resource-intensive. Most
crucially, they require prohibitively large amounts of extra memory, since they
rely on backpropagation which allocates almost twice as much GPU memory as the
forward pass. This makes it difficult, if not impossible, to use them in
production. We present AtMan that provides explanations of generative
transformer models at almost no extra cost. Specifically, AtMan is a
modality-agnostic perturbation method that manipulates the attention mechanisms
of transformers to produce relevance maps for the input with respect to the
output prediction. Instead of using backpropagation, AtMan applies a
parallelizable token-based search method based on cosine similarity
neighborhood in the embedding space. Our exhaustive experiments on text and
image-text benchmarks demonstrate that AtMan outperforms current
state-of-the-art gradient-based methods on several metrics while being
computationally efficient. As such, AtMan is suitable for use in large model
inference deployments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Broken Neural Scaling Laws 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.14891v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.14891v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Caballero, Kshitij Gupta, Irina Rish, David Krueger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a smoothly broken power law functional form that accurately models
and extrapolates the scaling behaviors of deep neural networks (i.e. how the
evaluation metric of interest varies as the amount of compute used for
training, number of model parameters, training dataset size, or upstream
performance varies) for various architectures and for each of various tasks
within a large and diverse set of upstream and downstream tasks, in zero-shot,
prompted, and fine-tuned settings. This set includes large-scale vision,
language, audio, video, diffusion generative modeling, multimodal learning,
contrastive learning, AI alignment, robotics, arithmetic,
unsupervised/self-supervised learning, and reinforcement learning (single agent
and multi-agent). When compared to other functional forms for neural scaling
behavior, this functional form yields extrapolations of scaling behavior that
are considerably more accurate on this set. Moreover, this functional form
accurately models and extrapolates scaling behavior that other functional forms
are incapable of expressing such as the non-monotonic transitions present in
the scaling behavior of phenomena such as double descent and the delayed, sharp
inflection points present in the scaling behavior of tasks such as arithmetic.
Lastly, we use this functional form to glean insights about the limit of the
predictability of scaling behavior. Code is available at
https://github.com/ethancaballero/broken_neural_scaling_laws
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Upper and Lower Bounds on the Performance of Kernel PCA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.10369v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.10369v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Haddouche, Benjamin Guedj, John Shawe-Taylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Principal Component Analysis (PCA) is a popular method for dimension
reduction and has attracted an unfailing interest for decades. More recently,
kernel PCA (KPCA) has emerged as an extension of PCA but, despite its use in
practice, a sound theoretical understanding of KPCA is missing. We contribute
several lower and upper bounds on the efficiency of KPCA, involving the
empirical eigenvalues of the kernel Gram matrix and new quantities involving a
notion of variance. These bounds show how much information is captured by KPCA
on average and contribute a better theoretical understanding of its efficiency.
We demonstrate that fast convergence rates are achievable for a widely used
class of kernels and we highlight the importance of some desirable properties
of datasets to ensure KPCA efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Proportional Fairness in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.01666v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.01666v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guojun Zhang, Saber Malekmohammadi, Xi Chen, Yaoliang Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasingly broad deployment of federated learning (FL) systems in
the real world, it is critical but challenging to ensure fairness in FL, i.e.
reasonably satisfactory performances for each of the numerous diverse clients.
In this work, we introduce and study a new fairness notion in FL, called
proportional fairness (PF), which is based on the relative change of each
client's performance. From its connection with the bargaining games, we propose
PropFair, a novel and easy-to-implement algorithm for finding proportionally
fair solutions in FL and study its convergence properties. Through extensive
experiments on vision and language datasets, we demonstrate that PropFair can
approximately find PF solutions, and it achieves a good balance between the
average performances of all clients and of the worst 10% clients.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at TMLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal learning with graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.03299v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.03299v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasha Ektefaie, George Dasoulas, Ayush Noori, Maha Farhat, Marinka Zitnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence for graphs has achieved remarkable success in
modeling complex systems, ranging from dynamic networks in biology to
interacting particle systems in physics. However, the increasingly
heterogeneous graph datasets call for multimodal methods that can combine
different inductive biases: the set of assumptions that algorithms use to make
predictions for inputs they have not encountered during training. Learning on
multimodal datasets presents fundamental challenges because the inductive
biases can vary by data modality and graphs might not be explicitly given in
the input. To address these challenges, multimodal graph AI methods combine
different modalities while leveraging cross-modal dependencies using graphs.
Diverse datasets are combined using graphs and fed into sophisticated
multimodal architectures, specified as image-intensive, knowledge-grounded and
language-intensive models. Using this categorization, we introduce a blueprint
for multimodal graph learning, use it to study existing methods and provide
guidelines to design new models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 5 figures, 2 boxes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topogivity: A Machine-Learned Chemical Rule for Discovering Topological
  Materials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.05255v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.05255v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Ma, Yang Zhang, Thomas Christensen, Hoi Chun Po, Li Jing, Liang Fu, Marin Soljačić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topological materials present unconventional electronic properties that make
them attractive for both basic science and next-generation technological
applications. The majority of currently known topological materials have been
discovered using methods that involve symmetry-based analysis of the quantum
wavefunction. Here we use machine learning to develop a simple-to-use heuristic
chemical rule that diagnoses with a high accuracy whether a material is
topological using only its chemical formula. This heuristic rule is based on a
notion that we term topogivity, a machine-learned numerical value for each
element that loosely captures its tendency to form topological materials. We
next implement a high-throughput procedure for discovering topological
materials based on the heuristic topogivity-rule prediction followed by ab
initio validation. This way, we discover new topological materials that are not
diagnosable using symmetry indicators, including several that may be promising
for experimental observation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main text: 6 pages, 3 figures; supplementary materials: 43 pages, 62
  figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neyman-Pearson Multi-class Classification via Cost-sensitive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.04597v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.04597v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Tian, Yang Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing classification methods aim to minimize the overall
misclassification error rate. However, in applications, different types of
errors can have different consequences. Two popular paradigms have been
developed to account for this asymmetry issue: the Neyman-Pearson (NP) paradigm
and the cost-sensitive (CS) paradigm. Compared to the CS paradigm, the NP
paradigm does not require a specification of costs. Most previous works on the
NP paradigm focused on the binary case. In this work, we study the multi-class
NP problem by connecting it to the CS problem and propose two algorithms. We
extend the NP oracle inequalities and consistency from the binary case to the
multi-class case, showing that our two algorithms enjoy these properties under
certain conditions. The simulation and real data studies demonstrate the
effectiveness of our algorithms. To our knowledge, this is the first work to
solve the multi-class NP problem via cost-sensitive learning techniques with
theoretical guarantees. The proposed algorithms are implemented in the R
package npcs on CRAN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>53 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Wholistic View of Continual Learning with Deep Neural Networks:
  Forgotten Lessons and the Bridge to Active and Open World Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2009.01797v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2009.01797v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Mundt, Yongwon Hong, Iuliia Pliushch, Visvanathan Ramesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current deep learning methods are regarded as favorable if they empirically
perform well on dedicated test sets. This mentality is seamlessly reflected in
the resurfacing area of continual learning, where consecutively arriving data
is investigated. The core challenge is framed as protecting previously acquired
representations from being catastrophically forgotten. However, comparison of
individual methods is nevertheless performed in isolation from the real world
by monitoring accumulated benchmark test set performance. The closed world
assumption remains predominant, i.e. models are evaluated on data that is
guaranteed to originate from the same distribution as used for training. This
poses a massive challenge as neural networks are well known to provide
overconfident false predictions on unknown and corrupted instances. In this
work we critically survey the literature and argue that notable lessons from
open set recognition, identifying unknown examples outside of the observed set,
and the adjacent field of active learning, querying data to maximize the
expected performance gain, are frequently overlooked in the deep learning era.
Hence, we propose a consolidated view to bridge continual learning, active
learning and open set recognition in deep neural networks. Finally, the
established synergies are supported empirically, showing joint improvement in
alleviating catastrophic forgetting, querying data, selecting task orders,
while exhibiting robust open world application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at Neural Networks in open-access form.
  Final version available at: https://doi.org/10.1016/j.neunet.2023.01.014</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gradient-adjusted Incremental Target Propagation Provides Effective
  Credit Assignment in Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.11598v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.11598v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sander Dalm, Nasir Ahmad, Luca Ambrogioni, Marcel van Gerven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many of the recent advances in the field of artificial intelligence have been
fueled by the highly successful backpropagation of error (BP) algorithm, which
efficiently solves the credit assignment problem in artificial neural networks.
However, it is unlikely that BP is implemented in its usual form within
biological neural networks, because of its reliance on non-local information in
propagating error gradients. Since biological neural networks are capable of
highly efficient learning and responses from BP trained models can be related
to neural responses, it seems reasonable that a biologically viable
approximation of BP underlies synaptic plasticity in the brain.
Gradient-adjusted incremental target propagation (GAIT-prop or GP for short)
has recently been derived directly from BP and has been shown to successfully
train networks in a more biologically plausible manner. However, so far, GP has
only been shown to work on relatively low-dimensional problems, such as
handwritten-digit recognition. This work addresses some of the scaling issues
in GP and shows it to perform effective multi-layer credit assignment in deeper
networks and on the much more challenging ImageNet dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A computational framework for physics-informed symbolic regression with
  straightforward integration of domain knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.06257v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.06257v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liron Simon Keren, Alex Liberzon, Teddy Lazebnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discovering a meaningful symbolic expression that explains experimental data
is a fundamental challenge in many scientific fields. We present a novel,
open-source computational framework called Scientist-Machine Equation Detector
(SciMED), which integrates scientific discipline wisdom in a
scientist-in-the-loop approach, with state-of-the-art symbolic regression (SR)
methods. SciMED combines a wrapper selection method, that is based on a genetic
algorithm, with automatic machine learning and two levels of SR methods. We
test SciMED on five configurations of a settling sphere, with and without
aerodynamic non-linear drag force, and with excessive noise in the
measurements. We show that SciMED is sufficiently robust to discover the
correct physically meaningful symbolic expressions from the data, and
demonstrate how the integration of domain knowledge enhances its performance.
Our results indicate better performance on these tasks than the
state-of-the-art SR software packages , even in cases where no knowledge is
integrated. Moreover, we demonstrate how SciMED can alert the user about
possible missing features, unlike the majority of current SR systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autoencoded sparse Bayesian in-IRT factorization, calibration, and
  amortized inference for the Work Disability Functional Assessment Battery <span class="chip">AISTATS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.10952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.10952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua C. Chang, Carson C. Chow, Julia Porcino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Work Disability Functional Assessment Battery (WD-FAB) is a
multidimensional item response theory (IRT) instrument designed for assessing
work-related mental and physical function based on responses to an item bank.
In prior iterations it was developed using traditional means -- linear
factorization and null hypothesis statistical testing for item
partitioning/selection, and finally, posthoc calibration of disjoint
unidimensional IRT models. As a result, the WD-FAB, like many other IRT
instruments, is a posthoc model. Its item partitioning, based on exploratory
factor analysis, is blind to the final nonlinear IRT model and is not performed
in a manner consistent with goodness of fit to the final model. In this
manuscript, we develop a Bayesian hierarchical model for self-consistently
performing the following simultaneous tasks: scale factorization, item
selection, parameter identification, and response scoring. This method uses
sparsity-based shrinkage to obviate the linear factorization and null
hypothesis statistical tests that are usually required for developing
multidimensional IRT models, so that item partitioning is consistent with the
ultimate nonlinear factor model. We also analogize our multidimensional IRT
model to probabilistic autoencoders, specifying an encoder function that
amortizes the inference of ability parameters from item responses. The encoder
function is equivalent to the "VBE" step in a stochastic variational Bayesian
expectation maximization (VBEM) procedure that we use for approxiamte Bayesian
inference on the entire model. We use the method on a sample of WD-FAB item
responses and compare the resulting item discriminations to those obtained
using the traditional posthoc method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AISTATS 2023 and the AAAI 2023 AI for Social Good
  workshop</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Context to Capture when Reconstructing Meaningful Spaces
  for Remote Instruction and Connecting in XR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanuma Teja Maddali, Amanda Lazar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent technological advances are enabling HCI researchers to explore
interaction possibilities for remote XR collaboration using high-fidelity
reconstructions of physical activity spaces. However, creating these
reconstructions often lacks user involvement with an overt focus on capturing
sensory context that does not necessarily augment an informal social
experience. This work seeks to understand social context that can be important
for reconstruction to enable XR applications for informal instructional
scenarios. Our study involved the evaluation of an XR remote guidance prototype
by 8 intergenerational groups of closely related gardeners using
reconstructions of personally meaningful spaces in their gardens. Our findings
contextualize physical objects and areas with various motivations related to
gardening and detail perceptions of XR that might affect the use of
reconstructions for remote interaction. We discuss implications for user
involvement to create reconstructions that better translate real-world
experience, encourage reflection, incorporate privacy considerations, and
preserve shared experiences with XR as a medium for informal intergenerational
activities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SoundSpaces 2.0: A Simulation Platform for Visual-Acoustic Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08312v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08312v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changan Chen, Carl Schissler, Sanchit Garg, Philip Kobernik, Alexander Clegg, Paul Calamia, Dhruv Batra, Philip W Robinson, Kristen Grauman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SoundSpaces 2.0, a platform for on-the-fly geometry-based audio
rendering for 3D environments. Given a 3D mesh of a real-world environment,
SoundSpaces can generate highly realistic acoustics for arbitrary sounds
captured from arbitrary microphone locations. Together with existing 3D visual
assets, it supports an array of audio-visual research tasks, such as
audio-visual navigation, mapping, source localization and separation, and
acoustic matching. Compared to existing resources, SoundSpaces 2.0 has the
advantages of allowing continuous spatial sampling, generalization to novel
environments, and configurable microphone and material properties. To our
knowledge, this is the first geometry-based acoustic simulation that offers
high fidelity and realism while also being fast enough to use for embodied
learning. We showcase the simulator's properties and benchmark its performance
against real-world audio measurements. In addition, we demonstrate two
downstream tasks -- embodied navigation and far-field automatic speech
recognition -- and highlight sim2real performance for the latter. SoundSpaces
2.0 is publicly available to facilitate wider research for perceptual systems
that can both see and hear.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version. Website: https://soundspaces.org. Project page:
  https://vision.cs.utexas.edu/projects/soundspaces2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Colorization of Structured Mobile Web Pages <span class="chip">WACV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.11541v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.11541v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kotaro Kikuchi, Naoto Inoue, Mayu Otani, Edgar Simo-Serra, Kota Yamaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Color is a critical design factor for web pages, affecting important factors
such as viewer emotions and the overall trust and satisfaction of a website.
Effective coloring requires design knowledge and expertise, but if this process
could be automated through data-driven modeling, efficient exploration and
alternative workflows would be possible. However, this direction remains
underexplored due to the lack of a formalization of the web page colorization
problem, datasets, and evaluation protocols. In this work, we propose a new
dataset consisting of e-commerce mobile web pages in a tractable format, which
are created by simplifying the pages and extracting canonical color styles with
a common web browser. The web page colorization problem is then formalized as a
task of estimating plausible color styles for a given web page content with a
given hierarchical structure of the elements. We present several
Transformer-based methods that are adapted to this task by prepending
structural message passing to capture hierarchical relationships between
elements. Experimental results, including a quantitative evaluation designed
for this task, demonstrate the advantages of our methods over statistical and
image colorization methods. The code is available at
https://github.com/CyberAgentAILab/webcolor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WACV 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-01-22T00:00:00Z">2023-01-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Study of Metrics to Measure Representational Harms in
  <span class="highlight-title">Pre-Train</span>ed Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saghar Hosseini, Hamid Palangi, Ahmed Hassan Awadallah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale Pre-Trained Language Models (PTLMs) capture knowledge from
massive human-written data which contains latent societal biases and toxic
contents. In this paper, we leverage the primary task of PTLMs, i.e., language
modeling, and propose a new metric to quantify manifested implicit
representational harms in PTLMs towards 13 marginalized demographics. Using
this metric, we conducted an empirical analysis of 24 widely used PTLMs. Our
analysis provides insights into the correlation between the proposed metric in
this work and other related metrics for representational harm. We observe that
our metric correlates with most of the gender-specific metrics in the
literature. Through extensive experiments, we explore the connections between
PTLMs architectures and representational harms across two dimensions: depth and
width of the networks. We found that prioritizing depth over width, mitigates
representational harms in some PTLMs. Our code and data can be found at
https://github.com/microsoft/SafeNLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Summarize the Past to Predict the Future: Natural Language Descriptions
  of Context Boost Multimodal Object Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Razvan-George Pasca, Alexey Gavryushin, Yen-Ling Kuo, Otmar Hilliges, Xi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the task of object interaction anticipation in egocentric videos.
Successful prediction of future actions and objects requires an understanding
of the spatio-temporal context formed by past actions and object relationships.
We propose TransFusion, a multimodal transformer-based architecture, that
effectively makes use of the representational power of language by summarizing
past actions concisely. TransFusion leverages pre-trained image captioning
models and summarizes the caption, focusing on past actions and objects. This
action context together with a single input frame is processed by a multimodal
fusion module to forecast the next object interactions. Our model enables more
efficient end-to-end learning by replacing dense video features with language
representations, allowing us to benefit from knowledge encoded in large
pre-trained models. Experiments on Ego4D and EPIC-KITCHENS-100 show the
effectiveness of our multimodal fusion model and the benefits of using
language-based context summaries. Our method outperforms state-of-the-art
approaches by 40.4% in overall mAP on the Ego4D test set. We show the
generality of TransFusion via experiments on EPIC-KITCHENS-100. Video and code
are available at: https://eth-ait.github.io/transfusion-proj/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ensemble Transfer Learning for Multilingual Coreference Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuan Manh Lai, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity coreference resolution is an important research problem with many
applications, including information extraction and question answering.
Coreference resolution for English has been studied extensively. However, there
is relatively little work for other languages. A problem that frequently occurs
when working with a non-English language is the scarcity of annotated training
data. To overcome this challenge, we design a simple but effective
ensemble-based framework that combines various transfer learning (TL)
techniques. We first train several models using different TL methods. Then,
during inference, we compute the unweighted average scores of the models'
predictions to extract the final set of predicted clusters. Furthermore, we
also propose a low-cost TL method that bootstraps coreference resolution models
by utilizing Wikipedia anchor texts. Leveraging the idea that the coreferential
links naturally exist between anchor texts pointing to the same article, our
method builds a sizeable distantly-supervised dataset for the target language
that consists of tens of thousands of documents. We can pre-train a model on
the pseudo-labeled dataset before finetuning it on the final target dataset.
Experimental results on two benchmark datasets, OntoNotes and SemEval, confirm
the effectiveness of our methods. Our best ensembles consistently outperform
the baseline approach of simple training by up to 7.68% in the F1 score. These
ensembles also achieve new state-of-the-art results for three languages:
Arabic, Dutch, and Spanish.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representing Interlingual Meaning in Lexical Databases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fausto Giunchiglia, Gabor Bella, Nandu Chandran Nair, Yang Chi, Hao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In today's multilingual lexical databases, the majority of the world's
languages are under-represented. Beyond a mere issue of resource
incompleteness, we show that existing lexical databases have structural
limitations that result in a reduced expressivity on culturally-specific words
and in mapping them across languages. In particular, the lexical meaning space
of dominant languages, such as English, is represented more accurately while
linguistically or culturally diverse languages are mapped in an approximate
manner. Our paper assesses state-of-the-art multilingual lexical databases and
evaluates their strengths and limitations with respect to their expressivity on
lexical phenomena of linguistic diversity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially Private Natural Language Models: Recent Advances and
  Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lijie Hu, Ivan Habernal, Lei Shen, Di Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in deep learning have led to great success in various
natural language processing (NLP) tasks. However, these applications may
involve data that contain sensitive information. Therefore, how to achieve good
performance while also protect privacy of sensitive data is a crucial challenge
in NLP. To preserve privacy, Differential Privacy (DP), which can prevent
reconstruction attacks and protect against potential side knowledge, is
becoming a de facto technique for private data analysis. In recent years, NLP
in DP models (DP-NLP) has been studied from different perspectives, which
deserves a comprehensive review. In this paper, we provide the first systematic
review of recent advances on DP deep learning models in NLP. In particular, we
first discuss some differences and additional challenges of DP-NLP compared
with the standard DP deep learning. Then we investigate some existing work on
DP-NLP and present its recent developments from two aspects: gradient
perturbation based methods and embedding vector perturbation based methods. We
also discuss some challenges and future directions of this topic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Data Selection for TTS: Using Arabic Broadcast News as a
  Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Massa Baali, Tomoki Hayashi, Hamdy Mubarak, Soumi Maiti, Shinji Watanabe, Wassim El-Hajj, Ahmed Ali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several high-resource Text to Speech (TTS) systems currently produce natural,
well-established human-like speech. In contrast, low-resource languages,
including Arabic, have very limited TTS systems due to the lack of resources.
We propose a fully unsupervised method for building TTS, including automatic
data selection and pre-training/fine-tuning strategies for TTS training, using
broadcast news as a case study. We show how careful selection of data, yet
smaller amounts, can improve the efficiency of TTS system in generating more
natural speech than a system trained on a bigger dataset. We adopt to propose
different approaches for the: 1) data: we applied automatic annotations using
DNSMOS, automatic vowelization, and automatic speech recognition (ASR) for
fixing transcriptions' errors; 2) model: we used transfer learning from
high-resource language in TTS model and fine-tuned it with one hour broadcast
recording then we used this model to guide a FastSpeech2-based Conformer model
for duration. Our objective evaluation shows 3.9% character error rate (CER),
while the groundtruth has 1.3% CER. As for the subjective evaluation, where 1
is bad and 5 is excellent, our FastSpeech2-based Conformer model achieved a
mean opinion score (MOS) of 4.4 for intelligibility and 4.2 for naturalness,
where many annotators recognized the voice of the broadcaster, which proves the
effectiveness of our proposed unsupervised method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BanglaNLG and BanglaT5: Benchmarks and Resources for Evaluating
  Low-Resource Natural Language Generation in Bangla <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.11081v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.11081v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Rifat Shahriyar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents BanglaNLG, a comprehensive benchmark for evaluating
natural language generation (NLG) models in Bangla, a widely spoken yet
low-resource language. We aggregate six challenging conditional text generation
tasks under the BanglaNLG benchmark, introducing a new dataset on dialogue
generation in the process. Then, using a clean corpus of 27.5 GB of Bangla
data, we pretrain BanglaT5, a sequence-to-sequence Transformer model for
Bangla. BanglaT5 achieves state-of-the-art performance in all of these tasks,
outperforming several multilingual models by up to 9% absolute gain and 32%
relative gain. We are making the new dataset, the BanglaT5 language model, and
a leaderboard publicly available at https://github.com/csebuetnlp/BanglaNLG in
the hope of advancing future research and evaluation on Bangla NLG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Findings of EACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Embedding Recycling for Language Models <span class="chip">EACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.04993v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.04993v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jon Saad-Falcon, Amanpreet Singh, Luca Soldaini, Mike D'Arcy, Arman Cohan, Doug Downey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world applications of neural language models often involve running many
different models over the same corpus. The high computational cost of these
runs has led to interest in techniques that can reuse the contextualized
embeddings produced in previous runs to speed training and inference of future
ones. We refer to this approach as embedding recycling (ER). While multiple ER
techniques have been proposed, their practical effectiveness is still unknown
because existing evaluations consider very few models and do not adequately
account for overhead costs. We perform an extensive evaluation of ER across
eight different models (17 to 900 million parameters) and fourteen tasks in
English. We show how a simple ER technique that caches activations from an
intermediate layer of a pretrained model, and learns task-specific adapters on
the later layers, is broadly effective. For the best-performing baseline in our
experiments (DeBERTa-v2 XL), adding a precomputed cache results in a >90%
speedup during training and 87-91% speedup for inference, with negligible
impact on accuracy. Our analysis reveals important areas of future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL Findings 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ASQA: Factoid Questions Meet Long-Form Answers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.06092v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.06092v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, Ming-Wei Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An abundance of datasets and availability of reliable evaluation metrics have
resulted in strong progress in factoid question answering (QA). This progress,
however, does not easily transfer to the task of long-form QA, where the goal
is to answer questions that require in-depth explanations. The hurdles include
(i) a lack of high-quality data, and (ii) the absence of a well-defined notion
of the answer's quality. In this work, we address these problems by (i)
releasing a novel dataset and a task that we call ASQA (Answer Summaries for
Questions which are Ambiguous); and (ii) proposing a reliable metric for
measuring performance on ASQA. Our task focuses on factoid questions that are
ambiguous, that is, have different correct answers depending on interpretation.
Answers to ambiguous questions should synthesize factual information from
multiple sources into a long-form summary that resolves the ambiguity. In
contrast to existing long-form QA tasks (such as ELI5), ASQA admits a clear
notion of correctness: a user faced with a good summary should be able to
answer different interpretations of the original ambiguous question. We use
this notion of correctness to define an automated metric of performance for
ASQA. Our analysis demonstrates an agreement between this metric and human
judgments, and reveals a considerable gap between human performance and strong
baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A minor bug in computing the ROUGE score was fixed. The fix **did
  not** result in any changes in observations and conclusions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Questions Are All You Need to Train a Dense Passage Retriever <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.10658v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.10658v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, Manzil Zaheer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ART, a new corpus-level autoencoding approach for training dense
retrieval models that does not require any labeled training data. Dense
retrieval is a central challenge for open-domain tasks, such as Open QA, where
state-of-the-art methods typically require large supervised datasets with
custom hard-negative mining and denoising of positive examples. ART, in
contrast, only requires access to unpaired inputs and outputs (e.g. questions
and potential answer documents). It uses a new document-retrieval autoencoding
scheme, where (1) an input question is used to retrieve a set of evidence
documents, and (2) the documents are then used to compute the probability of
reconstructing the original question. Training for retrieval based on question
reconstruction enables effective unsupervised learning of both document and
question encoders, which can be later incorporated into complete Open QA
systems without any further finetuning. Extensive experiments demonstrate that
ART obtains state-of-the-art results on multiple QA retrieval benchmarks with
only generic initialization from a pre-trained language model, removing the
need for labeled data and task-specific losses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TACL, pre MIT Press publication version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaQA: Combining Expert Agents for Multi-Skill Question Answering <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.01922v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.01922v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haritz Puerto, Gözde Gül Şahin, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent explosion of question answering (QA) datasets and models has
increased the interest in the generalization of models across multiple domains
and formats by either training on multiple datasets or by combining multiple
models. Despite the promising results of multi-dataset models, some domains or
QA formats may require specific architectures, and thus the adaptability of
these models might be limited. In addition, current approaches for combining
models disregard cues such as question-answer compatibility. In this work, we
propose to combine expert agents with a novel, flexible, and training-efficient
architecture that considers questions, answer predictions, and
answer-prediction confidence scores to select the best answer among a list of
answer candidates. Through quantitative and qualitative experiments we show
that our model i) creates a collaboration between agents that outperforms
previous multi-agent and multi-dataset approaches in both in-domain and
out-of-domain scenarios, ii) is highly data-efficient to train, and iii) can be
adapted to any QA format. We release our code and a dataset of answer
predictions from expert agents for 16 QA datasets to foster future developments
of multi-agent systems on https://github.com/UKPLab/MetaQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TrimTail: Low-Latency Streaming ASR with Simple but Effective
  Spectrogram-Level Length Penalty <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.00522v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.00522v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingchen Song, Di Wu, Zhiyong Wu, Binbin Zhang, Yuekai Zhang, Zhendong Peng, Wenpeng Li, Fuping Pan, Changbao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present TrimTail, a simple but effective emission
regularization method to improve the latency of streaming ASR models. The core
idea of TrimTail is to apply length penalty (i.e., by trimming trailing frames,
see Fig. 1-(b)) directly on the spectrogram of input utterances, which does not
require any alignment. We demonstrate that TrimTail is computationally cheap
and can be applied online and optimized with any training loss or any model
architecture on any dataset without any extra effort by applying it on various
end-to-end streaming ASR networks either trained with CTC loss [1] or
Transducer loss [2]. We achieve 100 $\sim$ 200ms latency reduction with equal
or even better accuracy on both Aishell-1 and Librispeech. Moreover, by using
TrimTail, we can achieve a 400ms algorithmic improvement of User Sensitive
Delay (USD) with an accuracy loss of less than 0.2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Applied Deep Learning to Identify and Localize Polyps from Endoscopic
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chandana Raju, Sumedh Vilas Datar, Kushala Hari, Kavin Vijay, Suma Ningappa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning based neural networks have gained popularity for a variety of
biomedical imaging applications. In the last few years several works have shown
the use of these methods for colon cancer detection and the early results have
been promising. These methods can potentially be utilized to assist doctor's
and may help in identifying the number of lesions or abnormalities in a
diagnosis session. From our literature survey we found out that there is a lack
of publicly available labeled data. Thus, as part of this work, we have aimed
at open sourcing a dataset which contains annotations of polyps and ulcers.
This is the first dataset that's coming from India containing polyp and ulcer
images. The dataset can be used for detection and classification tasks. We also
evaluated our dataset with several popular deep learning object detection
models that's trained on large publicly available datasets and found out
empirically that the model trained on one dataset works well on our dataset
that has data being captured in a different acquisition device.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FRAME: Fast and Robust Autonomous 3D point cloud Map-merging for
  Egocentric multi-robot exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Stathoulopoulos, Anton Koval, Ali-akbar Agha-mohammadi, George Nikolakopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article presents a 3D point cloud map-merging framework for egocentric
heterogeneous multi-robot exploration, based on overlap detection and
alignment, that is independent of a manual initial guess or prior knowledge of
the robots' poses. The novel proposed solution utilizes state-of-the-art place
recognition learned descriptors, that through the framework's main pipeline,
offer a fast and robust region overlap estimation, hence eliminating the need
for the time-consuming global feature extraction and feature matching process
that is typically used in 3D map integration. The region overlap estimation
provides a homogeneous rigid transform that is applied as an initial condition
in the point cloud registration algorithm Fast-GICP, which provides the final
and refined alignment. The efficacy of the proposed framework is experimentally
evaluated based on multiple field multi-robot exploration missions in
underground environments, where both ground and aerial robots are deployed,
with different sensor configurations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Summarize the Past to Predict the Future: Natural Language Descriptions
  of Context Boost Multimodal Object Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Razvan-George Pasca, Alexey Gavryushin, Yen-Ling Kuo, Otmar Hilliges, Xi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the task of object interaction anticipation in egocentric videos.
Successful prediction of future actions and objects requires an understanding
of the spatio-temporal context formed by past actions and object relationships.
We propose TransFusion, a multimodal transformer-based architecture, that
effectively makes use of the representational power of language by summarizing
past actions concisely. TransFusion leverages pre-trained image captioning
models and summarizes the caption, focusing on past actions and objects. This
action context together with a single input frame is processed by a multimodal
fusion module to forecast the next object interactions. Our model enables more
efficient end-to-end learning by replacing dense video features with language
representations, allowing us to benefit from knowledge encoded in large
pre-trained models. Experiments on Ego4D and EPIC-KITCHENS-100 show the
effectiveness of our multimodal fusion model and the benefits of using
language-based context summaries. Our method outperforms state-of-the-art
approaches by 40.4% in overall mAP on the Ego4D test set. We show the
generality of TransFusion via experiments on EPIC-KITCHENS-100. Video and code
are available at: https://eth-ait.github.io/transfusion-proj/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Apples and Oranges? Assessing Image Quality over Content Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09190v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09190v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyong You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image recognition and quality assessment are two important viewing tasks,
while potentially following different visual mechanisms. This paper
investigates if the two tasks can be performed in a multitask learning manner.
A sequential spatial-channel attention module is proposed to simulate the
visual attention and contrast sensitivity mechanisms that are crucial for
content recognition and quality assessment. Spatial attention is shared between
content recognition and quality assessment, while channel attention is solely
for quality assessment. Such attention module is integrated into Transformer to
build a uniform model for the two viewing tasks. The experimental results have
demonstrated that the proposed uniform model can achieve promising performance
for both quality assessment and content recognition tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MATT: Multimodal Attention Level Estimation for e-learning Platforms <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roberto Daza, Luis F. Gomez, Aythami Morales, Julian Fierrez, Ruben Tolosana, Ruth Cobos, Javier Ortega-Garcia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a new multimodal system for remote attention level
estimation based on multimodal face analysis. Our multimodal approach uses
different parameters and signals obtained from the behavior and physiological
processes that have been related to modeling cognitive load such as faces
gestures (e.g., blink rate, facial actions units) and user actions (e.g., head
pose, distance to the camera). The multimodal system uses the following modules
based on Convolutional Neural Networks (CNNs): Eye blink detection, head pose
estimation, facial landmark detection, and facial expression features. First,
we individually evaluate the proposed modules in the task of estimating the
student's attention level captured during online e-learning sessions. For that
we trained binary classifiers (high or low attention) based on Support Vector
Machines (SVM) for each module. Secondly, we find out to what extent multimodal
score level fusion improves the attention level estimation. The mEBAL database
is used in the experimental framework, a public multi-modal database for
attention level estimation obtained in an e-learning environment that contains
data from 38 users while conducting several e-learning tasks of variable
difficulty (creating changes in student cognitive loads).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint of the paper presented to the Workshop on Artificial
  Intelligence for Education (AI4EDU) of AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unifying Synergies between <span class="highlight-title">Self-supervised</span> Learning and Dynamic
  Computation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tarun Krishna, Ayush K Rai, Alexandru Drimbarean, Alan F Smeaton, Kevin McGuinness, Noel E O'Connor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) approaches have made major strides forward by
emulating the performance of their supervised counterparts on several computer
vision benchmarks. This, however, comes at a cost of substantially larger model
sizes, and computationally expensive training strategies, which eventually lead
to larger inference times making it impractical for resource constrained
industrial settings. Techniques like knowledge distillation (KD), dynamic
computation (DC), and pruning are often used to obtain a lightweight
sub-network, which usually involves multiple epochs of fine-tuning of a large
pre-trained model, making it more computationally challenging.
  In this work we propose a novel perspective on the interplay between SSL and
DC paradigms that can be leveraged to simultaneously learn a dense and gated
(sparse/lightweight) sub-network from scratch offering a good
accuracy-efficiency trade-off, and therefore yielding a generic and
multi-purpose architecture for application specific industrial settings. Our
study overall conveys a constructive message: exhaustive experiments on several
image classification benchmarks: CIFAR-10, STL-10, CIFAR-100, and ImageNet-100,
demonstrates that the proposed training strategy provides a dense and
corresponding sparse sub-network that achieves comparable (on-par) performance
compared with the vanilla self-supervised setting, but at a significant
reduction in computation in terms of FLOPs under a range of target budgets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Face Generation from Textual Features using Conditionally Trained Inputs
  to Generative Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandeep Shinde, Tejas Pradhan, Aniket Ghorpade, Mihir Tale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Networks have proved to be extremely effective in image
restoration and reconstruction in the past few years. Generating faces from
textual descriptions is one such application where the power of generative
algorithms can be used. The task of generating faces can be useful for a number
of applications such as finding missing persons, identifying criminals, etc.
This paper discusses a novel approach to generating human faces given a textual
description regarding the facial features. We use the power of state of the art
natural language processing models to convert face descriptions into learnable
latent vectors which are then fed to a generative adversarial network which
generates faces corresponding to those features. While this paper focuses on
high level descriptions of faces only, the same approach can be tailored to
generate any image based on fine grained textual features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Open-vocabulary Semantic Segmentation Models From Natural
  Language Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09121v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09121v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jilan Xu, Junlin Hou, Yuejie Zhang, Rui Feng, Yi Wang, Yu Qiao, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider the problem of open-vocabulary semantic
segmentation (OVS), which aims to segment objects of arbitrary classes instead
of pre-defined, closed-set categories. The main contributions are as follows:
First, we propose a transformer-based model for OVS, termed as OVSegmentor,
which only exploits web-crawled image-text pairs for pre-training without using
any mask annotations. OVSegmentor assembles the image pixels into a set of
learnable group tokens via a slot-attention based binding module, and aligns
the group tokens to the corresponding caption embedding. Second, we propose two
proxy tasks for training, namely masked entity completion and cross-image mask
consistency. The former aims to infer all masked entities in the caption given
the group tokens, that enables the model to learn fine-grained alignment
between visual groups and text entities. The latter enforces consistent mask
predictions between images that contain shared entities, which encourages the
model to learn visual invariance. Third, we construct CC4M dataset for
pre-training by filtering CC12M with frequently appeared entities, which
significantly improves training efficiency. Fourth, we perform zero-shot
transfer on three benchmark datasets, PASCAL VOC 2012, PASCAL Context, and COCO
Object. Our model achieves superior segmentation results over the
state-of-the-art method by using only 3\% data (4M vs 134M) for pre-training.
Code and pre-trained models will be released for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causality-based Dual-Contrastive Learning Framework for Domain
  Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zining Chen, Weiqiu Wang, Zhicheng Zhao, Aidong Men
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain Generalization (DG) is essentially a sub-branch of out-of-distribution
generalization, which trains models from multiple source domains and
generalizes to unseen target domains. Recently, some domain generalization
algorithms have emerged, but most of them were designed with non-transferable
complex architecture. Additionally, contrastive learning has become a promising
solution for simplicity and efficiency in DG. However, existing contrastive
learning neglected domain shifts that caused severe model confusions. In this
paper, we propose a Dual-Contrastive Learning (DCL) module on feature and
prototype contrast. Moreover, we design a novel Causal Fusion Attention (CFA)
module to fuse diverse views of a single image to attain prototype.
Furthermore, we introduce a Similarity-based Hard-pair Mining (SHM) strategy to
leverage information on diversity shift. Extensive experiments show that our
method outperforms state-of-the-art algorithms on three DG datasets. The
proposed algorithm can also serve as a plug-and-play module without usage of
domain labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BallGAN: 3D-aware Image Synthesis with a Spherical Background 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjung Shin, Yunji Seo, Jeongmin Bae, Young Sun Choi, Hyunsu Kim, Hyeran Byun, Youngjung Uh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D-aware GANs aim to synthesize realistic 3D scenes such that they can be
rendered in arbitrary perspectives to produce images. Although previous methods
produce realistic images, they suffer from unstable training or degenerate
solutions where the 3D geometry is unnatural. We hypothesize that the 3D
geometry is underdetermined due to the insufficient constraint, i.e., being
classified as real image to the discriminator is not enough. To solve this
problem, we propose to approximate the background as a spherical surface and
represent a scene as a union of the foreground placed in the sphere and the
thin spherical background. It reduces the degree of freedom in the background
field. Accordingly, we modify the volume rendering equation and incorporate
dedicated constraints to design a novel 3D-aware GAN framework named BallGAN.
BallGAN has multiple advantages as follows. 1) It produces more reasonable 3D
geometry; the images of a scene across different viewpoints have better
photometric consistency and fidelity than the state-of-the-art methods. 2) The
training becomes much more stable. 3) The foreground can be separately rendered
on top of different arbitrary backgrounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://minjung-s.github.io/ballgan</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bidirectional Propagation for Cross-Modal 3D Object Detection <span class="chip">ICLR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Zhang, Qijian Zhang, Junhui Hou, Yixuan Yuan, Guoliang Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have revealed the superiority of feature-level fusion for
cross-modal 3D object detection, where fine-grained feature propagation from 2D
image pixels to 3D LiDAR points has been widely adopted for performance
improvement. Still, the potential of heterogeneous feature propagation between
2D and 3D domains has not been fully explored. In this paper, in contrast to
existing pixel-to-point feature propagation, we investigate an opposite
point-to-pixel direction, allowing point-wise features to flow inversely into
the 2D image branch. Thus, when jointly optimizing the 2D and 3D streams, the
gradients back-propagated from the 2D image branch can boost the representation
ability of the 3D backbone network working on LiDAR point clouds. Then,
combining pixel-to-point and point-to-pixel information flow mechanisms, we
construct an bidirectional feature propagation framework, dubbed BiProDet. In
addition to the architectural design, we also propose normalized local
coordinates map estimation, a new 2D auxiliary task for the training of the 2D
image branch, which facilitates learning local spatial-aware features from the
image modality and implicitly enhances the overall 3D detection performance.
Extensive experiments and ablation studies validate the effectiveness of our
method. Notably, we rank $\mathbf{1^{\mathrm{st}}}$ on the highly competitive
KITTI benchmark on the cyclist class by the time of submission. The source code
is available at https://github.com/Eaphan/BiProDet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2023. Code is avaliable at
  https://github.com/Eaphan/BiProDet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variational Cross-Graph Reasoning and Adaptive Structured Semantics
  Learning for Compositional Temporal Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juncheng Li, Siliang Tang, Linchao Zhu, Wenqiao Zhang, Yi Yang, Tat-Seng Chua, Fei Wu, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal grounding is the task of locating a specific segment from an
untrimmed video according to a query sentence. This task has achieved
significant momentum in the computer vision community as it enables activity
grounding beyond pre-defined activity classes by utilizing the semantic
diversity of natural language descriptions. The semantic diversity is rooted in
the principle of compositionality in linguistics, where novel semantics can be
systematically described by combining known words in novel ways (compositional
generalization). However, existing temporal grounding datasets are not
carefully designed to evaluate the compositional generalizability. To
systematically benchmark the compositional generalizability of temporal
grounding models, we introduce a new Compositional Temporal Grounding task and
construct two new dataset splits, i.e., Charades-CG and ActivityNet-CG. When
evaluating the state-of-the-art methods on our new dataset splits, we
empirically find that they fail to generalize to queries with novel
combinations of seen words. We argue that the inherent structured semantics
inside the videos and language is the crucial factor to achieve compositional
generalization. Based on this insight, we propose a variational cross-graph
reasoning framework that explicitly decomposes video and language into
hierarchical semantic graphs, respectively, and learns fine-grained semantic
correspondence between the two graphs. Furthermore, we introduce a novel
adaptive structured semantics learning approach to derive the
structure-informed and domain-generalizable graph representations, which
facilitate the fine-grained semantic correspondence reasoning between the two
graphs. Extensive experiments validate the superior compositional
generalizability of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2203.13049</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DASTSiam: Spatio-Temporal Fusion and Discriminative Augmentation for
  Improved Siamese Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Huang, Eksan Firkat, Ziwang Xiao, Jihong Zhu, Askar Hamdulla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tracking tasks based on deep neural networks have greatly improved with the
emergence of Siamese trackers. However, the appearance of targets often changes
during tracking, which can reduce the robustness of the tracker when facing
challenges such as aspect ratio change, occlusion, and scale variation. In
addition, cluttered backgrounds can lead to multiple high response points in
the response map, leading to incorrect target positioning. In this paper, we
introduce two transformer-based modules to improve Siamese tracking called
DASTSiam: the spatio-temporal (ST) fusion module and the Discriminative
Augmentation (DA) module. The ST module uses cross-attention based accumulation
of historical cues to improve robustness against object appearance changes,
while the DA module associates semantic information between the template and
search region to improve target discrimination. Moreover, Modifying the label
assignment of anchors also improves the reliability of the object location. Our
modules can be used with all Siamese trackers and show improved performance on
several public datasets through comparative and ablation experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Reconstruction of Non-cooperative Resident Space Objects using
  Instant NGP-accelerated NeRF and D-NeRF 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trupti Mahendrakar, Basilio Caruso, Van Minh Nguyen, Ryan T. White, Todd Steffen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of non-cooperative resident space objects (RSOs) in orbit
has spurred the demand for active space debris removal, on-orbit servicing
(OOS), classification, and functionality identification of these RSOs. Recent
advances in computer vision have enabled high-definition 3D modeling of objects
based on a set of 2D images captured from different viewing angles. This work
adapts Instant NeRF and D-NeRF, variations of the neural radiance field (NeRF)
algorithm to the problem of mapping RSOs in orbit for the purposes of
functionality identification and assisting with OOS. The algorithms are
evaluated for 3D reconstruction quality and hardware requirements using
datasets of images of a spacecraft mock-up taken under two different lighting
and motion conditions at the Orbital Robotic Interaction, On-Orbit Servicing
and Navigation (ORION) Laboratory at Florida Institute of Technology. Instant
NeRF is shown to learn high-fidelity 3D models with a computational cost that
could feasibly be trained on on-board computers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at AAS/AIAA Spaceflight Mechanics Conference 2023, 14
  pages, 10 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Autonomous Rendezvous with Non-cooperative Target Objects with Swarm
  Chasers and Observers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trupti Mahendrakar, Steven Holmberg, Andrew Ekblad, Emma Conti, Ryan T. White, Markus Wilde, Isaac Silver
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Space debris is on the rise due to the increasing demand for spacecraft for
com-munication, navigation, and other applications. The Space Surveillance
Network (SSN) tracks over 27,000 large pieces of debris and estimates the
number of small, un-trackable fragments at over 1,00,000. To control the growth
of debris, the for-mation of further debris must be reduced. Some solutions
include deorbiting larger non-cooperative resident space objects (RSOs) or
servicing satellites in or-bit. Both require rendezvous with RSOs, and the
scale of the problem calls for autonomous missions. This paper introduces the
Multipurpose Autonomous Ren-dezvous Vision-Integrated Navigation system
(MARVIN) developed and tested at the ORION Facility at Florida Institution of
Technology. MARVIN consists of two sub-systems: a machine vision-aided
navigation system and an artificial po-tential field (APF) guidance algorithm
which work together to command a swarm of chasers to safely rendezvous with the
RSO. We present the MARVIN architec-ture and hardware-in-the-loop experiments
demonstrating autonomous, collabo-rative swarm satellite operations
successfully guiding three drones to rendezvous with a physical mockup of a
non-cooperative satellite in motion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at AAS/AIAA Spaceflight Mechanics Meeting 2023, 17 pages, 9
  figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performance Study of YOLOv5 and Faster R-CNN for Autonomous Navigation
  around Non-Cooperative Targets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trupti Mahendrakar, Andrew Ekblad, Nathan Fischer, Ryan T. White, Markus Wilde, Brian Kish, Isaac Silver
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous navigation and path-planning around non-cooperative space objects
is an enabling technology for on-orbit servicing and space debris removal
systems. The navigation task includes the determination of target object
motion, the identification of target object features suitable for grasping, and
the identification of collision hazards and other keep-out zones. Given this
knowledge, chaser spacecraft can be guided towards capture locations without
damaging the target object or without unduly the operations of a servicing
target by covering up solar arrays or communication antennas. One way to
autonomously achieve target identification, characterization and feature
recognition is by use of artificial intelligence algorithms. This paper
discusses how the combination of cameras and machine learning algorithms can
achieve the relative navigation task. The performance of two deep
learning-based object detection algorithms, Faster Region-based Convolutional
Neural Networks (R-CNN) and You Only Look Once (YOLOv5), is tested using
experimental data obtained in formation flight simulations in the ORION Lab at
Florida Institute of Technology. The simulation scenarios vary the yaw motion
of the target object, the chaser approach trajectory, and the lighting
conditions in order to test the algorithms in a wide range of realistic and
performance limiting situations. The data analyzed include the mean average
precision metrics in order to compare the performance of the object detectors.
The paper discusses the path to implementing the feature recognition algorithms
and towards integrating them into the spacecraft Guidance Navigation and
Control system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures, 9 tables, IEEE Aerospace Conference 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resource-constrained FPGA Design for Satellite Component Feature
  Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Ekblad, Trupti Mahendrakar, Ryan T. White, Markus Wilde, Isaac Silver, Brooke Wheeler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The effective use of computer vision and machine learning for on-orbit
applications has been hampered by limited computing capabilities, and therefore
limited performance. While embedded systems utilizing ARM processors have been
shown to meet acceptable but low performance standards, the recent availability
of larger space-grade field programmable gate arrays (FPGAs) show potential to
exceed the performance of microcomputer systems. This work proposes use of
neural network-based object detection algorithm that can be deployed on a
comparably resource-constrained FPGA to automatically detect components of
non-cooperative, satellites on orbit. Hardware-in-the-loop experiments were
performed on the ORION Maneuver Kinematics Simulator at Florida Tech to compare
the performance of the new model deployed on a small, resource-constrained FPGA
to an equivalent algorithm on a microcomputer system. Results show the FPGA
implementation increases the throughput and decreases latency while maintaining
comparable accuracy. These findings suggest future missions should consider
deploying computer vision algorithms on space-grade FPGAs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures, 4 tables, Accepted at IEEE Aerospace Conference
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Champion Solution for the WSDM2023 Toloka VQA Challenge <span class="chip">WSDM</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyi Gao, Zhe Chen, Guo Chen, Wenhai Wang, Tong Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this report, we present our champion solution to the WSDM2023 Toloka
Visual Question Answering (VQA) Challenge. Different from the common VQA and
visual grounding (VG) tasks, this challenge involves a more complex scenario,
i.e. inferring and locating the object implicitly specified by the given
interrogative question. For this task, we leverage ViT-Adapter, a
pre-training-free adapter network, to adapt multi-modal pre-trained
Uni-Perceiver for better cross-modal localization. Our method ranks first on
the leaderboard, achieving 77.5 and 76.347 IoU on public and private test sets,
respectively. It shows that ViT-Adapter is also an effective paradigm for
adapting the unified perception model to vision-language downstream tasks. Code
and models will be released at
https://github.com/czczup/ViT-Adapter/tree/main/wsdm2023.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report in WSDM Cup 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Using Language to Extend to Unseen Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.09520v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.09520v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisa Dunlap, Clara Mohri, Devin Guillory, Han Zhang, Trevor Darrell, Joseph E. Gonzalez, Aditi Raghunathan, Anja Rohrbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is expensive to collect training data for every possible domain that a
vision model may encounter when deployed. We instead consider how simply
verbalizing the training domain (e.g. "photos of birds") as well as domains we
want to extend to but do not have data for (e.g. "paintings of birds") can
improve robustness. Using a multimodal model with a joint image and language
embedding space, our method LADS learns a transformation of the image
embeddings from the training domain to each unseen test domain, while
preserving task relevant information. Without using any images from the unseen
test domain, we show that over the extended domain containing both training and
unseen test domains, LADS outperforms standard fine-tuning and ensemble
approaches over a suite of four benchmarks targeting domain adaptation and
dataset bias.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Look at Adjacent Frames: Video Anomaly Detection without Offline
  Training <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.13798v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.13798v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Ouyang, Guodong Shen, Victor Sanchez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a solution to detect anomalous events in videos without the need
to train a model offline. Specifically, our solution is based on a
randomly-initialized multilayer perceptron that is optimized online to
reconstruct video frames, pixel-by-pixel, from their frequency information.
Based on the information shifts between adjacent frames, an incremental learner
is used to update parameters of the multilayer perceptron after observing each
frame, thus allowing to detect anomalous events along the video stream.
Traditional solutions that require no offline training are limited to operating
on videos with only a few abnormal frames. Our solution breaks this limit and
achieves strong performance on benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ECCV 2022 RWS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HIER: Metric Learning Beyond Class Labels via Hierarchical
  Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.14258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.14258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungyeon Kim, Boseung Jeong, Suha Kwak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervision for metric learning has long been given in the form of
equivalence between human-labeled classes. Although this type of supervision
has been a basis of metric learning for decades, we argue that it hinders
further advances of the field. In this regard, we propose a new regularization
method, dubbed HIER, to discover the latent semantic hierarchy of training
data, and to deploy the hierarchy to provide richer and more fine-grained
supervision than inter-class separability induced by common metric learning
losses. HIER achieved this goal with no annotation for the semantic hierarchy
but by learning hierarchical proxies in hyperbolic spaces. The hierarchical
proxies are learnable parameters, and each of them is trained to serve as an
ancestor of a group of data or other proxies to approximate the semantic
hierarchy among them. HIER deals with the proxies along with data in hyperbolic
space since geometric properties of the space are well-suited to represent
their hierarchical structure. The efficacy of HIER was evaluated on four
standard benchmarks, where it consistently improved performance of conventional
methods when integrated with them, and consequently achieved the best records,
surpassing even the existing hyperbolic metric learning technique, in almost
all settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reversible Column Networks <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.11696v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.11696v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Cai, Yizhuang Zhou, Qi Han, Jianjian Sun, Xiangwen Kong, Jun Li, Xiangyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new neural network design paradigm Reversible Column Network
(RevCol). The main body of RevCol is composed of multiple copies of
subnetworks, named columns respectively, between which multi-level reversible
connections are employed. Such architectural scheme attributes RevCol very
different behavior from conventional networks: during forward propagation,
features in RevCol are learned to be gradually disentangled when passing
through each column, whose total information is maintained rather than
compressed or discarded as other network does. Our experiments suggest that
CNN-style RevCol models can achieve very competitive performances on multiple
computer vision tasks such as image classification, object detection and
semantic segmentation, especially with large parameter budget and large
dataset. For example, after ImageNet-22K pre-training, RevCol-XL obtains 88.2%
ImageNet-1K accuracy. Given more pre-training data, our largest model RevCol-H
reaches 90.0% on ImageNet-1K, 63.8% APbox on COCO detection minival set, 61.0%
mIoU on ADE20k segmentation. To our knowledge, it is the best COCO detection
and ADE20k segmentation result among pure (static) CNN models. Moreover, as a
general macro architecture fashion, RevCol can also be introduced into
transformers or other neural networks, which is demonstrated to improve the
performances in both computer vision and NLP tasks. We release code and models
at https://github.com/megvii-research/RevCol
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Delving into Semantic Scale Imbalance <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.14613v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.14613v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanbiao Ma, Licheng Jiao, Fang Liu, Yuxin Li, Shuyuan Yang, Xu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model bias triggered by long-tailed data has been widely studied. However,
measure based on the number of samples cannot explicate three phenomena
simultaneously: (1) Given enough data, the classification performance gain is
marginal with additional samples. (2) Classification performance decays
precipitously as the number of training samples decreases when there is
insufficient data. (3) Model trained on sample-balanced datasets still has
different biases for different classes. In this work, we define and quantify
the semantic scale of classes, which is used to measure the feature diversity
of classes. It is exciting to find experimentally that there is a marginal
effect of semantic scale, which perfectly describes the first two phenomena.
Further, the quantitative measurement of semantic scale imbalance is proposed,
which can accurately reflect model bias on multiple datasets, even on
sample-balanced data, revealing a novel perspective for the study of class
imbalance. Due to the prevalence of semantic scale imbalance, we propose
semantic-scale-balanced learning, including a general loss improvement scheme
and a dynamic re-weighting training framework that overcomes the challenge of
calculating semantic scales in real-time during iterations. Comprehensive
experiments show that dynamic semantic-scale-balanced learning consistently
enables the model to perform superiorly on large-scale long-tailed and
non-long-tailed natural and medical datasets, which is a good starting point
for mitigating the prevalent but unnoticed model bias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 pages, 26 figures, 12 tables, Published as a conference paper at
  ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Exponentially Modified Gaussian Oscillators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.12202v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.12202v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Hahne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acoustic modeling serves audio processing tasks such as de-noising, data
reconstruction, model-based testing and classification. Previous work dealt
with signal parameterization of wave envelopes either by multiple Gaussian
distributions or a single asymmetric Gaussian curve, which both fall short in
representing super-imposed echoes sufficiently well. This study presents a
three-stage Multimodal Exponentially Modified Gaussian (MEMG) model with an
optional oscillating term that regards captured echoes as a superposition of
univariate probability distributions in the temporal domain. With this,
synthetic ultrasound signals suffering from artifacts can be fully recovered,
which is backed by quantitative assessment. Real data experimentation is
carried out to demonstrate the classification capability of the acquired
features with object reflections being detected at different points in time.
The code is available at https://github.com/hahnec/multimodal_emg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE International Ultrasonic Symposium 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SUES-200: A Multi-height Multi-scene Cross-view Image Benchmark Across
  Drone and Satellite 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.10704v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.10704v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runzhe Zhu, Ling Yin, Mingze Yang, Fei Wu, Yuncheng Yang, Wenbo Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-view image matching aims to match images of the same target scene
acquired from different platforms. With the rapid development of drone
technology, cross-view matching by neural network models has been a widely
accepted choice for drone position or navigation. However, existing public
datasets do not include images obtained by drones at different heights, and the
types of scenes are relatively homogeneous, which yields issues in assessing a
model's capability to adapt to complex and changing scenes. In this end, we
present a new cross-view dataset called SUES-200 to address these issues.
SUES-200 contains 24120 images acquired by the drone at four different heights
and corresponding satellite view images of the same target scene. To the best
of our knowledge, SUES-200 is the first public dataset that considers the
differences generated in aerial photography captured by drones flying at
different heights. In addition, we developed an evaluation for efficient
training, testing and evaluation of cross-view matching models, under which we
comprehensively analyze the performance of nine architectures. Then, we propose
a robust baseline model for use with SUES-200. Experimental results show that
SUES-200 can help the model to learn highly discriminative features of the
height of the drone.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Debiasing the Cloze Task in Sequential Recommendation with Bidirectional
  <span class="highlight-title">Transformer</span>s <span class="chip">KDD '22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khalil Damak, Sami Khenissi, Olfa Nasraoui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bidirectional Transformer architectures are state-of-the-art sequential
recommendation models that use a bi-directional representation capacity based
on the Cloze task, a.k.a. Masked Language Modeling. The latter aims to predict
randomly masked items within the sequence. Because they assume that the true
interacted item is the most relevant one, an exposure bias results, where
non-interacted items with low exposure propensities are assumed to be
irrelevant. The most common approach to mitigating exposure bias in
recommendation has been Inverse Propensity Scoring (IPS), which consists of
down-weighting the interacted predictions in the loss function in proportion to
their propensities of exposure, yielding a theoretically unbiased learning. In
this work, we argue and prove that IPS does not extend to sequential
recommendation because it fails to account for the temporal nature of the
problem. We then propose a novel propensity scoring mechanism, which can
theoretically debias the Cloze task in sequential recommendation. Finally we
empirically demonstrate the debiasing capabilities of our proposed approach and
its robustness to the severity of exposure bias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures, Accepted at KDD '22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPEC5G: A <span class="highlight-title">Dataset</span> for 5G Cellular Network Protocol Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Imtiaz Karim, Kazi Samin Mubasshir, Mirza Masfiqur Rahman, Elisa Bertino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  5G is the 5th generation cellular network protocol. It is the
state-of-the-art global wireless standard that enables an advanced kind of
network designed to connect virtually everyone and everything with increased
speed and reduced latency. Therefore, its development, analysis, and security
are critical. However, all approaches to the 5G protocol development and
security analysis, e.g., property extraction, protocol summarization, and
semantic analysis of the protocol specifications and implementations are
completely manual. To reduce such manual effort, in this paper, we curate
SPEC5G the first-ever public 5G dataset for NLP research. The dataset contains
3,547,586 sentences with 134M words, from 13094 cellular network specifications
and 13 online websites. By leveraging large-scale pre-trained language models
that have achieved state-of-the-art results on NLP tasks, we use this dataset
for security-related text classification and summarization. Security-related
text classification can be used to extract relevant security-related properties
for protocol testing. On the other hand, summarization can help developers and
practitioners understand the high level of the protocol, which is itself a
daunting task. Our results show the value of our 5G-centric dataset in 5G
protocol analysis automation. We believe that SPEC5G will enable a new research
direction into automatic analyses for the 5G cellular network protocol and
numerous related downstream tasks. Our data and code are publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Durability and Availability of Erasure-Coded Storage Systems with
  Concurrent Maintenance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09057v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09057v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suayb S. Arslan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This initial version of this document was written back in 2014 for the sole
purpose of providing fundamentals of reliability theory as well as to identify
the theoretical types of machinery for the prediction of
durability/availability of erasure-coded storage systems. Since the definition
of a "system" is too broad, we specifically focus on warm/cold storage systems
where the data is stored in a distributed fashion across different storage
units with or without continuous operation. The contents of this document are
dedicated to a review of fundamentals, a few major improved stochastic models,
and several contributions of my work relevant to the field. One of the
contributions of this document is the introduction of the most general form of
Markov models for the estimation of mean time to failure. This work was
partially later published in IEEE Transactions on Reliability. Very good
approximations for the closed-form solutions for this general model are also
investigated. Various storage configurations under different policies are
compared using such advanced models. Later in a subsequent chapter, we have
also considered multi-dimensional Markov models to address detached
drive-medium combinations such as those found in optical disk and tape storage
systems. It is not hard to anticipate such a system structure would most likely
be part of future DNA storage libraries. This work is partially published in
Elsevier Reliability and System Safety. Topics that include simulation
modelings for more accurate estimations are included towards the end of the
document by noting the deficiencies of the simplified canonical as well as more
complex Markov models, due mainly to the stationary and static nature of
Markovinity. Throughout the document, we shall focus on concurrently maintained
systems although the discussions will only slightly change for the systems
repaired one device at a time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>58 pages, 20 figures, 9 tables. arXiv admin note: substantial text
  overlap with arXiv:1911.00329</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Questions Are All You Need to Train a Dense Passage Retriever <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.10658v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.10658v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, Manzil Zaheer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ART, a new corpus-level autoencoding approach for training dense
retrieval models that does not require any labeled training data. Dense
retrieval is a central challenge for open-domain tasks, such as Open QA, where
state-of-the-art methods typically require large supervised datasets with
custom hard-negative mining and denoising of positive examples. ART, in
contrast, only requires access to unpaired inputs and outputs (e.g. questions
and potential answer documents). It uses a new document-retrieval autoencoding
scheme, where (1) an input question is used to retrieve a set of evidence
documents, and (2) the documents are then used to compute the probability of
reconstructing the original question. Training for retrieval based on question
reconstruction enables effective unsupervised learning of both document and
question encoders, which can be later incorporated into complete Open QA
systems without any further finetuning. Extensive experiments demonstrate that
ART obtains state-of-the-art results on multiple QA retrieval benchmarks with
only generic initialization from a pre-trained language model, removing the
need for labeled data and task-specific losses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TACL, pre MIT Press publication version</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deterministic Online Classification: Non-iteratively Reweighted
  Recursive Least-Squares for Binary Class Rebalancing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Se-In Jang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deterministic solutions are becoming more critical for interpretability.
Weighted Least-Squares (WLS) has been widely used as a deterministic batch
solution with a specific weight design. In the online settings of WLS, exact
reweighting is necessary to converge to its batch settings. In order to comply
with its necessity, the iteratively reweighted least-squares algorithm is
mainly utilized with a linearly growing time complexity which is not attractive
for online learning. Due to the high and growing computational costs, an
efficient online formulation of reweighted least-squares is desired. We
introduce a new deterministic online classification algorithm of WLS with a
constant time complexity for binary class rebalancing. We demonstrate that our
proposed online formulation exactly converges to its batch formulation and
outperforms existing state-of-the-art stochastic online binary classification
algorithms in real-world data sets empirically.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Doubly Adversarial Federated Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialin Yi, Milan Vojnović
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a new non-stochastic federated multi-armed bandit problem with
multiple agents collaborating via a communication network. The losses of the
arms are assigned by an oblivious adversary that specifies the loss of each arm
not only for each time step but also for each agent, which we call ``doubly
adversarial". In this setting, different agents may choose the same arm in the
same time step but observe different feedback. The goal of each agent is to
find a globally best arm in hindsight that has the lowest cumulative loss
averaged over all agents, which necessities the communication among agents. We
provide regret lower bounds for any federated bandit algorithm under different
settings, when agents have access to full-information feedback, or the bandit
feedback. For the bandit feedback setting, we propose a near-optimal federated
bandit algorithm called FEDEXP3. Our algorithm gives a positive answer to an
open question proposed in Cesa-Bianchi et al. (2016): FEDEXP3 can guarantee a
sub-linear regret without exchanging sequences of selected arm identities or
loss sequences among agents. We also provide numerical evaluations of our
algorithm to validate our theoretical results and demonstrate its effectiveness
on synthetic and real-world datasets
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Debiasing the Cloze Task in Sequential Recommendation with Bidirectional
  <span class="highlight-title">Transformer</span>s <span class="chip">KDD '22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khalil Damak, Sami Khenissi, Olfa Nasraoui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bidirectional Transformer architectures are state-of-the-art sequential
recommendation models that use a bi-directional representation capacity based
on the Cloze task, a.k.a. Masked Language Modeling. The latter aims to predict
randomly masked items within the sequence. Because they assume that the true
interacted item is the most relevant one, an exposure bias results, where
non-interacted items with low exposure propensities are assumed to be
irrelevant. The most common approach to mitigating exposure bias in
recommendation has been Inverse Propensity Scoring (IPS), which consists of
down-weighting the interacted predictions in the loss function in proportion to
their propensities of exposure, yielding a theoretically unbiased learning. In
this work, we argue and prove that IPS does not extend to sequential
recommendation because it fails to account for the temporal nature of the
problem. We then propose a novel propensity scoring mechanism, which can
theoretically debias the Cloze task in sequential recommendation. Finally we
empirically demonstrate the debiasing capabilities of our proposed approach and
its robustness to the severity of exposure bias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures, Accepted at KDD '22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Relaxed Models for Adversarial Streaming: The Advice Model and the
  Bounded Interruptions Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Menachem Sadigurschi, Moshe Shechner, Uri Stemmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Streaming algorithms are typically analyzed in the oblivious setting, where
we assume that the input stream is fixed in advance. Recently, there is a
growing interest in designing adversarially robust streaming algorithms that
must maintain utility even when the input stream is chosen adaptively and
adversarially as the execution progresses. While several fascinating results
are known for the adversarial setting, in general, it comes at a very high cost
in terms of the required space. Motivated by this, in this work we set out to
explore intermediate models that allow us to interpolate between the oblivious
and the adversarial models. Specifically, we put forward the following two
models:
  (1) *The advice model*, in which the streaming algorithm may occasionally ask
for one bit of advice.
  (2) *The bounded interruptions model*, in which we assume that the adversary
is only partially adaptive.
  We present both positive and negative results for each of these two models.
In particular, we present generic reductions from each of these models to the
oblivious model. This allows us to design robust algorithms with significantly
improved space complexity compared to what is known in the plain adversarial
model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPEC5G: A <span class="highlight-title">Dataset</span> for 5G Cellular Network Protocol Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Imtiaz Karim, Kazi Samin Mubasshir, Mirza Masfiqur Rahman, Elisa Bertino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  5G is the 5th generation cellular network protocol. It is the
state-of-the-art global wireless standard that enables an advanced kind of
network designed to connect virtually everyone and everything with increased
speed and reduced latency. Therefore, its development, analysis, and security
are critical. However, all approaches to the 5G protocol development and
security analysis, e.g., property extraction, protocol summarization, and
semantic analysis of the protocol specifications and implementations are
completely manual. To reduce such manual effort, in this paper, we curate
SPEC5G the first-ever public 5G dataset for NLP research. The dataset contains
3,547,586 sentences with 134M words, from 13094 cellular network specifications
and 13 online websites. By leveraging large-scale pre-trained language models
that have achieved state-of-the-art results on NLP tasks, we use this dataset
for security-related text classification and summarization. Security-related
text classification can be used to extract relevant security-related properties
for protocol testing. On the other hand, summarization can help developers and
practitioners understand the high level of the protocol, which is itself a
daunting task. Our results show the value of our 5G-centric dataset in 5G
protocol analysis automation. We believe that SPEC5G will enable a new research
direction into automatic analyses for the 5G cellular network protocol and
numerous related downstream tasks. Our data and code are publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lower Bounds on Learning Pauli Channels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Fawzi, Aadil Oufkir, Daniel Stilck França
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the noise affecting a quantum device is of fundamental
importance for scaling quantum technologies. A particularly important class of
noise models is that of Pauli channels, as randomized compiling techniques can
effectively bring any quantum channel to this form and are significantly more
structured than general quantum channels. In this paper, we show fundamental
lower bounds on the sample complexity for learning Pauli channels in diamond
norm with unentangled measurements. We consider both adaptive and non-adaptive
strategies. In the non-adaptive setting, we show a lower bound of
$\Omega(2^{3n}\epsilon^{-2})$ to learn an $n$-qubit Pauli channel. In
particular, this shows that the recently introduced learning procedure by
Flammia and Wallman is essentially optimal. In the adaptive setting, we show a
lower bound of $\Omega(2^{2.5n}\epsilon^{-2})$ for
$\epsilon=\mathcal{O}(2^{-n})$, and a lower bound of
$\Omega(2^{2n}\epsilon^{-2} )$ for any $\epsilon > 0$. This last lower bound
even applies for arbitrarily many sequential uses of the channel, as long as
they are only interspersed with other unital operations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MATT: Multimodal Attention Level Estimation for e-learning Platforms <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roberto Daza, Luis F. Gomez, Aythami Morales, Julian Fierrez, Ruben Tolosana, Ruth Cobos, Javier Ortega-Garcia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a new multimodal system for remote attention level
estimation based on multimodal face analysis. Our multimodal approach uses
different parameters and signals obtained from the behavior and physiological
processes that have been related to modeling cognitive load such as faces
gestures (e.g., blink rate, facial actions units) and user actions (e.g., head
pose, distance to the camera). The multimodal system uses the following modules
based on Convolutional Neural Networks (CNNs): Eye blink detection, head pose
estimation, facial landmark detection, and facial expression features. First,
we individually evaluate the proposed modules in the task of estimating the
student's attention level captured during online e-learning sessions. For that
we trained binary classifiers (high or low attention) based on Support Vector
Machines (SVM) for each module. Secondly, we find out to what extent multimodal
score level fusion improves the attention level estimation. The mEBAL database
is used in the experimental framework, a public multi-modal database for
attention level estimation obtained in an e-learning environment that contains
data from 38 users while conducting several e-learning tasks of variable
difficulty (creating changes in student cognitive loads).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint of the paper presented to the Workshop on Artificial
  Intelligence for Education (AI4EDU) of AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy Prediction using Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meghana Bharadwaj, Sanjana Sarda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we demonstrate the viability of using federated learning to
successfully predict energy consumption as well as solar production for all
households within a certain network using low-power and low-space consuming
embedded devices. We also demonstrate our prediction performance improving over
time without the need for sharing private consumer energy data. We simulate a
system with four nodes using data for one year to show this.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unifying Synergies between <span class="highlight-title">Self-supervised</span> Learning and Dynamic
  Computation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tarun Krishna, Ayush K Rai, Alexandru Drimbarean, Alan F Smeaton, Kevin McGuinness, Noel E O'Connor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) approaches have made major strides forward by
emulating the performance of their supervised counterparts on several computer
vision benchmarks. This, however, comes at a cost of substantially larger model
sizes, and computationally expensive training strategies, which eventually lead
to larger inference times making it impractical for resource constrained
industrial settings. Techniques like knowledge distillation (KD), dynamic
computation (DC), and pruning are often used to obtain a lightweight
sub-network, which usually involves multiple epochs of fine-tuning of a large
pre-trained model, making it more computationally challenging.
  In this work we propose a novel perspective on the interplay between SSL and
DC paradigms that can be leveraged to simultaneously learn a dense and gated
(sparse/lightweight) sub-network from scratch offering a good
accuracy-efficiency trade-off, and therefore yielding a generic and
multi-purpose architecture for application specific industrial settings. Our
study overall conveys a constructive message: exhaustive experiments on several
image classification benchmarks: CIFAR-10, STL-10, CIFAR-100, and ImageNet-100,
demonstrates that the proposed training strategy provides a dense and
corresponding sparse sub-network that achieves comparable (on-par) performance
compared with the vanilla self-supervised setting, but at a significant
reduction in computation in terms of FLOPs under a range of target budgets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Abstracting Imperfect Information Away from Two-Player Zero-Sum Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Sokota, Ryan D'Orazio, Chun Kai Ling, David J. Wu, J. Zico Kolter, Noam Brown
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In their seminal work, Nayyar et al. (2013) showed that imperfect information
can be abstracted away from common-payoff games by having players publicly
announce their policies as they play. This insight underpins sound solvers and
decision-time planning algorithms for common-payoff games. Unfortunately, a
naive application of the same insight to two-player zero-sum games fails
because Nash equilibria of the game with public policy announcements may not
correspond to Nash equilibria of the original game. As a consequence, existing
sound decision-time planning algorithms require complicated additional
mechanisms that have unappealing properties. The main contribution of this work
is showing that certain regularized equilibria do not possess the
aforementioned non-correspondence problem -- thus, computing them can be
treated as perfect information problems. Because these regularized equilibria
can be made arbitrarily close to Nash equilibria, our result opens the door to
a new perspective on solving two-player zero-sum games and, in particular,
yields a simplified framework for decision-time planning in two-player zero-sum
games, void of the unappealing properties that plague existing decision-time
planning approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span> Federated Learning for Weather Forecasting: Toward Foundation
  Models on Meteorological Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengchao Chen, Guodong Long, Tao Shen, Jing Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To tackle the global climate challenge, it urgently needs to develop a
collaborative platform for comprehensive weather forecasting on large-scale
meteorological data. Despite urgency, heterogeneous meteorological sensors
across countries and regions, inevitably causing multivariate heterogeneity and
data exposure, become the main barrier. This paper develops a foundation model
across regions capable of understanding complex meteorological data and
providing weather forecasting. To relieve the data exposure concern across
regions, a novel federated learning approach has been proposed to
collaboratively learn a brand-new spatio-temporal Transformer-based foundation
model across participants with heterogeneous meteorological data. Moreover, a
novel prompt learning mechanism has been adopted to satisfy low-resourced
sensors' communication and computational constraints. The effectiveness of the
proposed method has been demonstrated on classical weather forecasting tasks
using three meteorological datasets with multivariate time series.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LF-checker: Machine Learning Acceleration of Bounded Model Checking for
  Concurrency Verification (Competition Contribution) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Wu, Edoardo Manino, Fatimah Aljaafari, Pavlos Petoumenos, Lucas C. Cordeiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe and evaluate LF-checker, a metaverifier tool based on machine
learning. It extracts multiple features of the program under test and predicts
the optimal configuration (flags) of a bounded model checker with a decision
tree. Our current work is specialised in concurrency verification and employs
ESBMC as a back-end verification engine. In the paper, we demonstrate that
LF-checker achieves better results than the default configuration of the
underlying verification engine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Quantum Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raoul Heese, Thore Gerlach, Sascha Mücke, Sabine Müller, Matthias Jakobs, Nico Piatkowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Methods of artificial intelligence (AI) and especially machine learning (ML)
have been growing ever more complex, and at the same time have more and more
impact on people's lives. This leads to explainable AI (XAI) manifesting itself
as an important research field that helps humans to better comprehend ML
systems. In parallel, quantum machine learning (QML) is emerging with the
ongoing improvement of quantum computing hardware combined with its increasing
availability via cloud services. QML enables quantum-enhanced ML in which
quantum mechanics is exploited to facilitate ML tasks, typically in form of
quantum-classical hybrid algorithms that combine quantum and classical
resources. Quantum gates constitute the building blocks of gate-based quantum
hardware and form circuits that can be used for quantum computations. For QML
applications, quantum circuits are typically parameterized and their parameters
are optimized classically such that a suitably defined objective function is
minimized. Inspired by XAI, we raise the question of explainability of such
circuits by quantifying the importance of (groups of) gates for specific goals.
To this end, we transfer and adapt the well-established concept of Shapley
values to the quantum realm. The resulting attributions can be interpreted as
explanations for why a specific circuit works well for a given task, improving
the understanding of how to construct parameterized (or variational) quantum
circuits, and fostering their human interpretability in general. An
experimental evaluation on simulators and two superconducting quantum hardware
devices demonstrates the benefits of the proposed framework for classification,
generative modeling, transpilation, and optimization. Furthermore, our results
shed some light on the role of specific gates in popular QML approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 27 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Design-based individual prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li-Chun Zhang, Danhyang Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A design-based individual prediction approach is developed based on the
expected cross-validation results, given the sampling design and the
sample-splitting design for cross-validation. Whether the predictor is selected
from an ensemble of models or a weighted average of them, valid inference of
the unobserved prediction errors is defined and obtained with respect to the
sampling design, while outcomes and features are treated as constants.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially Private Natural Language Models: Recent Advances and
  Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lijie Hu, Ivan Habernal, Lei Shen, Di Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in deep learning have led to great success in various
natural language processing (NLP) tasks. However, these applications may
involve data that contain sensitive information. Therefore, how to achieve good
performance while also protect privacy of sensitive data is a crucial challenge
in NLP. To preserve privacy, Differential Privacy (DP), which can prevent
reconstruction attacks and protect against potential side knowledge, is
becoming a de facto technique for private data analysis. In recent years, NLP
in DP models (DP-NLP) has been studied from different perspectives, which
deserves a comprehensive review. In this paper, we provide the first systematic
review of recent advances on DP deep learning models in NLP. In particular, we
first discuss some differences and additional challenges of DP-NLP compared
with the standard DP deep learning. Then we investigate some existing work on
DP-NLP and present its recent developments from two aspects: gradient
perturbation based methods and embedding vector perturbation based methods. We
also discuss some challenges and future directions of this topic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Recommendation with Additive Personalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Li, Guodong Long, Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With rising concerns about privacy, developing recommendation systems in a
federated setting become a new paradigm to develop next-generation Internet
service architecture. However, existing approaches are usually derived from a
distributed recommendation framework with an additional mechanism for privacy
protection, thus most of them fail to fully exploit personalization in the new
context of federated recommendation settings. In this paper, we propose a novel
approach called Federated Recommendation with Additive Personalization (FedRAP)
to enhance recommendation by learning user embedding and the user's personal
view of item embeddings. Specifically, the proposed additive personalization is
to add a personalized item embedding to a sparse global item embedding
aggregated from all users. Moreover, a curriculum learning mechanism has been
applied for additive personalization on item embeddings by gradually increasing
regularization weights to mitigate the performance degradation caused by large
variances among client-specific item embeddings. A unified formulation has been
proposed with a sparse regularization of global item embeddings for reducing
communication overhead. Experimental results on four real-world recommendation
datasets demonstrate the effectiveness of FedRAP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BallGAN: 3D-aware Image Synthesis with a Spherical Background 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjung Shin, Yunji Seo, Jeongmin Bae, Young Sun Choi, Hyunsu Kim, Hyeran Byun, Youngjung Uh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D-aware GANs aim to synthesize realistic 3D scenes such that they can be
rendered in arbitrary perspectives to produce images. Although previous methods
produce realistic images, they suffer from unstable training or degenerate
solutions where the 3D geometry is unnatural. We hypothesize that the 3D
geometry is underdetermined due to the insufficient constraint, i.e., being
classified as real image to the discriminator is not enough. To solve this
problem, we propose to approximate the background as a spherical surface and
represent a scene as a union of the foreground placed in the sphere and the
thin spherical background. It reduces the degree of freedom in the background
field. Accordingly, we modify the volume rendering equation and incorporate
dedicated constraints to design a novel 3D-aware GAN framework named BallGAN.
BallGAN has multiple advantages as follows. 1) It produces more reasonable 3D
geometry; the images of a scene across different viewpoints have better
photometric consistency and fidelity than the state-of-the-art methods. 2) The
training becomes much more stable. 3) The foreground can be separately rendered
on top of different arbitrary backgrounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://minjung-s.github.io/ballgan</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parallel Approaches to Accelerate Bayesian Decision Trees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Efthyvoulos Drousiotis, Paul G. Spirakis, Simon Maskell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Markov Chain Monte Carlo (MCMC) is a well-established family of algorithms
primarily used in Bayesian statistics to sample from a target distribution when
direct sampling is challenging. Existing work on Bayesian decision trees uses
MCMC. Unfortunately, this can be slow, especially when considering large
volumes of data. It is hard to parallelise the accept-reject component of the
MCMC. None-the-less, we propose two methods for exploiting parallelism in the
MCMC: in the first, we replace the MCMC with another numerical Bayesian
approach, the Sequential Monte Carlo (SMC) sampler, which has the appealing
property that it is an inherently parallel algorithm; in the second, we
consider data partitioning. Both methods use multi-core processing with a
HighPerformance Computing (HPC) resource. We test the two methods in various
study settings to determine which method is the most beneficial for each test
case. Experiments show that data partitioning has limited utility in the
settings we consider and that the use of the SMC sampler can improve run-time
(compared to the sequential implementation) by up to a factor of 343.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provable Unrestricted Adversarial Training without Compromise with
  Generalizability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lilin Zhang, Ning Yang, Yanchao Sun, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial training (AT) is widely considered as the most promising strategy
to defend against adversarial attacks and has drawn increasing interest from
researchers. However, the existing AT methods still suffer from two challenges.
First, they are unable to handle unrestricted adversarial examples (UAEs),
which are built from scratch, as opposed to restricted adversarial examples
(RAEs), which are created by adding perturbations bound by an $l_p$ norm to
observed examples. Second, the existing AT methods often achieve adversarial
robustness at the expense of standard generalizability (i.e., the accuracy on
natural examples) because they make a tradeoff between them. To overcome these
challenges, we propose a unique viewpoint that understands UAEs as
imperceptibly perturbed unobserved examples. Also, we find that the tradeoff
results from the separation of the distributions of adversarial examples and
natural examples. Based on these ideas, we propose a novel AT approach called
Provable Unrestricted Adversarial Training (PUAT), which can provide a target
classifier with comprehensive adversarial robustness against both UAE and RAE,
and simultaneously improve its standard generalizability. Particularly, PUAT
utilizes partially labeled data to achieve effective UAE generation by
accurately capturing the natural data distribution through a novel augmented
triple-GAN. At the same time, PUAT extends the traditional AT by introducing
the supervised loss of the target classifier into the adversarial loss and
achieves the alignment between the UAE distribution, the natural data
distribution, and the distribution learned by the classifier, with the
collaboration of the augmented triple-GAN. Finally, the solid theoretical
analysis and extensive experiments conducted on widely-used benchmarks
demonstrate the superiority of PUAT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DASTSiam: Spatio-Temporal Fusion and Discriminative Augmentation for
  Improved Siamese Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Huang, Eksan Firkat, Ziwang Xiao, Jihong Zhu, Askar Hamdulla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tracking tasks based on deep neural networks have greatly improved with the
emergence of Siamese trackers. However, the appearance of targets often changes
during tracking, which can reduce the robustness of the tracker when facing
challenges such as aspect ratio change, occlusion, and scale variation. In
addition, cluttered backgrounds can lead to multiple high response points in
the response map, leading to incorrect target positioning. In this paper, we
introduce two transformer-based modules to improve Siamese tracking called
DASTSiam: the spatio-temporal (ST) fusion module and the Discriminative
Augmentation (DA) module. The ST module uses cross-attention based accumulation
of historical cues to improve robustness against object appearance changes,
while the DA module associates semantic information between the template and
search region to improve target discrimination. Moreover, Modifying the label
assignment of anchors also improves the reliability of the object location. Our
modules can be used with all Siamese trackers and show improved performance on
several public datasets through comparative and ablation experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Speaker Embeddings with Adversarial Multi-task Learning for
  Age Group Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kwangje Baeg, Yeong-Gwan Kim, Young-Sub Han, Byoung-Ki Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, researchers have utilized neural network-based speaker embedding
techniques in speaker-recognition tasks to identify speakers accurately.
However, speaker-discriminative embeddings do not always represent speech
features such as age group well. In an embedding model that has been highly
trained to capture speaker traits, the task of age group classification is
closer to speech information leakage. Hence, to improve age group
classification performance, we consider the use of speaker-discriminative
embeddings derived from adversarial multi-task learning to align features and
reduce the domain discrepancy in age subgroups. In addition, we investigated
different types of speaker embeddings to learn and generalize the
domain-invariant representations for age groups. Experimental results on the
VoxCeleb Enrichment dataset verify the effectiveness of our proposed adaptive
adversarial network in multi-objective scenarios and leveraging speaker
embeddings for the domain adaptation task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Reject with a Fixed Predictor: Application to
  Decontextualization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Mohri, Daniel Andor, Eunsol Choi, Michael Collins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of classification with a reject option for a fixed
predictor, applicable in natural language processing. \ignore{where many
correct labels are often possible} We introduce a new problem formulation for
this scenario, and an algorithm minimizing a new surrogate loss function. We
provide a complete theoretical analysis of the surrogate loss function with a
strong $H$-consistency guarantee. For evaluation, we choose the
\textit{decontextualization} task, and provide a manually-labelled dataset of
$2\mathord,000$ examples. Our algorithm significantly outperforms the baselines
considered, with a $\sim\!\!25\%$ improvement in coverage when halving the
error rate, which is only $\sim\!\! 3 \%$ away from the theoretical limit.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Shape of Explanations: A Topological Account of Rule-Based
  Explanations in Machine Learning <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brett Mullins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rule-based explanations provide simple reasons explaining the behavior of
machine learning classifiers at given points in the feature space. Several
recent methods (Anchors, LORE, etc.) purport to generate rule-based
explanations for arbitrary or black-box classifiers. But what makes these
methods work in general? We introduce a topological framework for rule-based
explanation methods and provide a characterization of explainability in terms
of the definability of a classifier relative to an explanation scheme. We
employ this framework to consider various explanation schemes and argue that
the preferred scheme depends on how much the user knows about the domain and
the probability measure over the feature space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The AAAI 2023 Workshop on Representation Learning for
  Responsible Human-Centric AI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counterfactual (Non-)identifiability of Learned Structural Causal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arash Nasr-Esfahany, Emre Kiciman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in probabilistic generative modeling have motivated learning
Structural Causal Models (SCM) from observational datasets using deep
conditional generative models, also known as Deep Structural Causal Models
(DSCM). If successful, DSCMs can be utilized for causal estimation tasks, e.g.,
for answering counterfactual queries. In this work, we warn practitioners about
non-identifiability of counterfactual inference from observational data, even
in the absence of unobserved confounding and assuming known causal structure.
We prove counterfactual identifiability of monotonic generation mechanisms with
single dimensional exogenous variables. For general generation mechanisms with
multi-dimensional exogenous variables, we provide an impossibility result for
counterfactual identifiability, motivating the need for parametric assumptions.
As a practical approach, we propose a method for estimating worst-case errors
of learned DSCMs' counterfactual predictions. The size of this error can be an
essential metric for deciding whether or not DSCMs are a viable approach for
counterfactual inference in a specific problem setting. In evaluation, our
method confirms negligible counterfactual errors for an identifiable SCM from
prior work, and also provides informative error bounds on counterfactual errors
for a non-identifiable synthetic SCM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Condition monitoring and anomaly detection in cyber-physical systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Marfo, Deepak K. Tosh, Shirley V. Moore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The modern industrial environment is equipping myriads of smart manufacturing
machines where the state of each device can be monitored continuously. Such
monitoring can help identify possible future failures and develop a
cost-effective maintenance plan. However, it is a daunting task to perform
early detection with low false positives and negatives from the huge volume of
collected data. This requires developing a holistic machine learning framework
to address the issues in condition monitoring of high-priority components and
develop efficient techniques to detect anomalies that can detect and possibly
localize the faulty components. This paper presents a comparative analysis of
recent machine learning approaches for robust, cost-effective anomaly detection
in cyber-physical systems. While detection has been extensively studied, very
few researchers have analyzed the localization of the anomalies. We show that
supervised learning outperforms unsupervised algorithms. For supervised cases,
we achieve near-perfect accuracy of 98 percent (specifically for tree-based
algorithms). In contrast, the best-case accuracy in the unsupervised cases was
63 percent :the area under the receiver operating characteristic curve (AUC)
exhibits similar outcomes as an additional metric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Characterization and Learning of Causal Graphs with Small Conditioning
  Sets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09028v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09028v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Murat Kocaoglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constraint-based causal discovery algorithms learn part of the causal graph
structure by systematically testing conditional independences observed in the
data. These algorithms, such as the PC algorithm and its variants, rely on
graphical characterizations of the so-called equivalence class of causal graphs
proposed by Pearl. However, constraint-based causal discovery algorithms
struggle when data is limited since conditional independence tests quickly lose
their statistical power, especially when the conditioning set is large. To
address this, we propose using conditional independence tests where the size of
the conditioning set is upper bounded by some integer $k$ for robust causal
discovery. The existing graphical characterizations of the equivalence classes
of causal graphs are not applicable when we cannot leverage all the conditional
independence statements. We first define the notion of $k$-Markov equivalence:
Two causal graphs are $k$-Markov equivalent if they entail the same conditional
independence constraints where the conditioning set size is upper bounded by
$k$. We propose a novel representation that allows us to graphically
characterize $k$-Markov equivalence between two causal graphs. We propose a
sound constraint-based algorithm called the $k$-PC algorithm for learning this
equivalence class. Finally, we conduct synthetic, and semi-synthetic
experiments to demonstrate that the $k$-PC algorithm enables more robust causal
discovery in the small sample regime compared to the baseline PC algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cellular Network Speech Enhancement: Removing Background and
  Transmission Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amanda Shu, Hamza Khalid, Haohui Liu, Shikhar Agnihotri, Joseph Konan, Ojas Bhargave
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The primary objective of speech enhancement is to reduce background noise
while preserving the target's speech. A common dilemma occurs when a speaker is
confined to a noisy environment and receives a call with high background and
transmission noise. To address this problem, the Deep Noise Suppression (DNS)
Challenge focuses on removing the background noise with the next-generation
deep learning models to enhance the target's speech; however, researchers fail
to consider Voice Over IP (VoIP) applications their transmission noise.
Focusing on Google Meet and its cellular application, our work achieves
state-of-the-art performance on the Google Meet To Phone Track of the VoIP DNS
Challenge. This paper demonstrates how to beat industrial performance and
achieve 1.92 PESQ and 0.88 STOI, as well as superior acoustic fidelity,
perceptual quality, and intelligibility in various metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Regionally Decentralized AC Optimal Power Flows with ADMM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.03787v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.03787v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Terrence W. K. Mak, Minas Chatzos, Mathieu Tanneau, Pascal Van Hentenryck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One potential future for the next generation of smart grids is the use of
decentralized optimization algorithms and secured communications for
coordinating renewable generation (e.g., wind/solar), dispatchable devices
(e.g., coal/gas/nuclear generations), demand response, battery & storage
facilities, and topology optimization. The Alternating Direction Method of
Multipliers (ADMM) has been widely used in the community to address such
decentralized optimization problems and, in particular, the AC Optimal Power
Flow (AC-OPF). This paper studies how machine learning may help in speeding up
the convergence of ADMM for solving AC-OPF. It proposes a novel decentralized
machine-learning approach, namely ML-ADMM, where each agent uses deep learning
to learn the consensus parameters on the coupling branches. The paper also
explores the idea of learning only from ADMM runs that exhibit high-quality
convergence properties, and proposes filtering mechanisms to select these runs.
Experimental results on test cases based on the French system demonstrate the
potential of the approach in speeding up the convergence of ADMM significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Estimating individual treatment effects under unobserved confounding
  using binary instruments <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.08544v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.08544v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Frauen, Stefan Feuerriegel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating conditional average treatment effects (CATEs) from observational
data is relevant in many fields such as personalized medicine. However, in
practice, the treatment assignment is usually confounded by unobserved
variables and thus introduces bias. A remedy to remove the bias is the use of
instrumental variables (IVs). Such settings are widespread in medicine (e.g.,
trials where the treatment assignment is used as binary IV). In this paper, we
propose a novel, multiply robust machine learning framework, called MRIV, for
estimating CATEs using binary IVs and thus yield an unbiased CATE estimator.
Different from previous work for binary IVs, our framework estimates the CATE
directly via a pseudo outcome regression. (1)~We provide a theoretical analysis
where we show that our framework yields multiple robust convergence rates: our
CATE estimator achieves fast convergence even if several nuisance estimators
converge slowly. (2)~We further show that our framework asymptotically
outperforms state-of-the-art plug-in IV methods for CATE estimation, in the
sense that it achieves a faster rate of convergence if the CATE is smoother
than the individual outcome surfaces. (3)~We build upon our theoretical results
and propose a tailored deep neural network architecture called MRIV-Net for
CATE estimation using binary IVs. Across various computational experiments, we
demonstrate empirically that our MRIV-Net achieves state-of-the-art
performance. To the best of our knowledge, our MRIV is the first multiply
robust machine learning framework tailored to estimating CATEs in the binary IV
setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Interpretable Models Using an Oracle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1906.06852v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1906.06852v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Ghose, Balaraman Ravindran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We look at a specific aspect of model interpretability: models often need to
be constrained in size for them to be considered interpretable. But smaller
models also tend to have high bias. This suggests a trade-off between
interpretability and accuracy. Our work addresses this by: (a) showing that
learning a training distribution (often different from the test distribution)
can often increase accuracy of small models, and therefore may be used as a
strategy to compensate for small sizes, and (b) providing a model-agnostic
algorithm to learn such training distributions. We pose the distribution
learning problem as one of optimizing parameters for an Infinite Beta Mixture
Model based on a Dirichlet Process, so that the held-out accuracy of a model
trained on a sample from this distribution is maximized. To make computation
tractable, we project the training data onto one dimension: prediction
uncertainty scores as provided by a highly accurate oracle model. A Bayesian
Optimizer is used for learning the parameters. Empirical results using multiple
real world datasets, various oracles and interpretable models with different
notions of model sizes, are presented. We observe significant relative
improvements in the F1-score in most cases, occasionally seeing improvements
greater than 100% over baselines. Additionally we show that the proposed
algorithm provides the following benefits: (a) its a framework which allows for
flexibility in implementation, (b) it can be used across feature spaces, e.g.,
the text classification accuracy of a Decision Tree using character n-grams is
shown to improve when using a Gated Recurrent Unit as an oracle, which uses a
sequence of characters as its input, (c) it can be used to train models that
have a non-differentiable training loss, e.g., Decision Trees, and (d)
reasonable defaults exist for most parameters of the algorithm, which makes it
convenient to use.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Examples in Random Neural Networks with General Activations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.17209v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.17209v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Montanari, Yuchen Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A substantial body of empirical work documents the lack of robustness in deep
learning models to adversarial examples. Recent theoretical work proved that
adversarial examples are ubiquitous in two-layers networks with sub-exponential
width and ReLU or smooth activations, and multi-layer ReLU networks with
sub-exponential width. We present a result of the same type, with no
restriction on width and for general locally Lipschitz continuous activations.
  More precisely, given a neural network $f(\,\cdot\,;{\boldsymbol \theta})$
with random weights ${\boldsymbol \theta}$, and feature vector ${\boldsymbol
x}$, we show that an adversarial example ${\boldsymbol x}'$ can be found with
high probability along the direction of the gradient $\nabla_{{\boldsymbol
x}}f({\boldsymbol x};{\boldsymbol \theta})$. Our proof is based on a Gaussian
conditioning technique. Instead of proving that $f$ is approximately linear in
a neighborhood of ${\boldsymbol x}$, we characterize the joint distribution of
$f({\boldsymbol x};{\boldsymbol \theta})$ and $f({\boldsymbol x}';{\boldsymbol
\theta})$ for ${\boldsymbol x}' = {\boldsymbol x}-s({\boldsymbol
x})\nabla_{{\boldsymbol x}}f({\boldsymbol x};{\boldsymbol \theta})$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimism in Face of a Context: Regret Guarantees for Stochastic
  Contextual MDP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.11126v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.11126v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orin Levy, Yishay Mansour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present regret minimization algorithms for stochastic contextual MDPs
under minimum reachability assumption, using an access to an offline least
square regression oracle. We analyze three different settings: where the
dynamics is known, where the dynamics is unknown but independent of the context
and the most challenging setting where the dynamics is unknown and
context-dependent. For the latter, our algorithm obtains regret bound of
$\widetilde{O}(
(H+{1}/{p_{min}})H|S|^{3/2}\sqrt{|A|T\log(\max\{|\mathcal{G}|,|\mathcal{P}|\}/\delta)})$
with probability $1-\delta$, where $\mathcal{P}$ and $\mathcal{G}$ are finite
and realizable function classes used to approximate the dynamics and rewards
respectively, $p_{min}$ is the minimum reachability parameter, $S$ is the set
of states, $A$ the set of actions, $H$ the horizon, and $T$ the number of
episodes. To our knowledge, our approach is the first optimistic approach
applied to contextual MDPs with general function approximation (i.e., without
additional knowledge regarding the function class, such as it being linear and
etc.). We present a lower bound of $\Omega(\sqrt{T H |S| |A|
\ln(|\mathcal{G}|)/\ln(|A|)})$, on the expected regret which holds even in the
case of known dynamics. Lastly, we discuss an extension of our results to CMDPs
without minimum reachability, that obtains $\widetilde{O}(T^{3/4})$ regret.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Learning for Combinatorial Optimization Needs Meta-Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.03116v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.03116v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Wang, Pan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A general framework of unsupervised learning for combinatorial optimization
(CO) is to train a neural network (NN) whose output gives a problem solution by
directly optimizing the CO objective. Albeit with some advantages over
traditional solvers, the current framework optimizes an averaged performance
over the distribution of historical problem instances, which misaligns with the
actual goal of CO that looks for a good solution to every future encountered
instance. With this observation, we propose a new objective of unsupervised
learning for CO where the goal of learning is to search for good initialization
for future problem instances rather than give direct solutions. We propose a
meta-learning-based training pipeline for this new objective. Our method
achieves good empirical performance. We observe that even just the initial
solution given by our model before fine-tuning can significantly outperform the
baselines under various evaluation settings including evaluation across
multiple datasets, and the case with big shifts in the problem scale. The
reason we conjecture is that meta-learning-based training lets the model be
loosely tied to each local optima for a training instance while being more
adaptive to the changes of optimization landscapes across instances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available at: https://github.com/Graph-COM/Meta_CO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GANs and Closures: Micro-Macro Consistency in Multiscale Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.10715v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.10715v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ellis R. Crabtree, Juan M. Bello-Rivas, Andrew L. Ferguson, Ioannis G. Kevrekidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sampling the phase space of molecular systems -- and, more generally, of
complex systems effectively modeled by stochastic differential equations -- is
a crucial modeling step in many fields, from protein folding to materials
discovery. These problems are often multiscale in nature: they can be described
in terms of low-dimensional effective free energy surfaces parametrized by a
small number of "slow" reaction coordinates; the remaining "fast" degrees of
freedom populate an equilibrium measure on the reaction coordinate values.
Sampling procedures for such problems are used to estimate effective free
energy differences as well as ensemble averages with respect to the conditional
equilibrium distributions; these latter averages lead to closures for effective
reduced dynamic models. Over the years, enhanced sampling techniques coupled
with molecular simulation have been developed. An intriguing analogy arises
with the field of Machine Learning (ML), where Generative Adversarial Networks
can produce high dimensional samples from low dimensional probability
distributions. This sample generation returns plausible high dimensional space
realizations of a model state, from information about its low-dimensional
representation. In this work, we present an approach that couples physics-based
simulations and biasing methods for sampling conditional distributions with
ML-based conditional generative adversarial networks for the same task. The
"coarse descriptors" on which we condition the fine scale realizations can
either be known a priori, or learned through nonlinear dimensionality
reduction. We suggest that this may bring out the best features of both
approaches: we demonstrate that a framework that couples cGANs with
physics-based enhanced sampling techniques can improve multiscale SDE dynamical
systems sampling, and even shows promise for systems of increasing complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 10 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On A Mallows-type Model For (Ranked) Choices <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Feng, Yuxuan Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a preference learning setting where every participant chooses an
ordered list of $k$ most preferred items among a displayed set of candidates.
(The set can be different for every participant.) We identify a distance-based
ranking model for the population's preferences and their (ranked) choice
behavior. The ranking model resembles the Mallows model but uses a new distance
function called Reverse Major Index (RMJ). We find that despite the need to sum
over all permutations, the RMJ-based ranking distribution aggregates into
(ranked) choice probabilities with simple closed-form expression. We develop
effective methods to estimate the model parameters and showcase their
generalization power using real data, especially when there is a limited
variety of display sets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the Thirty-Sixth Annual Conference on Neural Information
  Processing Systems (NeurIPS 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive Learning for Unsupervised Domain Adaptation of Time Series <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.06243v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.06243v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilmazcan Ozyurt, Stefan Feuerriegel, Ce Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised domain adaptation (UDA) aims at learning a machine learning
model using a labeled source domain that performs well on a similar yet
different, unlabeled target domain. UDA is important in many applications such
as medicine, where it is used to adapt risk scores across different patient
cohorts. In this paper, we develop a novel framework for UDA of time series
data, called CLUDA. Specifically, we propose a contrastive learning framework
to learn contextual representations in multivariate time series, so that these
preserve label information for the prediction task. In our framework, we
further capture the variation in the contextual representations between source
and target domain via a custom nearest-neighbor contrastive learning. To the
best of our knowledge, ours is the first framework to learn domain-invariant,
contextual representation for UDA of time series data. We evaluate our
framework using a wide range of time series datasets to demonstrate its
effectiveness and show that it achieves state-of-the-art performance for time
series UDA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Delving into Semantic Scale Imbalance <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.14613v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.14613v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanbiao Ma, Licheng Jiao, Fang Liu, Yuxin Li, Shuyuan Yang, Xu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model bias triggered by long-tailed data has been widely studied. However,
measure based on the number of samples cannot explicate three phenomena
simultaneously: (1) Given enough data, the classification performance gain is
marginal with additional samples. (2) Classification performance decays
precipitously as the number of training samples decreases when there is
insufficient data. (3) Model trained on sample-balanced datasets still has
different biases for different classes. In this work, we define and quantify
the semantic scale of classes, which is used to measure the feature diversity
of classes. It is exciting to find experimentally that there is a marginal
effect of semantic scale, which perfectly describes the first two phenomena.
Further, the quantitative measurement of semantic scale imbalance is proposed,
which can accurately reflect model bias on multiple datasets, even on
sample-balanced data, revealing a novel perspective for the study of class
imbalance. Due to the prevalence of semantic scale imbalance, we propose
semantic-scale-balanced learning, including a general loss improvement scheme
and a dynamic re-weighting training framework that overcomes the challenge of
calculating semantic scales in real-time during iterations. Comprehensive
experiments show that dynamic semantic-scale-balanced learning consistently
enables the model to perform superiorly on large-scale long-tailed and
non-long-tailed natural and medical datasets, which is a good starting point
for mitigating the prevalent but unnoticed model bias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 pages, 26 figures, 12 tables, Published as a conference paper at
  ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaQA: Combining Expert Agents for Multi-Skill Question Answering <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.01922v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.01922v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haritz Puerto, Gözde Gül Şahin, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent explosion of question answering (QA) datasets and models has
increased the interest in the generalization of models across multiple domains
and formats by either training on multiple datasets or by combining multiple
models. Despite the promising results of multi-dataset models, some domains or
QA formats may require specific architectures, and thus the adaptability of
these models might be limited. In addition, current approaches for combining
models disregard cues such as question-answer compatibility. In this work, we
propose to combine expert agents with a novel, flexible, and training-efficient
architecture that considers questions, answer predictions, and
answer-prediction confidence scores to select the best answer among a list of
answer candidates. Through quantitative and qualitative experiments we show
that our model i) creates a collaboration between agents that outperforms
previous multi-agent and multi-dataset approaches in both in-domain and
out-of-domain scenarios, ii) is highly data-efficient to train, and iii) can be
adapted to any QA format. We release our code and a dataset of answer
predictions from expert agents for 16 QA datasets to foster future developments
of multi-agent systems on https://github.com/UKPLab/MetaQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pruning coupled with learning, ensembles of minimal neural networks, and
  future of XAI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2005.06284v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2005.06284v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander N. Gorban, Evgeny M. Mirkes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pruning coupled with learning aims to optimize the neural network (NN)
structure for solving specific problems. This optimization can be used for
various purposes: to prevent overfitting, to save resources for implementation
and training, to provide explainability of the trained NN, and many others. The
minimal structure that cannot be pruned further is not unique. Ensemble of
minimal structures can be used as a committee of intellectual agents that
solves problems by voting. Each minimal NN presents an "empirical knowledge"
about the problem and can be verbalized. The non-uniqueness of such knowledge
extracted from data is an important property of data-driven Artificial
Intelligence (AI). In this work, we review an approach to pruning based on the
principle: What controls training should control pruning. This principle is
expected to work both for artificial NN and for selection and modification of
important synaptic contacts in brain. In back-propagation artificial NN
learning is controlled by the gradient of loss functions. Therefore, the first
order sensitivity indicators are used for pruning and the algorithms based on
these indicators are reviewed. The notion of logically transparent NN was
introduced. The approach was illustrated on the problem of political
forecasting: predicting the results of the US presidential election. Eight
minimal NN were produced that give different forecasting algorithms. The
non-uniqueness of solution can be utilised by creation of expert panels
(committee). Another use of NN pluralism is to identify areas of input signals
where further data collection is most useful. In Conclusion, we discuss the
possible future of widely advertised XAI program.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Significantly modified and extended version, 23 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is Nash Equilibrium Approximator Learnable? <span class="chip">AAMAS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.07472v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.07472v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijian Duan, Wenhan Huang, Dinghuai Zhang, Yali Du, Jun Wang, Yaodong Yang, Xiaotie Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the learnability of the function approximator
that approximates Nash equilibrium (NE) for games generated from a
distribution. First, we offer a generalization bound using the Probably
Approximately Correct (PAC) learning model. The bound describes the gap between
the expected loss and empirical loss of the NE approximator. Afterward, we
prove the agnostic PAC learnability of the Nash approximator. In addition to
theoretical analysis, we demonstrate an application of NE approximator in
experiments. The trained NE approximator can be used to warm-start and
accelerate classical NE solvers. Together, our results show the practicability
of approximating NE through function approximation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAMAS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EvenNet: Ignoring Odd-Hop Neighbors Improves Robustness of Graph Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.13892v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.13892v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runlin Lei, Zhen Wang, Yaliang Li, Bolin Ding, Zhewei Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have received extensive research attention for
their promising performance in graph machine learning. Despite their
extraordinary predictive accuracy, existing approaches, such as GCN and GPRGNN,
are not robust in the face of homophily changes on test graphs, rendering these
models vulnerable to graph structural attacks and with limited capacity in
generalizing to graphs of varied homophily levels. Although many methods have
been proposed to improve the robustness of GNN models, most of these techniques
are restricted to the spatial domain and employ complicated defense mechanisms,
such as learning new graph structures or calculating edge attentions. In this
paper, we study the problem of designing simple and robust GNN models in the
spectral domain. We propose EvenNet, a spectral GNN corresponding to an
even-polynomial graph filter. Based on our theoretical analysis in both spatial
and spectral domains, we demonstrate that EvenNet outperforms full-order models
in generalizing across homophilic and heterophilic graphs, implying that
ignoring odd-hop neighbors improves the robustness of GNNs. We conduct
experiments on both synthetic and real-world datasets to demonstrate the
effectiveness of EvenNet. Notably, EvenNet outperforms existing defense models
against structural attacks without introducing additional computational costs
and maintains competitiveness in traditional node classification tasks on
homophilic and heterophilic graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Context-Integrated <span class="highlight-title">Transformer</span>-Based Neural Network for Auction Design <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.12489v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.12489v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijian Duan, Jingwu Tang, Yutong Yin, Zhe Feng, Xiang Yan, Manzil Zaheer, Xiaotie Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the central problems in auction design is developing an
incentive-compatible mechanism that maximizes the auctioneer's expected
revenue. While theoretical approaches have encountered bottlenecks in
multi-item auctions, recently, there has been much progress on finding the
optimal mechanism through deep learning. However, these works either focus on a
fixed set of bidders and items, or restrict the auction to be symmetric. In
this work, we overcome such limitations by factoring \emph{public} contextual
information of bidders and items into the auction learning framework. We
propose $\mathtt{CITransNet}$, a context-integrated transformer-based neural
network for optimal auction design, which maintains permutation-equivariance
over bids and contexts while being able to find asymmetric solutions. We show
by extensive experiments that $\mathtt{CITransNet}$ can recover the known
optimal solutions in single-item settings, outperform strong baselines in
multi-item auctions, and generalize well to cases other than those in training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML 2022. Code is available at
  https://github.com/zjduan/CITransNet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explicit Regularization in Overparametrized Models via Noise Injection <span class="chip">AISTATS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.04613v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.04613v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Orvieto, Anant Raj, Hans Kersting, Francis Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Injecting noise within gradient descent has several desirable features, such
as smoothing and regularizing properties. In this paper, we investigate the
effects of injecting noise before computing a gradient step. We demonstrate
that small perturbations can induce explicit regularization for simple models
based on the L1-norm, group L1-norms, or nuclear norms. However, when applied
to overparametrized neural networks with large widths, we show that the same
perturbations can cause variance explosion. To overcome this, we propose using
independent layer-wise perturbations, which provably allow for explicit
regularization without variance explosion. Our empirical results show that
these small perturbations lead to improved generalization performance compared
to vanilla gradient descent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AISTATS 2023 23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Overfitting in quantum machine learning and entangling dropout 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.11446v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.11446v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masahiro Kobayashi, Kouhei Nakaji, Naoki Yamamoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ultimate goal in machine learning is to construct a model function that
has a generalization capability for unseen dataset, based on given training
dataset. If the model function has too much expressibility power, then it may
overfit to the training data and as a result lose the generalization
capability. To avoid such overfitting issue, several techniques have been
developed in the classical machine learning regime, and the dropout is one such
effective method. This paper proposes a straightforward analogue of this
technique in the quantum machine learning regime, the entangling dropout,
meaning that some entangling gates in a given parametrized quantum circuit are
randomly removed during the training process to reduce the expressibility of
the circuit. Some simple case studies are given to show that this technique
actually suppresses the overfitting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dance2MIDI: Dance-driven multi-instruments music generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Han, Yi Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dance-driven music generation aims to generate musical pieces conditioned on
dance videos. Previous works focus on monophonic or raw audio generation, while
the multiinstruments scenario is under-explored. The challenges of the
dance-driven multi-instruments music (MIDI) generation are two-fold: 1) no
publicly available multi-instruments MIDI and video paired dataset and 2) the
weak correlation between music and video. To tackle these challenges, we build
the first multi-instruments MIDI and dance paired dataset (D2MIDI). Based on
our proposed dataset, we introduce a multi-instruments MIDI generation
framework (Dance2MIDI) conditioned on dance video. Specifically, 1) to model
the correlation between music and dance, we encode the dance motion using the
GCN, and 2) to generate harmonious and coherent music, we employ Transformer to
decode the MIDI sequence. We evaluate the generated music of our framework
trained on D2MIDI dataset and demonstrate that our method outperforms existing
methods. The data and code are available on
https://github.com/Dance2MIDI/Dance2MIDI
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-01-21T00:00:00Z">2023-01-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transfer Knowledge from Natural Language to Electrocardiography: Can We
  Detect Cardiovascular Disease Through Language Models? <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jielin Qiu, William Han, Jiacheng Zhu, Mengdi Xu, Michael Rosenberg, Emerson Liu, Douglas Weber, Ding Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have drawn increasing
attention since the learned embeddings pretrained on large-scale datasets have
shown powerful ability in various downstream applications. However, whether the
learned knowledge by LLMs can be transferred to clinical cardiology remains
unknown. In this work, we aim to bridge this gap by transferring the knowledge
of LLMs to clinical Electrocardiography (ECG). We propose an approach for
cardiovascular disease diagnosis and automatic ECG diagnosis report generation.
We also introduce an additional loss function by Optimal Transport (OT) to
align the distribution between ECG and language embedding. The learned
embeddings are evaluated on two downstream tasks: (1) automatic ECG diagnosis
report generation, and (2) zero-shot cardiovascular disease detection. Our
approach is able to generate high-quality cardiac diagnosis reports and also
achieves competitive zero-shot classification performance even compared with
supervised baselines, which proves the feasibility of transferring knowledge
from LLMs to the cardiac domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Poor Man's Quality Estimation: Predicting Reference-Based MT Metrics
  Without the Reference <span class="chip">EACL23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vilém Zouhar, Shehzaad Dhuliawala, Wangchunshu Zhou, Nico Daheim, Tom Kocmi, Yuchen Eleanor Jiang, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine translation quality estimation (QE) predicts human judgements of a
translation hypothesis without seeing the reference. State-of-the-art QE
systems based on pretrained language models have been achieving remarkable
correlations with human judgements yet they are computationally heavy and
require human annotations, which are slow and expensive to create. To address
these limitations, we define the problem of metric estimation (ME) where one
predicts the automated metric scores also without the reference. We show that
even without access to the reference, our model can estimate automated metrics
($\rho$=60% for BLEU, $\rho$=51% for other metrics) at the sentence-level.
Because automated metrics correlate with human judgements, we can leverage the
ME task for pre-training a QE model. For the QE task, we find that pre-training
on TER is better ($\rho$=23%) than training for scratch ($\rho$=20%).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EACL23 (main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Blacks is to Anger as Whites is to Joy? Understanding Latent Affective
  Bias in Large <span class="highlight-title">Pre-train</span>ed Neural Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anoop Kadan, Deepak P., Sahely Bhadra, Manjary P. Gangan, Lajish V. L
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Groundbreaking inventions and highly significant performance improvements in
deep learning based Natural Language Processing are witnessed through the
development of transformer based large Pre-trained Language Models (PLMs). The
wide availability of unlabeled data within human generated data deluge along
with self-supervised learning strategy helps to accelerate the success of large
PLMs in language generation, language understanding, etc. But at the same time,
latent historical bias/unfairness in human minds towards a particular gender,
race, etc., encoded unintentionally/intentionally into the corpora harms and
questions the utility and efficacy of large PLMs in many real-world
applications, particularly for the protected groups. In this paper, we present
an extensive investigation towards understanding the existence of "Affective
Bias" in large PLMs to unveil any biased association of emotions such as anger,
fear, joy, etc., towards a particular gender, race or religion with respect to
the downstream task of textual emotion detection. We conduct our exploration of
affective bias from the very initial stage of corpus level affective bias
analysis by searching for imbalanced distribution of affective words within a
domain, in large scale corpora that are used to pre-train and fine-tune PLMs.
Later, to quantify affective bias in model predictions, we perform an extensive
set of class-based and intensity-based evaluations using various bias
evaluation corpora. Our results show the existence of statistically significant
affective bias in the PLM based emotion detection systems, indicating biased
association of certain emotions towards a particular gender, race, and
religion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Syntax-guided Neural Module Distillation to Probe Compositionality in
  Sentence Embeddings <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohan Pandey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Past work probing compositionality in sentence embedding models faces issues
determining the causal impact of implicit syntax representations. Given a
sentence, we construct a neural module net based on its syntax parse and train
it end-to-end to approximate the sentence's embedding generated by a
transformer model. The distillability of a transformer to a Syntactic NeurAl
Module Net (SynNaMoN) then captures whether syntax is a strong causal model of
its compositional ability. Furthermore, we address questions about the geometry
of semantic composition by specifying individual SynNaMoN modules' internal
architecture & linearity. We find differences in the distillability of various
sentence embedding models that broadly correlate with their performance, but
observe that distillability doesn't considerably vary by model size. We also
present preliminary evidence that much syntax-guided composition in sentence
embedding models is linear, and that non-linearities may serve primarily to
handle non-compositional phrases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL 2023 (accepted)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REDAffectiveLM: Leveraging Affect Enriched Embedding and
  <span class="highlight-title">Transformer</span>-based Neural Language Model for Readers' Emotion Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anoop Kadan, Deepak P., Manjary P. Gangan, Savitha Sam Abraham, Lajish V. L
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Technological advancements in web platforms allow people to express and share
emotions towards textual write-ups written and shared by others. This brings
about different interesting domains for analysis; emotion expressed by the
writer and emotion elicited from the readers. In this paper, we propose a novel
approach for Readers' Emotion Detection from short-text documents using a deep
learning model called REDAffectiveLM. Within state-of-the-art NLP tasks, it is
well understood that utilizing context-specific representations from
transformer-based pre-trained language models helps achieve improved
performance. Within this affective computing task, we explore how incorporating
affective information can further enhance performance. Towards this, we
leverage context-specific and affect enriched representations by using a
transformer-based pre-trained language model in tandem with affect enriched
Bi-LSTM+Attention. For empirical evaluation, we procure a new dataset REN-20k,
besides using RENh-4k and SemEval-2007. We evaluate the performance of our
REDAffectiveLM rigorously across these datasets, against a vast set of
state-of-the-art baselines, where our model consistently outperforms baselines
and obtains statistically significant results. Our results establish that
utilizing affect enriched representation along with context-specific
representation within a neural architecture can considerably enhance readers'
emotion detection. Since the impact of affect enrichment specifically in
readers' emotion detection isn't well explored, we conduct a detailed analysis
over affect enriched Bi-LSTM+Attention using qualitative and quantitative model
behavior evaluation techniques. We observe that compared to conventional
semantic embedding, affect enriched embedding increases ability of the network
to effectively identify and assign weightage to key terms responsible for
readers' emotion detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adapting a Language Model While Preserving its General Knowledge <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Ke, Yijia Shao, Haowei Lin, Hu Xu, Lei Shu, Bing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain-adaptive pre-training (or DA-training for short), also known as
post-training, aims to train a pre-trained general-purpose language model (LM)
using an unlabeled corpus of a particular domain to adapt the LM so that
end-tasks in the domain can give improved performances. However, existing
DA-training methods are in some sense blind as they do not explicitly identify
what knowledge in the LM should be preserved and what should be changed by the
domain corpus. This paper shows that the existing methods are suboptimal and
proposes a novel method to perform a more informed adaptation of the knowledge
in the LM by (1) soft-masking the attention heads based on their importance to
best preserve the general knowledge in the LM and (2) contrasting the
representations of the general and the full (both general and domain knowledge)
to learn an integrated representation with both general and domain-specific
knowledge. Experimental results will demonstrate the effectiveness of the
proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Methods for Building Dialects-Mandarin Code-Mixing Corpora: A
  Case Study in Taiwanese Hokkien <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sin-En Lu, Bo-Han Lu, Chao-Yi Lu, Richard Tzong-Han Tsai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In natural language processing (NLP), code-mixing (CM) is a challenging task,
especially when the mixed languages include dialects. In Southeast Asian
countries such as Singapore, Indonesia, and Malaysia, Hokkien-Mandarin is the
most widespread code-mixed language pair among Chinese immigrants, and it is
also common in Taiwan. However, dialects such as Hokkien often have a scarcity
of resources and the lack of an official writing system, limiting the
development of dialect CM research. In this paper, we propose a method to
construct a Hokkien-Mandarin CM dataset to mitigate the limitation, overcome
the morphological issue under the Sino-Tibetan language family, and offer an
efficient Hokkien word segmentation method through a linguistics-based toolkit.
Furthermore, we use our proposed dataset and employ transfer learning to train
the XLM (cross-lingual language model) for translation tasks. To fit the
code-mixing scenario, we adapt XLM slightly. We found that by using linguistic
knowledge, rules, and language tags, the model produces good results on CM data
translation while maintaining monolingual translation quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper was accepted by EMNLP 2022 findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ExClaim: Explainable Neural Claim Verification Using Rationalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Gurrapu, Lifu Huang, Feras A. Batarseh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of deep learning, text generation language models have
improved dramatically, with text at a similar level as human-written text. This
can lead to rampant misinformation because content can now be created cheaply
and distributed quickly. Automated claim verification methods exist to validate
claims, but they lack foundational data and often use mainstream news as
evidence sources that are strongly biased towards a specific agenda. Current
claim verification methods use deep neural network models and complex
algorithms for a high classification accuracy but it is at the expense of model
explainability. The models are black-boxes and their decision-making process
and the steps it took to arrive at a final prediction are obfuscated from the
user. We introduce a novel claim verification approach, namely: ExClaim, that
attempts to provide an explainable claim verification system with foundational
evidence. Inspired by the legal system, ExClaim leverages rationalization to
provide a verdict for the claim and justifies the verdict through a natural
language explanation (rationale) to describe the model's decision-making
process. ExClaim treats the verdict classification task as a question-answer
problem and achieves a performance of 0.93 F1 score. It provides subtasks
explanations to also justify the intermediate outcomes. Statistical and
Explainable AI (XAI) evaluations are conducted to ensure valid and trustworthy
outcomes. Ensuring claim verification systems are assured, rational, and
explainable is an essential step toward improving Human-AI trust and the
accessibility of black-box systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at 2022 IEEE 29th STC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unifying Structure Reasoning and Language Model <span class="highlight-title">Pre-train</span>ing for Complex
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Wang, Zhongyu Wei, Jiarong Xu, Zhihao Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent knowledge enhanced pre-trained language models have shown remarkable
performance on downstream tasks by incorporating structured knowledge from
external sources into language models. However, they usually suffer from a
heterogeneous information alignment problem and a noisy knowledge injection
problem. For complex reasoning, the contexts contain rich knowledge that
typically exists in complex and sparse forms. In order to model structured
knowledge in the context and avoid these two problems, we propose to unify
structure reasoning and language model pre-training. It identifies four types
of elementary knowledge structures from contexts to construct structured
queries, and utilizes the box embedding method to conduct explicit structure
reasoning along queries during language modeling. To fuse textual and
structured semantics, we utilize contextual language representations of
knowledge structures to initialize their box embeddings for structure
reasoning. We conduct experiments on complex language reasoning and knowledge
graph (KG) reasoning tasks. The results show that our model can effectively
enhance the performance of complex reasoning of both language and KG
modalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rationalization for Explainable NLP: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Gurrapu, Ajay Kulkarni, Lifu Huang, Ismini Lourentzou, Laura Freeman, Feras A. Batarseh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in deep learning have improved the performance of many
Natural Language Processing (NLP) tasks such as translation,
question-answering, and text classification. However, this improvement comes at
the expense of model explainability. Black-box models make it difficult to
understand the internals of a system and the process it takes to arrive at an
output. Numerical (LIME, Shapley) and visualization (saliency heatmap)
explainability techniques are helpful; however, they are insufficient because
they require specialized knowledge. These factors led rationalization to emerge
as a more accessible explainable technique in NLP. Rationalization justifies a
model's output by providing a natural language explanation (rationale). Recent
improvements in natural language generation have made rationalization an
attractive technique because it is intuitive, human-comprehensible, and
accessible to non-technical users. Since rationalization is a relatively new
field, it is disorganized. As the first survey, rationalization literature in
NLP from 2007-2022 is analyzed. This survey presents available methods,
explainable evaluations, code, and datasets used across various NLP tasks that
use rationalization. Further, a new subfield in Explainable AI (XAI), namely,
Rational AI (RAI), is introduced to advance the current state of
rationalization. A discussion on observed insights, challenges, and future
directions is provided to point to promising research opportunities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dr.Spider: A Diagnostic Evaluation Benchmark towards Text-to-SQL
  Robustness <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuaichen Chang, Jun Wang, Mingwen Dong, Lin Pan, Henghui Zhu, Alexander Hanbo Li, Wuwei Lan, Sheng Zhang, Jiarong Jiang, Joseph Lilien, Steve Ash, William Yang Wang, Zhiguo Wang, Vittorio Castelli, Patrick Ng, Bing Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural text-to-SQL models have achieved remarkable performance in translating
natural language questions into SQL queries. However, recent studies reveal
that text-to-SQL models are vulnerable to task-specific perturbations. Previous
curated robustness test sets usually focus on individual phenomena. In this
paper, we propose a comprehensive robustness benchmark based on Spider, a
cross-domain text-to-SQL benchmark, to diagnose the model robustness. We design
17 perturbations on databases, natural language questions, and SQL queries to
measure the robustness from different angles. In order to collect more
diversified natural question perturbations, we utilize large pretrained
language models (PLMs) to simulate human behaviors in creating natural
questions. We conduct a diagnostic study of the state-of-the-art models on the
robustness set. Experimental results reveal that even the most robust model
suffers from a 14.0% performance drop overall and a 50.7% performance drop on
the most challenging perturbation. We also present a breakdown analysis
regarding text-to-SQL model designs and provide insights for improving model
robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProKD: An Unsupervised Prototypical Knowledge Distillation Network for
  Zero-Resource Cross-Lingual Named Entity Recognition <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ling Ge, Chuming Hu, Guanghui Ma, Hong Zhang, Jihong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For named entity recognition (NER) in zero-resource languages, utilizing
knowledge distillation methods to transfer language-independent knowledge from
the rich-resource source languages to zero-resource languages is an effective
means. Typically, these approaches adopt a teacher-student architecture, where
the teacher network is trained in the source language, and the student network
seeks to learn knowledge from the teacher network and is expected to perform
well in the target language. Despite the impressive performance achieved by
these methods, we argue that they have two limitations. Firstly, the teacher
network fails to effectively learn language-independent knowledge shared across
languages due to the differences in the feature distribution between the source
and target languages. Secondly, the student network acquires all of its
knowledge from the teacher network and ignores the learning of target
language-specific knowledge. Undesirably, these limitations would hinder the
model's performance in the target language. This paper proposes an unsupervised
prototype knowledge distillation network (ProKD) to address these issues.
Specifically, ProKD presents a contrastive learning-based prototype alignment
method to achieve class feature alignment by adjusting the distance among
prototypes in the source and target languages, boosting the teacher network's
capacity to acquire language-independent knowledge. In addition, ProKD
introduces a prototypical self-training method to learn the intrinsic structure
of the language by retraining the student network on the target data using
samples' distance information from prototypes, thereby enhancing the student
network's ability to acquire language-specific knowledge. Extensive experiments
on three benchmark cross-lingual NER datasets demonstrate the effectiveness of
our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Regeneration Learning: A Learning Paradigm for Data Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Tan, Tao Qin, Jiang Bian, Tie-Yan Liu, <span class="highlight-author">Yoshua Bengio</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning methods for conditional data generation usually build a
mapping from source conditional data X to target data Y. The target Y (e.g.,
text, speech, music, image, video) is usually high-dimensional and complex, and
contains information that does not exist in source data, which hinders
effective and efficient learning on the source-target mapping. In this paper,
we present a learning paradigm called regeneration learning for data
generation, which first generates Y' (an abstraction/representation of Y) from
X and then generates Y from Y'. During training, Y' is obtained from Y through
either handcrafted rules or self-supervised learning and is used to learn
X-->Y' and Y'-->Y. Regeneration learning extends the concept of representation
learning to data generation tasks, and can be regarded as a counterpart of
traditional representation learning, since 1) regeneration learning handles the
abstraction (Y') of the target data Y for data generation while traditional
representation learning handles the abstraction (X') of source data X for data
understanding; 2) both the processes of Y'-->Y in regeneration learning and
X-->X' in representation learning can be learned in a self-supervised way
(e.g., pre-training); 3) both the mappings from X to Y' in regeneration
learning and from X' to Y in representation learning are simpler than the
direct mapping from X to Y. We show that regeneration learning can be a
widely-used paradigm for data generation (e.g., text generation, speech
recognition, speech synthesis, music composition, image generation, and video
generation) and can provide valuable insights into developing data generation
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weakly-Supervised Questions for Zero-Shot Relation Extraction <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saeed Najafi, Alona Fyshe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-Shot Relation Extraction (ZRE) is the task of Relation Extraction where
the training and test sets have no shared relation types. This very challenging
domain is a good test of a model's ability to generalize. Previous approaches
to ZRE reframed relation extraction as Question Answering (QA), allowing for
the use of pre-trained QA models. However, this method required manually
creating gold question templates for each new relation. Here, we do away with
these gold templates and instead learn a model that can generate questions for
unseen relations. Our technique can successfully translate relation
descriptions into relevant questions, which are then leveraged to generate the
correct tail entity. On tail entity extraction, we outperform the previous
state-of-the-art by more than 16 F1 points without using gold question
templates. On the RE-QA dataset where no previous baseline for relation
extraction exists, our proposed algorithm comes within 0.7 F1 points of a
system that uses gold question templates. Our model also outperforms the
state-of-the-art ZRE baselines on the FewRel and WikiZSL datasets, showing that
QA models no longer need template questions to match the performance of models
specifically tailored to the ZRE task. Our implementation is available at
https://github.com/fyshelab/QA-ZRE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL 2023 (main track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MTTN: Multi-Pair Text to Text Narratives for <span class="highlight-title">Prompt</span> Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Archan Ghosh, Debgandhar Ghosh, Madhurima Maji, Suchinta Chanda, Kalporup Goswami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The explosive popularity of diffusion models[ 1][ 2][ 3 ] has provided a huge
stage for further development in generative-text modelling. As prompt based
models are very nuanced, such that a carefully generated prompt can produce
truely breath taking images, on the contrary producing powerful or even
meaningful prompt is a hit or a miss. To lavish on this we have introduced a
large scale derived and synthesized dataset built with on real prompts and
indexed with popular image-text datasets like MS-COCO[4 ], Flickr[ 5], etc. We
have also introduced staging for these sentences that sequentially reduce the
context and increase the complexity, that will further strengthen the output
because of the complex annotations that are being created. MTTN consists of
over 2.4M sentences that are divided over 5 stages creating a combination
amounting to over 12M pairs, along with a vocab size of consisting more than
300 thousands unique words that creates an abundance of variations. The
original 2.4M million pairs are broken down in such a manner that it produces a
true scenario of internet lingo that is used globally thereby heightening the
robustness of the dataset, and any model trained on it.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Purpose Audio-Visual Corpus for Multi-Modal Persian Speech
  Recognition: the Arman-AV <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javad Peymanfard, Samin Heydarian, Ali Lashini, Hossein Zeinali, Mohammad Reza Mohammadi, Nasser Mozayani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, significant progress has been made in automatic lip reading.
But these methods require large-scale datasets that do not exist for many
low-resource languages. In this paper, we have presented a new multipurpose
audio-visual dataset for Persian. This dataset consists of almost 220 hours of
videos with 1760 corresponding speakers. In addition to lip reading, the
dataset is suitable for automatic speech recognition, audio-visual speech
recognition, and speaker recognition. Also, it is the first large-scale lip
reading dataset in Persian. A baseline method was provided for each mentioned
task. In addition, we have proposed a technique to detect visemes (a visual
equivalent of a phoneme) in Persian. The visemes obtained by this method
increase the accuracy of the lip reading task by 7% relatively compared to the
previously proposed visemes, which can be applied to other languages as well.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Infinite Index: Information Retrieval on Generative Text-To-Image
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07476v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07476v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Deckers, Maik Fröbe, Johannes Kiesel, Gianluca Pandolfo, Christopher Schröder, Benno Stein, Martin Potthast
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conditional generative models such as DALL-E and Stable Diffusion generate
images based on a user-defined text, the prompt. Finding and refining prompts
that produce a desired image has become the art of prompt engineering.
Generative models do not provide a built-in retrieval model for a user's
information need expressed through prompts. In light of an extensive literature
review, we reframe prompt engineering for generative models as interactive
text-based retrieval on a novel kind of "infinite index". We apply these
insights for the first time in a case study on image generation for game design
with an expert. Finally, we envision how active learning may help to guide the
retrieval of generated images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Final version for CHIIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Incorporating Task-specific Concept Knowledge into Script Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.00068v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.00068v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenkai Sun, Tie Xu, ChengXiang Zhai, Heng ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present Tetris, a new task of Goal-Oriented Script
Completion. Unlike previous work, it considers a more realistic and general
setting, where the input includes not only the goal but also additional user
context, including preferences and history. To address this problem, we propose
a novel approach, which uses two techniques to improve performance: (1) concept
prompting, and (2) script-oriented contrastive learning that addresses step
repetition and hallucination problems. On our WikiHow-based dataset, we find
that both methods improve performance. The dataset, repository, and models will
be publicly available to facilitate further research on this new task.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ E$^3$Pose: Energy-Efficient Edge-assisted Multi-camera System for
  Multi-human 3D Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Letian Zhang, Jie Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-human 3D pose estimation plays a key role in establishing a seamless
connection between the real world and the virtual world. Recent efforts adopted
a two-stage framework that first builds 2D pose estimations in multiple camera
views from different perspectives and then synthesizes them into 3D poses.
However, the focus has largely been on developing new computer vision
algorithms on the offline video datasets without much consideration on the
energy constraints in real-world systems with flexibly-deployed and
battery-powered cameras. In this paper, we propose an energy-efficient
edge-assisted multiple-camera system, dubbed E$^3$Pose, for real-time
multi-human 3D pose estimation, based on the key idea of adaptive camera
selection. Instead of always employing all available cameras to perform 2D pose
estimations as in the existing works, E$^3$Pose selects only a subset of
cameras depending on their camera view qualities in terms of occlusion and
energy states in an adaptive manner, thereby reducing the energy consumption
(which translates to extended battery lifetime) and improving the estimation
accuracy. To achieve this goal, E$^3$Pose incorporates an attention-based LSTM
to predict the occlusion information of each camera view and guide camera
selection before cameras are selected to process the images of a scene, and
runs a camera selection algorithm based on the Lyapunov optimization framework
to make long-term adaptive selection decisions. We build a prototype of
E$^3$Pose on a 5-camera testbed, demonstrate its feasibility and evaluate its
performance. Our results show that a significant energy saving (up to 31.21%)
can be achieved while maintaining a high 3D pose estimation accuracy comparable
to state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiNet with <span class="highlight-title">Transformer</span>s: A Model for Cancer Diagnosis Using Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hosein Barzekar, Yash Patel, Ling Tong, Zeyun Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cancer is a leading cause of death in many countries. An early diagnosis of
cancer based on biomedical imaging ensures effective treatment and a better
prognosis. However, biomedical imaging presents challenges to both clinical
institutions and researchers. Physiological anomalies are often characterized
by slight abnormalities in individual cells or tissues, making them difficult
to detect visually. Traditionally, anomalies are diagnosed by radiologists and
pathologists with extensive training. This procedure, however, demands the
participation of professionals and incurs a substantial cost. The cost makes
large-scale biological image classification impractical. In this study, we
provide unique deep neural network designs for multiclass classification of
medical images, in particular cancer images. We incorporated transformers into
a multiclass framework to take advantage of data-gathering capability and
perform more accurate classifications. We evaluated models on publicly
accessible datasets using various measures to ensure the reliability of the
models. Extensive assessment metrics suggest this method can be used for a
multitude of classification tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Raw or Cooked? Object Detection on RAW Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Ljungbergh, Joakim Johnander, Christoffer Petersson, Michael Felsberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Images fed to a deep neural network have in general undergone several
handcrafted image signal processing (ISP) operations, all of which have been
optimized to produce visually pleasing images. In this work, we investigate the
hypothesis that the intermediate representation of visually pleasing images is
sub-optimal for downstream computer vision tasks compared to the RAW image
representation. We suggest that the operations of the ISP instead should be
optimized towards the end task, by learning the parameters of the operations
jointly during training. We extend previous works on this topic and propose a
new learnable operation that enables an object detector to achieve superior
performance when compared to both previous works and traditional RGB images. In
experiments on the open PASCALRAW dataset, we empirically confirm our
hypothesis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Successive Subspace Learning for Cardiac Disease Classification with
  Two-phase Deformation Fields from Cine MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Liu, Fangxu Xing, Hanna K. Gaggin, C. -C. Jay Kuo, Georges El Fakhri, Jonghye Woo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cardiac cine magnetic resonance imaging (MRI) has been used to characterize
cardiovascular diseases (CVD), often providing a noninvasive phenotyping
tool.~While recently flourished deep learning based approaches using cine MRI
yield accurate characterization results, the performance is often degraded by
small training samples. In addition, many deep learning models are deemed a
``black box," for which models remain largely elusive in how models yield a
prediction and how reliable they are. To alleviate this, this work proposes a
lightweight successive subspace learning (SSL) framework for CVD
classification, based on an interpretable feedforward design, in conjunction
with a cardiac atlas. Specifically, our hierarchical SSL model is based on (i)
neighborhood voxel expansion, (ii) unsupervised subspace approximation, (iii)
supervised regression, and (iv) multi-level feature integration. In addition,
using two-phase 3D deformation fields, including end-diastolic and end-systolic
phases, derived between the atlas and individual subjects as input offers
objective means of assessing CVD, even with small training samples. We evaluate
our framework on the ACDC2017 database, comprising one healthy group and four
disease groups. Compared with 3D CNN-based approaches, our framework achieves
superior classification performance with 140$\times$ fewer parameters, which
supports its potential value in clinical use.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ISBI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Slice <span class="highlight-title">Transformer</span> and <span class="highlight-title">Self-supervised</span> Learning for 6DoF Localization in
  3D Point Cloud Maps <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Ibrahim, Naveed Akhtar, Saeed Anwar, Michael Wise, Ajmal Mian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise localization is critical for autonomous vehicles. We present a
self-supervised learning method that employs Transformers for the first time
for the task of outdoor localization using LiDAR data. We propose a pre-text
task that reorganizes the slices of a $360^\circ$ LiDAR scan to leverage its
axial properties. Our model, called Slice Transformer, employs multi-head
attention while systematically processing the slices. To the best of our
knowledge, this is the first instance of leveraging multi-head attention for
outdoor point clouds. We additionally introduce the Perth-WA dataset, which
provides a large-scale LiDAR map of Perth city in Western Australia, covering
$\sim$4km$^2$ area. Localization annotations are provided for Perth-WA. The
proposed localization method is thoroughly evaluated on Perth-WA and
Appollo-SouthBay datasets. We also establish the efficacy of our
self-supervised learning approach for the common downstream task of object
classification using ModelNet40 and ScanNN datasets. The code and Perth-WA data
will be publicly released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IEEE International Conference on Robotics and Automation
  (ICRA), 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Time-Conditioned Generative Modeling of Object-Centric Representations
  for Video Decomposition and Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengmin Gao, Bin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When perceiving the world from multiple viewpoints, humans have the ability
to reason about the complete objects in a compositional manner even when the
object is completely occluded from partial viewpoints. Meanwhile, humans can
imagine the novel views after observing multiple viewpoints. The remarkable
recent advance in multi-view object-centric learning leaves some problems: 1)
the partially or completely occluded shape of objects can not be well
reconstructed. 2) the novel viewpoint prediction depends on expensive viewpoint
annotations rather than implicit view rules. This makes the agent fail to
perform like humans. In this paper, we introduce a time-conditioned generative
model for videos. To reconstruct the complete shape of the object accurately,
we enhance the disentanglement between different latent representations: view
latent representations are jointly inferred based on the Transformer and then
cooperate with the sequential extension of Slot Attention to learn
object-centric representations. The model also achieves the new ability:
Gaussian processes are employed as priors of view latent variables for
generation and novel-view prediction without viewpoint annotations. Experiments
on multiple specifically designed synthetic datasets have shown that the
proposed model can 1) make the video decomposition, 2) reconstruct the complete
shapes of objects, and 3) make the novel viewpoint prediction without viewpoint
annotations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counterfactual Explanation and Instance-Generation using
  Cycle-Consistent Generative Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08939v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08939v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tehseen Zia, Zeeshan Nisar, Shakeeb Murtaza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The image-based diagnosis is now a vital aspect of modern automation assisted
diagnosis. To enable models to produce pixel-level diagnosis, pixel-level
ground-truth labels are essentially required. However, since it is often not
straight forward to obtain the labels in many application domains such as in
medical image, classification-based approaches have become the de facto
standard to perform the diagnosis. Though they can identify class-salient
regions, they may not be useful for diagnosis where capturing all of the
evidences is important requirement. Alternatively, a counterfactual explanation
(CX) aims at providing explanations using a casual reasoning process of form
"If X has not happend, Y would not heppend". Existing CX approaches, however,
use classifier to explain features that can change its predictions. Thus, they
can only explain class-salient features, rather than entire object of interest.
This hence motivates us to propose a novel CX strategy that is not reliant on
image classification. This work is inspired from the recent developments in
generative adversarial networks (GANs) based image-to-image domain translation,
and leverages to translate an abnormal image to counterpart normal image (i.e.
counterfactual instance CI) to find discrepancy maps between the two. Since it
is generally not possible to obtain abnormal and normal image pairs, we
leverage Cycle-Consistency principle (a.k.a CycleGAN) to perform the
translation in unsupervised way. We formulate CX in terms of a discrepancy map
that, when added from the abnormal image, will make it indistinguishable from
the CI. We evaluate our method on three datasets including a synthetic,
tuberculosis and BraTS dataset. All these experiments confirm the supremacy of
propose method in generating accurate CX and CI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dense RGB SLAM with Neural Implicit Maps <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Li, Xiaodong Gu, Weihao Yuan, Luwei Yang, Zilong Dong, Ping Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is an emerging trend of using neural implicit functions for map
representation in Simultaneous Localization and Mapping (SLAM). Some pioneer
works have achieved encouraging results on RGB-D SLAM. In this paper, we
present a dense RGB SLAM method with neural implicit map representation. To
reach this challenging goal without depth input, we introduce a hierarchical
feature volume to facilitate the implicit map decoder. This design effectively
fuses shape cues across different scales to facilitate map reconstruction. Our
method simultaneously solves the camera motion and the neural implicit map by
matching the rendered and input video frames. To facilitate optimization, we
further propose a photometric warping loss in the spirit of multi-view stereo
to better constrain the camera pose and scene geometry. We evaluate our method
on commonly used benchmarks and compare it with modern RGB and RGB-D SLAM
systems. Our method achieves favorable results than previous methods and even
surpasses some recent RGB-D SLAM methods. Our source code will be publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2023; Pre-Camera-Ready Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Deep Regression with Ordinal Entropy <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08915v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08915v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihao Zhang, Linlin Yang, Michael Bi Mi, Xiaoxu Zheng, Angela Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In computer vision, it is often observed that formulating regression problems
as a classification task often yields better performance. We investigate this
curious phenomenon and provide a derivation to show that classification, with
the cross-entropy loss, outperforms regression with a mean squared error loss
in its ability to learn high-entropy feature representations. Based on the
analysis, we propose an ordinal entropy loss to encourage higher-entropy
feature spaces while maintaining ordinal relationships to improve the
performance of regression tasks. Experiments on synthetic and real-world
regression tasks demonstrate the importance and benefits of increasing entropy
for regression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2023. Project page:
  https://github.com/needylove/OrdinalEntropy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recurrent Contour-based Instance Segmentation with Progressive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Feng, Wengang Zhou, Yufei Yin, Jiajun Deng, Qi Sun, Houqiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contour-based instance segmentation has been actively studied, thanks to its
flexibility and elegance in processing visual objects within complex
backgrounds. In this work, we propose a novel deep network architecture, i.e.,
PolySnake, for contour-based instance segmentation. Motivated by the classic
Snake algorithm, the proposed PolySnake achieves superior and robust
segmentation performance with an iterative and progressive contour refinement
strategy. Technically, PolySnake introduces a recurrent update operator to
estimate the object contour iteratively. It maintains a single estimate of the
contour that is progressively deformed toward the object boundary. At each
iteration, PolySnake builds a semantic-rich representation for the current
contour and feeds it to the recurrent operator for further contour adjustment.
Through the iterative refinements, the contour finally progressively converges
to a stable status that tightly encloses the object instance. Moreover, with a
compact design of the recurrent architecture, we ensure the running efficiency
under multiple iterations. Extensive experiments are conducted to validate the
merits of our method, and the results demonstrate that the proposed PolySnake
outperforms the existing contour-based instance segmentation methods on several
prevalent instance segmentation benchmarks. The codes and models are available
at https://github.com/fh2019ustc/PolySnake.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pre-text Representation Transfer for Deep Learning with Limited
  Imbalanced Data : Application to CT-based COVID-19 Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fouzia Altaf, Syed M. S. Islam, Naeem K. Janjua, Naveed Akhtar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Annotating medical images for disease detection is often tedious and
expensive. Moreover, the available training samples for a given task are
generally scarce and imbalanced. These conditions are not conducive for
learning effective deep neural models. Hence, it is common to 'transfer' neural
networks trained on natural images to the medical image domain. However, this
paradigm lacks in performance due to the large domain gap between the natural
and medical image data. To address that, we propose a novel concept of Pre-text
Representation Transfer (PRT). In contrast to the conventional transfer
learning, which fine-tunes a source model after replacing its classification
layers, PRT retains the original classification layers and updates the
representation layers through an unsupervised pre-text task. The task is
performed with (original, not synthetic) medical images, without utilizing any
annotations. This enables representation transfer with a large amount of
training data. This high-fidelity representation transfer allows us to use the
resulting model as a more effective feature extractor. Moreover, we can also
subsequently perform the traditional transfer learning with this model. We
devise a collaborative representation based classification layer for the case
when we leverage the model as a feature extractor. We fuse the output of this
layer with the predictions of a model induced with the traditional transfer
learning performed over our pre-text transferred model. The utility of our
technique for limited and imbalanced data classification problem is
demonstrated with an extensive five-fold evaluation for three large-scale
models, tested for five different class-imbalance ratios for CT based COVID-19
detection. Our results show a consistent gain over the conventional transfer
learning with the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Best paper at IVCNZ</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Large-scale Film Style <span class="highlight-title">Dataset</span> for Learning Multi-frequency Driven
  Film Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08880v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08880v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuhang Chen, Zinuo Li, Chi-Man Pun, Shuqiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Film, a classic image style, is culturally significant to the whole
photographic industry since it marks the birth of photography. However, film
photography is time-consuming and expensive, necessitating a more efficient
method for collecting film-style photographs. Numerous datasets that have
emerged in the field of image enhancement so far are not film-specific. In
order to facilitate film-based image stylization research, we construct
FilmSet, a large-scale and high-quality film style dataset. Our dataset
includes three different film types and more than 5000 in-the-wild high
resolution images. Inspired by the features of FilmSet images, we propose a
novel framework called FilmNet based on Laplacian Pyramid for stylizing images
across frequency bands and achieving film style outcomes. Experiments reveal
that the performance of our model is superior than state-of-the-art techniques.
Our dataset and code will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Accuracy of Zero-Shot Action Recognition with Handcrafted
  Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08874v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08874v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Wu, Hiroshi Kera, Kazuhiko Kawamoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of machine learning, datasets for models are getting
increasingly larger. This leads to increased data annotation costs and training
time, which undoubtedly hinders the development of machine learning. To solve
this problem, zero-shot learning is gaining considerable attention. With
zero-shot learning, objects can be recognized or classified, even without
having been seen before. Nevertheless, the accuracy of this method is still
low, thus limiting its practical application. To solve this problem, we propose
a video-text matching model, which can learn from handcrafted features. Our
model can be used alone to predict the action classes and can also be added to
any other model to improve its accuracy. Moreover, our model can be
continuously optimized to improve its accuracy. We only need to manually
annotate some features, which incurs some labor costs; in many situations, the
costs are worth it. The results with UCF101 and HMDB51 show that our model
achieves the best accuracy and also improves the accuracies of other models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic MLP for MRI Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Zhang, Eric Z. Chen, Xiao Chen, Yikang Liu, Terrence Chen, Shanhui Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As convolutional neural networks (CNN) become the most successful
reconstruction technique for accelerated Magnetic Resonance Imaging (MRI), CNN
reaches its limit on image quality especially in sharpness. Further improvement
on image quality often comes at massive computational costs, hindering their
practicability in the clinic setting. MRI reconstruction is essentially a
deconvolution problem, which demands long-distance information that is
difficult to be captured by CNNs with small convolution kernels. The
multi-layer perceptron (MLP) is able to model such long-distance information,
but it restricts a fixed input size while the reconstruction of images in
flexible resolutions is required in the clinic setting. In this paper, we
proposed a hybrid CNN and MLP reconstruction strategy, featured by dynamic MLP
(dMLP) that accepts arbitrary image sizes. Experiments were conducted using 3D
multi-coil MRI. Our results suggested the proposed dMLP can improve image
sharpness compared to its pure CNN counterpart, while costing minor additional
GPU memory and computation time. We further compared the proposed dMLP with
CNNs using large kernels and studied pure MLP-based reconstruction using a
stack of 1D dMLPs, as well as its CNN counterpart using only 1D convolutions.
We observed the enlarged receptive field has noticeably improved image quality,
while simply using CNN with a large kernel leads to difficulties in training.
Noticeably, the pure MLP-based method has been outperformed by CNN-involved
methods, which matches the observations in other computer vision tasks for
natural images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CADA-GAN: Context-Aware GAN with Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sofie Daniels, Jiugeng Sun, Jiaqing Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current child face generators are restricted by the limited size of the
available datasets. In addition, feature selection can prove to be a
significant challenge, especially due to the large amount of features that need
to be trained for. To manage these problems, we proposed CADA-GAN, a
\textbf{C}ontext-\textbf{A}ware GAN that allows optimal feature extraction,
with added robustness from additional \textbf{D}ata \textbf{A}ugmentation.
CADA-GAN is adapted from the popular StyleGAN2-Ada model, with attention on
augmentation and segmentation of the parent images. The model has the lowest
\textit{Mean Squared Error Loss} (MSEloss) on latent feature representations
and the generated child image is robust compared with the one that generated
from baseline models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ETHDL2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Regeneration Learning: A Learning Paradigm for Data Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Tan, Tao Qin, Jiang Bian, Tie-Yan Liu, <span class="highlight-author">Yoshua Bengio</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning methods for conditional data generation usually build a
mapping from source conditional data X to target data Y. The target Y (e.g.,
text, speech, music, image, video) is usually high-dimensional and complex, and
contains information that does not exist in source data, which hinders
effective and efficient learning on the source-target mapping. In this paper,
we present a learning paradigm called regeneration learning for data
generation, which first generates Y' (an abstraction/representation of Y) from
X and then generates Y from Y'. During training, Y' is obtained from Y through
either handcrafted rules or self-supervised learning and is used to learn
X-->Y' and Y'-->Y. Regeneration learning extends the concept of representation
learning to data generation tasks, and can be regarded as a counterpart of
traditional representation learning, since 1) regeneration learning handles the
abstraction (Y') of the target data Y for data generation while traditional
representation learning handles the abstraction (X') of source data X for data
understanding; 2) both the processes of Y'-->Y in regeneration learning and
X-->X' in representation learning can be learned in a self-supervised way
(e.g., pre-training); 3) both the mappings from X to Y' in regeneration
learning and from X' to Y in representation learning are simpler than the
direct mapping from X to Y. We show that regeneration learning can be a
widely-used paradigm for data generation (e.g., text generation, speech
recognition, speech synthesis, music composition, image generation, and video
generation) and can provide valuable insights into developing data generation
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Problem-dependent attention and effort in neural networks with
  application to image resolution and model selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.01415v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.01415v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Rohlfs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a new ensemble-based approach to reduce the data and
computation costs of accurate classification. When faced with a new test case,
a low cost classifier is used first, only moving to a higher cost approach if
the initial classifier does not have a high degree of confidence in its
projection. This multi-stage strategy can be used with any set of classifiers
and does not require additional training. The approach is first applied to
reduce the amount of data required to classify test images; it is found to be
effective for problems in which at least some fraction of cases can be
correctly classified based upon coarser data than are typically used. For
neural networks performing digit recognition, for example, the proposed
approach reduces the number of bytes of data read by 60% to 85% with less than
5% reduction in accuracy. For the ImageNet data, the number of bytes read by
the typical network is reduced by 20% with less than 5% reduction in accuracy
-- and in some cases, the resource savings reach 40%. The second application is
to reduce computational complexity, with simpler neural networks used for test
cases that are easier to classify and complex networks used for more difficult
cases. For classification both of digits and of ImageNet images, computation
cost is reduced by as much as 82% to 89% with less than 5% reduction in
accuracy. The results also show that, for situations in which computational
cost is not a concern, calculating multiple models' projections and selecting
the one from the most confident classifier can increase classification accuracy
on ImageNet by as much as two percent over the best standalone classifier
considered here.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Infinite Index: Information Retrieval on Generative Text-To-Image
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07476v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07476v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Deckers, Maik Fröbe, Johannes Kiesel, Gianluca Pandolfo, Christopher Schröder, Benno Stein, Martin Potthast
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conditional generative models such as DALL-E and Stable Diffusion generate
images based on a user-defined text, the prompt. Finding and refining prompts
that produce a desired image has become the art of prompt engineering.
Generative models do not provide a built-in retrieval model for a user's
information need expressed through prompts. In light of an extensive literature
review, we reframe prompt engineering for generative models as interactive
text-based retrieval on a novel kind of "infinite index". We apply these
insights for the first time in a case study on image generation for game design
with an expert. Finally, we envision how active learning may help to guide the
retrieval of generated images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Final version for CHIIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SUPER-Net: Trustworthy Medical Image Segmentation with Uncertainty
  Propagation in Encoder-Decoder Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.05978v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.05978v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giuseppina Carannante, Dimah Dera, Nidhal C. Bouaynaya, Hassan M. Fathallah-Shaykh, Ghulam Rasool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning (DL) holds great promise in reshaping the healthcare industry
owing to its precision, efficiency, and objectivity. However, the brittleness
of DL models to noisy and out-of-distribution inputs is ailing their deployment
in the clinic. Most models produce point estimates without further information
about model uncertainty or confidence. This paper introduces a new Bayesian DL
framework for uncertainty quantification in segmentation neural networks:
SUPER-Net: trustworthy medical image Segmentation with Uncertainty Propagation
in Encoder-decodeR Networks. SUPER-Net analytically propagates, using Taylor
series approximations, the first two moments (mean and covariance) of the
posterior distribution of the model parameters across the nonlinear layers. In
particular, SUPER-Net simultaneously learns the mean and covariance without
expensive post-hoc Monte Carlo sampling or model ensembling. The output
consists of two simultaneous maps: the segmented image and its pixelwise
uncertainty map, which corresponds to the covariance matrix of the predictive
distribution. We conduct an extensive evaluation of SUPER-Net on medical image
segmentation of Magnetic Resonances Imaging and Computed Tomography scans under
various noisy and adversarial conditions. Our experiments on multiple benchmark
datasets demonstrate that SUPER-Net is more robust to noise and adversarial
attacks than state-of-the-art segmentation models. Moreover, the uncertainty
map of the proposed SUPER-Net associates low confidence (or equivalently high
uncertainty) to patches in the test input images that are corrupted with noise,
artifacts, or adversarial attacks. Perhaps more importantly, the model exhibits
the ability of self-assessment of its segmentation decisions, notably when
making erroneous predictions due to noise or adversarial examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Investigating the Conservative Property of Score-Based Generative
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.12753v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.12753v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen-Hao Chao, Wei-Fang Sun, Bo-Wun Cheng, Chun-Yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Score-based Generative Models (SGMs) can be categorized into
constrained SGMs (CSGMs) or unconstrained SGMs (USGMs) according to their
parameterization approaches. CSGMs model probability density functions as
Boltzmann distributions, and assign their predictions as the negative gradients
of some scalar-valued energy functions. On the other hand, USGMs employ
flexible architectures capable of directly estimating scores without the need
to explicitly model energy functions. In this paper, we demonstrate that the
architectural constraints of CSGMs may limit their modeling ability. In
addition, we show that USGMs' inability to preserve the property of
conservativeness may lead to degraded sampling performance in practice. To
address the above issues, we propose Quasi-Conservative Score-based Generative
Models (QCSGMs) for keeping the advantages of both CSGMs and USGMs. Our
theoretical derivations demonstrate that the training objective of QCSGMs can
be efficiently integrated into the training processes by leveraging the
Hutchinson trace estimator. In addition, our experimental results on the
CIFAR-10, CIFAR-100, ImageNet, and SVHN datasets validate the effectiveness of
QCSGMs. Finally, we justify the advantage of QCSGMs using an example of a
one-layered autoencoder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluation of Explanation Methods of AI -- CNNs in Image Classification
  Tasks with Reference-based and No-reference Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.01222v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.01222v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. Zhukov, J. Benois-Pineau, R. Giot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The most popular methods in AI-machine learning paradigm are mainly black
boxes. This is why explanation of AI decisions is of emergency. Although
dedicated explanation tools have been massively developed, the evaluation of
their quality remains an open research question. In this paper, we generalize
the methodologies of evaluation of post-hoc explainers of CNNs' decisions in
visual classification tasks with reference and no-reference based metrics. We
apply them on our previously developed explainers (FEM, MLFEM), and popular
Grad-CAM. The reference-based metrics are Pearson correlation coefficient and
Similarity computed between the explanation map and its ground truth
represented by a Gaze Fixation Density Map obtained with a psycho-visual
experiment. As a no-reference metric, we use stability metric, proposed by
Alvarez-Melis and Jaakkola. We study its behaviour, consensus with
reference-based metrics and show that in case of several kinds of degradation
on input images, this metric is in agreement with reference-based ones.
Therefore, it can be used for evaluation of the quality of explainers when the
ground truth is not available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Due to a bug found in the code, all tables and figures were redone.
  The new results did not change the main conclusion, except for the best
  explainer. FEM has performed better than MLFEM; 25 pages, 16 tables, 16
  figures; Submitted to "Advances in Artificial Intelligence and Machine
  Learning" (ISSN: 2582-9793)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Performance Evaluation of Vanilla, Residual, and Dense 2D U-Net
  Architectures for Skull Stripping of Augmented 3D T1-weighted MRI Head Scans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16570v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16570v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anway S. Pimpalkar, Rashmika K. Patole, Ketaki D. Kamble, Mahesh H. Shindikar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skull Stripping is a requisite preliminary step in most diagnostic
neuroimaging applications. Manual Skull Stripping methods define the gold
standard for the domain but are time-consuming and challenging to integrate
into processing pipelines with a high number of data samples. Automated methods
are an active area of research for head MRI segmentation, especially deep
learning methods such as U-Net architecture implementations. This study
compares Vanilla, Residual, and Dense 2D U-Net architectures for Skull
Stripping. The Dense 2D U-Net architecture outperforms the Vanilla and Residual
counterparts by achieving an accuracy of 99.75% on a test dataset. It is
observed that dense interconnections in a U-Net encourage feature reuse across
layers of the architecture and allow for shallower models with the strengths of
a deeper network.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Research Article submitted to the 2nd International Conference on
  Biomedical Engineering Science and Technology: Roadway from Laboratory to
  Market, at the National Institute of Technology Raipur, Chhattisgarh, India</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamics-aware Adversarial Attack of 3D Sparse Convolution Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        An Tao, Yueqi Duan, He Wang, Ziyi Wu, Pengliang Ji, Haowen Sun, Jie Zhou, Jiwen Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the dynamics-aware adversarial attack problem
in deep neural networks. Most existing adversarial attack algorithms are
designed under a basic assumption -- the network architecture is fixed
throughout the attack process. However, this assumption does not hold for many
recently proposed networks, e.g. 3D sparse convolution network, which contains
input-dependent execution to improve computational efficiency. It results in a
serious issue of lagged gradient, making the learned attack at the current step
ineffective due to the architecture changes afterward. To address this issue,
we propose a Leaded Gradient Method (LGM) and show the significant effects of
the lagged gradient. More specifically, we re-formulate the gradients to be
aware of the potential dynamic changes of network architectures, so that the
learned attack better "leads" the next step than the dynamics-unaware methods
when network architecture changes dynamically. Extensive experiments on various
datasets show that our LGM achieves impressive performance on semantic
segmentation and classification. Compared with the dynamic-unaware methods, LGM
achieves about 20% lower mIoU averagely on the ScanNet and S3DIS datasets. LGM
also outperforms the recent point cloud attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We have improved the quality of this work and updated a new version
  to address the limitations of the proposed method</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Passive Defense Against 3D Adversarial Point Clouds Through the Lens of
  3D Steganalysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.08738v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.08738v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, 3D data plays an indelible role in the computer vision field.
However, extensive studies have proved that deep neural networks (DNNs) fed
with 3D data, such as point clouds, are susceptible to adversarial examples,
which aim to misguide DNNs and might bring immeasurable losses. Currently, 3D
adversarial point clouds are chiefly generated in three fashions, i.e., point
shifting, point adding, and point dropping. These point manipulations would
modify geometrical properties and local correlations of benign point clouds
more or less. Motivated by this basic fact, we propose to defend such
adversarial examples with the aid of 3D steganalysis techniques. Specifically,
we first introduce an adversarial attack and defense model adapted from the
celebrated Prisoners' Problem in steganography to help us comprehend 3D
adversarial attack and defense more generally. Then we rethink two significant
but vague concepts in the field of adversarial example, namely, active defense
and passive defense, from the perspective of steganalysis. Most importantly, we
design a 3D adversarial point cloud detector through the lens of 3D
steganalysis. Our detector is double-blind, that is to say, it does not rely on
the exact knowledge of the adversarial attack means and victim models. To
enable the detector to effectively detect malicious point clouds, we craft a
64-D discriminant feature set, including features related to first-order and
second-order local descriptions of point clouds. To our knowledge, this work is
the first to apply 3D steganalysis to 3D adversarial example defense. Extensive
experimental results demonstrate that the proposed 3D adversarial point cloud
detector can achieve good detection performance on multiple types of 3D
adversarial point clouds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is out-of-date</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SlotFormer: Unsupervised Visual Dynamics Simulation with Object-Centric
  Models <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.05861v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.05861v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Wu, Nikita Dvornik, Klaus Greff, Thomas Kipf, Animesh Garg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding dynamics from visual observations is a challenging problem that
requires disentangling individual objects from the scene and learning their
interactions. While recent object-centric models can successfully decompose a
scene into objects, modeling their dynamics effectively still remains a
challenge. We address this problem by introducing SlotFormer -- a
Transformer-based autoregressive model operating on learned object-centric
representations. Given a video clip, our approach reasons over object features
to model spatio-temporal relationships and predicts accurate future object
states. In this paper, we successfully apply SlotFormer to perform video
prediction on datasets with complex object interactions. Moreover, the
unsupervised SlotFormer's dynamics model can be used to improve the performance
on supervised downstream tasks, such as Visual Question Answering (VQA), and
goal-conditioned planning. Compared to past works on dynamics modeling, our
method achieves significantly better long-term synthesis of object dynamics,
while retaining high quality visual generation. Besides, SlotFormer enables VQA
models to reason about the future without object-level labels, even
outperforming counterparts that use ground-truth annotations. Finally, we show
its ability to serve as a world model for model-based planning, which is
competitive with methods designed specifically for such tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2023. Project page: https://slotformer.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Switchable Lightweight Anti-symmetric Processing (SLAP) with CNN
  Outspeeds Data Augmentation by Smaller Sample -- Application in Gomoku
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.04746v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.04746v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi-Hang Suen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To replace data augmentation, this paper proposed a method called SLAP to
intensify experience to speed up machine learning and reduce the sample size.
SLAP is a model-independent protocol/function to produce the same output given
different transformation variants. SLAP improved the convergence speed of
convolutional neural network learning by 83% in the experiments with Gomoku
game states, with only one eighth of the sample size compared with data
augmentation. In reinforcement learning for Gomoku, using AlphaGo
Zero/AlphaZero algorithm with data augmentation as baseline, SLAP reduced the
number of training samples by a factor of 8 and achieved similar winning rate
against the same evaluator, but it was not yet evident that it could speed up
reinforcement learning. The benefits should at least apply to domains that are
invariant to symmetry or certain transformations. As future work, SLAP may aid
more explainable learning and transfer learning for domains that are not
invariant to symmetry, as a small step towards artificial general intelligence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Change title; 6 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Infinite Index: Information Retrieval on Generative Text-To-Image
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07476v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07476v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Deckers, Maik Fröbe, Johannes Kiesel, Gianluca Pandolfo, Christopher Schröder, Benno Stein, Martin Potthast
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conditional generative models such as DALL-E and Stable Diffusion generate
images based on a user-defined text, the prompt. Finding and refining prompts
that produce a desired image has become the art of prompt engineering.
Generative models do not provide a built-in retrieval model for a user's
information need expressed through prompts. In light of an extensive literature
review, we reframe prompt engineering for generative models as interactive
text-based retrieval on a novel kind of "infinite index". We apply these
insights for the first time in a case study on image generation for game design
with an expert. Finally, we envision how active learning may help to guide the
retrieval of generated images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Final version for CHIIR 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Statistically Optimal Robust Mean and Covariance Estimation for
  Anisotropic Gaussians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arshak Minasyan, Nikita Zhivotovskiy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assume that $X_{1}, \ldots, X_{N}$ is an $\varepsilon$-contaminated sample of
$N$ independent Gaussian vectors in $\mathbb{R}^d$ with mean $\mu$ and
covariance $\Sigma$. In the strong $\varepsilon$-contamination model we assume
that the adversary replaced an $\varepsilon$ fraction of vectors in the
original Gaussian sample by any other vectors. We show that there is an
estimator $\widehat \mu$ of the mean satisfying, with probability at least $1 -
\delta$, a bound of the form \[ \|\widehat{\mu} - \mu\|_2 \le
c\left(\sqrt{\frac{\operatorname{Tr}(\Sigma)}{N}} +
\sqrt{\frac{\|\Sigma\|\log(1/\delta)}{N}} +
\varepsilon\sqrt{\|\Sigma\|}\right), \] where $c > 0$ is an absolute constant
and $\|\Sigma\|$ denotes the operator norm of $\Sigma$. In the same
contaminated Gaussian setup, we construct an estimator $\widehat \Sigma$ of the
covariance matrix $\Sigma$ that satisfies, with probability at least $1 -
\delta$, \[ \left\|\widehat{\Sigma} - \Sigma\right\| \le
c\left(\sqrt{\frac{\|\Sigma\|\operatorname{Tr}(\Sigma)}{N}} +
\|\Sigma\|\sqrt{\frac{\log(1/\delta)}{N}} + \varepsilon\|\Sigma\|\right). \]
Both results are optimal up to multiplicative constant factors. Despite the
recent significant interest in robust statistics, achieving both dimension-free
bounds in the canonical Gaussian case remained open. In fact, several
previously known results were either dimension-dependent and required $\Sigma$
to be close to identity, or had a sub-optimal dependence on the contamination
level $\varepsilon$.
  As a part of the analysis, we derive sharp concentration inequalities for
central order statistics of Gaussian, folded normal, and chi-squared
distributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Semantic Modular Framework for Events Topic Modeling in Social Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arya Hadizadeh Moghaddam, Saeedeh Momtazi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of social media contributes to the growing amount of content
they share frequently. This framework provides a sophisticated place for people
to report various real-life events. Detecting these events with the help of
natural language processing has received researchers' attention, and various
algorithms have been developed for this goal. In this paper, we propose a
Semantic Modular Model (SMM) consisting of 5 different modules, namely
Distributional Denoising Autoencoder, Incremental Clustering, Semantic
Denoising, Defragmentation, and Ranking and Processing. The proposed model aims
to (1) cluster various documents and ignore the documents that might not
contribute to the identification of events, (2) identify more important and
descriptive keywords. Compared to the state-of-the-art methods, the results
show that the proposed model has a higher performance in identifying events
with lower ranks and extracting keywords for more important events in three
English Twitter datasets: FACup, SuperTuesday, and USElection. The proposed
method outperformed the best reported results in the mean keyword-precision
metric by 7.9\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tier Balancing: Towards Dynamic Fairness over Underlying Causal Factors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08987v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08987v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Tang, Yatong Chen, Yang Liu, Kun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pursuit of long-term fairness involves the interplay between
decision-making and the underlying data generating process. In this paper,
through causal modeling with a directed acyclic graph (DAG) on the
decision-distribution interplay, we investigate the possibility of achieving
long-term fairness from a dynamic perspective. We propose Tier Balancing, a
technically more challenging but more natural notion to achieve in the context
of long-term, dynamic fairness analysis. Different from previous fairness
notions that are defined purely on observed variables, our notion goes one step
further, capturing behind-the-scenes situation changes on the unobserved latent
causal factors that directly carry out the influence from the current decision
to the future data distribution. Under the specified dynamics, we prove that in
general one cannot achieve the long-term fairness goal only through one-step
interventions. Furthermore, in the effort of approaching long-term fairness, we
consider the mission of "getting closer to" the long-term fairness goal and
present possibility and impossibility results accordingly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adapting a Language Model While Preserving its General Knowledge <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Ke, Yijia Shao, Haowei Lin, Hu Xu, Lei Shu, Bing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain-adaptive pre-training (or DA-training for short), also known as
post-training, aims to train a pre-trained general-purpose language model (LM)
using an unlabeled corpus of a particular domain to adapt the LM so that
end-tasks in the domain can give improved performances. However, existing
DA-training methods are in some sense blind as they do not explicitly identify
what knowledge in the LM should be preserved and what should be changed by the
domain corpus. This paper shows that the existing methods are suboptimal and
proposes a novel method to perform a more informed adaptation of the knowledge
in the LM by (1) soft-masking the attention heads based on their importance to
best preserve the general knowledge in the LM and (2) contrasting the
representations of the general and the full (both general and domain knowledge)
to learn an integrated representation with both general and domain-specific
knowledge. Experimental results will demonstrate the effectiveness of the
proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Critic Sequential Monte Carlo <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.15460v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.15460v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vasileios Lioutas, Jonathan Wilder Lavington, Justice Sefas, Matthew Niedoba, Yunpeng Liu, Berend Zwartsenberg, Setareh Dabiri, Frank Wood, Adam Scibior
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce CriticSMC, a new algorithm for planning as inference built from
a composition of sequential Monte Carlo with learned Soft-Q function heuristic
factors. These heuristic factors, obtained from parametric approximations of
the marginal likelihood ahead, more effectively guide SMC towards the desired
target distribution, which is particularly helpful for planning in environments
with hard constraints placed sparsely in time. Compared with previous work, we
modify the placement of such heuristic factors, which allows us to cheaply
propose and evaluate large numbers of putative action particles, greatly
increasing inference and planning efficiency. CriticSMC is compatible with
informative priors, whose density function need not be known, and can be used
as a model-free control algorithm. Our experiments on collision avoidance in a
high-dimensional simulated driving task show that CriticSMC significantly
reduces collision rates at a low computational cost while maintaining realism
and diversity of driving behaviors across vehicles and environment scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DIVISION: Memory Efficient Training via Dual Activation Precision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.04187v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.04187v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanchu Wang, Zirui Liu, Zhimeng Jiang, Ninghao Liu, Na Zou, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing work of activation compressed training relies on searching for
optimal bit-width during DNN training to reduce the quantization noise, which
makes the procedure complicated and less transparent. To this end, we propose a
simple and effective method to compress DNN training. Our method is motivated
by an instructive observation: DNN backward propagation mainly utilizes the
low-frequency component (LFC) of the activation maps, while the majority of
memory is for caching the high-frequency component (HFC) during the training.
This indicates the HFC of activation maps is highly redundant and compressible
during DNN training, which inspires our proposed Dual Activation Precision
(DIVISION). During the training, DIVISION preserves the high-precision copy of
LFC and compresses the HFC into a light-weight copy with low numerical
precision. This can significantly reduce the memory cost without negatively
affecting the precision of backward propagation such that DIVISION maintains
competitive model accuracy. Experimental results show DIVISION achieves over
10x compression of activation maps, and significantly higher training
throughput than state-of-the-art ACT methods, without loss of model accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Problem-dependent attention and effort in neural networks with
  application to image resolution and model selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.01415v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.01415v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Rohlfs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a new ensemble-based approach to reduce the data and
computation costs of accurate classification. When faced with a new test case,
a low cost classifier is used first, only moving to a higher cost approach if
the initial classifier does not have a high degree of confidence in its
projection. This multi-stage strategy can be used with any set of classifiers
and does not require additional training. The approach is first applied to
reduce the amount of data required to classify test images; it is found to be
effective for problems in which at least some fraction of cases can be
correctly classified based upon coarser data than are typically used. For
neural networks performing digit recognition, for example, the proposed
approach reduces the number of bytes of data read by 60% to 85% with less than
5% reduction in accuracy. For the ImageNet data, the number of bytes read by
the typical network is reduced by 20% with less than 5% reduction in accuracy
-- and in some cases, the resource savings reach 40%. The second application is
to reduce computational complexity, with simpler neural networks used for test
cases that are easier to classify and complex networks used for more difficult
cases. For classification both of digits and of ImageNet images, computation
cost is reduced by as much as 82% to 89% with less than 5% reduction in
accuracy. The results also show that, for situations in which computational
cost is not a concern, calculating multiple models' projections and selecting
the one from the most confident classifier can increase classification accuracy
on ImageNet by as much as two percent over the best standalone classifier
considered here.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discriminative Multimodal Learning via Conditional Priors in Generative
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.04616v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.04616v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rogelio A. Mancisidor, Michael Kampffmeyer, Kjersti Aas, Robert Jenssen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep generative models with latent variables have been used lately to learn
joint representations and generative processes from multi-modal data. These two
learning mechanisms can, however, conflict with each other and representations
can fail to embed information on the data modalities. This research studies the
realistic scenario in which all modalities and class labels are available for
model training, but where some modalities and labels required for downstream
tasks are missing. We show, in this scenario, that the variational lower bound
limits mutual information between joint representations and missing modalities.
We, to counteract these problems, introduce a novel conditional multi-modal
discriminative model that uses an informative prior distribution and optimizes
a likelihood-free objective function that maximizes mutual information between
joint representations and missing modalities. Extensive experimentation
demonstrates the benefits of our proposed model, empirical results show that
our model achieves state-of-the-art results in representative problems such as
downstream classification, acoustic inversion, and image and annotation
generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FairGBM: Gradient Boosting with Fairness Constraints <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.07850v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.07850v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André F Cruz, Catarina Belém, Sérgio Jesus, João Bravo, Pedro Saleiro, Pedro Bizarro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tabular data is prevalent in many high stakes domains, such as financial
services or public policy. Gradient boosted decision trees (GBDT) are popular
in these settings due to performance guarantees and low cost. However, in
consequential decision-making fairness is a foremost concern. Despite GBDT's
popularity, existing in-processing Fair ML methods are either inapplicable to
GBDT, or incur in significant train time overhead, or are inadequate for
problems with high class imbalance -- a typical issue in these domains. We
present FairGBM, a dual ascent learning framework for training GBDT under
fairness constraints, with little to no impact on predictive performance when
compared to unconstrained GBDT. Since observational fairness metrics are
non-differentiable, we have to employ a "proxy-Lagrangian" formulation using
smooth convex error rate proxies to enable gradient-based optimization. Our
implementation shows an order of magnitude speedup in training time when
compared with related work, a pivotal aspect to foster the widespread adoption
of FairGBM by real-world practitioners.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unification Framework for Euclidean and Hyperbolic Graph Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.04285v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.04285v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehrdad Khatir, Nurendra Choudhary, Sutanay Choudhury, Khushbu Agarwal, Chandan K. Reddy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperbolic neural networks are able to capture the inherent hierarchy of
graph datasets, and consequently a powerful choice of GNNs. However, they
entangle multiple incongruent (gyro-)vector spaces within a layer, which makes
them limited in terms of generalization and scalability. In this work, we
propose to use Poincar\'e disk model as our search space, and apply all
approximations on the disk (as if the disk is a tangent space derived from the
origin), and thus getting rid of all inter-space transformations. Such an
approach enables us to propose a hyperbolic normalization layer, and to further
simplify the entire hyperbolic model to a Euclidean model cascaded with our
hyperbolic normalization layer. We applied our proposed nonlinear hyperbolic
normalization to the current state-of-the-art homogeneous and multi-relational
graph networks. We demonstrate that not only does the model leverage the power
of Euclidean networks such as interpretability and efficient execution of
various model components, but also it outperforms both Euclidean and hyperbolic
counterparts in our benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SUPER-Net: Trustworthy Medical Image Segmentation with Uncertainty
  Propagation in Encoder-Decoder Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.05978v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.05978v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giuseppina Carannante, Dimah Dera, Nidhal C. Bouaynaya, Hassan M. Fathallah-Shaykh, Ghulam Rasool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning (DL) holds great promise in reshaping the healthcare industry
owing to its precision, efficiency, and objectivity. However, the brittleness
of DL models to noisy and out-of-distribution inputs is ailing their deployment
in the clinic. Most models produce point estimates without further information
about model uncertainty or confidence. This paper introduces a new Bayesian DL
framework for uncertainty quantification in segmentation neural networks:
SUPER-Net: trustworthy medical image Segmentation with Uncertainty Propagation
in Encoder-decodeR Networks. SUPER-Net analytically propagates, using Taylor
series approximations, the first two moments (mean and covariance) of the
posterior distribution of the model parameters across the nonlinear layers. In
particular, SUPER-Net simultaneously learns the mean and covariance without
expensive post-hoc Monte Carlo sampling or model ensembling. The output
consists of two simultaneous maps: the segmented image and its pixelwise
uncertainty map, which corresponds to the covariance matrix of the predictive
distribution. We conduct an extensive evaluation of SUPER-Net on medical image
segmentation of Magnetic Resonances Imaging and Computed Tomography scans under
various noisy and adversarial conditions. Our experiments on multiple benchmark
datasets demonstrate that SUPER-Net is more robust to noise and adversarial
attacks than state-of-the-art segmentation models. Moreover, the uncertainty
map of the proposed SUPER-Net associates low confidence (or equivalently high
uncertainty) to patches in the test input images that are corrupted with noise,
artifacts, or adversarial attacks. Perhaps more importantly, the model exhibits
the ability of self-assessment of its segmentation decisions, notably when
making erroneous predictions due to noise or adversarial examples.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compose & Embellish: Well-Structured Piano Performance Generation via A
  Two-Stage Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.08212v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.08212v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shih-Lun Wu, Yi-Hsuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Even with strong sequence models like Transformers, generating expressive
piano performances with long-range musical structures remains challenging.
Meanwhile, methods to compose well-structured melodies or lead sheets (melody +
chords), i.e., simpler forms of music, gained more success. Observing the
above, we devise a two-stage Transformer-based framework that Composes a lead
sheet first, and then Embellishes it with accompaniment and expressive touches.
Such a factorization also enables pretraining on non-piano data. Our objective
and subjective experiments show that Compose & Embellish shrinks the gap in
structureness between a current state of the art and real performances by half,
and improves other musical aspects such as richness and coherence as well.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. 4 pages, 2 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Passive Defense Against 3D Adversarial Point Clouds Through the Lens of
  3D Steganalysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.08738v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.08738v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, 3D data plays an indelible role in the computer vision field.
However, extensive studies have proved that deep neural networks (DNNs) fed
with 3D data, such as point clouds, are susceptible to adversarial examples,
which aim to misguide DNNs and might bring immeasurable losses. Currently, 3D
adversarial point clouds are chiefly generated in three fashions, i.e., point
shifting, point adding, and point dropping. These point manipulations would
modify geometrical properties and local correlations of benign point clouds
more or less. Motivated by this basic fact, we propose to defend such
adversarial examples with the aid of 3D steganalysis techniques. Specifically,
we first introduce an adversarial attack and defense model adapted from the
celebrated Prisoners' Problem in steganography to help us comprehend 3D
adversarial attack and defense more generally. Then we rethink two significant
but vague concepts in the field of adversarial example, namely, active defense
and passive defense, from the perspective of steganalysis. Most importantly, we
design a 3D adversarial point cloud detector through the lens of 3D
steganalysis. Our detector is double-blind, that is to say, it does not rely on
the exact knowledge of the adversarial attack means and victim models. To
enable the detector to effectively detect malicious point clouds, we craft a
64-D discriminant feature set, including features related to first-order and
second-order local descriptions of point clouds. To our knowledge, this work is
the first to apply 3D steganalysis to 3D adversarial example defense. Extensive
experimental results demonstrate that the proposed 3D adversarial point cloud
detector can achieve good detection performance on multiple types of 3D
adversarial point clouds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is out-of-date</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-01-20T00:00:00Z">2023-01-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Peanuts Fall in Love with Distributional Semantics? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James A. Michaelov, Seana Coulson, Benjamin K. Bergen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The context in which a sentence appears can drastically alter our
expectations about upcoming words - for example, following a short story
involving an anthropomorphic peanut, experimental participants are more likely
to expect the sentence 'the peanut was in love' than 'the peanut was salted',
as indexed by N400 amplitude (Nieuwland & van Berkum, 2006). This rapid and
dynamic updating of comprehenders' expectations about the kind of events that a
peanut may take part in based on context has been explained using the construct
of Situation Models - updated mental representations of key elements of an
event under discussion, in this case, the peanut protagonist. However, recent
work showing that N400 amplitude can be predicted based on distributional
information alone raises the question whether situation models are in fact
necessary for the kinds of contextual effects observed in previous work. To
investigate this question, we attempt to model the results of Nieuwland and van
Berkum (2006) using six computational language models and three sets of word
vectors, none of which have explicit situation models or semantic grounding. We
find that the effect found by Nieuwland and van Berkum (2006) can be fully
modeled by two language models and two sets of word vectors, with others
showing a reduced effect. Thus, at least some processing effects normally
explained through situation models may not in fact require explicit situation
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Augmentation for Modeling Human Personality: The Dexter Machine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yair Neuman, Vladyslav Kozhukhov, Dan Vilenchik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling human personality is important for several AI challenges, from the
engineering of artificial psychotherapists to the design of persona bots.
However, the field of computational personality analysis heavily relies on
labeled data, which may be expensive, difficult or impossible to get. This
problem is amplified when dealing with rare personality types or disorders
(e.g., the anti-social psychopathic personality disorder). In this context, we
developed a text-based data augmentation approach for human personality
(PEDANT). PEDANT doesn't rely on the common type of labeled data but on the
generative pre-trained model (GPT) combined with domain expertise. Testing the
methodology on three different datasets, provides results that support the
quality of the generated data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Writing <span class="highlight-title">Prompt</span>s: Character-Grounded Story Generation with Curated
  Image Sequences <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xudong Hong, Asad Sayeed, Khushboo Mehra, Vera Demberg, Bernt Schiele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current work on image-based story generation suffers from the fact that the
existing image sequence collections do not have coherent plots behind them. We
improve visual story generation by producing a new image-grounded dataset,
Visual Writing Prompts (VWP). VWP contains almost 2K selected sequences of
movie shots, each including 5-10 images. The image sequences are aligned with a
total of 12K stories which were collected via crowdsourcing given the image
sequences and a set of grounded characters from the corresponding image
sequence. Our new image sequence collection and filtering process has allowed
us to obtain stories that are more coherent and have more narrativity compared
to previous work. We also propose a character-based story generation model
driven by coherence as a strong baseline. Evaluations show that our generated
stories are more coherent, visually grounded, and have more narrativity than
stories generated with the current state-of-the-art model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted by Transactions of the Association for Computational
  Linguistics (TACL). This is a pre-MIT Press publication version. 15 pages, 6
  figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transforming Unstructured Text into Data with Context Rule Assisted
  Machine Learning (CRAML) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephen Meisenbacher, Peter Norlander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe a method and new no-code software tools enabling domain experts
to build custom structured, labeled datasets from the unstructured text of
documents and build niche machine learning text classification models traceable
to expert-written rules. The Context Rule Assisted Machine Learning (CRAML)
method allows accurate and reproducible labeling of massive volumes of
unstructured text. CRAML enables domain experts to access uncommon constructs
buried within a document corpus, and avoids limitations of current
computational approaches that often lack context, transparency, and
interpetability. In this research methods paper, we present three use cases for
CRAML: we analyze recent management literature that draws from text data,
describe and release new machine learning models from an analysis of
proprietary job advertisement text, and present findings of social and economic
interest from a public corpus of franchise documents. CRAML produces
document-level coded tabular datasets that can be used for quantitative
academic research, and allows qualitative researchers to scale niche
classification schemes over massive text data. CRAML is a low-resource,
flexible, and scalable methodology for building training data for supervised
ML. We make available as open-source resources: the software, job advertisement
text classifiers, a novel corpus of franchise documents, and a fully replicable
start-to-finish trained example in the context of no poach clauses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Agnostic Data-Driven Inverse Text Normalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Szu-Jui Chen, Debjyoti Paul, Yutong Pang, Peng Su, Xuedong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of automatic speech recognition (ASR) models, converting
the spoken form text (from ASR) to the written form is in urgent need. This
inverse text normalization (ITN) problem attracts the attention of researchers
from various fields. Recently, several works show that data-driven ITN methods
can output high-quality written form text. Due to the scarcity of labeled
spoken-written datasets, the studies on non-English data-driven ITN are quite
limited. In this work, we propose a language-agnostic data-driven ITN framework
to fill this gap. Specifically, we leverage the data augmentation in
conjunction with neural machine translated data for low resource languages.
Moreover, we design an evaluation method for language agnostic ITN model when
only English data is available. Our empirical evaluation shows this language
agnostic modeling approach is effective for low resource languages while
preserving the performance for high resource languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Which Features are Learned by Code<span class="highlight-title">Bert</span>: An Empirical Study of the
  <span class="highlight-title">BERT</span>-based Source Code Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08427v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08427v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lan Zhang, Chen Cao, Zhilong Wang, Peng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Bidirectional Encoder Representations from Transformers (BERT) were
proposed in the natural language process (NLP) and shows promising results.
Recently researchers applied the BERT to source-code representation learning
and reported some good news on several downstream tasks. However, in this
paper, we illustrated that current methods cannot effectively understand the
logic of source codes. The representation of source code heavily relies on the
programmer-defined variable and function names. We design and implement a set
of experiments to demonstrate our conjecture and provide some insights for
future works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>1 table, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Translation for Accessible Multi-Language Text Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward W. Chew, William D. Weisman, Jingying Huang, Seth Frey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  English is the international standard of social research, but scholars are
increasingly conscious of their responsibility to meet the need for scholarly
insight into communication processes globally. This tension is as true in
computational methods as any other area, with revolutionary advances in the
tools for English language texts leaving most other languages far behind. In
this paper, we aim to leverage those very advances to demonstrate that
multi-language analysis is currently accessible to all computational scholars.
We show that English-trained measures computed after translation to English
have adequate-to-excellent accuracy compared to source-language measures
computed on original texts. We show this for three major analytics -- sentiment
analysis, topic analysis, and word embeddings -- over 16 languages, including
Spanish, Chinese, Hindi, and Arabic. We validate this claim by comparing
predictions on original language tweets and their backtranslations: double
translations from their source language to English and back to the source
language. Overall, our results suggest that Google Translate, a simple and
widely accessible tool, is effective in preserving semantic content across
languages and methods. Modern machine translation can thus help computational
scholars make more inclusive and general claims about human communication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5000 words, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Same Words, Different Meanings: Interpretable Predictions of
  Polarization Trends in Broadcast Media Language and Granger Causal Effects on
  Public Discourse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohan Ding, Mike Horning, Eugenia H. Rho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growth of online news over the past decade, empirical studies on
political discourse and news consumption have focused on the phenomenon of
filter bubbles and echo chambers. Yet recently, scholars have revealed limited
evidence around the impact of such phenomenon, leading some to argue that
partisan segregation across news audiences cannot be fully explained by online
news consumption alone and that the role of traditional legacy media may be as
salient in polarizing public discourse around current events. In this work, we
expand the scope of analysis to include both online and more traditional media
by investigating the relationship between broadcast news media language and
social media discourse. By analyzing a decade's worth of closed captions (2
million speaker turns) from CNN and Fox News along with topically corresponding
discourse from Twitter, we provide a novel framework for measuring semantic
polarization between America's two major broadcast networks to demonstrate how
semantic polarization between these outlets has evolved (Study 1), peaked
(Study 2) and influenced partisan discussions on Twitter (Study 3) across the
last decade. Our results demonstrate a sharp increase in polarization in how
topically important keywords are discussed between the two channels, especially
after 2016, with overall highest peaks occurring in 2020. The two stations
discuss identical topics in drastically distinct contexts in 2020, to the
extent that there is barely any linguistic overlap in how identical keywords
are contextually discussed. Further, we demonstrate at scale, how such partisan
division in broadcast media language significantly shapes semantic polarity
trends on Twitter (and vice-versa), empirically linking for the first time, how
online discussions are influenced by televised media.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages. 3 figures and 11 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Review</span> of the Trends and Challenges in Adopting Natural Language
  Processing Methods for Education Feedback Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thanveer Shaik, Xiaohui Tao, Yan Li, Christopher Dann, Jacquie Mcdonald, Petrea Redmond, Linda Galligan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) is a fast-growing area of study that stretching
its presence to many business and research domains. Machine learning, deep
learning, and natural language processing (NLP) are subsets of AI to tackle
different areas of data processing and modelling. This review article presents
an overview of AI impact on education outlining with current opportunities. In
the education domain, student feedback data is crucial to uncover the merits
and demerits of existing services provided to students. AI can assist in
identifying the areas of improvement in educational infrastructure, learning
management systems, teaching practices and study environment. NLP techniques
play a vital role in analyzing student feedback in textual format. This
research focuses on existing NLP methodologies and applications that could be
adapted to educational domain applications like sentiment annotations, entity
annotations, text summarization, and topic modelling. Trends and challenges in
adopting NLP in education were reviewed and explored. Contextbased challenges
in NLP like sarcasm, domain-specific language, ambiguity, and aspect-based
sentiment analysis are explained with existing methodologies to overcome them.
Research community approaches to extract the semantic meaning of emoticons and
special characters in feedback which conveys user opinion and challenges in
adopting NLP in education are explored.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Document Summarization with Text Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lesly Miculicich, Benjamin Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we exploit the innate document segment structure for improving
the extractive summarization task. We build two text segmentation models and
find the most optimal strategy to introduce their output predictions in an
extractive summarization model. Experimental results on a corpus of scientific
articles show that extractive summarization benefits from using a highly
accurate segmentation method. In particular, most of the improvement is in
documents where the most relevant information is not at the beginning thus, we
conclude that segmentation helps in reducing the lead bias problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Phoneme-Level <span class="highlight-title">BERT</span> for Enhanced Prosody of Text-to-Speech with Grapheme
  Predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghao Aaron Li, Cong Han, Xilin Jiang, Nima Mesgarani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale pre-trained language models have been shown to be helpful in
improving the naturalness of text-to-speech (TTS) models by enabling them to
produce more naturalistic prosodic patterns. However, these models are usually
word-level or sup-phoneme-level and jointly trained with phonemes, making them
inefficient for the downstream TTS task where only phonemes are needed. In this
work, we propose a phoneme-level BERT (PL-BERT) with a pretext task of
predicting the corresponding graphemes along with the regular masked phoneme
predictions. Subjective evaluations show that our phoneme-level BERT encoder
has significantly improved the mean opinion scores (MOS) of rated naturalness
of synthesized speech compared with the state-of-the-art (SOTA) StyleTTS
baseline on out-of-distribution (OOD) texts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Semantic Relatedness <span class="highlight-title">Dataset</span> for Image Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Sabir, Francesc Moreno-Noguer, Lluís Padró
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern image captioning system relies heavily on extracting knowledge from
images to capture the concept of a static story. In this paper, we propose a
textual visual context dataset for captioning, in which the publicly available
dataset COCO Captions (Lin et al., 2014) has been extended with information
about the scene (such as objects in the image). Since this information has a
textual form, it can be used to leverage any NLP task, such as text similarity
or semantic relation methods, into captioning systems, either as an end-to-end
training strategy or a post-processing based approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: bit.ly/3Zq6ATs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Matching Exemplar as Next Sentence Prediction (MeNSP): Zero-shot <span class="highlight-title">Prompt</span>
  Learning for Automatic Scoring in Science Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuansheng Wu, Xinyu He, Tianming Li, Ninghao Liu, Xiaoming Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing models to automatically score students' written responses to
science problems is critical for science education. However, collecting and
labeling sufficient student responses for training models is time and
cost-consuming. Recent studies suggest that pre-trained language models (PLMs)
can be adapted to downstream tasks without fine-tuning with prompts. However,
no research has employed such a prompt approach in science education. As
student responses are presented with natural language, aligning the scoring
procedure as the next sentence prediction task using prompts can skip the
costly fine-tuning stage. In this study, we developed a zero-shot approach to
automatically score student responses via Matching Exemplars as Next Sentence
Prediction (MeNSP). This approach employs no training samples. We first apply
MeNSP in scoring three assessment tasks of scientific argumentation and found
machine-human scoring agreements, Cohen's Kappa ranges from 0.30 to 0.57, and
F1 score ranges from 0.54 to 0.81. To improve the performance, we extend our
research to the few-shots setting, either randomly selecting labeled student
responses or manually constructing responses to fine-tune the models. We find
that one task's performance is improved with more samples, Cohen's Kappa from
0.30 to 0.38, and F1 score from 0.54 to 0.59; for the two others, scoring
performance is not improved. We also find that randomly selected few-shots
perform better than the human expert-crafted approach. This study suggests that
MeNSP can yield referable automatic scoring for student responses while
significantly reducing the cost of model training. This method can benefit
low-stakes classroom assessment practices in science education. Future research
should further explore the applicability of the MeNSP in different types of
assessment tasks in science education and improve the model performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10+3 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Chat<span class="highlight-title">GPT</span> A Good Translator? A Preliminary Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, Zhaopeng Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report provides a preliminary evaluation of ChatGPT for machine
translation, including translation prompt, multilingual translation, and
translation robustness. We adopt the prompts advised by ChatGPT to trigger its
translation ability and find that the candidate prompts generally work well and
show minor performance differences. By evaluating on a number of benchmark test
sets, we find that ChatGPT performs competitively with commercial translation
products (e.g., Google Translate) on high-resource European languages but lags
behind significantly on lowresource or distant languages. As for the
translation robustness, ChatGPT does not perform as well as the commercial
systems on biomedical abstracts or Reddit comments but is potentially a good
translator for spoken language. Scripts and data:
https://github.com/wxjiao/Is-ChatGPT-A-Good-Translator
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages; a delayed announcement</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CHAPTER: Exploiting Convolutional Neural Network Adapters for
  <span class="highlight-title">Self-supervised</span> Speech Models <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.01282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.01282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zih-Ching Chen, Yu-Shun Sung, Hung-yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) is a powerful technique for learning
representations from unlabeled data. Transformer based models such as HuBERT,
which consist a feature extractor and transformer layers, are leading the field
in the speech domain. SSL models are fine-tuned on a wide range of downstream
tasks, which involves re-training the majority of the model for each task.
Previous studies have introduced applying adapters, which are small lightweight
modules commonly used in Natural Language Processing (NLP) to adapt pre-trained
models to new tasks. However, such efficient tuning techniques only provide
adaptation at the transformer layer, but failed to perform adaptation at the
feature extractor. In this paper, we propose CHAPTER, an efficient tuning
method specifically designed for SSL speech model, by applying CNN adapters at
the feature extractor. Using this method, we can only fine-tune fewer than 5%
of parameters per task compared to fully fine-tuning and achieve better and
more stable performance. We empirically found that adding CNN adapters to the
feature extractor can help the adaptation on emotion and speaker tasks. For
instance, the accuracy of SID is improved from 87.71 to 91.56, and the accuracy
of ER is improved by 5%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2023. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analysis of Data Augmentation Methods for Low-Resource Maltese ASR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.07793v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.07793v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea DeMarco, Carlos Mena, Albert Gatt, Claudia Borg, Aiden Williams, Lonneke van der Plas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have seen an increased interest in the computational speech
processing of Maltese, but resources remain sparse. In this paper, we consider
data augmentation techniques for improving speech recognition for low-resource
languages, focusing on Maltese as a test case. We consider three different
types of data augmentation: unsupervised training, multilingual training and
the use of synthesized speech as training data. The goal is to determine which
of these techniques, or combination of them, is the most effective to improve
speech recognition for languages where the starting point is a small corpus of
approximately 7 hours of transcribed speech. Our results show that combining
the data augmentation techniques studied here lead us to an absolute WER
improvement of 15% without the use of a language model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ILLUME: Rationalizing Vision-Language Models through Human Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.08241v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.08241v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Brack, Patrick Schramowski, Björn Deiseroth, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bootstrapping from pre-trained language models has been proven to be an
efficient approach for building vision-language models (VLM) for tasks such as
image captioning or visual question answering. However, outputs of these models
rarely align with user's rationales for specific answers. In order to improve
this alignment and reinforce commonsense reasons, we propose a tuning paradigm
based on human interactions with machine generated data. Our ILLUME executes
the following loop: Given an image-question-answer prompt, the VLM samples
multiple candidate rationales, and a human critic provides minimal feedback via
preference selection, used for fine-tuning. This loop increases the training
data and gradually carves out the VLM's rationalization capabilities that are
aligned with human intend. Our exhaustive experiments demonstrate that ILLUME
is competitive with standard supervised fine-tuning while using significantly
fewer training data and only requiring minimal feedback.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GoSum: Extractive Summarization of Long Documents by Reinforcement
  Learning and Graph Organized discourse state 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10247v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10247v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyi Bian, Xiaodi Huang, Hong Zhou, Shanfeng Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting summaries from long documents can be regarded as sentence
classification using the structural information of the documents. How to use
such structural information to summarize a document is challenging. In this
paper, we propose GoSum, a novel graph and reinforcement learning based
extractive model for long-paper summarization. In particular, GoSum encodes
sentence states in reinforcement learning by building a heterogeneous graph for
each input document at different discourse levels. An edge in the graph
reflects the discourse hierarchy of a document for restraining the semantic
drifts across section boundaries. We evaluate GoSum on two datasets of
scientific articles summarization: PubMed and arXiv. The experimental results
have demonstrated that GoSum achieve state-of-the-art results compared with
strong baselines of both extractive and abstractive models. The ablation
studies further validate that the performance of our GoSum benefits from the
use of discourse information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards continually learning new languages <span class="chip">SP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ngoc-Quan Pham, Jan Niehues, Alexander Waibel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual speech recognition with neural networks is often implemented
with batch-learning, when all of the languages are available before training.
An ability to add new languages after the prior training sessions can be
economically beneficial, but the main challenge is catastrophic forgetting. In
this work, we combine the qualities of weight factorization and elastic weight
consolidation in order to counter catastrophic forgetting and facilitate
learning new languages quickly. Such combination allowed us to eliminate
catastrophic forgetting while still achieving performance for the new languages
comparable with having all languages at once, in experiments of learning from
an initial 10 languages to achieve 26 languages without catastrophic forgetting
and a reasonable performance compared to training all languages from scratch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICCASP 2023 - Revision 1.0</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging the Gap Between Indexing and Retrieval for Differentiable
  Search Index with Query Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.10128v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.10128v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyao Zhuang, Houxing Ren, Linjun Shou, Jian Pei, Ming Gong, Guido Zuccon, Daxin Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Differentiable Search Index (DSI) is an emerging paradigm for information
retrieval. Unlike traditional retrieval architectures where index and retrieval
are two different and separate components, DSI uses a single transformer model
to perform both indexing and retrieval.
  In this paper, we identify and tackle an important issue of current DSI
models: the data distribution mismatch that occurs between the DSI indexing and
retrieval processes. Specifically, we argue that, at indexing, current DSI
methods learn to build connections between the text of long documents and the
identifier of the documents, but then retrieval of document identifiers is
based on queries that are commonly much shorter than the indexed documents.
This problem is further exacerbated when using DSI for cross-lingual retrieval,
where document text and query text are in different languages.
  To address this fundamental problem of current DSI models, we propose a
simple yet effective indexing framework for DSI, called DSI-QG. When indexing,
DSI-QG represents documents with a number of potentially relevant queries
generated by a query generation model and re-ranked and filtered by a
cross-encoder ranker. The presence of these queries at indexing allows the DSI
models to connect a document identifier to a set of queries, hence mitigating
data distribution mismatches present between the indexing and the retrieval
phases. Empirical results on popular mono-lingual and cross-lingual passage
retrieval datasets show that DSI-QG significantly outperforms the original DSI
model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlatFormer: Flattened Window Attention for Efficient Point Cloud
  <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijian Liu, Xinyu Yang, Haotian Tang, Shang Yang, Song Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer, as an alternative to CNN, has been proven effective in many
modalities (e.g., texts and images). For 3D point cloud transformers, existing
efforts focus primarily on pushing their accuracy to the state-of-the-art
level. However, their latency lags behind sparse convolution-based models (3x
slower), hindering their usage in resource-constrained, latency-sensitive
applications (such as autonomous driving). This inefficiency comes from point
clouds' sparse and irregular nature, whereas transformers are designed for
dense, regular workloads. This paper presents FlatFormer to close this latency
gap by trading spatial proximity for better computational regularity. We first
flatten the point cloud with window-based sorting and partition points into
groups of equal sizes rather than windows of equal shapes. This effectively
avoids expensive structuring and padding overheads. We then apply
self-attention within groups to extract local features, alternate sorting axis
to gather features from different directions, and shift windows to exchange
features across groups. FlatFormer delivers state-of-the-art accuracy on Waymo
Open Dataset with 4.6x speedup over (transformer-based) SST and 1.4x speedup
over (sparse convolutional) CenterPoint. This is the first point cloud
transformer that achieves real-time performance on edge GPUs and is faster than
sparse convolutional methods while achieving on-par or even superior accuracy
on large-scale benchmarks. Code to reproduce our results will be made publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Novel-View Acoustic Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changan Chen, Alexander Richard, Roman Shapovalov, Vamsi Krishna Ithapu, Natalia Neverova, Kristen Grauman, Andrea Vedaldi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the novel-view acoustic synthesis (NVAS) task: given the sight
and sound observed at a source viewpoint, can we synthesize the \emph{sound} of
that scene from an unseen target viewpoint? We propose a neural rendering
approach: Visually-Guided Acoustic Synthesis (ViGAS) network that learns to
synthesize the sound of an arbitrary point in space by analyzing the input
audio-visual cues. To benchmark this task, we collect two first-of-their-kind
large-scale multi-view audio-visual datasets, one synthetic and one real. We
show that our model successfully reasons about the spatial cues and synthesizes
faithful audio on both datasets. To our knowledge, this work represents the
very first formulation, dataset, and approach to solve the novel-view acoustic
synthesis task, which has exciting potential applications ranging from AR/VR to
art and design. Unlocked by this work, we believe that the future of novel-view
synthesis is in multi-modal learning from videos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://vision.cs.utexas.edu/projects/nvas</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Holistically Explainable Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Böhle, Mario Fritz, Bernt Schiele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers increasingly dominate the machine learning landscape across many
tasks and domains, which increases the importance for understanding their
outputs. While their attention modules provide partial insight into their inner
workings, the attention scores have been shown to be insufficient for
explaining the models as a whole. To address this, we propose B-cos
transformers, which inherently provide holistic explanations for their
decisions. Specifically, we formulate each model component - such as the
multi-layer perceptrons, attention layers, and the tokenisation module - to be
dynamic linear, which allows us to faithfully summarise the entire transformer
via a single linear transform. We apply our proposed design to Vision
Transformers (ViTs) and show that the resulting models, dubbed Bcos-ViTs, are
highly interpretable and perform competitively to baseline ViTs on ImageNet.
Code will be made available soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingting Yuan, Liang Mi, Weijun Wang, Haipeng Dai, Xiaoming Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quality of the video stream is key to neural network-based video
analytics. However, low-quality video is inevitably collected by existing
surveillance systems because of poor quality cameras or over-compressed/pruned
video streaming protocols, e.g., as a result of upstream bandwidth limit. To
address this issue, existing studies use quality enhancers (e.g., neural
super-resolution) to improve the quality of videos (e.g., resolution) and
eventually ensure inference accuracy. Nevertheless, directly applying quality
enhancers does not work in practice because it will introduce unacceptable
latency. In this paper, we present AccDecoder, a novel accelerated decoder for
real-time and neural-enhanced video analytics. AccDecoder can select a few
frames adaptively via Deep Reinforcement Learning (DRL) to enhance the quality
by neural super-resolution and then up-scale the unselected frames that
reference them, which leads to 6-21% accuracy improvement. AccDecoder provides
efficient inference capability via filtering important frames using DRL for
DNN-based inference and reusing the results for the other frames via extracting
the reference relationship among frames and blocks, which results in a latency
reduction of 20-80% than baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2023 IEEE INFOCOM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated extraction of capacitive coupling for quantum dot systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Ziegler, Florian Luthi, Mick Ramsey, Felix Borjans, Guoji Zheng, Justyna P. Zwolak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gate-defined quantum dots (QDs) have appealing attributes as a quantum
computing platform, however, near-term devices possess a range of possible
imperfections that need to be accounted for during the tuning and operation of
QD devices. One such problem is the capacitive cross-talk between the metallic
gates that define and control QD qubits. A way to compensate for the capacitive
cross-talk and enable targeted control of specific QDs independent of coupling
is by the use of virtual gates. Here, we demonstrate a reliable automated
capacitive coupling identification method that combines machine learning with
traditional fitting to take advantage of the desirable properties of each. We
also show how the cross-capacitance measurement may be used for the
identification of spurious QDs sometimes formed during tuning experimental
devices. Our systems can autonomously flag devices with spurious dots near the
operating regime which is crucial information for reliable tuning to a regime
suitable for qubit operations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Memorability Prediction with Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Hagen, Thomas Espeseth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Behavioral studies have shown that the memorability of images is similar
across groups of people, suggesting that memorability is a function of the
intrinsic properties of images, and is unrelated to people's individual
experiences and traits. Deep learning networks can be trained on such
properties and be used to predict memorability in new data sets. Convolutional
neural networks (CNN) have pioneered image memorability prediction, but more
recently developed vision transformer (ViT) models may have the potential to
yield even better predictions. In this paper, we present the ViTMem, a new
memorability model based on ViT, and evaluate memorability predictions obtained
by it with state-of-the-art CNN-derived models. Results showed that ViTMem
performed equal to or better than state-of-the-art models on all data sets.
Additional semantic level analyses revealed that ViTMem is particularly
sensitive to the semantic content that drives memorability in images. We
conclude that ViTMem provides a new step forward, and propose that ViT-derived
models can replace CNNs for computational prediction of image memorability.
Researchers, educators, advertisers, visual designers and other interested
parties can leverage the model to improve the memorability of their image
material.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Deep Learning Approach for SAR Tomographic Imaging of Forested Areas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zoé Berenger, Loïc Denis, Florence Tupin, Laurent Ferro-Famil, Yue Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic aperture radar tomographic imaging reconstructs the
three-dimensional reflectivity of a scene from a set of coherent acquisitions
performed in an interferometric configuration. In forest areas, a large number
of elements backscatter the radar signal within each resolution cell. To
reconstruct the vertical reflectivity profile, state-of-the-art techniques
perform a regularized inversion implemented in the form of iterative
minimization algorithms. We show that light-weight neural networks can be
trained to perform the tomographic inversion with a single feed-forward pass,
leading to fast reconstructions that could better scale to the amount of data
provided by the future BIOMASS mission. We train our encoder-decoder network
using simulated data and validate our technique on real L-band and P-band data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Geoscience and Remote Sensing Letters, January 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Sketch Colorization using Adversarial Segmentation Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samet Hicsonmez, Nermin Samet, Emre Akbas, Pinar Duygulu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new method for producing color images from sketches. Current
solutions in sketch colorization either necessitate additional user instruction
or are restricted to the "paired" translation strategy. We leverage semantic
image segmentation from a general-purpose panoptic segmentation network to
generate an additional adversarial loss function. The proposed loss function is
compatible with any GAN model. Our method is not restricted to datasets with
segmentation labels and can be applied to unpaired translation tasks as well.
Using qualitative, and quantitative analysis, and based on a user study, we
demonstrate the efficacy of our method on four distinct image datasets. On the
FID metric, our model improves the baseline by up to 35 points. Our code,
pretrained models, scripts to produce newly introduced datasets and
corresponding sketch images are available at
https://github.com/giddyyupp/AdvSegLoss.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at Pattern Recognition Letters. arXiv admin note:
  substantial text overlap with arXiv:2102.06192</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Writing <span class="highlight-title">Prompt</span>s: Character-Grounded Story Generation with Curated
  Image Sequences <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xudong Hong, Asad Sayeed, Khushboo Mehra, Vera Demberg, Bernt Schiele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current work on image-based story generation suffers from the fact that the
existing image sequence collections do not have coherent plots behind them. We
improve visual story generation by producing a new image-grounded dataset,
Visual Writing Prompts (VWP). VWP contains almost 2K selected sequences of
movie shots, each including 5-10 images. The image sequences are aligned with a
total of 12K stories which were collected via crowdsourcing given the image
sequences and a set of grounded characters from the corresponding image
sequence. Our new image sequence collection and filtering process has allowed
us to obtain stories that are more coherent and have more narrativity compared
to previous work. We also propose a character-based story generation model
driven by coherence as a strong baseline. Evaluations show that our generated
stories are more coherent, visually grounded, and have more narrativity than
stories generated with the current state-of-the-art model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted by Transactions of the Association for Computational
  Linguistics (TACL). This is a pre-MIT Press publication version. 15 pages, 6
  figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prodromal Diagnosis of Lewy Body Diseases Based on the Assessment of
  Graphomotor and Handwriting Difficulties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zoltan Galaz, Jiri Mekyska, Jan Mucha, Vojtech Zvoncak, Zdenek Smekal, Marcos Faundez-Zanuy, Lubos Brabenec, Ivona Moravkova, Irena Rektorova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To this date, studies focusing on the prodromal diagnosis of Lewy body
diseases (LBDs) based on quantitative analysis of graphomotor and handwriting
difficulties are missing. In this work, we enrolled 18 subjects diagnosed with
possible or probable mild cognitive impairment with Lewy bodies (MCI-LB), 7
subjects having more than 50% probability of developing Parkinson's disease
(PD), 21 subjects with both possible/probable MCI-LB and probability of PD >
50%, and 37 age- and gender-matched healthy controls (HC). Each participant
performed three tasks: Archimedean spiral drawing (to quantify graphomotor
difficulties), sentence writing task (to quantify handwriting difficulties),
and pentagon copying test (to quantify cognitive decline). Next, we
parameterized the acquired data by various temporal, kinematic, dynamic,
spatial, and task-specific features. And finally, we trained classification
models for each task separately as well as a model for their combination to
estimate the predictive power of the features for the identification of LBDs.
Using this approach we were able to identify prodromal LBDs with 74% accuracy
and showed the promising potential of computerized objective and non-invasive
diagnosis of LBDs based on the assessment of graphomotor and handwriting
difficulties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Print ISBN 978-3-031-19744-4</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploration of Various Fractional Order Derivatives in Parkinson's
  Disease Dysgraphia Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08529v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08529v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Mucha, Zoltan Galaz, Jiri Mekyska, Marcos Faundez-Zanuy, Vojtech Zvoncak, Zdenek Smekal, Lubos Brabenec, Irena Rektorova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parkinson's disease (PD) is a common neurodegenerative disorder with a
prevalence rate estimated to 2.0% for people aged over 65 years. Cardinal motor
symptoms of PD such as rigidity and bradykinesia affect the muscles involved in
the handwriting process resulting in handwriting abnormalities called PD
dysgraphia. Nowadays, online handwritten signal (signal with temporal
information) acquired by the digitizing tablets is the most advanced approach
of graphomotor difficulties analysis. Although the basic kinematic features
were proved to effectively quantify the symptoms of PD dysgraphia, a recent
research identified that the theory of fractional calculus can be used to
improve the graphomotor difficulties analysis. Therefore, in this study, we
follow up on our previous research, and we aim to explore the utilization of
various approaches of fractional order derivative (FD) in the analysis of PD
dysgraphia. For this purpose, we used the repetitive loops task from the
Parkinson's disease handwriting database (PaHaW). Handwritten signals were
parametrized by the kinematic features employing three FD approximations:
Gr\"unwald-Letnikov's, Riemann-Liouville's, and Caputo's. Results of the
correlation analysis revealed a significant relationship between the clinical
state and the handwriting features based on the velocity. The extracted
features by Caputo's FD approximation outperformed the rest of the analyzed FD
approaches. This was also confirmed by the results of the classification
analysis, where the best model trained by Caputo's handwriting features
resulted in a balanced accuracy of 79.73% with a sensitivity of 83.78% and a
specificity of 75.68%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Print ISBN 978-3-031-19744-4</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pneumonia Detection in Chest X-Ray Images : Handling Class Imbalance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wardah Ali, Eesha Qureshi, Omama Ahmed Farooqi, Rizwan Ahmed Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  People all over the globe are affected by pneumonia but deaths due to it are
highest in Sub-Saharan Asia and South Asia. In recent years, the overall
incidence and mortality rate of pneumonia regardless of the utilization of
effective vaccines and compelling antibiotics has escalated. Thus, pneumonia
remains a disease that needs spry prevention and treatment. The widespread
prevalence of pneumonia has caused the research community to come up with a
framework that helps detect, diagnose and analyze diseases accurately and
promptly. One of the major hurdles faced by the Artificial Intelligence (AI)
research community is the lack of publicly available datasets for chest
diseases, including pneumonia . Secondly, few of the available datasets are
highly imbalanced (normal examples are over sampled, while samples with ailment
are in severe minority) making the problem even more challenging. In this
article we present a novel framework for the detection of pneumonia. The
novelty of the proposed methodology lies in the tackling of class imbalance
problem. The Generative Adversarial Network (GAN), specifically a combination
of Deep Convolutional Generative Adversarial Network (DCGAN) and Wasserstein
GAN gradient penalty (WGAN-GP) was applied on the minority class ``Pneumonia''
for augmentation, whereas Random Under-Sampling (RUS) was done on the majority
class ``No Findings'' to deal with the imbalance problem. The ChestX-Ray8
dataset, one of the biggest datasets, is used to validate the performance of
the proposed framework. The learning phase is completed using transfer learning
on state-of-the-art deep learning models i.e. ResNet-50, Xception, and VGG-16.
Results obtained exceed state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatial Steerability of GANs via Self-Supervision from Discriminator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyuan Wang, Ceyuan Yang, Yinghao Xu, Yujun Shen, Hongdong Li, Bolei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models make huge progress to the photorealistic image synthesis in
recent years. To enable human to steer the image generation process and
customize the output, many works explore the interpretable dimensions of the
latent space in GANs. Existing methods edit the attributes of the output image
such as orientation or color scheme by varying the latent code along certain
directions. However, these methods usually require additional human annotations
for each pretrained model, and they mostly focus on editing global attributes.
In this work, we propose a self-supervised approach to improve the spatial
steerability of GANs without searching for steerable directions in the latent
space or requiring extra annotations. Specifically, we design randomly sampled
Gaussian heatmaps to be encoded into the intermediate layers of generative
models as spatial inductive bias. Along with training the GAN model from
scratch, these heatmaps are being aligned with the emerging attention of the
GAN's discriminator in a self-supervised learning manner. During inference,
human users can intuitively interact with the spatial heatmaps to edit the
output image, such as varying the scene layout or moving objects in the scene.
Extensive experiments show that the proposed method not only enables spatial
editing over human faces, animal faces, outdoor scenes, and complicated indoor
scenes, but also brings improvement in synthesis quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript is a journal extension of our previous conference
  work (arXiv:2112.00718), submitted to TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Source-free Subject Adaptation for EEG-based Visual Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pilhyeon Lee, Seogkyu Jeon, Sunhee Hwang, Minjung Shin, Hyeran Byun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on subject adaptation for EEG-based visual recognition. It
aims at building a visual stimuli recognition system customized for the target
subject whose EEG samples are limited, by transferring knowledge from abundant
data of source subjects. Existing approaches consider the scenario that samples
of source subjects are accessible during training. However, it is often
infeasible and problematic to access personal biological data like EEG signals
due to privacy issues. In this paper, we introduce a novel and practical
problem setup, namely source-free subject adaptation, where the source subject
data are unavailable and only the pre-trained model parameters are provided for
subject adaptation. To tackle this challenging problem, we propose
classifier-based data generation to simulate EEG samples from source subjects
using classifier responses. Using the generated samples and target subject
data, we perform subject-independent feature learning to exploit the common
knowledge shared across different subjects. Notably, our framework is
generalizable and can adopt any subject-independent learning method. In the
experiments on the EEG-ImageNet40 benchmark, our model brings consistent
improvements regardless of the choice of subject-independent learning. Also,
our method shows promising performance, recording top-1 test accuracy of 74.6%
under the 5-shot setting even without relying on source data. Our code can be
found at
https://github.com/DeepBCI/Deep-BCI/tree/master/1_Intelligent_BCI/Source_Free_Subject_Adaptation_for_EEG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 11th IEEE International Winter Conference on
  Brain-Computer Interface (BCI 2023). Code is available at
  https://github.com/DeepBCI/Deep-BCI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DIFAI: Diverse Facial Inpainting using StyleGAN Inversion <span class="chip">ICIP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongsik Yoon, Jeong-gi Kwak, Yuanming Li, David Han, Hanseok Ko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image inpainting is an old problem in computer vision that restores occluded
regions and completes damaged images. In the case of facial image inpainting,
most of the methods generate only one result for each masked image, even though
there are other reasonable possibilities. To prevent any potential biases and
unnatural constraints stemming from generating only one image, we propose a
novel framework for diverse facial inpainting exploiting the embedding space of
StyleGAN. Our framework employs pSp encoder and SeFa algorithm to identify
semantic components of the StyleGAN embeddings and feed them into our proposed
SPARN decoder that adopts region normalization for plausible inpainting. We
demonstrate that our proposed method outperforms several state-of-the-art
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICIP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Light Field Depth Estimation via Multi-view Feature
  Matching with Occlusion Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shansi Zhang, Nan Meng, Edmund Y. Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth estimation from light field (LF) images is a fundamental step for some
applications. Recently, learning-based methods have achieved higher accuracy
and efficiency than the traditional methods. However, it is costly to obtain
sufficient depth labels for supervised training. In this paper, we propose an
unsupervised framework to estimate depth from LF images. First, we design a
disparity estimation network (DispNet) with a coarse-to-fine structure to
predict disparity maps from different view combinations by performing
multi-view feature matching to learn the correspondences more effectively. As
occlusions may cause the violation of photo-consistency, we design an occlusion
prediction network (OccNet) to predict the occlusion maps, which are used as
the element-wise weights of photometric loss to solve the occlusion issue and
assist the disparity learning. With the disparity maps estimated by multiple
input combinations, we propose a disparity fusion strategy based on the
estimated errors with effective occlusion handling to obtain the final
disparity map. Experimental results demonstrate that our method achieves
superior performance on both the dense and sparse LF images, and also has
better generalization ability to the real-world LF images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FG-Depth: Flow-Guided Unsupervised Monocular Depth Estimation <span class="chip">ICRA2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Zhu, Lina Liu, Yong Liu, Wanlong Li, Feng Wen, Hongbo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The great potential of unsupervised monocular depth estimation has been
demonstrated by many works due to low annotation cost and impressive accuracy
comparable to supervised methods. To further improve the performance, recent
works mainly focus on designing more complex network structures and exploiting
extra supervised information, e.g., semantic segmentation. These methods
optimize the models by exploiting the reconstructed relationship between the
target and reference images in varying degrees. However, previous methods prove
that this image reconstruction optimization is prone to get trapped in local
minima. In this paper, our core idea is to guide the optimization with prior
knowledge from pretrained Flow-Net. And we show that the bottleneck of
unsupervised monocular depth estimation can be broken with our simple but
effective framework named FG-Depth. In particular, we propose (i) a flow
distillation loss to replace the typical photometric loss that limits the
capacity of the model and (ii) a prior flow based mask to remove invalid pixels
that bring the noise in training loss. Extensive experiments demonstrate the
effectiveness of each component, and our approach achieves state-of-the-art
results on both KITTI and NYU-Depth-v2 datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Source-Free Domain Adaptation Meets Label Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunwei Wu, Guitao Cao, Yan Li, Xidong Xi, Wenming Cao, Hong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Source-free domain adaptation, where only a pre-trained source model is used
to adapt to the target distribution, is a more general approach to achieving
domain adaptation. However, it can be challenging to capture the inherent
structure of the target features accurately due to the lack of supervised
information on the target domain. To tackle this problem, we propose a novel
approach called Adaptive Local Transfer (ALT) that tries to achieve efficient
feature clustering from the perspective of label propagation. ALT divides the
target data into inner and outlier samples based on the adaptive threshold of
the learning state, and applies a customized learning strategy to best fits the
data property. Specifically, inner samples are utilized for learning
intra-class structure thanks to their relatively well-clustered properties. The
low-density outlier samples are regularized by input consistency to achieve
high accuracy with respect to the ground truth labels. In this way, local
clustering can be prevented from forming spurious clusters while effectively
propagating label information among subpopulations. Empirical evidence
demonstrates that ALT outperforms the state of the arts on three public
benchmarks: Office-31, Office-Home, and VisDA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identity masking effectiveness and gesture recognition: Effects of eye
  enhancement in seeing through the mask 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Madeline Rachow, Thomas Karnowski, Alice J. O'Toole
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face identity masking algorithms developed in recent years aim to protect the
privacy of people in video recordings. These algorithms are designed to
interfere with identification, while preserving information about facial
actions. An important challenge is to preserve subtle actions in the eye
region, while obscuring the salient identity cues from the eyes. We evaluated
the effectiveness of identity-masking algorithms based on Canny filters,
applied with and without eye enhancement, for interfering with identification
and preserving facial actions. In Experiments 1 and 2, we tested human
participants' ability to match the facial identity of a driver in a low
resolution video to a high resolution facial image. Results showed that both
masking methods impaired identification, and that eye enhancement did not alter
the effectiveness of the Canny filter mask. In Experiment 3, we tested action
preservation and found that neither method interfered significantly with driver
action perception. We conclude that relatively simple, filter-based masking
algorithms, which are suitable for application to low quality video, can be
used in privacy protection without compromising action perception.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-Set Likelihood Maximization for Few-Shot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08390v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08390v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malik Boudiaf, Etienne Bennequin, Myriam Tami, Antoine Toubhans, Pablo Piantanida, Céline Hudelot, Ismail Ben Ayed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle the Few-Shot Open-Set Recognition (FSOSR) problem, i.e. classifying
instances among a set of classes for which we only have a few labeled samples,
while simultaneously detecting instances that do not belong to any known class.
We explore the popular transductive setting, which leverages the unlabelled
query instances at inference. Motivated by the observation that existing
transductive methods perform poorly in open-set scenarios, we propose a
generalization of the maximum likelihood principle, in which latent scores
down-weighing the influence of potential outliers are introduced alongside the
usual parametric model. Our formulation embeds supervision constraints from the
support set and additional penalties discouraging overconfident predictions on
the query set. We proceed with a block-coordinate descent, with the latent
scores and parametric model co-optimized alternately, thereby benefiting from
each other. We call our resulting formulation \textit{Open-Set Likelihood
Optimization} (OSLO). OSLO is interpretable and fully modular; it can be
applied on top of any pre-trained model seamlessly. Through extensive
experiments, we show that our method surpasses existing inductive and
transductive methods on both aspects of open-set recognition, namely inlier
classification and outlier detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2206.09236</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Occlusion Reasoning for Skeleton Extraction of Self-Occluded Tree
  Canopies <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chung Hee Kim, George Kantor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a method to extract the skeleton of a self-occluded
tree canopy by estimating the unobserved structures of the tree. A tree
skeleton compactly describes the topological structure and contains useful
information such as branch geometry, positions and hierarchy. This can be
critical to planning contact interactions for agricultural manipulation, yet is
difficult to gain due to occlusion by leaves, fruits and other branches. Our
method uses an instance segmentation network to detect visible trunk, branches,
and twigs. Then, based on the observed tree structures, we build a custom 3D
likelihood map in the form of an occupancy grid to hypothesize on the presence
of occluded skeletons through a series of minimum cost path searches. We show
that our method outperforms baseline methods in highly occluded scenes,
demonstrated through a set of experiments on a synthetic tree dataset.
Qualitative results are also presented on a real tree dataset collected from
the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 10 figures, submitted to ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Retrospective $k$-space Subsampling schemes For Deep MRI
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Yiasemis, Clara I. Sánchez, Jan-Jakob Sonke, Jonas Teuwen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  $\textbf{Purpose:}$ The MRI $k$-space acquisition is time consuming.
Traditional techniques aim to acquire accelerated data, which in conjunction
with recent DL methods, aid in producing high-fidelity images in truncated
times. Conventionally, subsampling the $k$-space is performed by utilizing
Cartesian-rectilinear trajectories, which even with the use of DL, provide
imprecise reconstructions, though, a plethora of non-rectilinear or
non-Cartesian trajectories can be implemented in modern MRI scanners. This work
investigates the effect of the $k$-space subsampling scheme on the quality of
reconstructed accelerated MRI measurements produced by trained DL models.
  $\textbf{Methods:}$ The RecurrentVarNet was used as the DL-based
MRI-reconstruction architecture. Cartesian fully-sampled multi-coil $k$-space
measurements from three datasets with different accelerations were
retrospectively subsampled using eight distinct subsampling schemes (four
Cartesian-rectilinear, two Cartesian non-rectilinear, two non-Cartesian).
Experiments were conducted in two frameworks: Scheme-specific, where a distinct
model was trained and evaluated for each dataset-subsampling scheme pair, and
multi-scheme, where for each dataset a single model was trained on data
randomly subsampled by any of the eight schemes and evaluated on data
subsampled by all schemes.
  $\textbf{Results:}$ In the scheme-specific setting RecurrentVarNets trained
and evaluated on non-rectilinearly subsampled data demonstrated superior
performance especially for high accelerations, whilst in the multi-scheme
setting, reconstruction performance on rectilinearly subsampled data improved
when compared to the scheme-specific experiments.
  $\textbf{Conclusion:}$ Training DL-based MRI reconstruction algorithms on
non-rectilinearly subsampled measurements can produce more faithful
reconstructions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 13 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffusionCT: Latent Diffusion Model for CT Image Standardization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Selim, Jie Zhang, Michael A. Brooks, Ge Wang, Jin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computed tomography (CT) imaging is a widely used modality for early lung
cancer diagnosis, treatment, and prognosis. Features extracted from CT images
are now accepted to quantify spatial and temporal variations in tumor
architecture and function. However, CT images are often acquired using scanners
from different vendors with customized acquisition standards, resulting in
significantly different texture features even for the same patient, posing a
fundamental challenge to downstream studies. Existing CT image harmonization
models rely on supervised or semi-supervised techniques, with limited
performance. In this paper, we have proposed a diffusion-based CT image
standardization model called DiffusionCT which works on latent space by mapping
latent distribution into a standard distribution. DiffusionCT incorporates an
Unet-based encoder-decoder and a diffusion model embedded in its bottleneck
part. The Unet first trained without the diffusion model to learn the latent
representation of the input data. The diffusion model is trained in the next
training phase. All the trained models work together on image standardization.
The encoded representation outputted from the Unet encoder passes through the
diffusion model, and the diffusion model maps the distribution in to target
standard image domain. Finally, the decode takes that transformed latent
representation to synthesize a standardized image. The experimental results
show that DiffusionCT significantly improves the performance of the
standardization task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 03 figures and 01 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of PCA-based preprocessing and different CNN structures on
  deformable registration of sonograms <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Schmidt, Heinrich Martin Overhoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Central venous catheters (CVC) are commonly inserted into the large veins of
the neck, e.g. the internal jugular vein (IJV). CVC insertion may cause serious
complications like misplacement into an artery or perforation of cervical
vessels. Placing a CVC under sonographic guidance is an appropriate method to
reduce such adverse events, if anatomical landmarks like venous and arterial
vessels can be detected reliably. This task shall be solved by registration of
patient individual images vs. an anatomically labelled reference image. In this
work, a linear, affine transformation is performed on cervical sonograms,
followed by a non-linear transformation to achieve a more precise registration.
Voxelmorph (VM), a learning-based library for deformable image registration
using a convolutional neural network (CNN) with U-Net structure was used for
non-linear transformation. The impact of principal component analysis
(PCA)-based pre-denoising of patient individual images, as well as the impact
of modified net structures with differing complexities on registration results
were examined visually and quantitatively, the latter using metrics for
deformation and image similarity. Using the PCA-approximated cervical sonograms
resulted in decreased mean deformation lengths between 18% and 66% compared to
their original image counterparts, depending on net structure. In addition,
reducing the number of convolutional layers led to improved image similarity
with PCA images, while worsening in original images. Despite a large reduction
of network parameters, no overall decrease in registration quality was
observed, leading to the conclusion that the original net structure is
oversized for the task at hand.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures Presented at WSCG 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-situ Water quality monitoring in Oil and Gas operations <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satish Kumar, Rui Kou, Henry Hill, Jake Lempges, Eric Qian, Vikram Jayaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  From agriculture to mining, to energy, surface water quality monitoring is an
essential task. As oil and gas operators work to reduce the consumption of
freshwater, it is increasingly important to actively manage fresh and non-fresh
water resources over the long term. For large-scale monitoring, manual sampling
at many sites has become too time-consuming and unsustainable, given the sheer
number of dispersed ponds, small lakes, playas, and wetlands over a large area.
Therefore, satellite-based environmental monitoring presents great potential.
Many existing satellite-based monitoring studies utilize index-based methods to
monitor large water bodies such as rivers and oceans. However, these existing
methods fail when monitoring small ponds-the reflectance signal received from
small water bodies is too weak to detect. To address this challenge, we propose
a new Water Quality Enhanced Index (WQEI) Model, which is designed to enable
users to determine contamination levels in water bodies with weak reflectance
patterns. Our results show that 1) WQEI is a good indicator of water turbidity
validated with 1200 water samples measured in the laboratory, and 2) by
applying our method to commonly available satellite data (e.g. LandSat8), one
can achieve high accuracy water quality monitoring efficiently in large
regions. This provides a tool for operators to optimize the quality of water
stored within surface storage ponds and increasing the readiness and
availability of non-fresh water.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures, SPIE Defense + Commercial: Algorithms,
  Technologies, and Applications for Multispectral and Hyperspectral Imaging
  XXIX</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepCOVID-Fuse: A Multi-modality Deep Learning Model Fusing Chest
  X-Radiographs and Clinical Variables to Predict COVID-19 Risk Levels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunan Wu, Amil Dravid, Ramsey Michael Wehbe, Aggelos K. Katsaggelos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Propose: To present DeepCOVID-Fuse, a deep learning fusion model to predict
risk levels in patients with confirmed coronavirus disease 2019 (COVID-19) and
to evaluate the performance of pre-trained fusion models on full or partial
combination of chest x-ray (CXRs) or chest radiograph and clinical variables.
  Materials and Methods: The initial CXRs, clinical variables and outcomes
(i.e., mortality, intubation, hospital length of stay, ICU admission) were
collected from February 2020 to April 2020 with reverse-transcription
polymerase chain reaction (RT-PCR) test results as the reference standard. The
risk level was determined by the outcome. The fusion model was trained on 1657
patients (Age: 58.30 +/- 17.74; Female: 807) and validated on 428 patients
(56.41 +/- 17.03; 190) from Northwestern Memorial HealthCare system and was
tested on 439 patients (56.51 +/- 17.78; 205) from a single holdout hospital.
Performance of pre-trained fusion models on full or partial modalities were
compared on the test set using the DeLong test for the area under the receiver
operating characteristic curve (AUC) and the McNemar test for accuracy,
precision, recall and F1.
  Results: The accuracy of DeepCOVID-Fuse trained on CXRs and clinical
variables is 0.658, with an AUC of 0.842, which significantly outperformed (p <
0.05) models trained only on CXRs with an accuracy of 0.621 and AUC of 0.807
and only on clinical variables with an accuracy of 0.440 and AUC of 0.502. The
pre-trained fusion model with only CXRs as input increases accuracy to 0.632
and AUC to 0.813 and with only clinical variables as input increases accuracy
to 0.539 and AUC to 0.733.
  Conclusion: The fusion model learns better feature representations across
different modalities during training and achieves good outcome predictions even
when only some of the modalities are used in testing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robot Skill Learning Via Classical Robotics-Based Generated <span class="highlight-title">Dataset</span>s:
  Advantages, Disadvantages, and Future Improvement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Batu Kaan Oezen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Why do we not profit from our long-existing classical robotics knowledge and
look for some alternative way for data collection? The situation ignoring all
existing methods might be such a waste. This article argues that a dataset
created using a classical robotics algorithm is a crucial part of future
development. This developed classic algorithm has a perfect domain adaptation
and generalization property, and most importantly, collecting datasets based on
them is quite easy. It is well known that current robot skill-learning
approaches perform exceptionally badly in the unseen domain, and their
performance against adversarial attacks is quite limited as long as they do not
have a very exclusive big dataset. Our experiment is the initial steps of using
a dataset created by classical robotics codes. Our experiment investigated
possible trajectory collection based on classical robotics. It addressed some
advantages and disadvantages and pointed out other future development ideas.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Semantic Relatedness <span class="highlight-title">Dataset</span> for Image Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Sabir, Francesc Moreno-Noguer, Lluís Padró
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern image captioning system relies heavily on extracting knowledge from
images to capture the concept of a static story. In this paper, we propose a
textual visual context dataset for captioning, in which the publicly available
dataset COCO Captions (Lin et al., 2014) has been extended with information
about the scene (such as objects in the image). Since this information has a
textual form, it can be used to leverage any NLP task, such as text similarity
or semantic relation methods, into captioning systems, either as an end-to-end
training strategy or a post-processing based approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: bit.ly/3Zq6ATs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Asynchronous Intensity Representation for Framed and Event Video
  Sources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew C. Freeman, Montek Singh, Ketan Mayer-Patel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neuromorphic "event" cameras, designed to mimic the human vision system with
asynchronous sensing, unlock a new realm of high-speed and high dynamic range
applications. However, researchers often either revert to a framed
representation of event data for applications, or build bespoke applications
for a particular camera's event data type. To usher in the next era of video
systems, accommodate new event camera designs, and explore the benefits to
asynchronous video in classical applications, we argue that there is a need for
an asynchronous, source-agnostic video representation. In this paper, we
introduce a novel, asynchronous intensity representation for both framed and
non-framed data sources. We show that our representation can increase intensity
precision and greatly reduce the number of samples per pixel compared to
grid-based representations. With framed sources, we demonstrate that by
permitting a small amount of loss through the temporal averaging of similar
pixel values, we can reduce our representational sample rate by more than half,
while incurring a drop in VMAF quality score of only 4.5. We also demonstrate
lower latency than the state-of-the-art method for fusing and transcoding
framed and event camera data to an intensity representation, while maintaining
$2000\times$ the temporal resolution. We argue that our method provides the
computational efficiency and temporal granularity necessary to build real-time
intensity-based applications for event cameras.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimation of mitral valve hinge point coordinates -- deep neural net
  for echocardiogram segmentation <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Schmidt, Heinrich Martin Overhoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cardiac image segmentation is a powerful tool in regard to diagnostics and
treatment of cardiovascular diseases. Purely feature-based detection of
anatomical structures like the mitral valve is a laborious task due to
specifically required feature engineering and is especially challenging in
echocardiograms, because of their inherently low contrast and blurry boundaries
between some anatomical structures. With the publication of further annotated
medical datasets and the increase in GPU processing power, deep learning-based
methods in medical image segmentation became more feasible in the past years.
We propose a fully automatic detection method for mitral valve hinge points,
which uses a U-Net based deep neural net to segment cardiac chambers in
echocardiograms in a first step, and subsequently extracts the mitral valve
hinge points from the resulting segmentations in a second step. Results
measured with this automatic detection method were compared to reference
coordinate values, which with median absolute hinge point coordinate errors of
1.35 mm for the x- (15-85 percentile range: [0.3 mm; 3.15 mm]) and 0.75 mm for
the y- coordinate (15-85 percentile range: [0.15 mm; 1.88 mm]).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 Pages, 11 figures Presented at WSCG 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Robust Video Instance Segmentation with Temporal-Aware
  <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenghao Zhang, Fangtao Shao, Zuozhuo Dai, Siyu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing transformer based video instance segmentation methods extract
per frame features independently, hence it is challenging to solve the
appearance deformation problem. In this paper, we observe the temporal
information is important as well and we propose TAFormer to aggregate
spatio-temporal features both in transformer encoder and decoder. Specifically,
in transformer encoder, we propose a novel spatio-temporal joint multi-scale
deformable attention module which dynamically integrates the spatial and
temporal information to obtain enriched spatio-temporal features. In
transformer decoder, we introduce a temporal self-attention module to enhance
the frame level box queries with the temporal relation. Moreover, TAFormer
adopts an instance level contrastive loss to increase the discriminability of
instance query embeddings. Therefore the tracking error caused by visually
similar instances can be decreased. Experimental results show that TAFormer
effectively leverages the spatial and temporal information to obtain
context-aware feature representation and outperforms state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint reconstruction-segmentation on graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.05834v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.05834v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremy Budd, Yves van Gennip, Jonas Latz, Simone Parisotto, Carola-Bibiane Schönlieb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Practical image segmentation tasks concern images which must be reconstructed
from noisy, distorted, and/or incomplete observations. A recent approach for
solving such tasks is to perform this reconstruction jointly with the
segmentation, using each to guide the other. However, this work has so far
employed relatively simple segmentation methods, such as the Chan--Vese
algorithm. In this paper, we present a method for joint
reconstruction-segmentation using graph-based segmentation methods, which have
been seeing increasing recent interest. Complications arise due to the large
size of the matrices involved, and we show how these complications can be
managed. We then analyse the convergence properties of our scheme. Finally, we
apply this scheme to distorted versions of ``two cows'' images familiar from
previous graph-based segmentation literature, first to a highly noised version
and second to a blurred version, achieving highly accurate segmentations in
both cases. We compare these results to those obtained by sequential
reconstruction-segmentation approaches, finding that our method competes with,
or even outperforms, those approaches in terms of reconstruction and
segmentation accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low-Light Image and Video Enhancement: A Comprehensive <span class="highlight-title">Survey</span> and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10772v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10772v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shen Zheng, Yiling Ma, Jinqian Pan, Changjie Lu, Gaurav Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a comprehensive survey of low-light image and video
enhancement. We begin with the challenging mixed over-/under-exposed images,
which are under-performed by existing methods. To this end, we propose two
variants of the SICE dataset named SICE_Grad and SICE_Mix. Next, we introduce
Night Wenzhou, a large-scale, high-resolution video dataset, to address the
issue of the lack of a low-light video dataset that discount the use of
low-light image enhancement (LLIE) to videos. Our Night Wenzhou dataset is
challenging since it consists of fast-moving aerial scenes and streetscapes
with varying illuminations and degradation. We conduct extensive key technique
analysis and experimental comparisons for representative LLIE approaches using
these newly proposed datasets and the current benchmark datasets. Finally, we
address unresolved issues and propose future research topics for the LLIE
community. Our datasets are available at
https://github.com/ShenZheng2000/LLIE_Survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 tables, and 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ REx: Data-Free Residual Quantization Error Expansion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.14645v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.14645v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edouard Yvinec, Arnaud Dapgony, Matthieu Cord, Kevin Bailly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) are ubiquitous in computer vision and natural
language processing, but suffer from high inference cost. This problem can be
addressed by quantization, which consists in converting floating point
operations into a lower bit-width format. With the growing concerns on privacy
rights, we focus our efforts on data-free methods. However, such techniques
suffer from their lack of adaptability to the target devices, as a hardware
typically only support specific bit widths. Thus, to adapt to a variety of
devices, a quantization method shall be flexible enough to find good accuracy
v.s. speed trade-offs for every bit width and target device. To achieve this,
we propose REx, a quantization method that leverages residual error expansion,
along with group sparsity and an ensemble approximation for better
parallelization. REx is backed off by strong theoretical guarantees and
achieves superior performance on every benchmarked application (from vision to
NLP tasks), architecture (ConvNets, transformers) and bit-width (from int8 to
ternary quantization).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Image Segmentation With Noisy Labels: Characterization and Volume
  Properties of the Optimal Solutions to Accuracy and Dice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.06484v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.06484v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcus Nordström, Henrik Hult, Jonas Söderberg, Fredrik Löfman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study two of the most popular performance metrics in medical image
segmentation, Accuracy and Dice, when the target labels are noisy. For both
metrics, several statements related to characterization and volume properties
of the set of optimal segmentations are proved, and associated experiments are
provided. Our main insights are: (i) the volume of the solutions to both
metrics may deviate significantly from the expected volume of the target, (ii)
the volume of a solution to Accuracy is always less than or equal to the volume
of a solution to Dice and (iii) the optimal solutions to both of these metrics
coincide when the set of feasible segmentations is constrained to the set of
segmentations with the volume equal to the expected volume of the target.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating the Evaluators: Which UDA validation methods are most
  effective? Can they be improved? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.07360v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.07360v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Musgrave, Serge Belongie, Ser-Nam Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper compares and ranks 8 UDA validation methods. Validators estimate
model accuracy, which makes them an essential component of any UDA train-test
pipeline. We rank these validators to indicate which of them are most useful
for the purpose of selecting optimal model checkpoints and hyperparameters. To
the best of our knowledge, this large-scale benchmark study is the first of its
kind in the UDA field. In addition, we propose three new validators that
outperform all the existing checkpoint-based validators that we were able to
find in the existing literature. Code is available at
https://www.github.com/KevinMusgrave/powerful-benchmarker.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was previously titled Benchmarking Validation Methods for
  Unsupervised Domain Adaptation. This version contains new experiments,
  analysis, and figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MemeTector: Enforcing deep focus for meme detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.13268v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.13268v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christos Koutlis, Manos Schinas, Symeon Papadopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image memes and specifically their widely-known variation image macros, is a
special new media type that combines text with images and is used in social
media to playfully or subtly express humour, irony, sarcasm and even hate. It
is important to accurately retrieve image memes from social media to better
capture the cultural and social aspects of online phenomena and detect
potential issues (hate-speech, disinformation). Essentially, the background
image of an image macro is a regular image easily recognized as such by humans
but cumbersome for the machine to do so due to feature map similarity with the
complete image macro. Hence, accumulating suitable feature maps in such cases
can lead to deep understanding of the notion of image memes. To this end, we
propose a methodology, called Visual Part Utilization, that utilizes the visual
part of image memes as instances of the regular image class and the initial
image memes as instances of the image meme class to force the model to
concentrate on the critical parts that characterize an image meme.
Additionally, we employ a trainable attention mechanism on top of a standard
ViT architecture to enhance the model's ability to focus on these critical
parts and make the predictions interpretable. Several training and test
scenarios involving web-scraped regular images of controlled text presence are
considered for evaluating the model in terms of robustness and accuracy. The
findings indicate that light visual part utilization combined with sufficient
text presence during training provides the best and most robust model,
surpassing state of the art. Source code and dataset are available at
https://github.com/mever-team/memetector.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SgVA-CLIP: Semantic-guided Visual Adapting of Vision-Language Models for
  Few-shot Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16191v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16191v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fang Peng, Xiaoshan Yang, Linhui Xiao, Yaowei Wang, Changsheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although significant progress has been made in few-shot learning, most of
existing few-shot image classification methods require supervised pre-training
on a large amount of samples of base classes, which limits their generalization
ability in real world application. Recently, large-scale Vision-Language
Pre-trained models (VLPs) have been gaining increasing attention in few-shot
learning because they can provide a new paradigm for transferable visual
representation learning with easily available text on the Web. However, the
VLPs may neglect detailed visual information that is difficult to describe by
language sentences, but important for learning an effective classifier to
distinguish different images. To address the above problem, we propose a new
framework, named Semantic-guided Visual Adapting (SgVA), which can effectively
extend vision-language pre-trained models to produce discriminative adapted
visual features by comprehensively using an implicit knowledge distillation, a
vision-specific contrastive loss, and a cross-modal contrastive loss. The
implicit knowledge distillation is designed to transfer the fine-grained
cross-modal knowledge to guide the updating of the vision adapter.
State-of-the-art results on 13 datasets demonstrate that the adapted visual
features can well complement the cross-modal features to improve few-shot image
classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybrid Quantum-Classical Generative Adversarial Network for High
  Resolution Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.11614v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.11614v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Lok Tsang, Maxwell T. West, Sarah M. Erfani, Muhammad Usman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum machine learning (QML) has received increasing attention due to its
potential to outperform classical machine learning methods in problems
pertaining classification and identification tasks. A subclass of QML methods
is quantum generative adversarial networks (QGANs) which have been studied as a
quantum counterpart of classical GANs widely used in image manipulation and
generation tasks. The existing work on QGANs is still limited to small-scale
proof-of-concept examples based on images with significant downscaling. Here we
integrate classical and quantum techniques to propose a new hybrid
quantum-classical GAN framework. We demonstrate its superior learning
capabilities by generating $28 \times 28$ pixels grey-scale images without
dimensionality reduction or classical pre/post-processing on multiple classes
of the standard MNIST and Fashion MNIST datasets, which achieves comparable
results to classical frameworks with three orders of magnitude less trainable
generator parameters. To gain further insight into the working of our hybrid
approach, we systematically explore the impact of its parameter space by
varying the number of qubits, the size of image patches, the number of layers
in the generator, the shape of the patches and the choice of prior
distribution. Our results show that increasing the quantum generator size
generally improves the learning capability of the network. The developed
framework provides a foundation for future design of QGANs with optimal
parameter set tailored for complex image generation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting consistency for semi-supervised semantic segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.07075v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.07075v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Grubišić, Marin Oršić, Siniša Šegvić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning an attractive technique in practical deployments of
deep models since it relaxes the dependence on labeled data. It is especially
important in the scope of dense prediction because pixel-level annotation
requires significant effort. This paper considers semi-supervised algorithms
that enforce consistent predictions over perturbed unlabeled inputs. We study
the advantages of perturbing only one of the two model instances and preventing
the backward pass through the unperturbed instance. We also propose a
competitive perturbation model as a composition of geometric warp and
photometric jittering. We experiment with efficient models due to their
importance for real-time and low-power applications. Our experiments show clear
advantages of (1) one-way consistency, (2) perturbing only the student branch,
and (3) strong photometric and geometric perturbations. Our perturbation model
outperforms recent work and most of the contribution comes from photometric
component. Experiments with additional data from the large coarsely annotated
subset of Cityscapes suggest that semi-supervised training can outperform
supervised training with the coarse labels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The source code is available at
  https://github.com/Ivan1248/semisup-seg-efficient</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ILLUME: Rationalizing Vision-Language Models through Human Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.08241v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.08241v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Brack, Patrick Schramowski, Björn Deiseroth, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bootstrapping from pre-trained language models has been proven to be an
efficient approach for building vision-language models (VLM) for tasks such as
image captioning or visual question answering. However, outputs of these models
rarely align with user's rationales for specific answers. In order to improve
this alignment and reinforce commonsense reasons, we propose a tuning paradigm
based on human interactions with machine generated data. Our ILLUME executes
the following loop: Given an image-question-answer prompt, the VLM samples
multiple candidate rationales, and a human critic provides minimal feedback via
preference selection, used for fine-tuning. This loop increases the training
data and gradually carves out the VLM's rationalization capabilities that are
aligned with human intend. Our exhaustive experiments demonstrate that ILLUME
is competitive with standard supervised fine-tuning while using significantly
fewer training data and only requiring minimal feedback.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Theoretical Framework for AI Models Explainability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.14447v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.14447v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Rizzo, Alberto Veneri, Andrea Albarelli, Claudio Lucchese, Cristina Conati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  EXplainable Artificial Intelligence (XAI) is a vibrant research topic in the
artificial intelligence community, with growing interest across methods and
domains. Much has been written about the subject, yet XAI still lacks shared
terminology and a framework capable of providing structural soundness to
explanations. In our work, we address these issues by proposing a novel
definition of explanation that is a synthesis of what can be found in the
literature. We recognize that explanations are not atomic but the combination
of evidence stemming from the model and its input-output mapping, and the human
interpretation of this evidence. Furthermore, we fit explanations into the
properties of faithfulness (i.e., the explanation being a true description of
the model's inner workings and decision-making process) and plausibility (i.e.,
how much the explanation looks convincing to the user). Using our proposed
theoretical framework simplifies how these properties are operationalized and
it provides new insight into common explanation methods that we analyze as case
studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Point Discriminative Learning for Data-efficient 3D Point Cloud Analysis <span class="chip">3DV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.02104v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.02104v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fayao Liu, Guosheng Lin, Chuan-Sheng Foo, Chaitanya K. Joshi, Jie Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D point cloud analysis has drawn a lot of research attention due to its wide
applications. However, collecting massive labelled 3D point cloud data is both
time-consuming and labor-intensive. This calls for data-efficient learning
methods. In this work we propose PointDisc, a point discriminative learning
method to leverage self-supervisions for data-efficient 3D point cloud
classification and segmentation. PointDisc imposes a novel point discrimination
loss on the middle and global level features produced by the backbone network.
This point discrimination loss enforces learned features to be consistent with
points belonging to the corresponding local shape region and inconsistent with
randomly sampled noisy points. We conduct extensive experiments on 3D object
classification, 3D semantic and part segmentation, showing the benefits of
PointDisc for data-efficient learning. Detailed analysis demonstrate that
PointDisc learns unsupervised features that well capture local and global
geometry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work is published in 3DV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hypergraph <span class="highlight-title">Transformer</span> for Skeleton-based Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09590v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09590v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zhou, Chao Li, Zhi-Qi Cheng, Yifeng Geng, Xuansong Xie, Margret Keuper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skeleton-based action recognition aims to predict human actions given human
joint coordinates with skeletal interconnections. To model such off-grid data
points and their co-occurrences, Transformer-based formulations would be a
natural choice. However, Transformers still lag behind state-of-the-art methods
using graph convolutional networks (GCNs). Transformers assume that the input
is permutation-invariant and homogeneous (partially alleviated by positional
encoding), which ignores an important characteristic of skeleton data, i.e.,
bone connectivity. Furthermore, each type of body joint has a clear physical
meaning in human motion, i.e., motion retains an intrinsic relationship
regardless of the joint coordinates, which is not explored in Transformers. In
fact, certain re-occurring groups of body joints are often involved in specific
actions, such as the subconscious hand movement for keeping balance. Vanilla
attention is incapable of describing such underlying relations that are
persistent and beyond pair-wise. In this work, we aim to exploit these unique
aspects of skeleton data to close the performance gap between Transformers and
GCNs. Specifically, we propose a new self-attention (SA) extension, named
Hypergraph Self-Attention (HyperSA), to incorporate inherently higher-order
relations into the model. The K-hop relative positional embeddings are also
employed to take bone connectivity into account. We name the resulting model
Hyperformer, and it achieves comparable or better performance w.r.t. accuracy
and efficiency than state-of-the-art GCN architectures on NTU RGB+D, NTU RGB+D
120, and Northwestern-UCLA datasets. On the largest NTU RGB+D 120 dataset, the
significantly improved performance reached by our Hyperformer demonstrates the
underestimated potential of Transformer models in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Sequential Latent Variable Models from Multimodal Time Series
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.10419v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.10419v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliver Limoyo, Trevor Ablett, Jonathan Kelly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential modelling of high-dimensional data is an important problem that
appears in many domains including model-based reinforcement learning and
dynamics identification for control. Latent variable models applied to
sequential data (i.e., latent dynamics models) have been shown to be a
particularly effective probabilistic approach to solve this problem, especially
when dealing with images. However, in many application areas (e.g., robotics),
information from multiple sensing modalities is available -- existing latent
dynamics methods have not yet been extended to effectively make use of such
multimodal sequential data. Multimodal sensor streams can be correlated in a
useful manner and often contain complementary information across modalities. In
this work, we present a self-supervised generative modelling framework to
jointly learn a probabilistic latent state representation of multimodal data
and the respective dynamics. Using synthetic and real-world datasets from a
multimodal robotic planar pushing task, we demonstrate that our approach leads
to significant improvements in prediction and representation quality.
Furthermore, we compare to the common learning baseline of concatenating each
modality in the latent space and show that our principled probabilistic
formulation performs better. Finally, despite being fully self-supervised, we
demonstrate that our method is nearly as effective as an existing supervised
approach that relies on ground truth labels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In: Petrovic, I., Menegatti, E., Markovi\'c, I. (eds) Intelligent
  Autonomous Systems 17. IAS 2022. Lecture Notes in Networks and Systems, vol
  577. Springer, Cham</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Refining time-space traffic diagrams: A multiple linear regression model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.04457v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.04457v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengbing He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A time-space traffic (TS) diagram, which presents traffic states in
time-space cells with color, is an important traffic analysis and visualization
tool. Despite its importance for transportation research and engineering, most
TS diagrams that have already existed or are being produced are too coarse to
exhibit detailed traffic dynamics due to the limitations of existing
information technology and traffic infrastructure investment. To increase the
resolution of a TS diagram and enable it to present ample traffic details, this
paper introduces the TS diagram refinement problem and proposes a multiple
linear regression-based model to solve the problem. Two tests, which attempt to
increase the resolution of a TS diagram 4 and 16 times, are carried out to
evaluate the performance of the proposed model. Data collected at different
times, in different locations and even in different countries are employed to
thoroughly evaluate the accuracy and transferability of the proposed model.
Strict tests with diverse data show that the proposed model, despite its
simplicity, is able to refine a TS diagram with promising accuracy and reliable
transferability. The proposed refinement model will "save" widely existing TS
diagrams from their blurry "faces" and enable TS diagrams to show more traffic
details.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Implicit Representations for Physical Parameter Inference from a
  Single Video <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.14030v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.14030v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Hofherr, Lukas Koestler, Florian Bernard, Daniel Cremers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks have recently been used to analyze diverse physical systems
and to identify the underlying dynamics. While existing methods achieve
impressive results, they are limited by their strong demand for training data
and their weak generalization abilities to out-of-distribution data. To
overcome these limitations, in this work we propose to combine neural implicit
representations for appearance modeling with neural ordinary differential
equations (ODEs) for modelling physical phenomena to obtain a dynamic scene
representation that can be identified directly from visual observations. Our
proposed model combines several unique advantages: (i) Contrary to existing
approaches that require large training datasets, we are able to identify
physical parameters from only a single video. (ii) The use of neural implicit
representations enables the processing of high-resolution videos and the
synthesis of photo-realistic images. (iii) The embedded neural ODE has a known
parametric form that allows for the identification of interpretable physical
parameters, and (iv) long-term prediction in state space. (v) Furthermore, the
photo-realistic rendering of novel scenes with modified physical parameters
becomes possible.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in IEEE/CVF Winter Conference on Applications of Computer
  Vision (WACV) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty in Real-Time Semantic Segmentation on Embedded Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.01201v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.01201v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Goan, Clinton Fookes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Application for semantic segmentation models in areas such as autonomous
vehicles and human computer interaction require real-time predictive
capabilities. The challenges of addressing real-time application is amplified
by the need to operate on resource constrained hardware. Whilst development of
real-time methods for these platforms has increased, these models are unable to
sufficiently reason about uncertainty present. This paper addresses this by
combining deep feature extraction from pre-trained models with Bayesian
regression and moment propagation for uncertainty aware predictions. We
demonstrate how the proposed method can yield meaningful uncertainty on
embedded hardware in real-time whilst maintaining predictive performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty Quantification of Collaborative Detection for Self-Driving <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.08162v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.08162v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanbao Su, Yiming Li, Sihong He, Songyang Han, Chen Feng, Caiwen Ding, Fei Miao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sharing information between connected and autonomous vehicles (CAVs)
fundamentally improves the performance of collaborative object detection for
self-driving. However, CAVs still have uncertainties on object detection due to
practical challenges, which will affect the later modules in self-driving such
as planning and control. Hence, uncertainty quantification is crucial for
safety-critical systems such as CAVs. Our work is the first to estimate the
uncertainty of collaborative object detection. We propose a novel uncertainty
quantification method, called Double-M Quantification, which tailors a moving
block bootstrap (MBB) algorithm with direct modeling of the multivariant
Gaussian distribution of each corner of the bounding box. Our method captures
both the epistemic uncertainty and aleatoric uncertainty with one inference
pass based on the offline Double-M training process. And it can be used with
different collaborative object detectors. Through experiments on the
comprehensive collaborative perception dataset, we show that our Double-M
method achieves more than 4X improvement on uncertainty score and more than 3%
accuracy improvement, compared with the state-of-the-art uncertainty
quantification methods. Our code is public on
https://coperception.github.io/double-m-quantification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by the 2023 IEEE International
  Conference on Robotics and Automation (ICRA 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rich Action-semantic Consistent Knowledge for Early Action Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.09169v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.09169v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoli Liu, Jianqin Yin, Di Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early action prediction (EAP) aims to recognize human actions from a part of
action execution in ongoing videos, which is an important task for many
practical applications. Most prior works treat partial or full videos as a
whole, ignoring rich action knowledge hidden in videos, i.e., semantic
consistencies among different partial videos. In contrast, we partition
original partial or full videos to form a new series of partial videos and mine
the Action Semantic Consistent Knowledge (ASCK) among these new partial videos
evolving in arbitrary progress levels. Moreover, a novel Rich Action-semantic
Consistent Knowledge network (RACK) under the teacher-student framework is
proposed for EAP. Firstly, we use a two-stream pre-trained model to extract
features of videos. Secondly, we treat the RGB or flow features of the partial
videos as nodes and their action semantic consistencies as edges. Next, we
build a bi-directional semantic graph for the teacher network and a
single-directional semantic graph for the student network to model rich ASCK
among partial videos. The MSE and MMD losses are incorporated as our
distillation loss to enrich the ASCK of partial videos from the teacher to the
student network. Finally, we obtain the final prediction by summering the
logits of different sub-networks and applying a softmax layer. Extensive
experiments and ablative studies have been conducted, demonstrating the
effectiveness of modeling rich ASCK for EAP. With the proposed RACK, we have
achieved state-of-the-art performance on three benchmarks. The code will be
released if the paper is accepted.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Play and Self-Describe: Policy Adaptation with Vision-Language
  Foundation Models <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07398v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07398v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuying Ge, Annabella Macaluso, Li Erran Li, Ping Luo, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress on vision-language foundation models have brought significant
advancement to building general-purpose robots. By using the pre-trained models
to encode the scene and instructions as inputs for decision making, the
instruction-conditioned policy can generalize across different objects and
tasks. While this is encouraging, the policy still fails in most cases given an
unseen task or environment. To adapt the policy to unseen tasks and
environments, we explore a new paradigm on leveraging the pre-trained
foundation models with Self-PLAY and Self-Describe (SPLAYD). When deploying the
trained policy to a new task or a new environment, we first let the policy
self-play with randomly generated instructions to record the demonstrations.
While the execution could be wrong, we can use the pre-trained foundation
models to accurately self-describe (i.e., re-label or classify) the
demonstrations. This automatically provides new pairs of
demonstration-instruction data for policy fine-tuning. We evaluate our method
on a broad range of experiments with the focus on generalization on unseen
objects, unseen tasks, unseen environments, and sim-to-real transfer. We show
SPLAYD improves baselines by a large margin in all cases. Our project page is
available at https://geyuying.github.io/SPLAYD/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://geyuying.github.io/SPLAYD/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self Supervision Does Not Help Natural Language Supervision at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07836v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07836v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Floris Weers, Vaishaal Shankar, Angelos Katharopoulos, Yinfei Yang, Tom Gunter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self supervision and natural language supervision have emerged as two
exciting ways to train general purpose image encoders which excel at a variety
of downstream tasks. Recent works such as M3AE and SLIP have suggested that
these approaches can be effectively combined, but most notably their results
use small pre-training datasets (<50M samples) and don't effectively reflect
the large-scale regime (>100M examples) that is commonly used for these
approaches. Here we investigate whether a similar approach can be effective
when trained with a much larger amount of data. We find that a combination of
two state of the art approaches: masked auto-encoders, MAE and contrastive
language image pre-training, CLIP provides a benefit over CLIP when trained on
a corpus of 11.3M image-text pairs, but little to no benefit (as evaluated on a
suite of common vision tasks) over CLIP when trained on a large corpus of 1.4B
images. Our work provides some much needed clarity into the effectiveness (or
lack thereof) of self supervision for large-scale image-text training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedRolex: Model-Heterogeneous Federated Learning with Rolling Sub-Model
  Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.01548v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.01548v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samiul Alam, Luyang Liu, Ming Yan, Mi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most cross-device federated learning (FL) studies focus on the
model-homogeneous setting where the global server model and local client models
are identical. However, such constraint not only excludes low-end clients who
would otherwise make unique contributions to model training but also restrains
clients from training large models due to on-device resource bottlenecks. In
this work, we propose FedRolex, a partial training (PT)-based approach that
enables model-heterogeneous FL and can train a global server model larger than
the largest client model. At its core, FedRolex employs a rolling sub-model
extraction scheme that allows different parts of the global server model to be
evenly trained, which mitigates the client drift induced by the inconsistency
between individual client models and server model architectures. We show that
FedRolex outperforms state-of-the-art PT-based model-heterogeneous FL methods
(e.g. Federated Dropout) and reduces the gap between model-heterogeneous and
model-homogeneous FL, especially under the large-model large-dataset regime. In
addition, we provide theoretical statistical analysis on its advantage over
Federated Dropout and evaluate FedRolex on an emulated real-world device
distribution to show that FedRolex can enhance the inclusiveness of FL and
boost the performance of low-end devices that would otherwise not benefit from
FL. Our code is available at: https://github.com/AIoT-MLSys-Lab/FedRolex
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 Figures, Published in 36th Conference on Neural
  Information Processing And Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Trained Proposal Networks for the Open World 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.11050v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.11050v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Inkawhich, Nathan Inkawhich, Hai Li, Yiran Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current state-of-the-art object proposal networks are trained with a
closed-world assumption, meaning they learn to only detect objects of the
training classes. These models fail to provide high recall in open-world
environments where important novel objects may be encountered. While a handful
of recent works attempt to tackle this problem, they fail to consider that the
optimal behavior of a proposal network can vary significantly depending on the
data and application. Our goal is to provide a flexible proposal solution that
can be easily tuned to suit a variety of open-world settings. To this end, we
design a Self-Trained Proposal Network (STPN) that leverages an adjustable
hybrid architecture, a novel self-training procedure, and dynamic loss
components to optimize the tradeoff between known and unknown object detection
performance. To thoroughly evaluate our method, we devise several new
challenges which invoke varying degrees of label bias by altering known class
diversity and label count. We find that in every task, STPN easily outperforms
existing baselines (e.g., RPN, OLN). Our method is also highly data efficient,
surpassing baseline recall with a fraction of the labeled data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 9 figures, 10 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Slate Recommendation with Reinforcement Learning <span class="chip">WSDM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romain Deffayet, Thibaut Thonet, Jean-Michel Render, Maarten de Rijke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has employed reinforcement learning (RL) algorithms to
optimize long-term user engagement in recommender systems, thereby avoiding
common pitfalls such as user boredom and filter bubbles. They capture the
sequential and interactive nature of recommendations, and thus offer a
principled way to deal with long-term rewards and avoid myopic behaviors.
However, RL approaches are intractable in the slate recommendation scenario -
where a list of items is recommended at each interaction turn - due to the
combinatorial action space. In that setting, an action corresponds to a slate
that may contain any combination of items.
  While previous work has proposed well-chosen decompositions of actions so as
to ensure tractability, these rely on restrictive and sometimes unrealistic
assumptions. Instead, in this work we propose to encode slates in a continuous,
low-dimensional latent space learned by a variational auto-encoder. Then, the
RL agent selects continuous actions in this latent space, which are ultimately
decoded into the corresponding slates. By doing so, we are able to (i) relax
assumptions required by previous work, and (ii) improve the quality of the
action selection by modeling full slates instead of independent items, in
particular by enabling diversity. Our experiments performed on a wide array of
simulated environments confirm the effectiveness of our generative modeling of
slates over baselines in practical scenarios where the restrictive assumptions
underlying the baselines are lifted. Our findings suggest that representation
learning using generative models is a promising direction towards generalizable
RL-based slate recommendation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WSDM 2023, 9 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Evolution of Web Search User Interfaces -- An Archaeological
  Analysis of Google Search Engine Result Pages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        B. Oliveira, C. T. Lopes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Web search engines have marked everyone's life by transforming how one
searches and accesses information. Search engines give special attention to the
user interface, especially search engine result pages (SERP). The well-known
''10 blue links'' list has evolved into richer interfaces, often personalized
to the search query, the user, and other aspects. More than 20 years later, the
literature has not adequately portrayed this development. We present a study on
the evolution of SERP interfaces during the last two decades using Google
Search as a case study. We used the most searched queries by year to extract a
sample of SERP from the Internet Archive. Using this dataset, we analyzed how
SERP evolved in content, layout, design (e.g., color scheme, text styling,
graphics), navigation, and file size. We have also analyzed the user interface
design patterns associated with SERP elements. We found that SERP are becoming
more diverse in terms of elements, aggregating content from different verticals
and including more features that provide direct answers. This systematic
analysis portrays evolution trends in search engine user interfaces and, more
generally, web design. We expect this work will trigger other, more specific
studies that can take advantage of our dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, Full Paper of CHIIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Information Retrieval: Recent Advances and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kailash A. Hambarde, Hugo Proenca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we provide a detailed overview of the models used for
information retrieval in the first and second stages of the typical processing
chain. We discuss the current state-of-the-art models, including methods based
on terms, semantic retrieval, and neural. Additionally, we delve into the key
topics related to the learning process of these models. This way, this survey
offers a comprehensive understanding of the field and is of interest for for
researchers and practitioners entering/working in the information retrieval
domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robot Skill Learning Via Classical Robotics-Based Generated <span class="highlight-title">Dataset</span>s:
  Advantages, Disadvantages, and Future Improvement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Batu Kaan Oezen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Why do we not profit from our long-existing classical robotics knowledge and
look for some alternative way for data collection? The situation ignoring all
existing methods might be such a waste. This article argues that a dataset
created using a classical robotics algorithm is a crucial part of future
development. This developed classic algorithm has a perfect domain adaptation
and generalization property, and most importantly, collecting datasets based on
them is quite easy. It is well known that current robot skill-learning
approaches perform exceptionally badly in the unseen domain, and their
performance against adversarial attacks is quite limited as long as they do not
have a very exclusive big dataset. Our experiment is the initial steps of using
a dataset created by classical robotics codes. Our experiment investigated
possible trajectory collection based on classical robotics. It addressed some
advantages and disadvantages and pointed out other future development ideas.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging the Gap Between Indexing and Retrieval for Differentiable
  Search Index with Query Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.10128v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.10128v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyao Zhuang, Houxing Ren, Linjun Shou, Jian Pei, Ming Gong, Guido Zuccon, Daxin Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Differentiable Search Index (DSI) is an emerging paradigm for information
retrieval. Unlike traditional retrieval architectures where index and retrieval
are two different and separate components, DSI uses a single transformer model
to perform both indexing and retrieval.
  In this paper, we identify and tackle an important issue of current DSI
models: the data distribution mismatch that occurs between the DSI indexing and
retrieval processes. Specifically, we argue that, at indexing, current DSI
methods learn to build connections between the text of long documents and the
identifier of the documents, but then retrieval of document identifiers is
based on queries that are commonly much shorter than the indexed documents.
This problem is further exacerbated when using DSI for cross-lingual retrieval,
where document text and query text are in different languages.
  To address this fundamental problem of current DSI models, we propose a
simple yet effective indexing framework for DSI, called DSI-QG. When indexing,
DSI-QG represents documents with a number of potentially relevant queries
generated by a query generation model and re-ranked and filtered by a
cross-encoder ranker. The presence of these queries at indexing allows the DSI
models to connect a document identifier to a set of queries, hence mitigating
data distribution mismatches present between the indexing and the retrieval
phases. Empirical results on popular mono-lingual and cross-lingual passage
retrieval datasets show that DSI-QG significantly outperforms the original DSI
model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Architecture Search: Insights from 1000 Papers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Colin White, Mahmoud Safari, Rhea Sukthanker, Binxin Ru, Thomas Elsken, Arber Zela, Debadeepta Dey, Frank Hutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the past decade, advances in deep learning have resulted in breakthroughs
in a variety of areas, including computer vision, natural language
understanding, speech recognition, and reinforcement learning. Specialized,
high-performing neural architectures are crucial to the success of deep
learning in these areas. Neural architecture search (NAS), the process of
automating the design of neural architectures for a given task, is an
inevitable next step in automating machine learning and has already outpaced
the best human-designed architectures on many tasks. In the past few years,
research in NAS has been progressing rapidly, with over 1000 papers released
since 2020. In this survey, we provide an organized and comprehensive guide to
neural architecture search. We give a taxonomy of search spaces, algorithms,
and speedup techniques, and we discuss resources such as benchmarks, best
practices, other surveys, and open-source libraries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Baechi: Fast Device Placement of Machine Learning Graphs <span class="chip">SoCC 2020</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beomyeol Jeon, Linda Cai, Chirag Shetty, Pallavi Srivastava, Jintao Jiang, Xiaolan Ke, Yitao Meng, Cong Xie, Indranil Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Learning graphs (or models) can be challenging or impossible to train
when either devices have limited memory, or models are large. To split the
model across devices, learning-based approaches are still popular. While these
result in model placements that train fast on data (i.e., low step times),
learning-based model-parallelism is time-consuming, taking many hours or days
to create a placement plan of operators on devices. We present the Baechi
system, the first to adopt an algorithmic approach to the placement problem for
running machine learning training graphs on small clusters of
memory-constrained devices. We integrate our implementation of Baechi into two
popular open-source learning frameworks: TensorFlow and PyTorch. Our
experimental results using GPUs show that: (i) Baechi generates placement plans
654 X - 206K X faster than state-of-the-art learning-based approaches, and (ii)
Baechi-placed model's step (training) time is comparable to expert placements
in PyTorch, and only up to 6.2% worse than expert placements in TensorFlow. We
prove mathematically that our two algorithms are within a constant factor of
the optimal. Our work shows that compared to learning-based approaches,
algorithmic approaches can face different challenges for adaptation to Machine
learning systems, but also they offer proven bounds, and significant
performance benefits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of SoCC 2020 paper:
  https://dl.acm.org/doi/10.1145/3419111.3421302</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Asynchronous Deep Double Duelling Q-Learning for Trading-Signal
  Execution in Limit Order Book Markets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peer Nagy, Jan-Peter Calliess, Stefan Zohren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We employ deep reinforcement learning (RL) to train an agent to successfully
translate a high-frequency trading signal into a trading strategy that places
individual limit orders. Based on the ABIDES limit order book simulator, we
build a reinforcement learning OpenAI gym environment and utilise it to
simulate a realistic trading environment for NASDAQ equities based on historic
order book messages. To train a trading agent that learns to maximise its
trading return in this environment, we use Deep Duelling Double Q-learning with
the APEX (asynchronous prioritised experience replay) architecture. The agent
observes the current limit order book state, its recent history, and a
short-term directional forecast. To investigate the performance of RL for
adaptive trading independently from a concrete forecasting algorithm, we study
the performance of our approach utilising synthetic alpha signals obtained by
perturbing forward-looking returns with varying levels of noise. Here, we find
that the RL agent learns an effective trading strategy for inventory management
and order placing that outperforms a heuristic benchmark trading strategy
having access to the same signal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingting Yuan, Liang Mi, Weijun Wang, Haipeng Dai, Xiaoming Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quality of the video stream is key to neural network-based video
analytics. However, low-quality video is inevitably collected by existing
surveillance systems because of poor quality cameras or over-compressed/pruned
video streaming protocols, e.g., as a result of upstream bandwidth limit. To
address this issue, existing studies use quality enhancers (e.g., neural
super-resolution) to improve the quality of videos (e.g., resolution) and
eventually ensure inference accuracy. Nevertheless, directly applying quality
enhancers does not work in practice because it will introduce unacceptable
latency. In this paper, we present AccDecoder, a novel accelerated decoder for
real-time and neural-enhanced video analytics. AccDecoder can select a few
frames adaptively via Deep Reinforcement Learning (DRL) to enhance the quality
by neural super-resolution and then up-scale the unselected frames that
reference them, which leads to 6-21% accuracy improvement. AccDecoder provides
efficient inference capability via filtering important frames using DRL for
DNN-based inference and reusing the results for the other frames via extracting
the reference relationship among frames and blocks, which results in a latency
reduction of 20-80% than baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2023 IEEE INFOCOM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated extraction of capacitive coupling for quantum dot systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Ziegler, Florian Luthi, Mick Ramsey, Felix Borjans, Guoji Zheng, Justyna P. Zwolak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gate-defined quantum dots (QDs) have appealing attributes as a quantum
computing platform, however, near-term devices possess a range of possible
imperfections that need to be accounted for during the tuning and operation of
QD devices. One such problem is the capacitive cross-talk between the metallic
gates that define and control QD qubits. A way to compensate for the capacitive
cross-talk and enable targeted control of specific QDs independent of coupling
is by the use of virtual gates. Here, we demonstrate a reliable automated
capacitive coupling identification method that combines machine learning with
traditional fitting to take advantage of the desirable properties of each. We
also show how the cross-capacitance measurement may be used for the
identification of spurious QDs sometimes formed during tuning experimental
devices. Our systems can autonomously flag devices with spurious dots near the
operating regime which is crucial information for reliable tuning to a regime
suitable for qubit operations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Offline Policy Evaluation with Out-of-Sample Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sofia Ek, Dave Zachariah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of evaluating the performance of a decision policy
using past observational data. The outcome of a policy is measured in terms of
a loss or disutility (or negative reward) and the problem is to draw valid
inferences about the out-of-sample loss of the specified policy when the past
data is observed under a, possibly unknown, policy. Using a sample-splitting
method, we show that it is possible to draw such inferences with finite-sample
coverage guarantees that evaluate the entire loss distribution. Importantly,
the method takes into account model misspecifications of the past policy --
including unmeasured confounding. The evaluation method can be used to certify
the performance of a policy using observational data under an explicitly
specified range of credible model assumptions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STORM-GAN: Spatio-Temporal Meta-GAN for Cross-City Estimation of Human
  Mobility Responses to COVID-19 <span class="chip">ICDM 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Bao, Xun Zhou, Yiqun Xie, Yanhua Li, Xiaowei Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human mobility estimation is crucial during the COVID-19 pandemic due to its
significant guidance for policymakers to make non-pharmaceutical interventions.
While deep learning approaches outperform conventional estimation techniques on
tasks with abundant training data, the continuously evolving pandemic poses a
significant challenge to solving this problem due to data nonstationarity,
limited observations, and complex social contexts. Prior works on mobility
estimation either focus on a single city or lack the ability to model the
spatio-temporal dependencies across cities and time periods. To address these
issues, we make the first attempt to tackle the cross-city human mobility
estimation problem through a deep meta-generative framework. We propose a
Spatio-Temporal Meta-Generative Adversarial Network (STORM-GAN) model that
estimates dynamic human mobility responses under a set of social and policy
conditions related to COVID-19. Facilitated by a novel spatio-temporal
task-based graph (STTG) embedding, STORM-GAN is capable of learning shared
knowledge from a spatio-temporal distribution of estimation tasks and quickly
adapting to new cities and time periods with limited training samples. The STTG
embedding component is designed to capture the similarities among cities to
mitigate cross-task heterogeneity. Experimental results on real-world data show
that the proposed approach can greatly improve estimation performance and
out-perform baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 22nd IEEE International Conference on Data Mining
  (ICDM 2022) Full Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Slate Recommendation with Reinforcement Learning <span class="chip">WSDM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romain Deffayet, Thibaut Thonet, Jean-Michel Render, Maarten de Rijke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has employed reinforcement learning (RL) algorithms to
optimize long-term user engagement in recommender systems, thereby avoiding
common pitfalls such as user boredom and filter bubbles. They capture the
sequential and interactive nature of recommendations, and thus offer a
principled way to deal with long-term rewards and avoid myopic behaviors.
However, RL approaches are intractable in the slate recommendation scenario -
where a list of items is recommended at each interaction turn - due to the
combinatorial action space. In that setting, an action corresponds to a slate
that may contain any combination of items.
  While previous work has proposed well-chosen decompositions of actions so as
to ensure tractability, these rely on restrictive and sometimes unrealistic
assumptions. Instead, in this work we propose to encode slates in a continuous,
low-dimensional latent space learned by a variational auto-encoder. Then, the
RL agent selects continuous actions in this latent space, which are ultimately
decoded into the corresponding slates. By doing so, we are able to (i) relax
assumptions required by previous work, and (ii) improve the quality of the
action selection by modeling full slates instead of independent items, in
particular by enabling diversity. Our experiments performed on a wide array of
simulated environments confirm the effectiveness of our generative modeling of
slates over baselines in practical scenarios where the restrictive assumptions
underlying the baselines are lifted. Our findings suggest that representation
learning using generative models is a promising direction towards generalizable
RL-based slate recommendation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WSDM 2023, 9 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Coupled Physics-informed Neural Networks for Inferring Solutions of
  Partial Differential Equations with Unknown Source Terms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aina Wang, Pan Qin, Xi-Ming Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-informed neural networks (PINNs) provide a transformative development
for approximating the solutions to partial differential equations (PDEs). This
work proposes a coupled physics-informed neural network (C-PINN) for the
nonhomogeneous PDEs with unknown dynamical source terms, which is used to
describe the systems with external forces and cannot be well approximated by
the existing PINNs. In our method, two neural networks, NetU and NetG, are
proposed. NetU is constructed to generate a quasi-solution satisfying PDEs
under study. NetG is used to regularize the training of NetU. Then, the two
networks are integrated into a data-physics-hybrid cost function. Finally, we
propose a hierarchical training strategy to optimize and couple the two
networks. The performance of C-PINN is proved by approximating several
classical PDEs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ontology <span class="highlight-title">Pre-train</span>ing for Poison Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Glauer, Fabian Neuhaus, Till Mossakowski, Janna Hastings
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating human knowledge into neural networks has the potential to improve
their robustness and interpretability. We have developed a novel approach to
integrate knowledge from ontologies into the structure of a Transformer network
which we call ontology pre-training: we train the network to predict membership
in ontology classes as a way to embed the structure of the ontology into the
network, and subsequently fine-tune the network for the particular prediction
task. We apply this approach to a case study in predicting the potential
toxicity of a small molecule based on its molecular structure, a challenging
task for machine learning in life sciences chemistry. Our approach improves on
the state of the art, and moreover has several additional benefits. First, we
are able to show that the model learns to focus attention on more meaningful
chemical groups when making predictions with ontology pre-training than
without, paving a path towards greater robustness and interpretability. Second,
the training time is reduced after ontology pre-training, indicating that the
model is better placed to learn what matters for toxicity prediction with the
ontology pre-training than without. This strategy has general applicability as
a neuro-symbolic approach to embed meaningful semantics into neural networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Writing <span class="highlight-title">Prompt</span>s: Character-Grounded Story Generation with Curated
  Image Sequences <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xudong Hong, Asad Sayeed, Khushboo Mehra, Vera Demberg, Bernt Schiele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current work on image-based story generation suffers from the fact that the
existing image sequence collections do not have coherent plots behind them. We
improve visual story generation by producing a new image-grounded dataset,
Visual Writing Prompts (VWP). VWP contains almost 2K selected sequences of
movie shots, each including 5-10 images. The image sequences are aligned with a
total of 12K stories which were collected via crowdsourcing given the image
sequences and a set of grounded characters from the corresponding image
sequence. Our new image sequence collection and filtering process has allowed
us to obtain stories that are more coherent and have more narrativity compared
to previous work. We also propose a character-based story generation model
driven by coherence as a strong baseline. Evaluations show that our generated
stories are more coherent, visually grounded, and have more narrativity than
stories generated with the current state-of-the-art model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted by Transactions of the Association for Computational
  Linguistics (TACL). This is a pre-MIT Press publication version. 15 pages, 6
  figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Surface Texture in Steel Manufacturing at Speed 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander J. M. Milne, Xianghua Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Control of the surface texture of steel strip during the galvanizing and
temper rolling processes is essential to satisfy customer requirements and is
conventionally measured post-production using a stylus. In-production laser
reflection measurement is less consistent than physical measurement but enables
real time adjustment of processing parameters to optimize product surface
characteristics. We propose the use of machine learning to improve accuracy of
the transformation from inline laser reflection measurements to a prediction of
surface properties. In addition to accuracy, model evaluation speed is
important for fast feedback control. The ROCKET model is one of the fastest
state of the art models, however it can be sped up by utilizing a GPU. Our
contribution is to implement the model in PyTorch for fast GPU kernel
transforms and provide a soft version of the Proportion of Positive Values
(PPV) nonlinear pooling function, allowing gradient flow. We perform timing and
performance experiments comparing the implementations
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Promises and pitfalls of deep neural networks in neuroimaging-based
  psychiatric research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Eitel, Marc-André Schulz, Moritz Seiler, Henrik Walter, Kerstin Ritter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By promising more accurate diagnostics and individual treatment
recommendations, deep neural networks and in particular convolutional neural
networks have advanced to a powerful tool in medical imaging. Here, we first
give an introduction into methodological key concepts and resulting
methodological promises including representation and transfer learning, as well
as modelling domain-specific priors. After reviewing recent applications within
neuroimaging-based psychiatric research, such as the diagnosis of psychiatric
diseases, delineation of disease subtypes, normative modeling, and the
development of neuroimaging biomarkers, we discuss current challenges. This
includes for example the difficulty of training models on small, heterogeneous
and biased data sets, the lack of validity of clinical labels, algorithmic
bias, and the influence of confounding variables.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clustering Human Mobility with Multiple Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoji Hu, Haowen Lin, Yao-Yi Chiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human mobility clustering is an important problem for understanding human
mobility behaviors (e.g., work and school commutes). Existing methods typically
contain two steps: choosing or learning a mobility representation and applying
a clustering algorithm to the representation. However, these methods rely on
strict visiting orders in trajectories and cannot take advantage of multiple
types of mobility representations. This paper proposes a novel mobility
clustering method for mobility behavior detection. First, the proposed method
contains a permutation-equivalent operation to handle sub-trajectories that
might have different visiting orders but similar impacts on mobility behaviors.
Second, the proposed method utilizes a variational autoencoder architecture to
simultaneously perform clustering in both latent and original spaces. Also, in
order to handle the bias of a single latent space, our clustering assignment
prediction considers multiple learned latent spaces at different epochs. This
way, the proposed method produces accurate results and can provide reliability
estimates of each trajectory's cluster assignment. The experiment shows that
the proposed method outperformed state-of-the-art methods in mobility behavior
detection from trajectories with better accuracy and more interpretability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Regular Time-series Generation using SGM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haksoo Lim, Minjung Kim, Sewon Park, Noseong Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Score-based generative models (SGMs) are generative models that are in the
spotlight these days. Time-series frequently occurs in our daily life, e.g.,
stock data, climate data, and so on. Especially, time-series forecasting and
classification are popular research topics in the field of machine learning.
SGMs are also known for outperforming other generative models. As a result, we
apply SGMs to synthesize time-series data by learning conditional score
functions. We propose a conditional score network for the time-series
generation domain. Furthermore, we also derive the loss function between the
score matching and the denoising score matching in the time-series generation
domain. Finally, we achieve state-of-the-art results on real-world datasets in
terms of sampling diversity and quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, appendix 3 pages, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Agnostic Data-Driven Inverse Text Normalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Szu-Jui Chen, Debjyoti Paul, Yutong Pang, Peng Su, Xuedong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of automatic speech recognition (ASR) models, converting
the spoken form text (from ASR) to the written form is in urgent need. This
inverse text normalization (ITN) problem attracts the attention of researchers
from various fields. Recently, several works show that data-driven ITN methods
can output high-quality written form text. Due to the scarcity of labeled
spoken-written datasets, the studies on non-English data-driven ITN are quite
limited. In this work, we propose a language-agnostic data-driven ITN framework
to fill this gap. Specifically, we leverage the data augmentation in
conjunction with neural machine translated data for low resource languages.
Moreover, we design an evaluation method for language agnostic ITN model when
only English data is available. Our empirical evaluation shows this language
agnostic modeling approach is effective for low resource languages while
preserving the performance for high resource languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Plan To Predict: Learning an Uncertainty-Foreseeing Model for
  Model-Based Reinforcement Learning <span class="chip">NeurIPS2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifan Wu, Chao Yu, Chen Chen, Jianye Hao, Hankz Hankui Zhuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Model-based Reinforcement Learning (MBRL), model learning is critical
since an inaccurate model can bias policy learning via generating misleading
samples. However, learning an accurate model can be difficult since the policy
is continually updated and the induced distribution over visited states used
for model learning shifts accordingly. Prior methods alleviate this issue by
quantifying the uncertainty of model-generated samples. However, these methods
only quantify the uncertainty passively after the samples were generated,
rather than foreseeing the uncertainty before model trajectories fall into
those highly uncertain regions. The resulting low-quality samples can induce
unstable learning targets and hinder the optimization of the policy. Moreover,
while being learned to minimize one-step prediction errors, the model is
generally used to predict for multiple steps, leading to a mismatch between the
objectives of model learning and model usage. To this end, we propose
\emph{Plan To Predict} (P2P), an MBRL framework that treats the model rollout
process as a sequential decision making problem by reversely considering the
model as a decision maker and the current policy as the dynamics. In this way,
the model can quickly adapt to the current policy and foresee the multi-step
future uncertainty when generating trajectories. Theoretically, we show that
the performance of P2P can be guaranteed by approximately optimizing a lower
bound of the true environment return. Empirical results demonstrate that P2P
achieves state-of-the-art performance on several challenging benchmark tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Introducing Expertise Logic into Graph Representation Learning from A
  Causal Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Gao, Jiangmeng Li, Wenwen Qiang, Lingyu Si, Xingzhe Su, Fengge Wu, Changwen Zheng, Fuchun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benefiting from the injection of human prior knowledge, graphs, as derived
discrete data, are semantically dense so that models can efficiently learn the
semantic information from such data. Accordingly, graph neural networks (GNNs)
indeed achieve impressive success in various fields. Revisiting the GNN
learning paradigms, we discover that the relationship between human expertise
and the knowledge modeled by GNNs still confuses researchers. To this end, we
introduce motivating experiments and derive an empirical observation that the
human expertise is gradually learned by the GNNs in general domains. By further
observing the ramifications of introducing expertise logic into graph
representation learning, we conclude that leading the GNNs to learn human
expertise can improve the model performance. By exploring the intrinsic
mechanism behind such observations, we elaborate the Structural Causal Model
for the graph representation learning paradigm. Following the theoretical
guidance, we innovatively introduce the auxiliary causal logic learning
paradigm to improve the model to learn the expertise logic causally related to
the graph representation learning task. In practice, the counterfactual
technique is further performed to tackle the insufficient training issue during
optimization. Plentiful experiments on the crafted and real-world domains
support the consistent effectiveness of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Moral Choices in Social Dilemmas with Multi-Agent Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizaveta Tennant, Stephen Hailes, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Practical uses of Artificial Intelligence (AI) in the real world have
demonstrated the importance of embedding moral choices into intelligent agents.
They have also highlighted that defining top-down ethical constraints on AI
according to any one type of morality is extremely challenging and can pose
risks. A bottom-up learning approach may be more appropriate for studying and
developing ethical behavior in AI agents. In particular, we believe that an
interesting and insightful starting point is the analysis of emergent behavior
of Reinforcement Learning (RL) agents that act according to a predefined set of
moral rewards in social dilemmas.
  In this work, we present a systematic analysis of the choices made by
intrinsically-motivated RL agents whose rewards are based on moral theories. We
aim to design reward structures that are simplified yet representative of a set
of key ethical systems. Therefore, we first define moral reward functions that
distinguish between consequence- and norm-based agents, between morality based
on societal norms or internal virtues, and between single- and mixed-virtue
(e.g., multi-objective) methodologies. Then, we evaluate our approach by
modeling repeated dyadic interactions between learning moral agents in three
iterated social dilemma games (Prisoner's Dilemma, Volunteer's Dilemma and Stag
Hunt). We analyze the impact of different types of morality on the emergence of
cooperation, defection or exploitation, and the corresponding social outcomes.
Finally, we discuss the implications of these findings for the development of
moral agents in artificial and mixed human-AI societies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, currently under review for a conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pneumonia Detection in Chest X-Ray Images : Handling Class Imbalance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wardah Ali, Eesha Qureshi, Omama Ahmed Farooqi, Rizwan Ahmed Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  People all over the globe are affected by pneumonia but deaths due to it are
highest in Sub-Saharan Asia and South Asia. In recent years, the overall
incidence and mortality rate of pneumonia regardless of the utilization of
effective vaccines and compelling antibiotics has escalated. Thus, pneumonia
remains a disease that needs spry prevention and treatment. The widespread
prevalence of pneumonia has caused the research community to come up with a
framework that helps detect, diagnose and analyze diseases accurately and
promptly. One of the major hurdles faced by the Artificial Intelligence (AI)
research community is the lack of publicly available datasets for chest
diseases, including pneumonia . Secondly, few of the available datasets are
highly imbalanced (normal examples are over sampled, while samples with ailment
are in severe minority) making the problem even more challenging. In this
article we present a novel framework for the detection of pneumonia. The
novelty of the proposed methodology lies in the tackling of class imbalance
problem. The Generative Adversarial Network (GAN), specifically a combination
of Deep Convolutional Generative Adversarial Network (DCGAN) and Wasserstein
GAN gradient penalty (WGAN-GP) was applied on the minority class ``Pneumonia''
for augmentation, whereas Random Under-Sampling (RUS) was done on the majority
class ``No Findings'' to deal with the imbalance problem. The ChestX-Ray8
dataset, one of the biggest datasets, is used to validate the performance of
the proposed framework. The learning phase is completed using transfer learning
on state-of-the-art deep learning models i.e. ResNet-50, Xception, and VGG-16.
Results obtained exceed state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature Relevance Analysis to Explain Concept Drift -- A Case Study in
  Human Activity Recognition <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pekka Siirtola, Juha Röning
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article studies how to detect and explain concept drift. Human activity
recognition is used as a case study together with a online batch learning
situation where the quality of the labels used in the model updating process
starts to decrease. Drift detection is based on identifying a set of features
having the largest relevance difference between the drifting model and a model
that is known to be accurate and monitoring how the relevance of these features
changes over time. As a main result of this article, it is shown that feature
relevance analysis cannot only be used to detect the concept drift but also to
explain the reason for the drift when a limited number of typical reasons for
the concept drift are predefined. To explain the reason for the concept drift,
it is studied how these predefined reasons effect to feature relevance. In
fact, it is shown that each of these has an unique effect to features relevance
and these can be used to explain the reason for concept drift.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to HASCA 2022 workshop in conjunction with UbiComp/ISWC2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Multi-Agent Planning Using Graph <span class="highlight-title">Transformer</span>s with Bounded
  Suboptimality <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenning Yu, Qingbiao Li, Sicun Gao, Amanda Prorok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conflict-Based Search is one of the most popular methods for multi-agent path
finding. Though it is complete and optimal, it does not scale well. Recent
works have been proposed to accelerate it by introducing various heuristics.
However, whether these heuristics can apply to non-grid-based problem settings
while maintaining their effectiveness remains an open question. In this work,
we find that the answer is prone to be no. To this end, we propose a
learning-based component, i.e., the Graph Transformer, as a heuristic function
to accelerate the planning. The proposed method is provably complete and
bounded-suboptimal with any desired factor. We conduct extensive experiments on
two environments with dense graphs. Results show that the proposed Graph
Transformer can be trained in problem instances with relatively few agents and
generalizes well to a larger number of agents, while achieving better
performance than state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Source-free Subject Adaptation for EEG-based Visual Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pilhyeon Lee, Seogkyu Jeon, Sunhee Hwang, Minjung Shin, Hyeran Byun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on subject adaptation for EEG-based visual recognition. It
aims at building a visual stimuli recognition system customized for the target
subject whose EEG samples are limited, by transferring knowledge from abundant
data of source subjects. Existing approaches consider the scenario that samples
of source subjects are accessible during training. However, it is often
infeasible and problematic to access personal biological data like EEG signals
due to privacy issues. In this paper, we introduce a novel and practical
problem setup, namely source-free subject adaptation, where the source subject
data are unavailable and only the pre-trained model parameters are provided for
subject adaptation. To tackle this challenging problem, we propose
classifier-based data generation to simulate EEG samples from source subjects
using classifier responses. Using the generated samples and target subject
data, we perform subject-independent feature learning to exploit the common
knowledge shared across different subjects. Notably, our framework is
generalizable and can adopt any subject-independent learning method. In the
experiments on the EEG-ImageNet40 benchmark, our model brings consistent
improvements regardless of the choice of subject-independent learning. Also,
our method shows promising performance, recording top-1 test accuracy of 74.6%
under the 5-shot setting even without relying on source data. Our code can be
found at
https://github.com/DeepBCI/Deep-BCI/tree/master/1_Intelligent_BCI/Source_Free_Subject_Adaptation_for_EEG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 11th IEEE International Winter Conference on
  Brain-Computer Interface (BCI 2023). Code is available at
  https://github.com/DeepBCI/Deep-BCI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Estimation Bias in Policy Gradients for Deep Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxuan Pan, Deheng Ye, Xiaoming Duan, Qiang Fu, Wei Yang, Jianping He, Mingfei Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We revisit the estimation bias in policy gradients for the discounted
episodic Markov decision process (MDP) from Deep Reinforcement Learning (DRL)
perspective. The objective is formulated theoretically as the expected returns
discounted over the time horizon. One of the major policy gradient biases is
the state distribution shift: the state distribution used to estimate the
gradients differs from the theoretical formulation in that it does not take
into account the discount factor. Existing discussion of the influence of this
bias was limited to the tabular and softmax cases in the literature. Therefore,
in this paper, we extend it to the DRL setting where the policy is
parameterized and demonstrate how this bias can lead to suboptimal policies
theoretically. We then discuss why the empirically inaccurate implementations
with shifted state distribution can still be effective. We show that, despite
such state distribution shift, the policy gradient estimation bias can be
reduced in the following three ways: 1) a small learning rate; 2) an
adaptive-learning-rate-based optimizer; and 3) KL regularization. Specifically,
we show that a smaller learning rate, or, an adaptive learning rate, such as
that used by Adam and RSMProp optimizers, makes the policy optimization robust
to the bias. We further draw connections between optimizers and the
optimization regularization to show that both the KL and the reverse KL
regularization can significantly rectify this bias. Moreover, we provide
extensive experiments on continuous control tasks to support our analysis. Our
paper sheds light on how successful PG algorithms optimize policies in the DRL
setting, and contributes insights into the practical issues in DRL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-Supervised</span> Learning for Data Scarcity in a Fatigue Damage
  Prognostic Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anass Akrim, Christian Gogu, Rob Vingerhoeds, Michel Salaün
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing availability of data for Prognostics and Health
Management (PHM), Deep Learning (DL) techniques are now the subject of
considerable attention for this application, often achieving more accurate
Remaining Useful Life (RUL) predictions. However, one of the major challenges
for DL techniques resides in the difficulty of obtaining large amounts of
labelled data on industrial systems. To overcome this lack of labelled data, an
emerging learning technique is considered in our work: Self-Supervised
Learning, a sub-category of unsupervised learning approaches. This paper aims
to investigate whether pre-training DL models in a self-supervised way on
unlabelled sensors data can be useful for RUL estimation with only Few-Shots
Learning, i.e. with scarce labelled data. In this research, a fatigue damage
prognostics problem is addressed, through the estimation of the RUL of aluminum
alloy panels (typical of aerospace structures) subject to fatigue cracks from
strain gauge data. Synthetic datasets composed of strain data are used allowing
to extensively investigate the influence of the dataset size on the predictive
performance. Results show that the self-supervised pre-trained models are able
to significantly outperform the non-pre-trained models in downstream RUL
prediction task, and with less computational expense, showing promising results
in prognostic tasks when only limited labelled data is available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimization of body configuration and joint-driven attitude
  stabilization for transformable spacecrafts under solar radiation pressure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuki Kubo, Toshihiro Chujo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A solar sail is one of the most promising space exploration system because of
its theoretically infinite specific impulse using solar radiation pressure
(SRP). Recently, some researchers proposed "transformable spacecrafts" that can
actively reconfigure their body configurations with actuatable joints. The
transformable spacecrafts are expected to greatly enhance orbit and attitude
control capability due to its high redundancy in control degree of freedom if
they are used as solar sails. However, its large number of input poses
difficulties in control, and therefore, previous researchers imposed strong
constraints to limit its potential control capabilities. This paper addresses
novel attitude control techniques for the transformable spacecrafts under SRP.
The authors have constructed two proposed methods; one of those is a joint
angle optimization to acquire arbitrary SRP force and torque, and the other is
a momentum damping control driven by joint angle actuation. Our proposed
methods are formulated in general forms and applicable to any transformable
solar sail that consists of flat and thin body components. Validity of the
proposed methods are confirmed by numerical simulations. This paper contributes
to making most of the high control redundancy of transformable solar sails
without consuming any expendable propellants, which is expected to greatly
enhance orbit and attitude control capability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 11 figures, submitted to Astrodynamics published by
  Tsinghua University Press and Springer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Which Features are Learned by Code<span class="highlight-title">Bert</span>: An Empirical Study of the
  <span class="highlight-title">BERT</span>-based Source Code Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08427v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08427v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lan Zhang, Chen Cao, Zhilong Wang, Peng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Bidirectional Encoder Representations from Transformers (BERT) were
proposed in the natural language process (NLP) and shows promising results.
Recently researchers applied the BERT to source-code representation learning
and reported some good news on several downstream tasks. However, in this
paper, we illustrated that current methods cannot effectively understand the
logic of source codes. The representation of source code heavily relies on the
programmer-defined variable and function names. We design and implement a set
of experiments to demonstrate our conjecture and provide some insights for
future works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>1 table, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fair Credit Scorer through Bayesian Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuo Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning currently plays an increasingly important role in people's
lives in areas such as credit scoring, auto-driving, disease diagnosing, and
insurance quoting. However, in many of these areas, machine learning models
have performed unfair behaviors against some sub-populations, such as some
particular groups of race, sex, and age. These unfair behaviors can be on
account of the pre-existing bias in the training dataset due to historical and
social factors. In this paper, we focus on a real-world application of credit
scoring and construct a fair prediction model by introducing latent variables
to remove the correlation between protected attributes, such as sex and age,
with the observable feature inputs, including house and job. For detailed
implementation, we apply Bayesian approaches, including the Markov Chain Monte
Carlo simulation, to estimate our proposed fair model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sequence Generation via Subsequence Similarity: Theory and Application
  to UAV Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Kazemi, Salar Basiri, Volodymyr Kindratenko, Srinivasa Salapaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to generate synthetic sequences is crucial for a wide range of
applications, and recent advances in deep learning architectures and generative
frameworks have greatly facilitated this process. Particularly, unconditional
one-shot generative models constitute an attractive line of research that
focuses on capturing the internal information of a single image, video, etc. to
generate samples with similar contents. Since many of those one-shot models are
shifting toward efficient non-deep and non-adversarial approaches, we examine
the versatility of a one-shot generative model for augmenting whole datasets.
In this work, we focus on how similarity at the subsequence level affects
similarity at the sequence level, and derive bounds on the optimal transport of
real and generated sequences based on that of corresponding subsequences. We
use a one-shot generative model to sample from the vicinity of individual
sequences and generate subsequence-similar ones and demonstrate the improvement
of this approach by applying it to the problem of Unmanned Aerial Vehicle (UAV)
identification using limited radio-frequency (RF) signals. In the context of
UAV identification, RF fingerprinting is an effective method for distinguishing
legitimate devices from malicious ones, but heterogenous environments and
channel impairments can impose data scarcity and affect the performance of
classification models. By using subsequence similarity to augment sequences of
RF data with a low ratio (5\%-20\%) of training dataset, we achieve significant
improvements in performance metrics such as accuracy, precision, recall, and F1
score.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Who Should I Engage with At What Time? A Missing Event Aware Temporal
  Graph Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyi Liu, Zhiying Tu, Xiaofei Xu, Zhongjie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal graph neural network has recently received significant attention due
to its wide application scenarios, such as bioinformatics, knowledge graphs,
and social networks. There are some temporal graph neural networks that achieve
remarkable results. However, these works focus on future event prediction and
are performed under the assumption that all historical events are observable.
In real-world applications, events are not always observable, and estimating
event time is as important as predicting future events. In this paper, we
propose MTGN, a missing event-aware temporal graph neural network, which
uniformly models evolving graph structure and timing of events to support
predicting what will happen in the future and when it will happen.MTGN models
the dynamic of both observed and missing events as two coupled temporal point
processes, thereby incorporating the effects of missing events into the
network. Experimental results on several real-world temporal graphs demonstrate
that MTGN significantly outperforms existing methods with up to 89% and 112%
more accurate time and link prediction. Code can be found on
https://github.com/HIT-ICES/TNNLS-MTGN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to TNNLS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Brain Model State Space Reconstruction Using an LSTM Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueyang Liu, Artemio Soto-Breceda, Yun Zhao, Phillipa Karoly, Mark J. Cook, David B. Grayden, Daniel Schmidt, Levin Kuhlmann1
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective
  Kalman filtering has previously been applied to track neural model states and
parameters, particularly at the scale relevant to EEG. However, this approach
lacks a reliable method to determine the initial filter conditions and assumes
that the distribution of states remains Gaussian. This study presents an
alternative, data-driven method to track the states and parameters of neural
mass models (NMMs) from EEG recordings using deep learning techniques,
specifically an LSTM neural network.
  Approach
  An LSTM filter was trained on simulated EEG data generated by a neural mass
model using a wide range of parameters. With an appropriately customised loss
function, the LSTM filter can learn the behaviour of NMMs. As a result, it can
output the state vector and parameters of NMMs given observation data as the
input.
  Main Results
  Test results using simulated data yielded correlations with R squared of
around 0.99 and verified that the method is robust to noise and can be more
accurate than a nonlinear Kalman filter when the initial conditions of the
Kalman filter are not accurate. As an example of real-world application, the
LSTM filter was also applied to real EEG data that included epileptic seizures,
and revealed changes in connectivity strength parameters at the beginnings of
seizures.
  Significance
  Tracking the state vector and parameters of mathematical brain models is of
great importance in the area of brain modelling, monitoring, imaging and
control. This approach has no need to specify the initial state vector and
parameters, which is very difficult to do in practice because many of the
variables being estimated cannot be measured directly in physiological
experiments. This method may be applied using any neural mass model and,
therefore, provides a general, novel, efficient approach to estimate brain
model variables that are often difficult to measure.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-Set Likelihood Maximization for Few-Shot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08390v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08390v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malik Boudiaf, Etienne Bennequin, Myriam Tami, Antoine Toubhans, Pablo Piantanida, Céline Hudelot, Ismail Ben Ayed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle the Few-Shot Open-Set Recognition (FSOSR) problem, i.e. classifying
instances among a set of classes for which we only have a few labeled samples,
while simultaneously detecting instances that do not belong to any known class.
We explore the popular transductive setting, which leverages the unlabelled
query instances at inference. Motivated by the observation that existing
transductive methods perform poorly in open-set scenarios, we propose a
generalization of the maximum likelihood principle, in which latent scores
down-weighing the influence of potential outliers are introduced alongside the
usual parametric model. Our formulation embeds supervision constraints from the
support set and additional penalties discouraging overconfident predictions on
the query set. We proceed with a block-coordinate descent, with the latent
scores and parametric model co-optimized alternately, thereby benefiting from
each other. We call our resulting formulation \textit{Open-Set Likelihood
Optimization} (OSLO). OSLO is interpretable and fully modular; it can be
applied on top of any pre-trained model seamlessly. Through extensive
experiments, we show that our method surpasses existing inductive and
transductive methods on both aspects of open-set recognition, namely inlier
classification and outlier detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2206.09236</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Asynchronously Trained Distributed Topographic Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abbas Siddiqui, Dionysios Georgiadis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topographic feature maps are low dimensional representations of data, that
preserve spatial dependencies. Current methods of training such maps (e.g. self
organizing maps - SOM, generative topographic maps) require centralized control
and synchronous execution, which restricts scalability. We present an algorithm
that uses $N$ autonomous units to generate a feature map by distributed
asynchronous training. Unit autonomy is achieved by sparse interaction in time
\& space through the combination of a distributed heuristic search, and a
cascade-driven weight updating scheme governed by two rules: a unit i) adapts
when it receives either a sample, or the weight vector of a neighbor, and ii)
broadcasts its weight vector to its neighbors after adapting for a predefined
number of times. Thus, a vector update can trigger an avalanche of adaptation.
We map avalanching to a statistical mechanics model, which allows us to
parametrize the statistical properties of cascading. Using MNIST, we
empirically investigate the effect of the heuristic search accuracy and the
cascade parameters on map quality. We also provide empirical evidence that
algorithm complexity scales at most linearly with system size $N$. The proposed
approach is found to perform comparably with similar methods in classification
tasks across multiple datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 Pages, 8 Figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Within-group fairness: A guidance for more sound between-group fairness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Kim, Kyusang Yu, Yongdai Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As they have a vital effect on social decision-making, AI algorithms not only
should be accurate and but also should not pose unfairness against certain
sensitive groups (e.g., non-white, women). Various specially designed AI
algorithms to ensure trained AI models to be fair between sensitive groups have
been developed. In this paper, we raise a new issue that between-group fair AI
models could treat individuals in a same sensitive group unfairly. We introduce
a new concept of fairness so-called within-group fairness which requires that
AI models should be fair for those in a same sensitive group as well as those
in different sensitive groups. We materialize the concept of within-group
fairness by proposing corresponding mathematical definitions and developing
learning algorithms to control within-group fairness and between-group fairness
simultaneously. Numerical studies show that the proposed learning algorithms
improve within-group fairness without sacrificing accuracy as well as
between-group fairness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Efficient Quadrature Sequence and Sparsifying Methodology for
  Mean-Field Variational Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jed A. Duersch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a quasirandom sequence of quadratures for high-dimensional
mean-field variational inference and a related sparsifying methodology. Each
iterate of the sequence contains two evaluations points that combine to
correctly integrate all univariate quadratic functions, as well as univariate
cubics if the mean-field factors are symmetric. More importantly, averaging
results over short subsequences achieves periodic exactness on a much larger
space of multivariate polynomials of quadratic total degree. This framework is
devised by first considering stochastic blocked mean-field quadratures, which
may be useful in other contexts. By replacing pseudorandom sequences with
quasirandom sequences, over half of all multivariate quadratic basis functions
integrate exactly with only 4 function evaluations, and the exactness dimension
increases for longer subsequences. Analysis shows how these efficient integrals
characterize the dominant log-posterior contributions to mean-field variational
approximations, including diagonal Hessian approximations, to support a robust
sparsifying methodology in deep learning algorithms. A numerical demonstration
of this approach on a simple Convolutional Neural Network for MNIST retains
high test accuracy, 96.9%, while training over 98.9% of parameters to zero in
only 10 epochs, bearing potential to reduce both storage and energy
requirements for deep learning models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning-Rate-Free Learning by D-Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07733v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07733v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron Defazio, Konstantin Mishchenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The speed of gradient descent for convex Lipschitz functions is highly
dependent on the choice of learning rate. Setting the learning rate to achieve
the optimal convergence rate requires knowing the distance D from the initial
point to the solution set. In this work, we describe a single-loop method, with
no back-tracking or line searches, which does not require knowledge of $D$ yet
asymptotically achieves the optimal rate of convergence for the complexity
class of convex Lipschitz functions. Our approach is the first parameter-free
method for this class without additional multiplicative log factors in the
convergence rate. We present extensive experiments for SGD and Adam variants of
our method, where the method automatically matches hand-tuned learning rates
across more than a dozen diverse machine learning problems, including
large-scale vision and language problems. Our method is practical, efficient
and requires no additional function value or gradient evaluations each step. An
open-source implementation is available
(https://github.com/facebookresearch/dadaptation).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SamBaS: Sampling-Based Stochastic Block Partitioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.06651v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.06651v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frank Wanye, Vitaliy Gleyzer, Edward Kao, Wu-chun Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Community detection is a well-studied problem with applications in domains
ranging from networking to bioinformatics. Due to the rapid growth in the
volume of real-world data, there is growing interest in accelerating
contemporary community detection algorithms. However, the more accurate and
statistically robust methods tend to be hard to parallelize. One such method is
stochastic block partitioning (SBP) - a community detection algorithm that
works well on graphs with complex and heterogeneous community structure. In
this paper, we present a sampling-based SBP (SamBaS) for accelerating SBP on
sparse graphs. We characterize how various graph parameters affect the speedup
and result quality of community detection with SamBaS and quantify the
trade-offs therein. To evaluate SamBas on real-world web graphs without known
ground-truth communities, we introduce partition quality score (PQS), an
evaluation metric that outperforms modularity in terms of correlation with F1
score. Overall, SamBaS achieves speedups of up to 10X while maintaining result
quality (and even improving result quality by over 150% on certain graphs,
relative to F1 score).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated to latest submitted version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Blind Restoration of Real-World Audio by 1D Operational GANs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.14618v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.14618v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Turker Ince, Serkan Kiranyaz, Ozer Can Devecioglu, Muhammad Salman Khan, Muhammad Chowdhury, Moncef Gabbouj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: Despite numerous studies proposed for audio restoration in the
literature, most of them focus on an isolated restoration problem such as
denoising or dereverberation, ignoring other artifacts. Moreover, assuming a
noisy or reverberant environment with limited number of fixed
signal-to-distortion ratio (SDR) levels is a common practice. However,
real-world audio is often corrupted by a blend of artifacts such as
reverberation, sensor noise, and background audio mixture with varying types,
severities, and duration. In this study, we propose a novel approach for blind
restoration of real-world audio signals by Operational Generative Adversarial
Networks (Op-GANs) with temporal and spectral objective metrics to enhance the
quality of restored audio signal regardless of the type and severity of each
artifact corrupting it. Methods: 1D Operational-GANs are used with generative
neuron model optimized for blind restoration of any corrupted audio signal.
Results: The proposed approach has been evaluated extensively over the
benchmark TIMIT-RAR (speech) and GTZAN-RAR (non-speech) datasets corrupted with
a random blend of artifacts each with a random severity to mimic real-world
audio signals. Average SDR improvements of over 7.2 dB and 4.9 dB are achieved,
respectively, which are substantial when compared with the baseline methods.
Significance: This is a pioneer study in blind audio restoration with the
unique capability of direct (time-domain) restoration of real-world audio
whilst achieving an unprecedented level of performance for a wide SDR range and
artifact types. Conclusion: 1D Op-GANs can achieve robust and computationally
effective real-world audio restoration with significantly improved performance.
The source codes and the generated real-world audio datasets are shared
publicly with the research community in a dedicated GitHub repository1.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intrinsic persistent homology via density-based metric learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.07621v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.07621v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ximena Fernández, Eugenio Borghini, Gabriel Mindlin, Pablo Groisman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of estimating topological features from data in high
dimensional Euclidean spaces under the manifold assumption. Our approach is
based on the computation of persistent homology of the space of data points
endowed with a sample metric known as Fermat distance. We prove that such
metric space converges almost surely to the manifold itself endowed with an
intrinsic metric that accounts for both the geometry of the manifold and the
density that produces the sample. This fact implies the convergence of the
associated persistence diagrams. The use of this intrinsic distance when
computing persistent homology presents advantageous properties such as
robustness to the presence of outliers in the input data and less sensitiveness
to the particular embedding of the underlying manifold in the ambient space. We
use these ideas to propose and implement a method for pattern recognition and
anomaly detection in time series, which is evaluated in applications to real
data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages. v3: major revision. Final version accepted for publication
  at Journal of Machine Learning Research</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainable prediction of Qcodes for NOTAMs using column generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.04955v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.04955v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krunal Kishor Patel, Guy Desaulniers, Andrea Lodi, Freddy Lecue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A NOtice To AirMen (NOTAM) contains important flight route related
information. To search and filter them, NOTAMs are grouped into categories
called QCodes. In this paper, we develop a tool to predict, with some
explanations, a Qcode for a NOTAM. We present a way to extend the interpretable
binary classification using column generation proposed in Dash, Gunluk, and Wei
(2018) to a multiclass text classification method. We describe the techniques
used to tackle the issues related to one vs-rest classification, such as
multiple outputs and class imbalances. Furthermore, we introduce some
heuristics, including the use of a CP-SAT solver for the subproblems, to reduce
the training time. Finally, we show that our approach compares favorably with
state-of-the-art machine learning algorithms like Linear SVM and small neural
networks while adding the needed interpretability component.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Convergence Rates of Deep Convolutional Neural Networks:
  Additive Ridge Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.12119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.12119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiying Fang, Guang Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks have shown impressive abilities in many
applications, especially those related to the classification tasks. However,
for the regression problem, the abilities of convolutional structures have not
been fully understood, and further investigation is needed. In this paper, we
consider the mean squared error analysis for deep convolutional neural
networks. We show that, for additive ridge functions, convolutional neural
networks followed by one fully connected layer with ReLU activation functions
can reach optimal mini-max rates (up to a log factor). The input dimension only
appears in the constant of convergence rates. This work shows the statistical
optimality of convolutional neural networks and may shed light on why
convolutional neural networks are able to behave well for high dimensional
input.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StratDef: Strategic Defense Against Adversarial Attacks in ML-based
  Malware Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.07568v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.07568v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aqib Rashid, Jose Such
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the years, most research towards defenses against adversarial attacks on
machine learning models has been in the image recognition domain. The malware
detection domain has received less attention despite its importance. Moreover,
most work exploring these defenses has focused on several methods but with no
strategy when applying them. In this paper, we introduce StratDef, which is a
strategic defense system based on a moving target defense approach. We overcome
challenges related to the systematic construction, selection, and strategic use
of models to maximize adversarial robustness. StratDef dynamically and
strategically chooses the best models to increase the uncertainty for the
attacker while minimizing critical aspects in the adversarial ML domain, like
attack transferability. We provide the first comprehensive evaluation of
defenses against adversarial attacks on machine learning for malware detection,
where our threat model explores different levels of threat, attacker knowledge,
capabilities, and attack intensities. We show that StratDef performs better
than other defenses even when facing the peak adversarial threat. We also show
that, of the existing defenses, only a few adversarially-trained models provide
substantially better protection than just using vanilla models but are still
outperformed by StratDef.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CHAPTER: Exploiting Convolutional Neural Network Adapters for
  <span class="highlight-title">Self-supervised</span> Speech Models <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.01282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.01282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zih-Ching Chen, Yu-Shun Sung, Hung-yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) is a powerful technique for learning
representations from unlabeled data. Transformer based models such as HuBERT,
which consist a feature extractor and transformer layers, are leading the field
in the speech domain. SSL models are fine-tuned on a wide range of downstream
tasks, which involves re-training the majority of the model for each task.
Previous studies have introduced applying adapters, which are small lightweight
modules commonly used in Natural Language Processing (NLP) to adapt pre-trained
models to new tasks. However, such efficient tuning techniques only provide
adaptation at the transformer layer, but failed to perform adaptation at the
feature extractor. In this paper, we propose CHAPTER, an efficient tuning
method specifically designed for SSL speech model, by applying CNN adapters at
the feature extractor. Using this method, we can only fine-tune fewer than 5%
of parameters per task compared to fully fine-tuning and achieve better and
more stable performance. We empirically found that adding CNN adapters to the
feature extractor can help the adaptation on emotion and speaker tasks. For
instance, the accuracy of SID is improved from 87.71 to 91.56, and the accuracy
of ER is improved by 5%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2023. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Estimating Categorical Counterfactuals via Deep Twin Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.01904v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.01904v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Athanasios Vlontzos, Bernhard Kainz, Ciaran M. Gilligan-Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual inference is a powerful tool, capable of solving challenging
problems in high-profile sectors. To perform counterfactual inference, one
requires knowledge of the underlying causal mechanisms. However, causal
mechanisms cannot be uniquely determined from observations and interventions
alone. This raises the question of how to choose the causal mechanisms so that
resulting counterfactual inference is trustworthy in a given domain. This
question has been addressed in causal models with binary variables, but the
case of categorical variables remains unanswered. We address this challenge by
introducing for causal models with categorical variables the notion of
counterfactual ordering, a principle that posits desirable properties causal
mechanisms should posses, and prove that it is equivalent to specific
functional constraints on the causal mechanisms. To learn causal mechanisms
satisfying these constraints, and perform counterfactual inference with them,
we introduce deep twin networks. These are deep neural networks that, when
trained, are capable of twin network counterfactual inference -- an alternative
to the abduction, action, & prediction method. We empirically test our approach
on diverse real-world and semi-synthetic data from medicine, epidemiology, and
finance, reporting accurate estimation of counterfactual probabilities while
demonstrating the issues that arise with counterfactual reasoning when
counterfactual ordering is not enforced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enforcing the consensus between Trajectory Optimization and Policy
  Learning for precise robot control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.09006v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.09006v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quentin Le Lidec, Wilson Jallet, Ivan Laptev, Cordelia Schmid, Justin Carpentier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) and trajectory optimization (TO) present strong
complementary advantages. On one hand, RL approaches are able to learn global
control policies directly from data, but generally require large sample sizes
to properly converge towards feasible policies. On the other hand, TO methods
are able to exploit gradient-based information extracted from simulators to
quickly converge towards a locally optimal control trajectory which is only
valid within the vicinity of the solution. Over the past decade, several
approaches have aimed to adequately combine the two classes of methods in order
to obtain the best of both worlds. Following on from this line of research, we
propose several improvements on top of these approaches to learn global control
policies quicker, notably by leveraging sensitivity information stemming from
TO methods via Sobolev learning, and augmented Lagrangian techniques to enforce
the consensus between TO and policy learning. We evaluate the benefits of these
improvements on various classical tasks in robotics through comparison with
existing approaches in the literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tight bounds for maximum $\ell_1$-margin classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.03783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.03783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Stojanovic, Konstantin Donhauser, Fanny Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Popular iterative algorithms such as boosting methods and coordinate descent
on linear models converge to the maximum $\ell_1$-margin classifier, a.k.a.
sparse hard-margin SVM, in high dimensional regimes where the data is linearly
separable. Previous works consistently show that many estimators relying on the
$\ell_1$-norm achieve improved statistical rates for hard sparse ground truths.
We show that surprisingly, this adaptivity does not apply to the maximum
$\ell_1$-margin classifier for a standard discriminative setting. In
particular, for the noiseless setting, we prove tight upper and lower bounds
for the prediction error that match existing rates of order
$\frac{\|w^*\|_1^{2/3}}{n^{1/3}}$ for general ground truths. To complete the
picture, we show that when interpolating noisy observations, the error vanishes
at a rate of order $\frac{1}{\sqrt{\log(d/n)}}$. We are therefore first to show
benign overfitting for the maximum $\ell_1$-margin classifier.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Image Segmentation With Noisy Labels: Characterization and Volume
  Properties of the Optimal Solutions to Accuracy and Dice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.06484v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.06484v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcus Nordström, Henrik Hult, Jonas Söderberg, Fredrik Löfman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study two of the most popular performance metrics in medical image
segmentation, Accuracy and Dice, when the target labels are noisy. For both
metrics, several statements related to characterization and volume properties
of the set of optimal segmentations are proved, and associated experiments are
provided. Our main insights are: (i) the volume of the solutions to both
metrics may deviate significantly from the expected volume of the target, (ii)
the volume of a solution to Accuracy is always less than or equal to the volume
of a solution to Dice and (iii) the optimal solutions to both of these metrics
coincide when the set of feasible segmentations is constrained to the set of
segmentations with the volume equal to the expected volume of the target.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FewSOME: Few Shot Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06957v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06957v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niamh Belton, Misgina Tsighe Hagos, Aonghus Lawlor, Kathleen M. Curran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have seen considerable progress in the field of Anomaly
Detection but at the cost of increasingly complex training pipelines. Such
techniques require large amounts of training data, resulting in computationally
expensive algorithms. We propose Few Shot anomaly detection (FewSOME), a deep
One-Class Anomaly Detection algorithm with the ability to accurately detect
anomalies having trained on 'few' examples of the normal class and no examples
of the anomalous class. We describe FewSOME to be of low complexity given its
low data requirement and short training time. FewSOME is aided by pretrained
weights with an architecture based on Siamese Networks. By means of an ablation
study, we demonstrate how our proposed loss, 'Stop Loss', improves the
robustness of FewSOME. Our experiments demonstrate that FewSOME performs at
state-of-the-art level on benchmark datasets MNIST, CIFAR-10, F-MNIST and MVTec
AD while training on only 30 normal samples, a minute fraction of the data that
existing methods are trained on. Most notably, we found that FewSOME
outperforms even highly complex models in the setting where only few examples
of the normal class exist. Moreover, our extensive experiments show FewSOME to
be robust to contaminated datasets. We also report F1 score and Balanced
Accuracy in addition to AUC as a benchmark for future techniques to be compared
against.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time-Warping Invariant Quantum Recurrent Neural Networks via
  Quantum-Classical Adaptive Gating 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08173v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08173v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivana Nikoloska, Osvaldo Simeone, Leonardo Banchi, Petar Veličković
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adaptive gating plays a key role in temporal data processing via classical
recurrent neural networks (RNN), as it facilitates retention of past
information necessary to predict the future, providing a mechanism that
preserves invariance to time warping transformations. This paper builds on
quantum recurrent neural networks (QRNNs), a dynamic model with quantum memory,
to introduce a novel class of temporal data processing quantum models that
preserve invariance to time-warping transformations of the (classical)
input-output sequences. The model, referred to as time warping-invariant QRNN
(TWI-QRNN), augments a QRNN with a quantum-classical adaptive gating mechanism
that chooses whether to apply a parameterized unitary transformation at each
time step as a function of the past samples of the input sequence via a
classical recurrent model. The TWI-QRNN model class is derived from first
principles, and its capacity to successfully implement time-warping
transformations is experimentally demonstrated on examples with classical or
quantum dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating the Evaluators: Which UDA validation methods are most
  effective? Can they be improved? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.07360v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.07360v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Musgrave, Serge Belongie, Ser-Nam Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper compares and ranks 8 UDA validation methods. Validators estimate
model accuracy, which makes them an essential component of any UDA train-test
pipeline. We rank these validators to indicate which of them are most useful
for the purpose of selecting optimal model checkpoints and hyperparameters. To
the best of our knowledge, this large-scale benchmark study is the first of its
kind in the UDA field. In addition, we propose three new validators that
outperform all the existing checkpoint-based validators that we were able to
find in the existing literature. Code is available at
https://www.github.com/KevinMusgrave/powerful-benchmarker.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was previously titled Benchmarking Validation Methods for
  Unsupervised Domain Adaptation. This version contains new experiments,
  analysis, and figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LaF: Labeling-Free Model Selection for Automated Deep Neural Network
  Reusing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.03994v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.03994v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Hu, Yuejun Guo, Maxime Cordy, Xiaofei Xie, Mike Papadakis, Yves Le Traon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applying deep learning to science is a new trend in recent years which leads
DL engineering to become an important problem. Although training data
preparation, model architecture design, and model training are the normal
processes to build DL models, all of them are complex and costly. Therefore,
reusing the open-sourced pre-trained model is a practical way to bypass this
hurdle for developers. Given a specific task, developers can collect massive
pre-trained deep neural networks from public sources for re-using. However,
testing the performance (e.g., accuracy and robustness) of multiple DNNs and
recommending which model should be used is challenging regarding the scarcity
of labeled data and the demand for domain expertise. In this paper, we propose
a labeling-free (LaF) model selection approach to overcome the limitations of
labeling efforts for automated model reusing. The main idea is to statistically
learn a Bayesian model to infer the models' specialty only based on predicted
labels. We evaluate LaF using 9 benchmark datasets including image, text, and
source code, and 165 DNNs, considering both the accuracy and robustness of
models. The experimental results demonstrate that LaF outperforms the baseline
methods by up to 0.74 and 0.53 on Spearman's correlation and Kendall's $\tau$,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autonomous Drug Design with Multi-Armed Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01393v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01393v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hampus Gummesson Svensson, Esben Jannik Bjerrum, Christian Tyrchan, Ola Engkvist, Morteza Haghir Chehreghani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in artificial intelligence and automation support a new
drug design paradigm: autonomous drug design. Under this paradigm, generative
models can provide suggestions on thousands of molecules with specific
properties, and automated laboratories can potentially make, test and analyze
molecules with minimal human supervision. However, since still only a limited
number of molecules can be synthesized and tested, an obvious challenge is how
to efficiently select among provided suggestions in a closed-loop system. We
formulate this task as a stochastic multi-armed bandit problem with multiple
plays, volatile arms and similarity information. To solve this task, we adapt
previous work on multi-armed bandits to this setting, and compare our solution
with random sampling, greedy selection and decaying-epsilon-greedy selection
strategies. According to our simulation results, our approach has the potential
to perform better exploration and exploitation of the chemical space for
autonomous drug design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High Dimensional Statistical Estimation under Uniformly Dithered One-bit
  Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.13157v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.13157v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junren Chen, Cheng-Long Wang, Michael K. Ng, Di Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a uniformly dithered 1-bit quantization scheme for
high-dimensional statistical estimation. The scheme contains truncation,
dithering, and quantization as typical steps. As canonical examples, the
quantization scheme is applied to the estimation problems of sparse covariance
matrix estimation, sparse linear regression (i.e., compressed sensing), and
matrix completion. We study both sub-Gaussian and heavy-tailed regimes, where
the underlying distribution of heavy-tailed data is assumed to have bounded
moments of some order. We propose new estimators based on 1-bit quantized data.
In sub-Gaussian regime, our estimators achieve near minimax rates, indicating
that our quantization scheme costs very little. In heavy-tailed regime, while
the rates of our estimators become essentially slower, these results are either
the first ones in an 1-bit quantized and heavy-tailed setting, or already
improve on existing comparable results from some respect. Under the
observations in our setting, the rates are almost tight in compressed sensing
and matrix completion. Our 1-bit compressed sensing results feature general
sensing vector that is sub-Gaussian or even heavy-tailed. We also first
investigate a novel setting where both the covariate and response are
quantized. In addition, our approach to 1-bit matrix completion does not rely
on likelihood and represent the first method robust to pre-quantization noise
with unknown distribution. Experimental results on synthetic data are presented
to support our theoretical analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We add lower bounds for 1-bit quantization of heavy-tailed data
  (Theorems 11, 14)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing model-agnostic Random Subspace ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.03099v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.03099v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vân Anh Huynh-Thu, Pierre Geurts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a model-agnostic ensemble approach for supervised
learning. The proposed approach is based on a parametric version of Random
Subspace, in which each base model is learned from a feature subset sampled
according to a Bernoulli distribution. Parameter optimization is performed
using gradient descent and is rendered tractable by using an importance
sampling approach that circumvents frequent re-training of the base models
after each gradient descent step. The degree of randomization in our parametric
Random Subspace is thus automatically tuned through the optimization of the
feature selection probabilities. This is an advantage over the standard Random
Subspace approach, where the degree of randomization is controlled by a
hyper-parameter. Furthermore, the optimized feature selection probabilities can
be interpreted as feature importance scores. Our algorithm can also easily
incorporate any differentiable regularization term to impose constraints on
these importance scores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybrid Quantum-Classical Generative Adversarial Network for High
  Resolution Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.11614v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.11614v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Lok Tsang, Maxwell T. West, Sarah M. Erfani, Muhammad Usman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum machine learning (QML) has received increasing attention due to its
potential to outperform classical machine learning methods in problems
pertaining classification and identification tasks. A subclass of QML methods
is quantum generative adversarial networks (QGANs) which have been studied as a
quantum counterpart of classical GANs widely used in image manipulation and
generation tasks. The existing work on QGANs is still limited to small-scale
proof-of-concept examples based on images with significant downscaling. Here we
integrate classical and quantum techniques to propose a new hybrid
quantum-classical GAN framework. We demonstrate its superior learning
capabilities by generating $28 \times 28$ pixels grey-scale images without
dimensionality reduction or classical pre/post-processing on multiple classes
of the standard MNIST and Fashion MNIST datasets, which achieves comparable
results to classical frameworks with three orders of magnitude less trainable
generator parameters. To gain further insight into the working of our hybrid
approach, we systematically explore the impact of its parameter space by
varying the number of qubits, the size of image patches, the number of layers
in the generator, the shape of the patches and the choice of prior
distribution. Our results show that increasing the quantum generator size
generally improves the learning capability of the network. The developed
framework provides a foundation for future design of QGANs with optimal
parameter set tailored for complex image generation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The European AI Liability Directives -- Critique of a Half-Hearted
  Approach and Lessons for the Future 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.13960v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.13960v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Hacker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As ChatGPT et al. conquer the world, the optimal liability framework for AI
systems remains an unsolved problem across the globe. In a much-anticipated
move, the European Commission advanced two proposals outlining the European
approach to AI liability in September 2022: a novel AI Liability Directive and
a revision of the Product Liability Directive. They constitute the final
cornerstone of EU AI regulation. Crucially, the liability proposals and the EU
AI Act are inherently intertwined: the latter does not contain any individual
rights of affected persons, and the former lack specific, substantive rules on
AI development and deployment. Taken together, these acts may well trigger a
Brussels Effect in AI regulation, with significant consequences for the US and
beyond.
  This paper makes three novel contributions. First, it examines in detail the
Commission proposals and shows that, while making steps in the right direction,
they ultimately represent a half-hearted approach: if enacted as foreseen, AI
liability in the EU will primarily rest on disclosure of evidence mechanisms
and a set of narrowly defined presumptions concerning fault, defectiveness and
causality. Hence, second, the article suggests amendments, which are collected
in an Annex at the end of the paper. Third, based on an analysis of the key
risks AI poses, the final part of the paper maps out a road for the future of
AI liability and regulation, in the EU and beyond. This includes: a
comprehensive framework for AI liability; provisions to support innovation; an
extension to non-discrimination/algorithmic fairness, as well as explainable
AI; and sustainability. I propose to jump-start sustainable AI regulation via
sustainability impact assessments in the AI Act and sustainable design defects
in the liability regime. In this way, the law may help spur not only fair AI
and XAI, but potentially also sustainable AI (SAI).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under peer-review; contains 3 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predicting the Masses of Exotic Hadrons with Data Augmentation Using
  Multilayer Perceptron 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.09538v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.09538v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huseyin Bahtiyar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there have been significant developments in neural networks, which
led to the frequent use of neural networks in the physics literature. This work
is focused on predicting the masses of exotic hadrons, doubly charmed and
bottomed baryons using neural networks trained on meson and baryon masses that
are determined by experiments. The original data set has been extended using
the recently proposed artificial data augmentation methods. We have observed
that the neural network's predictive ability increases with the use of
augmented data. The results indicated that data augmentation techniques play an
essential role in improving neural network predictions; moreover, neural
networks can make reasonable predictions for exotic hadrons, doubly charmed,
and doubly bottomed baryons. The results are also comparable to Gaussian
Process and Constituent Quark Model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Decision Making for Trading Wind Energy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.02009v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.02009v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miguel Angel Muñoz, Pierre Pinson, Jalal Kazempour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes and develops a new algorithm for trading wind energy in
electricity markets, within an online learning and optimization framework. In
particular, we combine a component-wise adaptive variant of the gradient
descent algorithm with recent advances in the feature-driven newsvendor model.
This results in an online offering approach capable of leveraging data-rich
environments, while adapting to non-stationary characteristics of energy
generation and electricity markets, and with a minimal computational burden.
The performance of our approach is analyzed based on several numerical
experiments, showing both better adaptability to non-stationary uncertain
parameters and significant economic gains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting consistency for semi-supervised semantic segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.07075v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.07075v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Grubišić, Marin Oršić, Siniša Šegvić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning an attractive technique in practical deployments of
deep models since it relaxes the dependence on labeled data. It is especially
important in the scope of dense prediction because pixel-level annotation
requires significant effort. This paper considers semi-supervised algorithms
that enforce consistent predictions over perturbed unlabeled inputs. We study
the advantages of perturbing only one of the two model instances and preventing
the backward pass through the unperturbed instance. We also propose a
competitive perturbation model as a composition of geometric warp and
photometric jittering. We experiment with efficient models due to their
importance for real-time and low-power applications. Our experiments show clear
advantages of (1) one-way consistency, (2) perturbing only the student branch,
and (3) strong photometric and geometric perturbations. Our perturbation model
outperforms recent work and most of the contribution comes from photometric
component. Experiments with additional data from the large coarsely annotated
subset of Cityscapes suggest that semi-supervised training can outperform
supervised training with the coarse labels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The source code is available at
  https://github.com/Ivan1248/semisup-seg-efficient</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ILLUME: Rationalizing Vision-Language Models through Human Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.08241v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.08241v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Brack, Patrick Schramowski, Björn Deiseroth, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bootstrapping from pre-trained language models has been proven to be an
efficient approach for building vision-language models (VLM) for tasks such as
image captioning or visual question answering. However, outputs of these models
rarely align with user's rationales for specific answers. In order to improve
this alignment and reinforce commonsense reasons, we propose a tuning paradigm
based on human interactions with machine generated data. Our ILLUME executes
the following loop: Given an image-question-answer prompt, the VLM samples
multiple candidate rationales, and a human critic provides minimal feedback via
preference selection, used for fine-tuning. This loop increases the training
data and gradually carves out the VLM's rationalization capabilities that are
aligned with human intend. Our exhaustive experiments demonstrate that ILLUME
is competitive with standard supervised fine-tuning while using significantly
fewer training data and only requiring minimal feedback.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Theoretical Framework for AI Models Explainability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.14447v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.14447v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Rizzo, Alberto Veneri, Andrea Albarelli, Claudio Lucchese, Cristina Conati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  EXplainable Artificial Intelligence (XAI) is a vibrant research topic in the
artificial intelligence community, with growing interest across methods and
domains. Much has been written about the subject, yet XAI still lacks shared
terminology and a framework capable of providing structural soundness to
explanations. In our work, we address these issues by proposing a novel
definition of explanation that is a synthesis of what can be found in the
literature. We recognize that explanations are not atomic but the combination
of evidence stemming from the model and its input-output mapping, and the human
interpretation of this evidence. Furthermore, we fit explanations into the
properties of faithfulness (i.e., the explanation being a true description of
the model's inner workings and decision-making process) and plausibility (i.e.,
how much the explanation looks convincing to the user). Using our proposed
theoretical framework simplifies how these properties are operationalized and
it provides new insight into common explanation methods that we analyze as case
studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Estimation of Large Financial Covariances: A Cross-Validation Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.05757v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.05757v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Tan, Stefan Zohren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel covariance estimator for portfolio selection that adapts
to the non-stationary or persistent heteroskedastic environments of financial
time series by employing exponentially weighted averages and nonlinearly
shrinking the sample eigenvalues through cross-validation. Our estimator is
structure agnostic, transparent, and computationally feasible in large
dimensions. By correcting the biases in the sample eigenvalues and aligning our
estimator to more recent risk, we demonstrate that our estimator performs well
in large dimensions against existing state-of-the-art static and dynamic
covariance shrinkage estimators through simulations and with an empirical
application in active portfolio management.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A shallow physics-informed neural network for solving partial
  differential equations on surfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.01581v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.01581v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei-Fan Hu, Yi-Jun Shih, Te-Sheng Lin, Ming-Chih Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a shallow (one-hidden-layer) physics-informed
neural network for solving partial differential equations on static and
evolving surfaces. For the static surface case, with the aid of level set
function, the surface normal and mean curvature used in the surface
differential expressions can be computed easily. So instead of imposing the
normal extension constraints used in literature, we write the surface
differential operators in the form of traditional Cartesian differential
operators and use them in the loss function directly. We perform a series of
performance study for the present methodology by solving Laplace-Beltrami
equation and surface diffusion equation on complex static surfaces. With just a
moderate number of neurons used in the hidden layer, we are able to attain
satisfactory prediction results. Then we extend the present methodology to
solve the advection-diffusion equation on an evolving surface with given
velocity. To track the surface, we additionally introduce a prescribed hidden
layer to enforce the topological structure of the surface and use the network
to learn the homeomorphism between the surface and the prescribed topology. The
proposed network structure is designed to track the surface and solve the
equation simultaneously. Again, the numerical results show comparable accuracy
as the static cases. As an application, we simulate the surfactant transport on
the droplet surface under shear flow and obtain some physically plausible
results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning from non-irreducible Markov chains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.04338v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.04338v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikola Sandrić, Stjepan Šebek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mostof the existing literature on supervised machine learning problems
focuses on the case when the training data set is drawn from an i.i.d. sample.
However, many practical problems are characterized by temporal dependence and
strong correlation between the marginals of the data-generating process,
suggesting that the i.i.d. assumption is not always justified. This problem has
been already considered in the context of Markov chains satisfying the Doeblin
condition. This condition, among other things, implies that the chain is not
singular in its behavior, i.e. it is irreducible. In this article, we focus on
the case when the training data set is drawn from a not necessarily irreducible
Markov chain. Under the assumption that the chain is uniformly ergodic with
respect to the $\mathrm{L}^1$-Wasserstein distance, and certain regularity
assumptions on the hypothesis class and the state space of the chain, we first
obtain a uniform convergence result for the corresponding sample error, and
then we conclude learnability of the approximate sample error minimization
algorithm and find its generalization bounds. At the end, a relative uniform
convergence result for the sample error is also discussed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding the diffusion models by conditional expectations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07882v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07882v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubin Lu, Zhongjian Wang, Guillaume Bal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provide several mathematical analyses of the diffusion model in
machine learning. The drift term of the backwards sampling process is
represented as a conditional expectation involving the data distribution and
the forward diffusion. The training process aims to find such a drift function
by minimizing the mean-squared residue related to the conditional expectation.
Using small-time approximations of the Green's function of the forward
diffusion, we show that the analytical mean drift function in DDPM and the
score function in SGM asymptotically blow up in the final stages of the
sampling process for singular data distributions such as those concentrated on
lower-dimensional manifolds, and is therefore difficult to approximate by a
network. To overcome this difficulty, we derive a new target function and
associated loss, which remains bounded even for singular data distributions. We
illustrate the theoretical findings with several numerical examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Metric Residual Networks for Sample Efficient Goal-Conditioned
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.08133v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.08133v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Liu, Yihao Feng, Qiang Liu, Peter Stone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Goal-conditioned reinforcement learning (GCRL) has a wide range of potential
real-world applications, including manipulation and navigation problems in
robotics. Especially in such robotics tasks, sample efficiency is of the utmost
importance for GCRL since, by default, the agent is only rewarded when it
reaches its goal. While several methods have been proposed to improve the
sample efficiency of GCRL, one relatively under-studied approach is the design
of neural architectures to support sample efficiency. In this work, we
introduce a novel neural architecture for GCRL that achieves significantly
better sample efficiency than the commonly-used monolithic network
architecture. The key insight is that the optimal action-value function Q^*(s,
a, g) must satisfy the triangle inequality in a specific sense. Furthermore, we
introduce the metric residual network (MRN) that deliberately decomposes the
action-value function Q(s,a,g) into the negated summation of a metric plus a
residual asymmetric component. MRN provably approximates any optimal
action-value function Q^*(s,a,g), thus making it a fitting neural architecture
for GCRL. We conduct comprehensive experiments across 12 standard benchmark
environments in GCRL. The empirical results demonstrate that MRN uniformly
outperforms other state-of-the-art GCRL neural architectures in terms of sample
efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Goal-conditioned reinforcement learning, neural architecture design</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Sequential Latent Variable Models from Multimodal Time Series
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.10419v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.10419v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliver Limoyo, Trevor Ablett, Jonathan Kelly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential modelling of high-dimensional data is an important problem that
appears in many domains including model-based reinforcement learning and
dynamics identification for control. Latent variable models applied to
sequential data (i.e., latent dynamics models) have been shown to be a
particularly effective probabilistic approach to solve this problem, especially
when dealing with images. However, in many application areas (e.g., robotics),
information from multiple sensing modalities is available -- existing latent
dynamics methods have not yet been extended to effectively make use of such
multimodal sequential data. Multimodal sensor streams can be correlated in a
useful manner and often contain complementary information across modalities. In
this work, we present a self-supervised generative modelling framework to
jointly learn a probabilistic latent state representation of multimodal data
and the respective dynamics. Using synthetic and real-world datasets from a
multimodal robotic planar pushing task, we demonstrate that our approach leads
to significant improvements in prediction and representation quality.
Furthermore, we compare to the common learning baseline of concatenating each
modality in the latent space and show that our principled probabilistic
formulation performs better. Finally, despite being fully self-supervised, we
demonstrate that our method is nearly as effective as an existing supervised
approach that relies on ground truth labels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In: Petrovic, I., Menegatti, E., Markovi\'c, I. (eds) Intelligent
  Autonomous Systems 17. IAS 2022. Lecture Notes in Networks and Systems, vol
  577. Springer, Cham</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating Synthetic Clinical Data that Capture Class Imbalanced
  Distributions with Generative Adversarial Networks: Example using
  Antiretroviral Therapy for HIV 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.08655v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.08655v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas I-Hsien Kuo, Federico Garcia, Anders Sönnerborg, Maurizio Zazzi, Michael Böhm, Rolf Kaiser, Mark Polizzotto, Louisa Jorm, Sebastiano Barbieri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical data usually cannot be freely distributed due to their highly
confidential nature and this hampers the development of machine learning in the
healthcare domain. One way to mitigate this problem is by generating realistic
synthetic datasets using generative adversarial networks (GANs). However, GANs
are known to suffer from mode collapse thus creating outputs of low diversity.
This lowers the quality of the synthetic healthcare data, and may cause it to
omit patients of minority demographics or neglect less common clinical
practices. In this paper, we extend the classic GAN setup with an additional
variational autoencoder (VAE) and include an external memory to replay latent
features observed from the real samples to the GAN generator. Using
antiretroviral therapy for human immunodeficiency virus (ART for HIV) as a case
study, we show that our extended setup overcomes mode collapse and generates a
synthetic dataset that accurately describes severely imbalanced class
distributions commonly found in real-world clinical variables. In addition, we
demonstrate that our synthetic dataset is associated with a very low patient
disclosure risk, and that it retains a high level of utility from the ground
truth dataset to support the development of downstream machine learning
algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In the near future, we will make our codes and synthetic datasets
  publicly available to facilitate future research. Follow us on
  https://healthgym.ai/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Estimation of Network Point Processes for Event Streams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2009.01742v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2009.01742v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanhua Fang, Owen G. Ward, Tian Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A common goal in network modeling is to uncover the latent community
structure present among nodes. For many real-world networks, the true
connections consist of events arriving as streams, which are then aggregated to
form edges, ignoring the dynamic temporal component. A natural way to take
account of these temporal dynamics of interactions is to use point processes as
the foundation of network models for community detection. Computational
complexity hampers the scalability of such approaches to large sparse networks.
To circumvent this challenge, we propose a fast online variational inference
algorithm for estimating the latent structure underlying dynamic event arrivals
on a network, using continuous-time point process latent network models. We
describe this procedure for networks models capturing community structure. This
structure can be learned as new events are observed on the network, updating
the inferred community assignments. We investigate the theoretical properties
of such an inference scheme, and provide regret bounds on the loss function of
this procedure. The proposed inference procedure is then thoroughly compared,
using both simulation studies and real data, to non-online variants. We
demonstrate that online inference can obtain comparable performance, in terms
of community recovery, to non-online variants, while realising computational
gains. Our proposed inference framework can also be readily modified to
incorporate other popular network structures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Failure Tolerant Training with Persistent Memory Disaggregation over CXL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miryeong Kwon, Junhyeok Jang, Hanjin Choi, Sangwon Lee, Myoungsoo Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes TRAININGCXL that can efficiently process large-scale
recommendation datasets in the pool of disaggregated memory while making
training fault tolerant with low overhead. To this end, i) we integrate
persistent memory (PMEM) and GPU into a cache-coherent domain as Type-2.
Enabling CXL allows PMEM to be directly placed in GPU's memory hierarchy, such
that GPU can access PMEM without software intervention. TRAININGCXL introduces
computing and checkpointing logic near the CXL controller, thereby training
data and managing persistency in an active manner. Considering PMEM's
vulnerability, ii) we utilize the unique characteristics of recommendation
models and take the checkpointing overhead off the critical path of their
training. Lastly, iii) TRAININGCXL employs an advanced checkpointing technique
that relaxes the updating sequence of model parameters and embeddings across
training batches. The evaluation shows that TRAININGCXL achieves 5.2x training
performance improvement and 76% energy savings, compared to the modern
PMEM-based recommendation systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixed-Integer Optimization with Constraint Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.04469v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.04469v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donato Maragno, Holly Wiberg, Dimitris Bertsimas, S. Ilker Birbil, Dick den Hertog, Adejuyigbe Fajemisin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We establish a broad methodological foundation for mixed-integer optimization
with learned constraints. We propose an end-to-end pipeline for data-driven
decision making in which constraints and objectives are directly learned from
data using machine learning, and the trained models are embedded in an
optimization formulation. We exploit the mixed-integer
optimization-representability of many machine learning methods, including
linear models, decision trees, ensembles, and multi-layer perceptrons, which
allows us to capture various underlying relationships between decisions,
contextual variables, and outcomes. We also introduce two approaches for
handling the inherent uncertainty of learning from data. First, we characterize
a decision trust region using the convex hull of the observations, to ensure
credible recommendations and avoid extrapolation. We efficiently incorporate
this representation using column generation and propose a more flexible
formulation to deal with low-density regions and high-dimensional datasets.
Then, we propose an ensemble learning approach that enforces constraint
satisfaction over multiple bootstrapped estimators or multiple algorithms. In
combination with domain-driven components, the embedded models and trust region
define a mixed-integer optimization problem for prescription generation. We
implement this framework as a Python package (OptiCL) for practitioners. We
demonstrate the method in both World Food Programme planning and chemotherapy
optimization. The case studies illustrate the framework's ability to generate
high-quality prescriptions as well as the value added by the trust region, the
use of ensembles to control model robustness, the consideration of multiple
machine learning methods, and the inclusion of multiple learned constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty in Real-Time Semantic Segmentation on Embedded Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.01201v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.01201v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Goan, Clinton Fookes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Application for semantic segmentation models in areas such as autonomous
vehicles and human computer interaction require real-time predictive
capabilities. The challenges of addressing real-time application is amplified
by the need to operate on resource constrained hardware. Whilst development of
real-time methods for these platforms has increased, these models are unable to
sufficiently reason about uncertainty present. This paper addresses this by
combining deep feature extraction from pre-trained models with Bayesian
regression and moment propagation for uncertainty aware predictions. We
demonstrate how the proposed method can yield meaningful uncertainty on
embedded hardware in real-time whilst maintaining predictive performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Play and Self-Describe: Policy Adaptation with Vision-Language
  Foundation Models <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07398v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07398v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuying Ge, Annabella Macaluso, Li Erran Li, Ping Luo, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress on vision-language foundation models have brought significant
advancement to building general-purpose robots. By using the pre-trained models
to encode the scene and instructions as inputs for decision making, the
instruction-conditioned policy can generalize across different objects and
tasks. While this is encouraging, the policy still fails in most cases given an
unseen task or environment. To adapt the policy to unseen tasks and
environments, we explore a new paradigm on leveraging the pre-trained
foundation models with Self-PLAY and Self-Describe (SPLAYD). When deploying the
trained policy to a new task or a new environment, we first let the policy
self-play with randomly generated instructions to record the demonstrations.
While the execution could be wrong, we can use the pre-trained foundation
models to accurately self-describe (i.e., re-label or classify) the
demonstrations. This automatically provides new pairs of
demonstration-instruction data for policy fine-tuning. We evaluate our method
on a broad range of experiments with the focus on generalization on unseen
objects, unseen tasks, unseen environments, and sim-to-real transfer. We show
SPLAYD improves baselines by a large margin in all cases. Our project page is
available at https://geyuying.github.io/SPLAYD/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://geyuying.github.io/SPLAYD/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Frame-Scoring <span class="highlight-title">Transformer</span> for Video Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01814v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01814v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeiyoon Park, Kiho Kwoun, Chanhee Lee, Heuiseok Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the number of video content has mushroomed in recent years, automatic
video summarization has come useful when we want to just peek at the content of
the video. However, there are two underlying limitations in generic video
summarization task. First, most previous approaches read in just visual
features as input, leaving other modality features behind. Second, existing
datasets for generic video summarization are relatively insufficient to train a
caption generator used for extracting text information from a video and to
train the multimodal feature extractors. To address these two problems, this
paper proposes the Multimodal Frame-Scoring Transformer (MFST), a framework
exploiting visual, text, and audio features and scoring a video with respect to
frames. Our MFST framework first extracts each modality features
(audio-visual-text) using pretrained encoders. Then, MFST trains the multimodal
frame-scoring transformer that uses multimodal representation based on
extracted features as inputs and predicts frame-level scores. Our extensive
experiments with previous models and ablation studies on TVSum and SumMe
datasets demonstrate the effectiveness and superiority of our proposed method
by a large margin in both F1 score and Rank-based evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingting Yuan, Liang Mi, Weijun Wang, Haipeng Dai, Xiaoming Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quality of the video stream is key to neural network-based video
analytics. However, low-quality video is inevitably collected by existing
surveillance systems because of poor quality cameras or over-compressed/pruned
video streaming protocols, e.g., as a result of upstream bandwidth limit. To
address this issue, existing studies use quality enhancers (e.g., neural
super-resolution) to improve the quality of videos (e.g., resolution) and
eventually ensure inference accuracy. Nevertheless, directly applying quality
enhancers does not work in practice because it will introduce unacceptable
latency. In this paper, we present AccDecoder, a novel accelerated decoder for
real-time and neural-enhanced video analytics. AccDecoder can select a few
frames adaptively via Deep Reinforcement Learning (DRL) to enhance the quality
by neural super-resolution and then up-scale the unselected frames that
reference them, which leads to 6-21% accuracy improvement. AccDecoder provides
efficient inference capability via filtering important frames using DRL for
DNN-based inference and reusing the results for the other frames via extracting
the reference relationship among frames and blocks, which results in a latency
reduction of 20-80% than baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2023 IEEE INFOCOM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Developing a Framework for Heterotopias as Discursive Playgrounds: A
  Comparative Analysis of Non-Immersive and Immersive Technologies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elif Hilal Korkut, Elif Surer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The discursive space represents the reordering of knowledge gained through
accumulation. In the digital age, multimedia has become the language of
information, and the space for archival practices is provided by non-immersive
technologies, resulting in the disappearance of several layers from discursive
activities. Heterotopias are unique, multilayered epistemic contexts that
connect other systems through the exchange of information. This paper describes
a process to create a framework for Virtual Reality, Mixed Reality, and
personal computer environments based on heterotopias to provide absent layers.
This study provides virtual museum space as an informational terrain that
contains a "world within worlds" and presents place production as a layer of
heterotopia and the subject of discourse. Automation for the individual
multimedia content is provided via various sorting and grouping algorithms, and
procedural content generation algorithms such as Binary Space Partitioning,
Cellular Automata, Growth Algorithm, and Procedural Room Generation. Versions
of the framework were comparatively evaluated through a user study involving 30
participants, considering factors such as usability, technology acceptance, and
presence. The results of the study show that the framework can serve diverse
contexts to construct multilayered digital habitats and is flexible for
integration into professional and daily life practices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Asynchronous Intensity Representation for Framed and Event Video
  Sources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew C. Freeman, Montek Singh, Ketan Mayer-Patel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neuromorphic "event" cameras, designed to mimic the human vision system with
asynchronous sensing, unlock a new realm of high-speed and high dynamic range
applications. However, researchers often either revert to a framed
representation of event data for applications, or build bespoke applications
for a particular camera's event data type. To usher in the next era of video
systems, accommodate new event camera designs, and explore the benefits to
asynchronous video in classical applications, we argue that there is a need for
an asynchronous, source-agnostic video representation. In this paper, we
introduce a novel, asynchronous intensity representation for both framed and
non-framed data sources. We show that our representation can increase intensity
precision and greatly reduce the number of samples per pixel compared to
grid-based representations. With framed sources, we demonstrate that by
permitting a small amount of loss through the temporal averaging of similar
pixel values, we can reduce our representational sample rate by more than half,
while incurring a drop in VMAF quality score of only 4.5. We also demonstrate
lower latency than the state-of-the-art method for fusing and transcoding
framed and event camera data to an intensity representation, while maintaining
$2000\times$ the temporal resolution. We argue that our method provides the
computational efficiency and temporal granularity necessary to build real-time
intensity-based applications for event cameras.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimized learned entropy coding parameters for practical neural-based
  image and video compression <span class="chip">ICIP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Said, Reza Pourreza, Hoang Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural-based image and video codecs are significantly more power-efficient
when weights and activations are quantized to low-precision integers. While
there are general-purpose techniques for reducing quantization effects, large
losses can occur when specific entropy coding properties are not considered.
This work analyzes how entropy coding is affected by parameter quantizations,
and provides a method to minimize losses. It is shown that, by using a certain
type of coding parameters to be learned, uniform quantization becomes
practically optimal, also simplifying the minimization of code memory
requirements. The mathematical properties of the new representation are
presented, and its effectiveness is demonstrated by coding experiments, showing
that good results can be obtained with precision as low as 4~bits per network
output, and practically no loss with 8~bits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2022 IEEE International Conference on Image Processing (ICIP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SgVA-CLIP: Semantic-guided Visual Adapting of Vision-Language Models for
  Few-shot Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16191v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16191v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fang Peng, Xiaoshan Yang, Linhui Xiao, Yaowei Wang, Changsheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although significant progress has been made in few-shot learning, most of
existing few-shot image classification methods require supervised pre-training
on a large amount of samples of base classes, which limits their generalization
ability in real world application. Recently, large-scale Vision-Language
Pre-trained models (VLPs) have been gaining increasing attention in few-shot
learning because they can provide a new paradigm for transferable visual
representation learning with easily available text on the Web. However, the
VLPs may neglect detailed visual information that is difficult to describe by
language sentences, but important for learning an effective classifier to
distinguish different images. To address the above problem, we propose a new
framework, named Semantic-guided Visual Adapting (SgVA), which can effectively
extend vision-language pre-trained models to produce discriminative adapted
visual features by comprehensively using an implicit knowledge distillation, a
vision-specific contrastive loss, and a cross-modal contrastive loss. The
implicit knowledge distillation is designed to transfer the fine-grained
cross-modal knowledge to guide the updating of the vision adapter.
State-of-the-art results on 13 datasets demonstrate that the adapted visual
features can well complement the cross-modal features to improve few-shot image
classification.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-01-19T00:00:00Z">2023-01-19</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JCSE: Contrastive Learning of Japanese Sentence Embeddings and Its
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Chen, Hisashi Handa, Kimiaki Shirahama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning is widely used for sentence representation learning.
Despite this prevalence, most studies have focused exclusively on English and
few concern domain adaptation for domain-specific downstream tasks, especially
for low-resource languages like Japanese, which are characterized by
insufficient target domain data and the lack of a proper training strategy. To
overcome this, we propose a novel Japanese sentence representation framework,
JCSE (derived from ``Contrastive learning of Sentence Embeddings for
Japanese''), that creates training data by generating sentences and
synthesizing them with sentences available in a target domain. Specifically, a
pre-trained data generator is finetuned to a target domain using our collected
corpus. It is then used to generate contradictory sentence pairs that are used
in contrastive learning for adapting a Japanese language model to a specific
task in the target domain.
  Another problem of Japanese sentence representation learning is the
difficulty of evaluating existing embedding methods due to the lack of
benchmark datasets. Thus, we establish a comprehensive Japanese Semantic
Textual Similarity (STS) benchmark on which various embedding models are
evaluated. Based on this benchmark result, multiple embedding methods are
chosen and compared with JCSE on two domain-specific tasks, STS in a clinical
domain and information retrieval in an educational domain. The results show
that JCSE achieves significant performance improvement surpassing direct
transfer and other training strategies. This empirically demonstrates JCSE's
effectiveness and practicability for downstream tasks of a low-resource
language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Embeddings Sometimes Contain Typological Generalizations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Östling, Murathan Kurfalı
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To what extent can neural network models learn generalizations about language
structure, and how do we find out what they have learned? We explore these
questions by training neural models for a range of natural language processing
tasks on a massively multilingual dataset of Bible translations in 1295
languages. The learned language representations are then compared to existing
typological databases as well as to a novel set of quantitative syntactic and
morphological features obtained through annotation projection. We conclude that
some generalizations are surprisingly close to traditional features from
linguistic typology, but that most of our models, as well as those of previous
work, do not appear to have made linguistically meaningful generalizations.
Careful attention to details in the evaluation turns out to be essential to
avoid false positives. Furthermore, to encourage continued work in this field,
we release several resources covering most or all of the languages in our data:
(i) multiple sets of language representations, (ii) multilingual word
embeddings, (iii) projected and predicted syntactic and morphological features,
(iv) software to provide linguistically sound evaluations of language
representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Author as Character and Narrator: Deconstructing Personal Narratives
  from the r/AmITheAsshole Reddit Community 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salvatore Giorgi, Ke Zhao, Alexander H. Feng, Lara J. Martin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the r/AmITheAsshole subreddit, people anonymously share first person
narratives that contain some moral dilemma or conflict and ask the community to
judge who is at fault (i.e., who is "the asshole"). In general, first person
narratives are a unique storytelling domain where the author is the narrator
(the person telling the story) but can also be a character (the person living
the story) and, thus, the author has two distinct voices presented in the
story. In this study, we identify linguistic and narrative features associated
with the author as the character or as a narrator. We use these features to
answer the following questions: (1) what makes an asshole character and (2)
what makes an asshole narrator? We extract both Author-as-Character features
(e.g., demographics, narrative event chain, and emotional arc) and
Author-as-Narrator features (i.e., the style and emotion of the story as a
whole) in order to identify which aspects of the narrative are correlated with
the final moral judgment. Our work shows that "assholes" as Characters frame
themselves as lacking agency with a more positive personal arc, while
"assholes" as Narrators will tell emotional and opinionated stories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Machine Translation with Phrase Pair Injection and Corpus
  Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshay Batheja, Pushpak Bhattacharyya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we show that the combination of Phrase Pair Injection and
Corpus Filtering boosts the performance of Neural Machine Translation (NMT)
systems. We extract parallel phrases and sentences from the pseudo-parallel
corpus and augment it with the parallel corpus to train the NMT models. With
the proposed approach, we observe an improvement in the Machine Translation
(MT) system for 3 low-resource language pairs, Hindi-Marathi, English-Marathi,
and English-Pashto, and 6 translation directions by up to 2.7 BLEU points, on
the FLORES test data. These BLEU score improvements are over the models trained
using the whole pseudo-parallel corpus augmented with the parallel corpus.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Keyword Embeddings for Query Suggestion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jorge Gabín, M. Eduardo Ares, Javier Parapar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, search engine users commonly rely on query suggestions to improve
their initial inputs. Current systems are very good at recommending lexical
adaptations or spelling corrections to users' queries. However, they often
struggle to suggest semantically related keywords given a user's query. The
construction of a detailed query is crucial in some tasks, such as legal
retrieval or academic search. In these scenarios, keyword suggestion methods
are critical to guide the user during the query formulation. This paper
proposes two novel models for the keyword suggestion task trained on scientific
literature. Our techniques adapt the architecture of Word2Vec and FastText to
generate keyword embeddings by leveraging documents' keyword co-occurrence.
Along with these models, we also present a specially tailored negative sampling
approach that exploits how keywords appear in academic publications. We devise
a ranking-based evaluation methodology following both known-item and ad-hoc
search scenarios. Finally, we evaluate our proposals against the
state-of-the-art word and sentence embedding models showing considerable
improvements over the baselines for the tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuously Reliable Detection of New-Normal Misinformation: Semantic
  Masking and Contrastive Smoothing in High-Density Latent Regions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhijit Suprem, Joao Eduardo Ferreira, Calton Pu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Toxic misinformation campaigns have caused significant societal harm, e.g.,
affecting elections and COVID-19 information awareness. Unfortunately, despite
successes of (gold standard) retrospective studies of misinformation that
confirmed their harmful effects after the fact, they arrive too late for timely
intervention and reduction of such harm. By design, misinformation evades
retrospective classifiers by exploiting two properties we call new-normal: (1)
never-seen-before novelty that cause inescapable generalization challenges for
previous classifiers, and (2) massive but short campaigns that end before they
can be manually annotated for new classifier training. To tackle these
challenges, we propose UFIT, which combines two techniques: semantic masking of
strong signal keywords to reduce overfitting, and intra-proxy smoothness
regularization of high-density regions in the latent space to improve
reliability and maintain accuracy. Evaluation of UFIT on public new-normal
misinformation data shows over 30% improvement over existing approaches on
future (and unseen) campaigns. To the best of our knowledge, UFIT is the first
successful effort to achieve such high level of generalization on new-normal
misinformation data with minimal concession (1 to 5%) of accuracy compared to
oracles trained with full knowledge of all campaigns.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic-aware Contrastive Learning for More Accurate Semantic Parsing <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shan Wu, Chunlei Xin, Bo Chen, Xianpei Han, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the meaning representations are detailed and accurate annotations which
express fine-grained sequence-level semtantics, it is usually hard to train
discriminative semantic parsers via Maximum Likelihood Estimation (MLE) in an
autoregressive fashion. In this paper, we propose a semantic-aware contrastive
learning algorithm, which can learn to distinguish fine-grained meaning
representations and take the overall sequence-level semantic into
consideration. Specifically, a multi-level online sampling algorithm is
proposed to sample confusing and diverse instances. Three semantic-aware
similarity functions are designed to accurately measure the distance between
meaning representations as a whole. And a ranked contrastive loss is proposed
to pull the representations of the semantic-identical instances together and
push negative instances away. Experiments on two standard datasets show that
our approach achieves significant improvements over MLE baselines and gets
state-of-the-art performances by simply applying semantic-aware contrastive
learning on a vanilla Seq2Seq model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sentiment Analysis for Measuring Hope and Fear from Reddit Posts During
  the 2022 Russo-Ukrainian Conflict 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessio Guerra, Oktay Karakuş
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel lexicon-based unsupervised sentimental analysis
method to measure the $``\textit{hope}"$ and $``\textit{fear}"$ for the 2022
Ukrainian-Russian Conflict. $\textit{Reddit.com}$ is utilised as the main
source of human reactions to daily events during nearly the first three months
of the conflict. The top 50 $``hot"$ posts of six different subreddits about
Ukraine and news (Ukraine, worldnews, Ukraina, UkrainianConflict,
UkraineWarVideoReport, UkraineWarReports) and their relative comments are
scraped and a data set is created. On this corpus, multiple analyses such as
(1) public interest, (2) hope/fear score, (3) stock price interaction are
employed. We promote using a dictionary approach, which scores the hopefulness
of every submitted user post. The Latent Dirichlet Allocation (LDA) algorithm
of topic modelling is also utilised to understand the main issues raised by
users and what are the key talking points. Experimental analysis shows that the
hope strongly decreases after the symbolic and strategic losses of Azovstal
(Mariupol) and Severodonetsk. Spikes in hope/fear, both positives and
negatives, are present after important battles, but also some non-military
events, such as Eurovision and football games.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 8 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reversing The Twenty Questions Game 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parth Parikh, Anisha Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Twenty questions is a widely popular verbal game. In recent years, many
computerized versions of this game have been developed in which a user thinks
of an entity and a computer attempts to guess this entity by asking a series of
boolean-type (yes/no) questions. In this research, we aim to reverse this game
by making the computer choose an entity at random. The human aims to guess this
entity by quizzing the computer with natural language queries which the
computer will then attempt to parse using a boolean question answering model.
The game ends when the human is successfully able to guess the entity of the
computer's choice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures, 2 tables, This paper is a graduate course
  project for North Carolina State University, written for the Natural Language
  Processing class in Fall 2021. The paper was submitted to and graded by Dr.
  Munindar P. Singh</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Batch <span class="highlight-title">Prompt</span>ing: Efficient Inference with Large Language Model APIs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhoujun Cheng, Jungo Kasai, Tao Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performing inference on hundreds of thousands of samples with large language
models (LLMs) can be computationally and financially costly. We propose batch
prompting, a simple alternative prompting approach that enables the LLM to run
inference in batches, instead of one sample at a time. Our method reduces both
token and time costs while retaining downstream performance. We theoretically
demonstrate that under a few-shot in-context learning setting, the inference
costs decrease almost inverse linearly with the number of samples in each
batch. We extensively validate the effectiveness of batch prompting on ten
datasets across commonsense QA, arithmetic reasoning, and NLI/NLU: batch
prompting significantly~(up to $5\times$ with six samples in batch) reduces the
LLM (Codex) inference token and time costs while achieving better or comparable
performance. Our analysis shows that the number of samples in each batch and
the complexity of tasks affect its performance. Further, batch prompting can be
applied across different LLMs and reasoning methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InferEM: Inferring the Speaker's Intention for Empathetic Dialogue
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06373v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06373v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoqing Lv, Xiaoping Wang, Jiang Li, Zhigang Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current approaches to empathetic response generation typically encode the
entire dialogue history directly and put the output into a decoder to generate
friendly feedback. These methods focus on modelling contextual information but
neglect capturing the direct intention of the speaker. We argue that the last
utterance in the dialogue empirically conveys the intention of the speaker.
Consequently, we propose a novel model named InferEM for empathetic response
generation. We separately encode the last utterance and fuse it with the entire
dialogue through the multi-head attention based intention fusion module to
capture the speaker's intention. Besides, we utilize previous utterances to
predict the last utterance, which simulates human's psychology to guess what
the interlocutor may speak in advance. To balance the optimizing rates of the
utterance prediction and response generation, a multi-task learning strategy is
designed for InferEM. Experimental results demonstrate the plausibility and
validity of InferEM in improving empathetic expression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Life is a Circus and We are the Clowns: Automatically Finding Analogies
  between Situations and Processes <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.12197v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.12197v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oren Sultan, Dafna Shahaf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analogy-making gives rise to reasoning, abstraction, flexible categorization
and counterfactual inference -- abilities lacking in even the best AI systems
today. Much research has suggested that analogies are key to non-brittle
systems that can adapt to new domains. Despite their importance, analogies
received little attention in the NLP community, with most research focusing on
simple word analogies. Work that tackled more complex analogies relied heavily
on manually constructed, hard-to-scale input representations. In this work, we
explore a more realistic, challenging setup: our input is a pair of natural
language procedural texts, describing a situation or a process (e.g., how the
heart works/how a pump works). Our goal is to automatically extract entities
and their relations from the text and find a mapping between the different
domains based on relational similarity (e.g., blood is mapped to water). We
develop an interpretable, scalable algorithm and demonstrate that it identifies
the correct mappings 87% of the time for procedural texts and 94% for stories
from cognitive-psychology literature. We show it can extract analogies from a
large dataset of procedural texts, achieving 79% precision (analogy prevalence
in data: 3%). Lastly, we demonstrate that our algorithm is robust to
paraphrasing the input texts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2022 main conference (long paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Training Vision Language <span class="highlight-title">BERT</span>s with a Unified Conditional Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.02010v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.02010v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Yang, Fengmao Lv, Fayao Liu, Guosheng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language BERTs are trained with language corpus in a self-supervised
manner. Unlike natural language BERTs, vision language BERTs need paired data
to train, which restricts the scale of VL-BERT pretraining. We propose a
self-training approach that allows training VL-BERTs from unlabeled image data.
The proposed method starts with our unified conditional model -- a vision
language BERT model that can perform zero-shot conditional generation. Given
different conditions, the unified conditional model can generate captions,
dense captions, and even questions. We use the labeled image data to train a
teacher model and use the trained model to generate pseudo captions on
unlabeled image data. We then combine the labeled data and pseudo labeled data
to train a student model. The process is iterated by putting the student model
as a new teacher. By using the proposed self-training approach and only 300k
unlabeled extra data, we are able to get competitive or even better
performances compared to the models of similar model size trained with 3
million extra image data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Analysis of Semantically-Aligned Speech-Text Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.01235v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.01235v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Huzaifah, Ivan Kukanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embeddings play an important role in end-to-end solutions for multi-modal
language processing problems. Although there has been some effort to understand
the properties of single-modality embedding spaces, particularly that of text,
their cross-modal counterparts are less understood. In this work, we study some
intrinsic properties of a joint speech-text embedding space, constructed by
minimizing the distance between paired utterance and transcription inputs in a
teacher-student model setup, that are informative for several prominent use
cases. We found that incorporating automatic speech recognition through both
pretraining and multitask scenarios aid semantic alignment significantly,
resulting in more tightly coupled embeddings. To analyse cross-modal embeddings
we utilise a quantitative retrieval accuracy metric for semantic alignment,
zero-shot classification for generalisability, and probing of the encoders to
observe the extent of knowledge transfer from one modality to another.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is the accepted version of the paper published at IEEE Spoken
  Language Technology (SLT) Workshop 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Dialogue Breakdown Detection with Semi-Supervised Learning <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.00136v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.00136v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Ng, Marzyeh Ghassemi, Narendran Thangarajan, Jiacheng Pan, Qi Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building user trust in dialogue agents requires smooth and consistent
dialogue exchanges. However, agents can easily lose conversational context and
generate irrelevant utterances. These situations are called dialogue breakdown,
where agent utterances prevent users from continuing the conversation. Building
systems to detect dialogue breakdown allows agents to recover appropriately or
avoid breakdown entirely. In this paper we investigate the use of
semi-supervised learning methods to improve dialogue breakdown detection,
including continued pre-training on the Reddit dataset and a manifold-based
data augmentation method. We demonstrate the effectiveness of these methods on
the Dialogue Breakdown Detection Challenge (DBDC) English shared task. Our
submissions to the 2020 DBDC5 shared task place first, beating baselines and
other submissions by over 12\% accuracy. In ablations on DBDC4 data from 2019,
our semi-supervised learning methods improve the performance of a baseline BERT
model by 2\% accuracy. These methods are applicable generally to any dialogue
task and provide a simple way to improve model performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 1 figure, accepted at the NeurIPS Workshop on Human in the
  Loop Dialogue Systems</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiview Compressive Coding for 3D Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph Feichtenhofer, Georgia Gkioxari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A central goal of visual recognition is to understand objects and scenes from
a single image. 2D recognition has witnessed tremendous progress thanks to
large-scale learning and general-purpose representations. Comparatively, 3D
poses new challenges stemming from occlusions not depicted in the image. Prior
works try to overcome these by inferring from multiple views or rely on scarce
CAD models and category-specific priors which hinder scaling to novel settings.
In this work, we explore single-view 3D reconstruction by learning
generalizable representations inspired by advances in self-supervised learning.
We introduce a simple framework that operates on 3D points of single objects or
whole scenes coupled with category-agnostic large-scale training from diverse
RGB-D videos. Our model, Multiview Compressive Coding (MCC), learns to compress
the input appearance and geometry to predict the 3D structure by querying a
3D-aware decoder. MCC's generality and efficiency allow it to learn from
large-scale and diverse data sources with strong generalization to novel
objects imagined by DALL$\cdot$E 2 or captured in-the-wild with an iPhone.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://mcc3d.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Booster: a Benchmark for Depth from Images of Specular and Transparent
  Surfaces <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierluigi Zama Ramirez, Alex Costanzino, Fabio Tosi, Matteo Poggi, Samuele Salti, Stefano Mattoccia, Luigi Di Stefano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating depth from images nowadays yields outstanding results, both in
terms of in-domain accuracy and generalization. However, we identify two main
challenges that remain open in this field: dealing with non-Lambertian
materials and effectively processing high-resolution images. Purposely, we
propose a novel dataset that includes accurate and dense ground-truth labels at
high resolution, featuring scenes containing several specular and transparent
surfaces. Our acquisition pipeline leverages a novel deep space-time stereo
framework, enabling easy and accurate labeling with sub-pixel precision. The
dataset is composed of 606 samples collected in 85 different scenes, each
sample includes both a high-resolution pair (12 Mpx) as well as an unbalanced
stereo pair (Left: 12 Mpx, Right: 1.1 Mpx). Additionally, we provide manually
annotated material segmentation masks and 15K unlabeled samples. We divide the
dataset into a training set, and two testing sets, the latter devoted to the
evaluation of stereo and monocular depth estimation networks respectively to
highlight the open challenges and future research directions in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extension of the paper "Open Challenges in Deep Stereo: the Booster
  Dataset" that was presented at CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> <span class="highlight-title">Self-Supervised</span> Learning from Images with a Joint-Embedding Predictive
  Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, <span class="highlight-author">Yann LeCun</span>, Nicolas Ballas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper demonstrates an approach for learning highly semantic image
representations without relying on hand-crafted data-augmentations. We
introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a
non-generative approach for self-supervised learning from images. The idea
behind I-JEPA is simple: from a single context block, predict the
representations of various target blocks in the same image. A core design
choice to guide I-JEPA towards producing semantic representations is the
masking strategy; specifically, it is crucial to (a) predict several target
blocks in the image, (b) sample target blocks with sufficiently large scale
(occupying 15%-20% of the image), and (c) use a sufficiently informative
(spatially distributed) context block. Empirically, when combined with Vision
Transformers, we find I-JEPA to be highly scalable. For instance, we train a
ViT-Huge/16 on ImageNet using 32 A100 GPUs in under 38 hours to achieve strong
downstream performance across a wide range of tasks requiring various levels of
abstraction, from linear classification to object counting and depth
prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoCoNet: Long-Short Context Network for Active Speaker Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xizi Wang, Feng Cheng, Gedas Bertasius, David Crandall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active Speaker Detection (ASD) aims to identify who is speaking in each frame
of a video. ASD reasons from audio and visual information from two contexts:
long-term intra-speaker context and short-term inter-speaker context. Long-term
intra-speaker context models the temporal dependencies of the same speaker,
while short-term inter-speaker context models the interactions of speakers in
the same scene. These two contexts are complementary to each other and can help
infer the active speaker. Motivated by these observations, we propose LoCoNet,
a simple yet effective Long-Short Context Network that models the long-term
intra-speaker context and short-term inter-speaker context. We use
self-attention to model long-term intra-speaker context due to its
effectiveness in modeling long-range dependencies, and convolutional blocks
that capture local patterns to model short-term inter-speaker context.
Extensive experiments show that LoCoNet achieves state-of-the-art performance
on multiple datasets, achieving an mAP of 95.2%(+1.1%) on AVA-ActiveSpeaker,
68.1%(+22%) on Columbia dataset, 97.2%(+2.8%) on Talkies dataset and
59.7%(+8.0%) on Ego4D dataset. Moreover, in challenging cases where multiple
speakers are present, or face of active speaker is much smaller than other
faces in the same scene, LoCoNet outperforms previous state-of-the-art methods
by 3.4% on the AVA-ActiveSpeaker dataset. The code will be released at
https://github.com/SJTUwxz/LoCoNet_ASD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>tech report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating Remaining Lifespan from the Face 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Fekrazad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The face is a rich source of information that can be utilized to infer a
person's biological age, sex, phenotype, genetic defects, and health status.
All of these factors are relevant for predicting an individual's remaining
lifespan. In this study, we collected a dataset of over 24,000 images (from
Wikidata/Wikipedia) of individuals who died of natural causes, along with the
number of years between when the image was taken and when the person passed
away. We made this dataset publicly available. We fine-tuned multiple
Convolutional Neural Network (CNN) models on this data, at best achieving a
mean absolute error of 8.3 years in the validation data using VGGFace. However,
the model's performance diminishes when the person was younger at the time of
the image. To demonstrate the potential applications of our remaining lifespan
model, we present examples of using it to estimate the average loss of life (in
years) due to the COVID-19 pandemic and to predict the increase in life
expectancy that might result from a health intervention such as weight loss.
Additionally, we discuss the ethical considerations associated with such
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking YOLOv5 and YOLOv7 models with DeepSORT for droplet tracking
  applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihir Durve, Sibilla Orsini, Adriano Tiribocchi, Andrea Montessori, Jean-Michel Tucny, Marco Lauricella, Andrea Camposeo, Dario Pisignano, Sauro Succi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tracking droplets in microfluidics is a challenging task. The difficulty
arises in choosing a tool to analyze general microfluidic videos to infer
physical quantities. The state-of-the-art object detector algorithm You Only
Look Once (YOLO) and the object tracking algorithm Simple Online and Realtime
Tracking with a Deep Association Metric (DeepSORT) are customizable for droplet
identification and tracking. The customization includes training YOLO and
DeepSORT networks to identify and track the objects of interest. We trained
several YOLOv5 and YOLOv7 models and the DeepSORT network for droplet
identification and tracking from microfluidic experimental videos. We compare
the performance of the droplet tracking applications with YOLOv5 and YOLOv7 in
terms of training time and time to analyze a given video across various
hardware configurations. Despite the latest YOLOv7 being 10% faster, the
real-time tracking is only achieved by lighter YOLO models on RTX 3070 Ti GPU
machine due to additional significant droplet tracking costs arising from the
DeepSORT algorithm. This work is a benchmark study for the YOLOv5 and YOLOv7
networks with DeepSORT in terms of the training time and inference time for a
custom dataset of microfluidic droplets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Resolution Framework for U-Nets with Applications to
  Hierarchical VAEs <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Falck, Christopher Williams, Dominic Danks, George Deligiannidis, Christopher Yau, Chris Holmes, Arnaud Doucet, Matthew Willetts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  U-Net architectures are ubiquitous in state-of-the-art deep learning, however
their regularisation properties and relationship to wavelets are understudied.
In this paper, we formulate a multi-resolution framework which identifies
U-Nets as finite-dimensional truncations of models on an infinite-dimensional
function space. We provide theoretical results which prove that average pooling
corresponds to projection within the space of square-integrable functions and
show that U-Nets with average pooling implicitly learn a Haar wavelet basis
representation of the data. We then leverage our framework to identify
state-of-the-art hierarchical VAEs (HVAEs), which have a U-Net architecture, as
a type of two-step forward Euler discretisation of multi-resolution diffusion
processes which flow from a point mass, introducing sampling instabilities. We
also demonstrate that HVAEs learn a representation of time which allows for
improved parameter efficiency through weight-sharing. We use this observation
to achieve state-of-the-art HVAE performance with half the number of parameters
of existing models, exploiting the properties of our continuous-time
formulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022 (selected as oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative Robotic Ultrasound Tissue Scanning for Surgical Resection
  Guidance in Neurosurgery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alistair Weld, Michael Dyck, Julian Klodmann, Giulio Anichini, Luke Dixon, Sophie Camp, Alin Albu-Schäffer, Stamatia Giannarou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The aim of this paper is to introduce a robotic platform for autonomous iUS
tissue scanning to optimise intraoperative diagnosis and improve surgical
resection during robot-assisted operations. To guide anatomy specific robotic
scanning and generate a representation of the robot task space, fast and
accurate techniques for the recovery of 3D morphological structures of the
surgical cavity are developed. The prototypic DLR MIRO surgical robotic arm is
used to control the applied force and the in-plane motion of the US transducer.
A key application of the proposed platform is the scanning of brain tissue to
guide tumour resection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FECANet: Boosting Few-Shot Semantic Segmentation with Feature-Enhanced
  Context-Aware Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huafeng Liu, Pai Peng, Tao Chen, Qiong Wang, Yazhou Yao, Xian-Sheng Hua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot semantic segmentation is the task of learning to locate each pixel
of the novel class in the query image with only a few annotated support images.
The current correlation-based methods construct pair-wise feature correlations
to establish the many-to-many matching because the typical prototype-based
approaches cannot learn fine-grained correspondence relations. However, the
existing methods still suffer from the noise contained in naive correlations
and the lack of context semantic information in correlations. To alleviate
these problems mentioned above, we propose a Feature-Enhanced Context-Aware
Network (FECANet). Specifically, a feature enhancement module is proposed to
suppress the matching noise caused by inter-class local similarity and enhance
the intra-class relevance in the naive correlation. In addition, we propose a
novel correlation reconstruction module that encodes extra correspondence
relations between foreground and background and multi-scale context semantic
features, significantly boosting the encoder to capture a reliable matching
pattern. Experiments on PASCAL-$5^i$ and COCO-$20^i$ datasets demonstrate that
our proposed FECANet leads to remarkable improvement compared to previous
state-of-the-arts, demonstrating its effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by IEEE Transactions on Multimedia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SoftEnNet: Symbiotic Monocular Depth Estimation and Lumen Segmentation
  for Colonoscopy Endorobots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alwyn Mathew, Ludovic Magerand, Emanuele Trucco, Luigi Manfredi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Colorectal cancer is the third most common cause of cancer death worldwide.
Optical colonoscopy is the gold standard for detecting colorectal cancer;
however, about 25 percent of polyps are missed during the procedure. A
vision-based autonomous endorobot can improve colonoscopy procedures
significantly through systematic, complete screening of the colonic mucosa. The
reliable robot navigation needed requires a three-dimensional understanding of
the environment and lumen tracking to support autonomous tasks. We propose a
novel multi-task model that simultaneously predicts dense depth and lumen
segmentation with an ensemble of deep networks. The depth estimation
sub-network is trained in a self-supervised fashion guided by view synthesis;
the lumen segmentation sub-network is supervised. The two sub-networks are
interconnected with pathways that enable information exchange and thereby
mutual learning. As the lumen is in the image's deepest visual space, lumen
segmentation helps with the depth estimation at the farthest location. In turn,
the estimated depth guides the lumen segmentation network as the lumen location
defines the farthest scene location. Unlike other environments, view synthesis
often fails in the colon because of the deformable wall, textureless surface,
specularities, and wide field of view image distortions, all challenges that
our pipeline addresses. We conducted qualitative analysis on a synthetic
dataset and quantitative analysis on a colon training model and real
colonoscopy videos. The experiments show that our model predicts accurate
scale-invariant depth maps and lumen segmentation from colonoscopy images in
near real-time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SwiftAvatar: Efficient Auto-Creation of Parameterized Stylized Character
  on Arbitrary Avatar Engines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shizun Wang, Weihong Zeng, Xu Wang, Hao Yang, Li Chen, Chuang Zhang, Ming Wu, Yi Yuan, Yunzhao Zeng, Min Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The creation of a parameterized stylized character involves careful selection
of numerous parameters, also known as the "avatar vectors" that can be
interpreted by the avatar engine. Existing unsupervised avatar vector
estimation methods that auto-create avatars for users, however, often fail to
work because of the domain gap between realistic faces and stylized avatar
images. To this end, we propose SwiftAvatar, a novel avatar auto-creation
framework that is evidently superior to previous works. SwiftAvatar introduces
dual-domain generators to create pairs of realistic faces and avatar images
using shared latent codes. The latent codes can then be bridged with the avatar
vectors as pairs, by performing GAN inversion on the avatar images rendered
from the engine using avatar vectors. Through this way, we are able to
synthesize paired data in high-quality as many as possible, consisting of
avatar vectors and their corresponding realistic faces. We also propose
semantic augmentation to improve the diversity of synthesis. Finally, a
light-weight avatar vector estimator is trained on the synthetic pairs to
implement efficient auto-creation. Our experiments demonstrate the
effectiveness and efficiency of SwiftAvatar on two different avatar engines.
The superiority and advantageous flexibility of SwiftAvatar are also verified
in both subjective and objective evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RGB-D-Based Categorical Object Pose and Shape Estimation: Methods,
  <span class="highlight-title">Dataset</span>s, and Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonard Bruns, Patric Jensfelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, various methods for 6D pose and shape estimation of objects at a
per-category level have been proposed. This work provides an overview of the
field in terms of methods, datasets, and evaluation protocols. First, an
overview of existing works and their commonalities and differences is provided.
Second, we take a critical look at the predominant evaluation protocol,
including metrics and datasets. Based on the findings, we propose a new set of
metrics, contribute new annotations for the Redwood dataset, and evaluate
state-of-the-art methods in a fair comparison. The results indicate that
existing methods do not generalize well to unconstrained orientations and are
actually heavily biased towards objects being upright. We provide an
easy-to-use evaluation toolbox with well-defined metrics, methods, and dataset
interfaces, which allows evaluation and comparison with various
state-of-the-art approaches
(https://github.com/roym899/pose_and_shape_evaluation).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2202.10346</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Regularizing disparity estimation via multi task learning with
  structured light reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alistair Weld, Joao Cartucho, Chi Xu, Joseph Davids, Stamatia Giannarou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D reconstruction is a useful tool for surgical planning and guidance.
However, the lack of available medical data stunts research and development in
this field, as supervised deep learning methods for accurate disparity
estimation rely heavily on large datasets containing ground truth information.
Alternative approaches to supervision have been explored, such as
self-supervision, which can reduce or remove entirely the need for ground
truth. However, no proposed alternatives have demonstrated performance
capabilities close to what would be expected from a supervised setup. This work
aims to alleviate this issue. In this paper, we investigate the learning of
structured light projections to enhance the development of direct disparity
estimation networks. We show for the first time that it is possible to
accurately learn the projection of structured light on a scene, implicitly
learning disparity. Secondly, we \textcolor{black}{explore the use of a multi
task learning (MTL) framework for the joint training of structured light and
disparity. We present results which show that MTL with structured light
improves disparity training; without increasing the number of model parameters.
Our MTL setup outperformed the single task learning (STL) network in every
validation test. Notably, in the medical generalisation test, the STL error was
1.4 times worse than that of the best MTL performance. The benefit of using MTL
is emphasised when the training data is limited.} A dataset containing
stereoscopic images, disparity maps and structured light projections on medical
phantoms and ex vivo tissue was created for evaluation together with virtual
scenes. This dataset will be made publicly available in the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diagnose Like a Pathologist: <span class="highlight-title">Transformer</span>-Enabled Hierarchical
  Attention-Guided Multiple Instance Learning for Whole Slide Image
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Conghao Xiong, Hao Chen, Joseph Sung, Irwin King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple Instance Learning (MIL) and transformers are increasingly popular in
histopathology Whole Slide Image (WSI) classification. However, unlike human
pathologists who selectively observe specific regions of histopathology tissues
under different magnifications, most methods do not incorporate multiple
resolutions of the WSIs, hierarchically and attentively, thereby leading to a
loss of focus on the WSIs and information from other resolutions. To resolve
this issue, we propose the Hierarchical Attention-Guided Multiple Instance
Learning framework to fully exploit the WSIs, which can dynamically and
attentively discover the discriminative regions across multiple resolutions of
the WSIs. Within this framework, to further enhance the performance of the
transformer and obtain a more holistic WSI (bag) representation, we propose an
Integrated Attention Transformer, consisting of multiple Integrated Attention
Modules, which is the combination of a transformer layer and an aggregation
module that produces a bag representation based on every instance
representation in that bag. The results of the experiments show that our method
achieved state-of-the-art performances on multiple datasets, including
Camelyon16, TCGA-RCC, TCGA-NSCLC, and our in-house IMGC dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Soft Thresholding for Visual Image Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Dalitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thresholding converts a greyscale image into a binary image, and is thus
often a necessary segmentation step in image processing. For a human viewer
however, thresholding usually has a negative impact on the legibility of
document images. This report describes a simple method for "smearing out" the
threshold and transforming the greyscale image into a different greyscale
image. The method is similar to fuzzy thresholding, but is discussed here in
the simpler context of greyscale transformations and, unlike fuzzy
thresholding, it is independent from the method for finding the threshold. A
simple formula is presented for automatically determining the width of the
threshold spread. The method can be used, e.g., for enhancing images for the
presentation in online facsimile repositories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RNAS-CL: Robust Neural Architecture Search by Cross-Layer Knowledge
  Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Utkarsh Nath, Yancheng Wang, Yingzhen Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks are vulnerable to adversarial attacks. Neural
Architecture Search (NAS), one of the driving tools of deep neural networks,
demonstrates superior performance in prediction accuracy in various machine
learning applications. However, it is unclear how it performs against
adversarial attacks. Given the presence of a robust teacher, it would be
interesting to investigate if NAS would produce robust neural architecture by
inheriting robustness from the teacher. In this paper, we propose Robust Neural
Architecture Search by Cross-Layer Knowledge Distillation (RNAS-CL), a novel
NAS algorithm that improves the robustness of NAS by learning from a robust
teacher through cross-layer knowledge distillation. Unlike previous knowledge
distillation methods that encourage close student/teacher output only in the
last layer, RNAS-CL automatically searches for the best teacher layer to
supervise each student layer. Experimental result evidences the effectiveness
of RNAS-CL and shows that RNAS-CL produces small and robust neural
architecture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dif-Fusion: Towards High Color Fidelity in Infrared and Visible Image
  Fusion with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Yue, Leyuan Fang, Shaobo Xia, Yue Deng, Jiayi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Color plays an important role in human visual perception, reflecting the
spectrum of objects. However, the existing infrared and visible image fusion
methods rarely explore how to handle multi-spectral/channel data directly and
achieve high color fidelity. This paper addresses the above issue by proposing
a novel method with diffusion models, termed as Dif-Fusion, to generate the
distribution of the multi-channel input data, which increases the ability of
multi-source information aggregation and the fidelity of colors. In specific,
instead of converting multi-channel images into single-channel data in existing
fusion methods, we create the multi-channel data distribution with a denoising
network in a latent space with forward and reverse diffusion process. Then, we
use the the denoising network to extract the multi-channel diffusion features
with both visible and infrared information. Finally, we feed the multi-channel
diffusion features to the multi-channel fusion module to directly generate the
three-channel fused image. To retain the texture and intensity information, we
propose multi-channel gradient loss and intensity loss. Along with the current
evaluation metrics for measuring texture and intensity fidelity, we introduce a
new evaluation metric to quantify color fidelity. Extensive experiments
indicate that our method is more effective than other state-of-the-art image
fusion methods, especially in color fidelity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpreting CNN Predictions using Conditional Generative Adversarial
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akash Guna R T, Raul Benitez, Sikha O K
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel method that trains a conditional Generative Adversarial
Network (GAN) to generate visual interpretations of a Convolutional Neural
Network (CNN). To comprehend a CNN, the GAN is trained with information on how
the CNN processes an image when making predictions. Supplying that information
has two main challenges: how to represent this information in a form that is
feedable to the GANs and how to effectively feed the representation to the GAN.
To address these issues, we developed a suitable representation of CNN
architectures by cumulatively averaging intermediate interpretation maps. We
also propose two alternative approaches to feed the representations to the GAN
and to choose an effective training strategy. Our approach learned the general
aspects of CNNs and was agnostic to datasets and CNN architectures. The study
includes both qualitative and quantitative evaluations and compares the
proposed GANs with state-of-the-art approaches. We found that the initial
layers of CNNs and final layers are equally crucial for interpreting CNNs upon
interpreting the proposed GAN. We believe training a GAN to interpret CNNs
would open doors for improved interpretations by leveraging fast-paced deep
learning advancements. The code used for experimentation is publicly available
at https://github.com/Akash-guna/Explain-CNN-With-GANS
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Article submitted to JMLR. 19 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Position Regression for Unsupervised Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florentin Bieder, Julia Wolleb, Robin Sandkühler, Philippe C. Cattin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, anomaly detection has become an essential field in medical
image analysis. Most current anomaly detection methods for medical images are
based on image reconstruction. In this work, we propose a novel anomaly
detection approach based on coordinate regression. Our method estimates the
position of patches within a volume, and is trained only on data of healthy
subjects. During inference, we can detect and localize anomalies by considering
the error of the position estimate of a given patch. We apply our method to 3D
CT volumes and evaluate it on patients with intracranial haemorrhages and
cranial fractures. The results show that our method performs well in detecting
these anomalies. Furthermore, we show that our method requires less memory than
comparable approaches that involve image reconstruction. This is highly
relevant for processing large 3D volumes, for instance, CT or MRI scans.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reference Guided Image Inpainting using Facial Attributes <span class="chip">BMVC 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongsik Yoon, Jeonggi Kwak, Yuanming Li, David Han, Youngsaeng Jin, Hanseok Ko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image inpainting is a technique of completing missing pixels such as occluded
region restoration, distracting objects removal, and facial completion. Among
these inpainting tasks, facial completion algorithm performs face inpainting
according to the user direction. Existing approaches require delicate and well
controlled input by the user, thus it is difficult for an average user to
provide the guidance sufficiently accurate for the algorithm to generate
desired results. To overcome this limitation, we propose an alternative
user-guided inpainting architecture that manipulates facial attributes using a
single reference image as the guide. Our end-to-end model consists of attribute
extractors for accurate reference image attribute transfer and an inpainting
model to map the attributes realistically and accurately to generated images.
We customize MS-SSIM loss and learnable bidirectional attention maps in which
importance structures remain intact even with irregular shaped masks. Based on
our evaluation using the publicly available dataset CelebA-HQ, we demonstrate
that the proposed method delivers superior performance compared to some
state-of-the-art methods specialized in inpainting tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>BMVC 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Inference in Denoising Diffusion Models via MMD Finetuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuele Aiello, Diego Valsesia, Enrico Magli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising Diffusion Models (DDMs) have become a popular tool for generating
high-quality samples from complex data distributions. These models are able to
capture sophisticated patterns and structures in the data, and can generate
samples that are highly diverse and representative of the underlying
distribution. However, one of the main limitations of diffusion models is the
complexity of sample generation, since a large number of inference timesteps is
required to faithfully capture the data distribution. In this paper, we present
MMD-DDM, a novel method for fast sampling of diffusion models. Our approach is
based on the idea of using the Maximum Mean Discrepancy (MMD) to finetune the
learned distribution with a given budget of timesteps. This allows the
finetuned model to significantly improve the speed-quality trade-off, by
substantially increasing fidelity in inference regimes with few steps or,
equivalently, by reducing the required number of steps to reach a target
fidelity, thus paving the way for a more practical adoption of diffusion models
in a wide range of applications. We evaluate our approach on unconditional
image generation with extensive experiments across the CIFAR-10, CelebA,
ImageNet and LSUN-Church datasets. Our findings show that the proposed method
is able to produce high-quality samples in a fraction of the time required by
widely-used diffusion models, and outperforms state-of-the-art techniques for
accelerated sampling. Code is available at:
https://github.com/diegovalsesia/MMD-DDM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RecolorNeRF: Layer Decomposed Radiance Field for Efficient Color Editing
  of 3D Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07958v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07958v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingchen Gong, Yuehao Wang, Xiaoguang Han, Qi Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiance fields have gradually become a main representation of media.
Although its appearance editing has been studied, how to achieve
view-consistent recoloring in an efficient manner is still under explored. We
present RecolorNeRF, a novel user-friendly color editing approach for the
neural radiance field. Our key idea is to decompose the scene into a set of
pure-colored layers, forming a palette. Thus, color manipulation can be
conducted by altering the color components of the palette directly. To support
efficient palette-based editing, the color of each layer needs to be as
representative as possible. In the end, the problem is formulated as in an
optimization formula, where the layers and their blending way are jointly
optimized with the NeRF itself. Extensive experiments show that our
jointly-optimized layer decomposition can be used against multiple backbones
and produce photo-realistic recolored novel-view renderings. We demonstrate
that RecolorNeRF outperforms baseline methods both quantitatively and
qualitatively for color editing even in complex real-world scenes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Point Cloud Data Simulation and Modelling with Aize Workspace 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boris Mocialov, Eirik Eythorsson, Reza Parseh, Hoang Tran, Vegard Flovik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work takes a look at data models often used in digital twins and
presents preliminary results specifically from surface reconstruction and
semantic segmentation models trained using simulated data. This work is
expected to serve as a ground work for future endeavours in data
contextualisation inside a digital twin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended abstract, Northern Lights Deep Learning Conference, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting the Spatial and Temporal Modeling for Few-shot Action
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazheng Xing, Mengmeng Wang, Boyu Mu, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial and temporal modeling is one of the most core aspects of few-shot
action recognition. Most previous works mainly focus on long-term temporal
relation modeling based on high-level spatial representations, without
considering the crucial low-level spatial features and short-term temporal
relations. Actually, the former feature could bring rich local semantic
information, and the latter feature could represent motion characteristics of
adjacent frames, respectively. In this paper, we propose SloshNet, a new
framework that revisits the spatial and temporal modeling for few-shot action
recognition in a finer manner. First, to exploit the low-level spatial
features, we design a feature fusion architecture search module to
automatically search for the best combination of the low-level and high-level
spatial features. Next, inspired by the recent transformer, we introduce a
long-term temporal modeling module to model the global temporal relations based
on the extracted spatial appearance features. Meanwhile, we design another
short-term temporal modeling module to encode the motion characteristics
between adjacent frame representations. After that, the final predictions can
be obtained by feeding the embedded rich spatial-temporal features to a common
frame-level class prototype matcher. We extensively validate the proposed
SloshNet on four few-shot action recognition datasets, including
Something-Something V2, Kinetics, UCF101, and HMDB51. It achieves favorable
results against state-of-the-art methods in all datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Style Transfer-based Task Augmentation for Cross-Domain
  Few-Shot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuzhen Rao, Jun Huang, Zengming Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In cross-domain few-shot learning, the core issue is that the model trained
on source tasks from source domains can not generalize well to target tasks
from the target domain, especially when the domain shift is very large.
Motivated by the observation that the domain shift between training tasks and
target tasks usually can reflect in their style variation, we propose Task
Augmented Meta-Learning (TAML) to conduct style transfer-based task
augmentation to improve the domain generalization ability. Firstly, Multi-task
Interpolation (MTI) is introduced to perform feature fusion on tasks from
different tasks with different styles, which makes more diverse styles
available. Furthermore, a novel task-augmentation strategy called Multi-Task
Style Transfer (MTST) is put forward to perform style transfer on existing
tasks to learn discriminative style-independent features. At last, we introduce
Feature Modulation module (FM) to add random styles, which aims to improve the
generalization of our model. The proposed TAML increases the diversity of
styles of training tasks, and contributes to training a model with better
domain generalization ability. The effectiveness is demonstrated via
theoretical analysis and thorough experiments on two popular cross-domain
few-shot benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-Scene Network: A Novel Baseline with Self-rectifying Loss for
  Weakly supervised Video Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Snehashis Majhi, Rui Dai, Quan Kong, Lorenzo Garattoni, Gianpiero Francesca, Francois Bremond
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video anomaly detection in surveillance systems with only video-level labels
(i.e. weakly-supervised) is challenging. This is due to, (i) the complex
integration of human and scene based anomalies comprising of subtle and sharp
spatio-temporal cues in real-world scenarios, (ii) non-optimal optimization
between normal and anomaly instances under weak supervision. In this paper, we
propose a Human-Scene Network to learn discriminative representations by
capturing both subtle and strong cues in a dissociative manner. In addition, a
self-rectifying loss is also proposed that dynamically computes the pseudo
temporal annotations from video-level labels for optimizing the Human-Scene
Network effectively. The proposed Human-Scene Network optimized with
self-rectifying loss is validated on three publicly available datasets i.e.
UCF-Crime, ShanghaiTech and IITB-Corridor, outperforming recently reported
state-of-the-art approaches on five out of the six scenarios considered.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatio-Temporal Context Modeling for Road Obstacle Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiuen Wu, Tao Wang, Lingyu Liang, Zuoyong Li, Fum Yew Ching
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Road obstacle detection is an important problem for vehicle driving safety.
In this paper, we aim to obtain robust road obstacle detection based on
spatio-temporal context modeling. Firstly, a data-driven spatial context model
of the driving scene is constructed with the layouts of the training data.
Then, obstacles in the input image are detected via the state-of-the-art object
detection algorithms, and the results are combined with the generated scene
layout. In addition, to further improve the performance and robustness,
temporal information in the image sequence is taken into consideration, and the
optical flow is obtained in the vicinity of the detected objects to track the
obstacles across neighboring frames. Qualitative and quantitative experiments
were conducted on the Small Obstacle Detection (SOD) dataset and the Lost and
Found dataset. The results indicate that our method with spatio-temporal
context modeling is superior to existing methods for road obstacle detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted by the 4th International Conference on Machine
  Learning for Cyber Security (ML4CS 2022), Guangzhou, China</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatially Covariant Lesion Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07895v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07895v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Zhang, Rongguang Wang, Jinwei Zhang, Dongdong Liu, Chao Li, Jiahao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared to natural images, medical images usually show stronger visual
patterns and therefore this adds flexibility and elasticity to resource-limited
clinical applications by injecting proper priors into neural networks. In this
paper, we propose spatially covariant pixel-aligned classifier (SCP) to improve
the computational efficiency and meantime maintain or increase accuracy for
lesion segmentation. SCP relaxes the spatial invariance constraint imposed by
convolutional operations and optimizes an underlying implicit function that
maps image coordinates to network weights, the parameters of which are obtained
along with the backbone network training and later used for generating network
weights to capture spatially covariant contextual information. We demonstrate
the effectiveness and efficiency of the proposed SCP using two lesion
segmentation tasks from different imaging modalities: white matter
hyperintensity segmentation in magnetic resonance imaging and liver tumor
segmentation in contrast-enhanced abdominal computerized tomography. The
network using SCP has achieved 23.8%, 64.9% and 74.7% reduction in GPU memory
usage, FLOPs, and network size with similar or better accuracy for lesion
segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures, and 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unposed: Unsupervised Pose Estimation based Product Image
  Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurabh Sharma, Faizan Ahemad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Product images are the most impressing medium of customer interaction on the
product detail pages of e-commerce websites. Millions of products are onboarded
on to webstore catalogues daily and maintaining a high quality bar for a
product's set of images is a problem at scale. Grouping products by categories,
clothing is a very high volume and high velocity category and thus deserves its
own attention. Given the scale it is challenging to monitor the completeness of
image set, which adequately details the product for the consumers, which in
turn often leads to a poor customer experience and thus customer drop off.
  To supervise the quality and completeness of the images in the product pages
for these product types and suggest improvements, we propose a Human Pose
Detection based unsupervised method to scan the image set of a product for the
missing ones. The unsupervised approach suggests a fair approach to sellers
based on product and category irrespective of any biases. We first create a
reference image set of popular products with wholesome imageset. Then we create
clusters of images to label most desirable poses to form the classes for the
reference set from these ideal products set. Further, for all test products we
scan the images for all desired pose classes w.r.t. reference set poses,
determine the missing ones and sort them in the order of potential impact.
These missing poses can further be used by the sellers to add enriched product
listing image. We gathered data from popular online webstore and surveyed ~200
products manually, a large fraction of which had at least 1 repeated image or
missing variant, and sampled 3K products(~20K images) of which a significant
proportion had scope for adding many image variants as compared to high rated
products which had more than double image variants, indicating that our model
can potentially be used on a large scale.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast-BEV: Towards Real-time On-vehicle Bird's-Eye View Perception <span class="chip">NeurIPS2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Huang, Yangguang Li, Enze Xie, Feng Liang, Luya Wang, Mingzhu Shen, Fenggang Liu, Tianqi Wang, Ping Luo, Jing Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the pure camera-based Bird's-Eye-View (BEV) perception removes
expensive Lidar sensors, making it a feasible solution for economical
autonomous driving. However, most existing BEV solutions either suffer from
modest performance or require considerable resources to execute on-vehicle
inference. This paper proposes a simple yet effective framework, termed
Fast-BEV, which is capable of performing real-time BEV perception on the
on-vehicle chips. Towards this goal, we first empirically find that the BEV
representation can be sufficiently powerful without expensive view
transformation or depth representation. Starting from M2BEV baseline, we
further introduce (1) a strong data augmentation strategy for both image and
BEV space to avoid over-fitting (2) a multi-frame feature fusion mechanism to
leverage the temporal information (3) an optimized deployment-friendly view
transformation to speed up the inference. Through experiments, we show Fast-BEV
model family achieves considerable accuracy and efficiency on edge. In
particular, our M1 model (R18@256x704) can run over 50FPS on the Tesla T4
platform, with 47.0% NDS on the nuScenes validation set. Our largest model
(R101@900x1600) establishes a new state-of-the-art 53.5% NDS on the nuScenes
validation set. The code is released at: https://github.com/Sense-GVT/Fast-BEV.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2022_ML4AD on October 22, 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Video Adapter for Parameter Efficient Video Text Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Zhang, Xiaojie Jin, Weibo Gong, Kai Xu, Zhao Zhang, Peng Wang, Xiaohui Shen, Jiashi Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art video-text retrieval (VTR) methods usually fully fine-tune
the pre-trained model (e.g. CLIP) on specific datasets, which may suffer from
substantial storage costs in practical applications since a separate model per
task needs to be stored. To overcome this issue, we present the premier work on
performing parameter-efficient VTR from the pre-trained model, i.e., only a
small number of parameters are tunable while freezing the backbone. Towards
this goal, we propose a new method dubbed Multimodal Video Adapter (MV-Adapter)
for efficiently transferring the knowledge in the pre-trained CLIP from
image-text to video-text. Specifically, MV-Adapter adopts bottleneck structures
in both video and text branches and introduces two novel components. The first
is a Temporal Adaptation Module employed in the video branch to inject global
and local temporal contexts. We also learn weights calibrations to adapt to the
dynamic variations across frames. The second is a Cross-Modal Interaction
Module that generates weights for video/text branches through a shared
parameter space, for better aligning between modalities. Thanks to above
innovations, MV-Adapter can achieve on-par or better performance than standard
fine-tuning with negligible parameters overhead. Notably, on five widely used
VTR benchmarks (MSR-VTT, MSVD, LSMDC, DiDemo, and ActivityNet), MV-Adapter
consistently outperforms various competing methods in V2T/T2V tasks with large
margins. Codes will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Food Detection For Images From a Wearable Egocentric Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Han, Sri Kalyan Yarlagadda, Tonmoy Ghosh, Fengqing Zhu, Edward Sazonov, Edward J. Delp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diet is an important aspect of our health. Good dietary habits can contribute
to the prevention of many diseases and improve the overall quality of life. To
better understand the relationship between diet and health, image-based dietary
assessment systems have been developed to collect dietary information. We
introduce the Automatic Ingestion Monitor (AIM), a device that can be attached
to one's eye glasses. It provides an automated hands-free approach to capture
eating scene images. While AIM has several advantages, images captured by the
AIM are sometimes blurry. Blurry images can significantly degrade the
performance of food image analysis such as food detection. In this paper, we
propose an approach to pre-process images collected by the AIM imaging sensor
by rejecting extremely blurry images to improve the performance of food
detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures, Conference Paper for Imaging and Multimedia
  Analytics in a Web and Mobile World Conference, IS&T Electronic Imaging
  Symposium, Burlingame, CA (Virtual), January, 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Foresee What You Will Learn: Data Augmentation for Domain Generalization
  in Non-Stationary Environments <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuhao Zeng, Wei Wang, Fan Zhou, Charles Ling, Boyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing domain generalization aims to learn a generalizable model to perform
well even on unseen domains. For many real-world machine learning applications,
the data distribution often shifts gradually along domain indices. For example,
a self-driving car with a vision system drives from dawn to dusk, with the sky
darkening gradually. Therefore, the system must be able to adapt to changes in
ambient illumination and continue to drive safely on the road. In this paper,
we formulate such problems as Evolving Domain Generalization, where a model
aims to generalize well on a target domain by discovering and leveraging the
evolving pattern of the environment. We then propose Directional Domain
Augmentation (DDA), which simulates the unseen target features by mapping
source data as augmentations through a domain transformer. Specifically, we
formulate DDA as a bi-level optimization problem and solve it through a novel
meta-learning approach in the representation space. We evaluate the proposed
method on both synthetic datasets and realworld datasets, and empirical results
show that our approach can outperform other existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures, accepted by AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self Supervision Does Not Help Natural Language Supervision at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Floris Weers, Vaishaal Shankar, Angelos Katharopoulos, Yinfei Yang, Tom Gunter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self supervision and natural language supervision have emerged as two
exciting ways to train general purpose image encoders which excel at a variety
of downstream tasks. Recent works such as M3AE and SLIP have suggested that
these approaches can be effectively combined, but most notably their results
use small pre-training datasets (<50M samples) and don't effectively reflect
the large-scale regime (>100M examples) that is commonly used for these
approaches. Here we investigate whether a similar approach can be effective
when trained with a much larger amount of data. We find that a combination of
two state of the art approaches: masked auto-encoders, MAE and contrastive
language image pre-training, CLIP provides a benefit over CLIP when trained on
a corpus of 11.3M image-text pairs, but little to no benefit (as evaluated on a
suite of common vision tasks) over CLIP when trained on a large corpus of 1.4B
images. Our work provides some much needed clarity into the effectiveness (or
lack thereof) of self supervision for large-scale image-text training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The role of noise in denoising models for anomaly detection in medical
  images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antanas Kascenas, Pedro Sanchez, Patrick Schrempf, Chaoyang Wang, William Clackett, Shadia S. Mikhael, Jeremy P. Voisey, Keith Goatman, Alexander Weir, Nicolas Pugeault, Sotirios A. Tsaftaris, Alison Q. O'Neil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pathological brain lesions exhibit diverse appearance in brain images, in
terms of intensity, texture, shape, size, and location. Comprehensive sets of
data and annotations are difficult to acquire. Therefore, unsupervised anomaly
detection approaches have been proposed using only normal data for training,
with the aim of detecting outlier anomalous voxels at test time. Denoising
methods, for instance classical denoising autoencoders (DAEs) and more recently
emerging diffusion models, are a promising approach, however naive application
of pixelwise noise leads to poor anomaly detection performance. We show that
optimization of the spatial resolution and magnitude of the noise improves the
performance of different model training regimes, with similar noise parameter
adjustments giving good performance for both DAEs and diffusion models. Visual
inspection of the reconstructions suggests that the training noise influences
the trade-off between the extent of the detail that is reconstructed and the
extent of erasure of anomalies, both of which contribute to better anomaly
detection performance. We validate our findings on two real-world datasets
(tumor detection in brain MRI and hemorrhage/ischemia/tumor detection in brain
CT), showing good detection on diverse anomaly appearances. Overall, we find
that a DAE trained with coarse noise is a fast and simple method that gives
state-of-the-art accuracy. Diffusion models applied to anomaly detection are as
yet in their infancy and provide a promising avenue for further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Medical Image Analysis special issue for MIDL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning ultrasound plane pose regression: assessing generalized pose
  coordinates in the fetal brain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chiara Di Vece, Maela Le Lous, Brian Dromey, Francisco Vasconcelos, Anna L David, Donald Peebles, Danail Stoyanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In obstetric ultrasound (US) scanning, the learner's ability to mentally
build a three-dimensional (3D) map of the fetus from a two-dimensional (2D) US
image represents a significant challenge in skill acquisition. We aim to build
a US plane localization system for 3D visualization, training, and guidance
without integrating additional sensors. This work builds on top of our previous
work, which predicts the six-dimensional (6D) pose of arbitrarily-oriented US
planes slicing the fetal brain with respect to a normalized reference frame
using a convolutional neural network (CNN) regression network. Here, we analyze
in detail the assumptions of the normalized fetal brain reference frame and
quantify its accuracy with respect to the acquisition of transventricular (TV)
standard plane (SP) for fetal biometry. We investigate the impact of
registration quality in the training and testing data and its subsequent effect
on trained models. Finally, we introduce data augmentations and larger training
sets that improve the results of our previous work, achieving median errors of
3.53 mm and 6.42 degrees for translation and rotation, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures, 2 tables. This work has been submitted to the
  IEEE for possible publication (IEEE TMRB). Copyright may be transferred
  without notice, after which this version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of the potential of Near Infrared Hyperspectral Imaging for
  monitoring the invasive brown marmorated stink bug 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Veronica Ferrari, Rosalba Calvini, Bas Boom, Camilla Menozzi, Aravind Krishnaswamy Rangarajan, Lara Maistrello, Peter Offermans, Alessandro Ulrici
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The brown marmorated stink bug (BMSB), Halyomorpha halys, is an invasive
insect pest of global importance that damages several crops, compromising
agri-food production. Field monitoring procedures are fundamental to perform
risk assessment operations, in order to promptly face crop infestations and
avoid economical losses. To improve pest management, spectral cameras mounted
on Unmanned Aerial Vehicles (UAVs) and other Internet of Things (IoT) devices,
such as smart traps or unmanned ground vehicles, could be used as an innovative
technology allowing fast, efficient and real-time monitoring of insect
infestations. The present study consists in a preliminary evaluation at the
laboratory level of Near Infrared Hyperspectral Imaging (NIR-HSI) as a possible
technology to detect BMSB specimens on different vegetal backgrounds,
overcoming the problem of BMSB mimicry. Hyperspectral images of BMSB were
acquired in the 980-1660 nm range, considering different vegetal backgrounds
selected to mimic a real field application scene. Classification models were
obtained following two different chemometric approaches. The first approach was
focused on modelling spectral information and selecting relevant spectral
regions for discrimination by means of sparse-based variable selection coupled
with Soft Partial Least Squares Discriminant Analysis (s-Soft PLS-DA)
classification algorithm. The second approach was based on modelling spatial
and spectral features contained in the hyperspectral images using Convolutional
Neural Networks (CNN). Finally, to further improve BMSB detection ability, the
two strategies were merged, considering only the spectral regions selected by
s-Soft PLS-DA for CNN modelling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted manuscript</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Open-set Segmentation with Synthetic Negative Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matej Grcić, Siniša Šegvić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-set segmentation is often conceived by complementing closed-set
classification with anomaly detection. Existing dense anomaly detectors operate
either through generative modelling of regular training data or by
discriminating with respect to negative training data. These two approaches
optimize different objectives and therefore exhibit different failure modes.
Consequently, we propose the first dense hybrid anomaly score that fuses
generative and discriminative cues. The proposed score can be efficiently
implemented by upgrading any semantic segmentation model with
translation-equivariant estimates of data likelihood and dataset posterior. Our
design is a remarkably good fit for efficient inference on large images due to
negligible computational overhead over the closed-set baseline. The resulting
dense hybrid open-set models require negative training images that can be
sampled either from an auxiliary negative dataset or from a jointly trained
generative model. We evaluate our contributions on benchmarks for dense anomaly
detection and open-set segmentation of traffic scenes. The experiments reveal
strong open-set performance in spite of negligible computational overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Models for <span class="highlight-title">Dataset</span> Drift Controls in Machine Learning With Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.02578v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.02578v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Oala, Marco Aversa, Gabriel Nobis, Kurt Willis, Yoan Neuenschwander, Michèle Buck, Christian Matek, Jerome Extermann, Enrico Pomarico, Wojciech Samek, Roderick Murray-Smith, Christoph Clausen, Bruno Sanguinetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camera images are ubiquitous in machine learning research. They also play a
central role in the delivery of important services spanning medicine and
environmental surveying. However, the application of machine learning models in
these domains has been limited because of robustness concerns. A primary
failure mode are performance drops due to differences between the training and
deployment data. While there are methods to prospectively validate the
robustness of machine learning models to such dataset drifts, existing
approaches do not account for explicit models of the primary object of
interest: the data. This limits our ability to study and understand the
relationship between data generation and downstream machine learning model
performance in a physically accurate manner. In this study, we demonstrate how
to overcome this limitation by pairing traditional machine learning with
physical optics to obtain explicit and differentiable data models. We
demonstrate how such data models can be constructed for image data and used to
control downstream machine learning model performance related to dataset drift.
The findings are distilled into three applications. First, drift synthesis
enables the controlled generation of physically faithful drift test cases to
power model selection and targeted generalization. Second, the gradient
connection between machine learning task model and data model allows advanced,
precise tolerancing of task model sensitivity to changes in the data
generation. These drift forensics can be used to precisely specify the
acceptable data environments in which a task model may be run. Third, drift
optimization opens up the possibility to create drifts that can help the task
model learn better faster, effectively optimizing the data generating process
itself. A guide to access the open code and datasets is available at
https://github.com/aiaudit-org/raw2logit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LO and MA contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Soft-labeling Strategies for Rapid Sub-Typing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.12684v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.12684v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grant Rosario, David Noever, Matt Ciolino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The challenge of labeling large example datasets for computer vision
continues to limit the availability and scope of image repositories. This
research provides a new method for automated data collection, curation,
labeling, and iterative training with minimal human intervention for the case
of overhead satellite imagery and object detection. The new operational scale
effectively scanned an entire city (68 square miles) in grid search and yielded
a prediction of car color from space observations. A partially trained yolov5
model served as an initial inference seed to output further, more refined model
predictions in iterative cycles. Soft labeling here refers to accepting label
noise as a potentially valuable augmentation to reduce overfitting and enhance
generalized predictions to previously unseen test data. The approach takes
advantage of a real-world instance where a cropped image of a car can
automatically receive sub-type information as white or colorful from pixel
values alone, thus completing an end-to-end pipeline without overdependence on
human labor.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discriminative Feature Learning through Feature Distance Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.11606v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.11606v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Schlagenhauf, Yiwen Lin, Benjamin Noack
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensembles of Convolutional neural networks have shown remarkable results in
learning discriminative semantic features for image classification tasks.
Though, the models in the ensemble often concentrate on similar regions in
images. This work proposes a novel method that forces a set of base models to
learn different features for a classification task. These models are combined
in an ensemble to make a collective classification. The key finding is that by
forcing the models to concentrate on different features, the classification
accuracy is increased. To learn different feature concepts, a so-called feature
distance loss is implemented on the feature maps. The experiments on benchmark
convolutional neural networks (VGG16, ResNet, AlexNet), popular datasets
(Cifar10, Cifar100, miniImageNet, NEU, BSD, TEX), and different training
samples (3, 5, 10, 20, 50, 100 per class) show the effectiveness of the
proposed feature loss. The proposed method outperforms classical ensemble
versions of the base models. The Class Activation Maps explicitly prove the
ability to learn different feature concepts. The code is available at:
https://github.com/2Obe/Feature-Distance-Loss.git
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-bandwidth Close-Range Information Transport through Light Pipes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06496v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06496v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joowon Lim, Jannes Gladrow, Douglas Kelly, Greg O'Shea, Govert Verkes, Ioan Stefanovici, Sebastian Nowozin, Benn Thomsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image retrieval after propagation through multi-mode fibers is gaining
attention due to their capacity to confine light and efficiently transport it
over distances in a compact system. Here, we propose a generally applicable
information-theoretic framework to transmit maximal-entropy (data) images and
maximize the information transmission over sub-meter distances, a crucial
capability that allows optical storage applications to scale and address
different parts of storage media. To this end, we use millimeter-sized square
optical waveguides to image a megapixel 8-bit spatial-light modulator. Data is
thus represented as a 2D array of 8-bit values (symbols). Transmitting 100000s
of symbols requires innovation beyond transmission matrix approaches. Deep
neural networks have been recently utilized to retrieve images, but have been
limited to small (thousands of symbols) and natural looking (low entropy)
images. We maximize information transmission by combining a bandwidth-optimized
homodyne detector with a differentiable hybrid neural-network consisting of a
digital twin of the experiment setup and a U-Net. For the digital twin, we
implement and compare a differentiable mode-based twin with a differentiable
ray-based twin. Importantly, the latter can adapt to manufacturing-related
setup imperfections during training which we show to be crucial. Our pipeline
is trained end-to-end to recover digital input images while maximizing the
achievable information page size based on a differentiable mutual-information
estimator. We demonstrate retrieval of 66 kB at maximum with 1.7 bit per symbol
on average with a range of 0.3 - 3.4 bit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Building Scalable Video Understanding Benchmarks through Sports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06866v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06866v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniket Agarwal, Alex Zhang, Karthik Narasimhan, Igor Gilitschenski, Vishvak Murahari, Yash Kant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing benchmarks for evaluating long video understanding falls short on
multiple aspects, either lacking in scale or quality of annotations. These
limitations arise from the difficulty in collecting dense annotations for long
videos (e.g. actions, dialogues, etc.), which are often obtained by manually
labeling many frames per second. In this work, we introduce an automated
Annotation and Video Stream Alignment Pipeline (abbreviated ASAP). We
demonstrate the generality of ASAP by aligning unlabeled videos of four
different sports (Cricket, Football, Basketball, and American Football) with
their corresponding dense annotations (i.e. commentary) freely available on the
web. Our human studies indicate that ASAP can align videos and annotations with
high fidelity, precision, and speed. We then leverage ASAP scalability to
create LCric, a large-scale long video understanding benchmark, with over 1000
hours of densely annotated long Cricket videos (with an average sample length
of 50 mins) collected at virtually zero annotation cost. We benchmark and
analyze state-of-the-art video understanding models on LCric through a large
set of compositional multi-choice and regression queries. We establish a human
baseline that indicates significant room for new research to explore. The
dataset along with the code for ASAP and baselines can be accessed here:
https://asap-benchmark.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dense Prediction with Attentive Feature Aggregation <span class="chip">WACV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.00770v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.00770v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yung-Hsu Yang, Thomas E. Huang, Min Sun, Samuel Rota Bulò, Peter Kontschieder, Fisher Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aggregating information from features across different layers is an essential
operation for dense prediction models. Despite its limited expressiveness,
feature concatenation dominates the choice of aggregation operations. In this
paper, we introduce Attentive Feature Aggregation (AFA) to fuse different
network layers with more expressive non-linear operations. AFA exploits both
spatial and channel attention to compute weighted average of the layer
activations. Inspired by neural volume rendering, we extend AFA with
Scale-Space Rendering (SSR) to perform late fusion of multi-scale predictions.
AFA is applicable to a wide range of existing network designs. Our experiments
show consistent and significant improvements on challenging semantic
segmentation benchmarks, including Cityscapes, BDD100K, and Mapillary Vistas,
at negligible computational and parameter overhead. In particular, AFA improves
the performance of the Deep Layer Aggregation (DLA) model by nearly 6% mIoU on
Cityscapes. Our experimental analyses show that AFA learns to progressively
refine segmentation maps and to improve boundary details, leading to new
state-of-the-art results on boundary detection benchmarks on BSDS500 and
NYUDv2. Code and video resources are available at http://vis.xyz/pub/dla-afa.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 14 figures, WACV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Motion Matters: A Novel Motion Modeling For Cross-View Gait Feature
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.11817v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.11817v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingqi Li, Jiaqi Gao, Yuzhen Zhang, Hongming Shan, Junping Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a unique biometric that can be perceived at a distance, gait has broad
applications in person authentication, social security, and so on. Existing
gait recognition methods suffer from changes in viewpoint and clothing and
barely consider extracting diverse motion features, a fundamental
characteristic in gaits, from gait sequences. This paper proposes a novel
motion modeling method to extract the discriminative and robust representation.
Specifically, we first extract the motion features from the encoded motion
sequences in the shallow layer. Then we continuously enhance the motion feature
in deep layers. This motion modeling approach is independent of mainstream work
in building network architectures. As a result, one can apply this motion
modeling method to any backbone to improve gait recognition performance. In
this paper, we combine motion modeling with one commonly used backbone~(GaitGL)
as GaitGL-M to illustrate motion modeling. Extensive experimental results on
two commonly-used cross-view gait datasets demonstrate the superior performance
of GaitGL-M over existing state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Zero-Shot Learning for Semantic Segmentation of 3D Point
  Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.06230v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.06230v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Björn Michele, Alexandre Boulch, Gilles Puy, Maxime Bucher, Renaud Marlet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While there has been a number of studies on Zero-Shot Learning (ZSL) for 2D
images, its application to 3D data is still recent and scarce, with just a few
methods limited to classification. We present the first generative approach for
both ZSL and Generalized ZSL (GZSL) on 3D data, that can handle both
classification and, for the first time, semantic segmentation. We show that it
reaches or outperforms the state of the art on ModelNet40 classification for
both inductive ZSL and inductive GZSL. For semantic segmentation, we created
three benchmarks for evaluating this new ZSL task, using S3DIS, ScanNet and
SemanticKITTI. Our experiments show that our method outperforms strong
baselines, which we additionally propose for this task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For the published code, see https://github.com/valeoai/3DGenZ</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TruFor: Leveraging all-round clues for trustworthy image forgery
  detection and localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10957v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10957v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabrizio Guillaro, Davide Cozzolino, Avneesh Sud, Nicholas Dufour, Luisa Verdoliva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present TruFor, a forensic framework that can be applied to
a large variety of image manipulation methods, from classic cheapfakes to more
recent manipulations based on deep learning. We rely on the extraction of both
high-level and low-level traces through a transformer-based fusion architecture
that combines the RGB image and a learned noise-sensitive fingerprint. The
latter learns to embed the artifacts related to the camera internal and
external processing by training only on real data in a self-supervised manner.
Forgeries are detected as deviations from the expected regular pattern that
characterizes each pristine image. Looking for anomalies makes the approach
able to robustly detect a variety of local manipulations, ensuring
generalization. In addition to a pixel-level localization map and a whole-image
integrity score, our approach outputs a reliability map that highlights areas
where localization predictions may be error-prone. This is particularly
important in forensic applications in order to reduce false alarms and allow
for a large scale analysis. Extensive experiments on several datasets show that
our method is able to reliably detect and localize both cheapfakes and
deepfakes manipulations outperforming state-of-the-art works. Code will be
publicly available at https://grip-unina.github.io/TruFor/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LWSIS: LiDAR-guided Weakly Supervised Instance Segmentation for
  Autonomous Driving <span class="chip">AAAI2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.03504v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.03504v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Junbo Yin, Botian Shi, Yikang Li, Ruigang Yang, Jianbing Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image instance segmentation is a fundamental research topic in autonomous
driving, which is crucial for scene understanding and road safety. Advanced
learning-based approaches often rely on the costly 2D mask annotations for
training. In this paper, we present a more artful framework, LiDAR-guided
Weakly Supervised Instance Segmentation (LWSIS), which leverages the
off-the-shelf 3D data, i.e., Point Cloud, together with the 3D boxes, as
natural weak supervisions for training the 2D image instance segmentation
models. Our LWSIS not only exploits the complementary information in multimodal
data during training, but also significantly reduces the annotation cost of the
dense 2D masks. In detail, LWSIS consists of two crucial modules, Point Label
Assignment (PLA) and Graph-based Consistency Regularization (GCR). The former
module aims to automatically assign the 3D point cloud as 2D point-wise labels,
while the latter further refines the predictions by enforcing geometry and
appearance consistency of the multimodal data. Moreover, we conduct a secondary
instance segmentation annotation on the nuScenes, named nuInsSeg, to encourage
further research on multimodal perception tasks. Extensive experiments on the
nuInsSeg, as well as the large-scale Waymo, show that LWSIS can substantially
improve existing weakly supervised segmentation models by only involving 3D
data during training. Additionally, LWSIS can also be incorporated into 3D
object detectors like PointPainting to boost the 3D detection performance for
free. The code and dataset are available at https://github.com/Serenos/LWSIS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedPara: Low-Rank Hadamard Product for Communication-Efficient Federated
  Learning <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.06098v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.06098v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nam Hyeon-Woo, Moon Ye-Bin, Tae-Hyun Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a communication-efficient parameterization, FedPara,
for federated learning (FL) to overcome the burdens on frequent model uploads
and downloads. Our method re-parameterizes weight parameters of layers using
low-rank weights followed by the Hadamard product. Compared to the conventional
low-rank parameterization, our FedPara method is not restricted to low-rank
constraints, and thereby it has a far larger capacity. This property enables to
achieve comparable performance while requiring 3 to 10 times lower
communication costs than the model with the original layers, which is not
achievable by the traditional low-rank methods. The efficiency of our method
can be further improved by combining with other efficient FL optimizers. In
addition, we extend our method to a personalized FL application, pFedPara,
which separates parameters into global and local ones. We show that pFedPara
outperforms competing personalized FL methods with more than three times fewer
parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Training Vision Language <span class="highlight-title">BERT</span>s with a Unified Conditional Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.02010v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.02010v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Yang, Fengmao Lv, Fayao Liu, Guosheng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language BERTs are trained with language corpus in a self-supervised
manner. Unlike natural language BERTs, vision language BERTs need paired data
to train, which restricts the scale of VL-BERT pretraining. We propose a
self-training approach that allows training VL-BERTs from unlabeled image data.
The proposed method starts with our unified conditional model -- a vision
language BERT model that can perform zero-shot conditional generation. Given
different conditions, the unified conditional model can generate captions,
dense captions, and even questions. We use the labeled image data to train a
teacher model and use the trained model to generate pseudo captions on
unlabeled image data. We then combine the labeled data and pseudo labeled data
to train a student model. The process is iterated by putting the student model
as a new teacher. By using the proposed self-training approach and only 300k
unlabeled extra data, we are able to get competitive or even better
performances compared to the models of similar model size trained with 3
million extra image data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MN-Pair Contrastive Damage Representation and Clustering for Prognostic
  Explanation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06077v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06077v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takato Yasuno, Masahiro Okano, Junichiro Fujii
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is essential for infrastructure managers to maintain a high standard to
ensure user satisfaction during daily operations. Surveillance cameras and
drone inspections have enabled progress toward automating the inspection of
damaged features and assessing the health condition of the deterioration. When
we prepare a pair of raw images and damage class labels, we can train
supervised learning toward the predefined damage grade, displacement. However,
such a damage representation does not constantly match the predefined classes
of damage grade, hence, there may be some detailed clusters from the unseen
damage space or more complex clusters from overlapped space between two damage
grades. The damage representation has fundamentally complex features,
consequently, all the damage classes could not be perfectly predefined. Our
proposed MN-pair contrastive learning method enables us to explore the
embedding damage representation beyond the predefined classes including more
detailed clusters. It maximizes the similarity of M-1 positive images close to
the anchor, and simultaneously maximize the dissimilarity of N-1 negative ones,
using both weighting loss functions. It has been learning faster than the
N-pair algorithm, instead of using one positive image. We propose a pipeline to
learn damage representation and use density-based clustering on the 2-D
reduction space to automate finer cluster discrimination. We also visualize the
explanation of the damage feature using Grad-CAM for MN-pair damage metric
learning. We demonstrate our method in three experimental studies: steel
product defect, concrete crack of deck and pavement, and sewer pipe defect and
mention its effectiveness and discuss potential future works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 14 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Skeletal Video Anomaly Detection using Deep Learning: <span class="highlight-title">Survey</span>, Challenges
  and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pratik K. Mishra, Alex Mihailidis, Shehroz S. Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The existing methods for video anomaly detection mostly utilize videos
containing identifiable facial and appearance-based features. The use of videos
with identifiable faces raises privacy concerns, especially when used in a
hospital or community-based setting. Appearance-based features can also be
sensitive to pixel-based noise, straining the anomaly detection methods to
model the changes in the background and making it difficult to focus on the
actions of humans in the foreground. Structural information in the form of
skeletons describing the human motion in the videos is privacy-protecting and
can overcome some of the problems posed by appearance-based features. In this
paper, we present a survey of privacy-protecting deep learning anomaly
detection methods using skeletons extracted from videos. We present a novel
taxonomy of algorithms based on the various learning approaches. We conclude
that skeleton-based approaches for anomaly detection can be a plausible
privacy-protecting alternative for video anomaly detection. Lastly, we identify
major open research questions and provide guidelines to address them.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boosting Few-shot Fine-grained Recognition with Background Suppression
  and Foreground Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01439v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01439v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zican Zha, Hao Tang, Yunlian Sun, Jinhui Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot fine-grained recognition (FS-FGR) aims to recognize novel
fine-grained categories with the help of limited available samples.
Undoubtedly, this task inherits the main challenges from both few-shot learning
and fine-grained recognition. First, the lack of labeled samples makes the
learned model easy to overfit. Second, it also suffers from high intra-class
variance and low inter-class differences in the datasets. To address this
challenging task, we propose a two-stage background suppression and foreground
alignment framework, which is composed of a background activation suppression
(BAS) module, a foreground object alignment (FOA) module, and a local-to-local
(L2L) similarity metric. Specifically, the BAS is introduced to generate a
foreground mask for localization to weaken background disturbance and enhance
dominative foreground objects. The FOA then reconstructs the feature map of
each support sample according to its correction to the query ones, which
addresses the problem of misalignment between support-query image pairs. To
enable the proposed method to have the ability to capture subtle differences in
confused samples, we present a novel L2L similarity metric to further measure
the local similarity between a pair of aligned spatial features in the
embedding space. What's more, considering that background interference brings
poor robustness, we infer the pairwise similarity of feature maps using both
the raw image and the refined image. Extensive experiments conducted on
multiple popular fine-grained benchmarks demonstrate that our method
outperforms the existing state of the art by a large margin. The source codes
are available at: https://github.com/CSer-Tang-hao/BSFA-FSFG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Circuits and Systems for Video
  Technology (TCSVT) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ General Greedy De-bias Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.10572v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.10572v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinzhe Han, Shuhui Wang, Chi Su, Qingming Huang, Qi Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks often make predictions relying on the spurious correlations
from the datasets rather than the intrinsic properties of the task of interest,
facing sharp degradation on out-of-distribution (OOD) test data. Existing
de-bias learning frameworks try to capture specific dataset bias by annotations
but they fail to handle complicated OOD scenarios. Others implicitly identify
the dataset bias by special design low capability biased models or losses, but
they degrade when the training and testing data are from the same distribution.
In this paper, we propose a General Greedy De-bias learning framework (GGD),
which greedily trains the biased models and the base model. The base model is
encouraged to focus on examples that are hard to solve with biased models, thus
remaining robust against spurious correlations in the test stage. GGD largely
improves models' OOD generalization ability on various tasks, but sometimes
over-estimates the bias level and degrades on the in-distribution test. We
further re-analyze the ensemble process of GGD and introduce the Curriculum
Regularization inspired by curriculum learning, which achieves a good trade-off
between in-distribution and out-of-distribution performance. Extensive
experiments on image classification, adversarial question answering, and visual
question answering demonstrate the effectiveness of our method. GGD can learn a
more robust base model under the settings of both task-specific biased models
with prior knowledge and self-ensemble biased model without prior knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted by IEEE T-PAMI. Copyright is transferred
  without notice, after which this version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAFE: Sensitivity-Aware Features for Out-of-Distribution Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.13930v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.13930v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Wilson, Tobias Fischer, Feras Dayoub, Dimity Miller, Niko Sünderhauf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature-based out-of-distribution (OOD) detectors have received significant
attention under the image classification setting lately. However, the
practicality of these works in the object detection setting is limited due to
the current lack of understanding of the characteristics of the feature space
in this setting. Our approach, SAFE (Sensitivity-Aware FEatures), leverages the
innate sensitivity of residual networks to detect OOD samples. Key to our
method, we build on foundational theory from image classification to identify
that shortcut convolutional layers followed immediately by batch normalisation
are uniquely powerful at detecting OOD samples. SAFE circumvents the need for
realistic OOD training data, expensive generative models and retraining of the
base object detector by training a 3-layer multilayer perceptron (MLP) on the
surrogate task of distinguishing noise-perturbed and clean in-distribution
object detections, using only the concatenated features from the identified
most sensitive layers. We show that this MLP can identify OOD object detections
more reliably than previous approaches, achieving a new state-of-the-art on
multiple benchmarks, e.g. reducing the FPR95 by an absolute 30% from 48.3% to
18.4% on the OpenImages dataset. We provide empirical evidence for our claims
through our ablations, demonstrating that the identified critical subset of
layers is disproportionately powerful at detecting OOD samples in comparison to
the rest of the network.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Vision <span class="highlight-title">Transformer</span>s with HiLo Attention <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.13213v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.13213v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zizheng Pan, Jianfei Cai, Bohan Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers (ViTs) have triggered the most recent and significant
breakthroughs in computer vision. Their efficient designs are mostly guided by
the indirect metric of computational complexity, i.e., FLOPs, which however has
a clear gap with the direct metric such as throughput. Thus, we propose to use
the direct speed evaluation on the target platform as the design principle for
efficient ViTs. Particularly, we introduce LITv2, a simple and effective ViT
which performs favourably against the existing state-of-the-art methods across
a spectrum of different model sizes with faster speed. At the core of LITv2 is
a novel self-attention mechanism, which we dub HiLo. HiLo is inspired by the
insight that high frequencies in an image capture local fine details and low
frequencies focus on global structures, whereas a multi-head self-attention
layer neglects the characteristic of different frequencies. Therefore, we
propose to disentangle the high/low frequency patterns in an attention layer by
separating the heads into two groups, where one group encodes high frequencies
via self-attention within each local window, and another group encodes low
frequencies by performing global attention between the average-pooled
low-frequency keys and values from each window and each query position in the
input feature map. Benefiting from the efficient design for both groups, we
show that HiLo is superior to the existing attention mechanisms by
comprehensively benchmarking FLOPs, speed and memory consumption on GPUs and
CPUs. For example, HiLo is 1.4x faster than spatial reduction attention and
1.6x faster than local window attention on CPUs. Powered by HiLo, LITv2 serves
as a strong backbone for mainstream vision tasks including image
classification, dense detection and segmentation. Code is available at
https://github.com/ziplab/LITv2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022 camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hi-LASSIE: High-Fidelity Articulated Shape and Skeleton Discovery from
  Sparse Image Ensemble 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.11042v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.11042v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chun-Han Yao, Wei-Chih Hung, Yuanzhen Li, Michael Rubinstein, Ming-Hsuan Yang, Varun Jampani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatically estimating 3D skeleton, shape, camera viewpoints, and part
articulation from sparse in-the-wild image ensembles is a severely
under-constrained and challenging problem. Most prior methods rely on
large-scale image datasets, dense temporal correspondence, or human annotations
like camera pose, 2D keypoints, and shape templates. We propose Hi-LASSIE,
which performs 3D articulated reconstruction from only 20-30 online images in
the wild without any user-defined shape or skeleton templates. We follow the
recent work of LASSIE that tackles a similar problem setting and make two
significant advances. First, instead of relying on a manually annotated 3D
skeleton, we automatically estimate a class-specific skeleton from the selected
reference image. Second, we improve the shape reconstructions with novel
instance-specific optimization strategies that allow reconstructions to
faithful fit on each instance while preserving the class-specific priors
learned across all images. Experiments on in-the-wild image ensembles show that
Hi-LASSIE obtains higher fidelity state-of-the-art 3D reconstructions despite
requiring minimum user input.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://chhankyao.github.io/hi-lassie/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WaveMix: A Resource-efficient Neural Network for Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.14375v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.14375v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Jeevan, Kavitha Viswanathan, Anandu A S, Amit Sethi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To allow image analysis in resource-constrained scenarios without
compromising generalizability, we introduce WaveMix -- a novel and flexible
neural framework that reduces the GPU RAM (memory) and compute (latency)
compared to CNNs and transformers. In addition to using convolutional layers
that exploit shift-invariant image statistics, the proposed framework uses
multi-level two-dimensional discrete wavelet transform (2D-DWT) modules to
exploit scale-invariance and edge sparseness, which gives it the following
advantages. Firstly, the fixed weights of wavelet modules do not add to the
parameter count while reorganizing information based on these image priors.
Secondly, the wavelet modules scale the spatial extents of feature maps by
integral powers of $\frac{1}{2}\times\frac{1}{2}$, which reduces the memory and
latency required for forward and backward passes. Finally, a multi-level 2D-DWT
leads to a quicker expansion of the receptive field per layer than pooling
(which we do not use) and it is a more effective spatial token mixer. WaveMix
also generalizes better than other token mixing models, such as ConvMixer,
MLP-Mixer, PoolFormer, random filters, and Fourier basis, because the wavelet
transform is much better suited for image decomposition and spatial token
mixing. WaveMix is a flexible model that can perform well on multiple image
tasks without needing architectural modifications. WaveMix achieves a semantic
segmentation mIoU of 83% on the Cityscapes validation set outperforming
transformer and CNN-based architectures. We also demonstrate the advantages of
WaveMix for classification on multiple datasets and show that WaveMix
establishes new state-of-the-results in Places-365, EMNIST, and iNAT-mini
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures. arXiv admin note: text overlap with
  arXiv:2203.03689</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NAS-Bench-360: Benchmarking Neural Architecture Search on Diverse Tasks <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.05668v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.05668v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renbo Tu, Nicholas Roberts, Mikhail Khodak, Junhong Shen, Frederic Sala, Ameet Talwalkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing neural architecture search (NAS) benchmarks and algorithms
prioritize well-studied tasks, e.g. image classification on CIFAR or ImageNet.
This makes the performance of NAS approaches in more diverse areas poorly
understood. In this paper, we present NAS-Bench-360, a benchmark suite to
evaluate methods on domains beyond those traditionally studied in architecture
search, and use it to address the following question: do state-of-the-art NAS
methods perform well on diverse tasks? To construct the benchmark, we curate
ten tasks spanning a diverse array of application domains, dataset sizes,
problem dimensionalities, and learning objectives. Each task is carefully
chosen to interoperate with modern CNN-based search methods while possibly
being far-afield from its original development domain. To speed up and reduce
the cost of NAS research, for two of the tasks we release the precomputed
performance of 15,625 architectures comprising a standard CNN search space.
Experimentally, we show the need for more robust NAS evaluation of the kind
NAS-Bench-360 enables by showing that several modern NAS procedures perform
inconsistently across the ten tasks, with many catastrophically poor results.
We also demonstrate how NAS-Bench-360 and its associated precomputed results
will enable future scientific discoveries by testing whether several recent
hypotheses promoted in the NAS literature hold on diverse tasks. NAS-Bench-360
is hosted at https://nb360.ml.cmu.edu.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022 Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HITNet: Hierarchical Iterative Tile Refinement Network for Real-time
  Stereo Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2007.12140v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2007.12140v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladimir Tankovich, Christian Häne, Yinda Zhang, Adarsh Kowdle, Sean Fanello, Sofien Bouaziz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents HITNet, a novel neural network architecture for real-time
stereo matching. Contrary to many recent neural network approaches that operate
on a full cost volume and rely on 3D convolutions, our approach does not
explicitly build a volume and instead relies on a fast multi-resolution
initialization step, differentiable 2D geometric propagation and warping
mechanisms to infer disparity hypotheses. To achieve a high level of accuracy,
our network not only geometrically reasons about disparities but also infers
slanted plane hypotheses allowing to more accurately perform geometric warping
and upsampling operations. Our architecture is inherently multi-resolution
allowing the propagation of information across different levels. Multiple
experiments prove the effectiveness of the proposed approach at a fraction of
the computation required by state-of-the-art methods. At the time of writing,
HITNet ranks 1st-3rd on all the metrics published on the ETH3D website for two
view stereo, ranks 1st on most of the metrics among all the end-to-end learning
approaches on Middlebury-v3, ranks 1st on the popular KITTI 2012 and 2015
benchmarks among the published methods faster than 100ms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The pretrained models used for submission to benchmarks and sample
  evaluation scripts can be found at
  https://github.com/google-research/google-research/tree/master/hitnet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Seamless Multimodal Biometrics for Continuous Personalised Wellbeing
  Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.03045v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.03045v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Ribeiro Pinto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificially intelligent perception is increasingly present in the lives of
every one of us. Vehicles are no exception, (...) In the near future, pattern
recognition will have an even stronger role in vehicles, as self-driving cars
will require automated ways to understand what is happening around (and within)
them and act accordingly. (...) This doctoral work focused on advancing
in-vehicle sensing through the research of novel computer vision and pattern
recognition methodologies for both biometrics and wellbeing monitoring. The
main focus has been on electrocardiogram (ECG) biometrics, a trait well-known
for its potential for seamless driver monitoring. Major efforts were devoted to
achieving improved performance in identification and identity verification in
off-the-person scenarios, well-known for increased noise and variability. Here,
end-to-end deep learning ECG biometric solutions were proposed and important
topics were addressed such as cross-database and long-term performance,
waveform relevance through explainability, and interlead conversion. Face
biometrics, a natural complement to the ECG in seamless unconstrained
scenarios, was also studied in this work. The open challenges of masked face
recognition and interpretability in biometrics were tackled in an effort to
evolve towards algorithms that are more transparent, trustworthy, and robust to
significant occlusions. Within the topic of wellbeing monitoring, improved
solutions to multimodal emotion recognition in groups of people and
activity/violence recognition in in-vehicle scenarios were proposed. At last,
we also proposed a novel way to learn template security within end-to-end
models, dismissing additional separate encryption processes, and a
self-supervised learning approach tailored to sequential data, in order to
ensure data security and optimal performance. (...)
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Doctoral thesis presented and approved on the 21st of December 2022
  to the University of Porto</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TTT-UCDR: Test-time Training for Universal Cross-Domain Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.09198v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.09198v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumava Paul, Titir Dutta, Aheli Saha, Abhishek Samanta, Soma Biswas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image retrieval under generalized test scenarios has gained significant
momentum in literature, and the recently proposed protocol of Universal
Cross-domain Retrieval is a pioneer in this direction. A common practice in any
such generalized classification or retrieval algorithm is to exploit samples
from multiple domains during training to learn a domain-invariant
representation of data. Such criterion is often restrictive, and thus in this
work, for the first time, we explore the challenges associated with generalized
retrieval problems under a low-data regime, which is quite relevant in many
real-world scenarios. We attempt to make any retrieval model trained on a small
cross-domain dataset (containing just two training domains) more generalizable
towards any unknown query domain or category by quickly adapting it to the test
data during inference. This form of test-time training or adaptation of the
retrieval model is explored by means of a number of self-supervision-based loss
functions, for example, Rotnet, Jigsaw-puzzle, Barlow twins, etc., in this
work. Extensive experiments on multiple large-scale datasets demonstrate the
effectiveness of the proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 1 figure, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Look, Listen, and Attack: Backdoor Attacks Against Video Action
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00986v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00986v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hasan Abed Al Kader Hammoud, Shuming Liu, Mohammed Alkhrashi, Fahad AlBalawi, Bernard Ghanem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) are vulnerable to a class of attacks called
"backdoor attacks", which create an association between a backdoor trigger and
a target label the attacker is interested in exploiting. A backdoored DNN
performs well on clean test images, yet persistently predicts an
attacker-defined label for any sample in the presence of the backdoor trigger.
Although backdoor attacks have been extensively studied in the image domain,
there are very few works that explore such attacks in the video domain, and
they tend to conclude that image backdoor attacks are less effective in the
video domain. In this work, we revisit the traditional backdoor threat model
and incorporate additional video-related aspects to that model. We show that
poisoned-label image backdoor attacks could be extended temporally in two ways,
statically and dynamically, leading to highly effective attacks in the video
domain. In addition, we explore natural video backdoors to highlight the
seriousness of this vulnerability in the video domain. And, for the first time,
we study multi-modal (audiovisual) backdoor attacks against video action
recognition models, where we show that attacking a single modality is enough
for achieving a high attack success rate.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ New Metrics to Encourage Innovation and Diversity in Information
  Retrieval Approaches <span class="chip">ECIR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08062v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08062v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehmet Deniz Türkmen, Matthew Lease, Mucahid Kutlu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In evaluation campaigns, participants often explore variations of popular,
state-of-the-art baselines as a low-risk strategy to achieve competitive
results. While effective, this can lead to local "hill climbing" rather than
more radical and innovative departure from standard methods. Moreover, if many
participants build on similar baselines, the overall diversity of approaches
considered may be limited. In this work, we propose a new class of IR
evaluation metrics intended to promote greater diversity of approaches in
evaluation campaigns. Whereas traditional IR metrics focus on user experience,
our two "innovation" metrics instead reward exploration of more divergent,
higher-risk strategies finding relevant documents missed by other systems.
Experiments on four TREC collections show that our metrics do change system
rankings by rewarding systems that find such rare, relevant documents. This
result is further supported by a controlled, synthetic data experiment, and a
qualitative analysis. In addition, we show that our metrics achieve higher
evaluation stability and discriminative power than the standard metrics we
modify. To support reproducibility, we share our source code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages + references, 6 figures, to be published in ECIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From 10 Blue Links Pages to Feature-Full Search Engine Results Pages --
  Analysis of the Temporal Evolution of SERP Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        B. Oliveira, C. T. Lopes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Web Search Engine Results Pages (SERP) are one of the most well-known and
used web pages. These pages have started as simple ``10 blue links'' pages, but
the information in SERP currently goes way beyond these links. Several features
have been included in these pages to complement organic and sponsored results
and attempt to provide answers to the query instead of just pointing to
websites that might deliver that information. In this work, we analyze the
appearance and evolution of SERP features in the two leading web search
engines, Google Search and Microsoft Bing. Using a sample of SERP from the
Internet Archive, we analyzed the appearance and evolution of these features.
We found that SERP are becoming more diverse in terms of elements, aggregating
content from different verticals and including more features that provide
direct answers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, CHIIR 2023 Conference Short Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Keyword Embeddings for Query Suggestion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jorge Gabín, M. Eduardo Ares, Javier Parapar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, search engine users commonly rely on query suggestions to improve
their initial inputs. Current systems are very good at recommending lexical
adaptations or spelling corrections to users' queries. However, they often
struggle to suggest semantically related keywords given a user's query. The
construction of a detailed query is crucial in some tasks, such as legal
retrieval or academic search. In these scenarios, keyword suggestion methods
are critical to guide the user during the query formulation. This paper
proposes two novel models for the keyword suggestion task trained on scientific
literature. Our techniques adapt the architecture of Word2Vec and FastText to
generate keyword embeddings by leveraging documents' keyword co-occurrence.
Along with these models, we also present a specially tailored negative sampling
approach that exploits how keywords appear in academic publications. We devise
a ranking-based evaluation methodology following both known-item and ad-hoc
search scenarios. Finally, we evaluate our proposals against the
state-of-the-art word and sentence embedding models showing considerable
improvements over the baselines for the tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Job recommendations: benchmarking of collaborative filtering methods for
  classifieds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Kwieciński, Agata Filipowska, Tomasz Górecki, Viacheslav Dubrov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classifieds provide many challenges for recommendation methods, due to the
limited information regarding users and items. In this paper, we explore
recommendation methods for classifieds using the example of OLX Jobs.
  The goal of the paper is to benchmark different recommendation methods for
jobs classifieds in order to improve advertisements' conversion rate and user
satisfaction.
  In our research, we implemented methods that are scalable and represent
different approaches to recommendation, namely ALS, LightFM, Prod2Vec, RP3beta,
and SLIM. We performed a laboratory comparison of methods with regard to
accuracy, diversity, and scalability (memory and time consumption during
training and in prediction). Online A/B tests were also carried out by sending
millions of messages with recommendations to evaluate models in a real-world
setting.
  In addition, we have published the dataset that we created for the needs of
our research. To the best of our knowledge, this is the first dataset of this
kind. The dataset contains 65,502,201 events performed on OLX Jobs by 3,295,942
users, who interacted with (displayed, replied to, or bookmarked) 185,395 job
ads in two weeks of 2020.
  We demonstrate that RP3beta, SLIM, and ALS perform significantly better than
Prod2Vec and LightFM when tested in a laboratory setting. Online A/B tests also
demonstrated that sending messages with recommendations generated by the ALS
and RP3beta models increases the number of users contacting advertisers.
Additionally, RP3beta had a 20% greater impact on this metric than ALS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FE-TCM: Filter-Enhanced <span class="highlight-title">Transformer</span> Click Model for Web Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingfei Wang, Jianping Liu, Meng Wang, Xintao Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constructing click models and extracting implicit relevance feedback
information from the interaction between users and search engines are very
important to improve the ranking of search results. Using neural network to
model users' click behaviors has become one of the effective methods to
construct click models. In this paper, We use Transformer as the backbone
network of feature extraction, add filter layer innovatively, and propose a new
Filter-Enhanced Transformer Click Model (FE-TCM) for web search. Firstly, in
order to reduce the influence of noise on user behavior data, we use the
learnable filters to filter log noise. Secondly, following the examination
hypothesis, we model the attraction estimator and examination predictor
respectively to output the attractiveness scores and examination probabilities.
A novel transformer model is used to learn the deeper representation among
different features. Finally, we apply the combination functions to integrate
attractiveness scores and examination probabilities into the click prediction.
From our experiments on two real-world session datasets, it is proved that
FE-TCM outperforms the existing click models for the click prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is it worth it? Comparing six deep and classical methods for
  unsupervised anomaly detection in time series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.11080v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.11080v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ferdinand Rewicki, Joachim Denzler, Julia Niebling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting anomalies in time series data is important in a variety of fields,
including system monitoring, healthcare, and cybersecurity. While the abundance
of available methods makes it difficult to choose the most appropriate method
for a given application, each method has its strengths in detecting certain
types of anomalies. In this study, we compare six unsupervised anomaly
detection methods of varying complexity to determine whether more complex
methods generally perform better and if certain methods are better suited to
certain types of anomalies. We evaluated the methods using the UCR anomaly
archive, a recent benchmark dataset for anomaly detection. We analyzed the
results on a dataset and anomaly type level after adjusting the necessary
hyperparameters for each method. Additionally, we assessed the ability of each
method to incorporate prior knowledge about anomalies and examined the
differences between point-wise and sequence-wise features. Our experiments show
that classical machine learning methods generally outperform deep learning
methods across a range of anomaly types.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 Pages, The repository to reproduce the results is available at
  https://gitlab.com/dlr-dw/is-it-worth-it-benchmark</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TINKER: A framework for Open source Cy<span class="highlight-title">bert</span>hreat Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.05571v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.05571v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nidhi Rastogi, Sharmishtha Dutta, Mohammed J. Zaki, Alex Gittens, Charu Aggarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Threat intelligence on malware attacks and campaigns is increasingly being
shared with other security experts for a cost or for free. Other security
analysts use this intelligence to inform them of indicators of compromise,
attack techniques, and preventative actions. Security analysts prepare threat
analysis reports after investigating an attack, an emerging cyber threat, or a
recently discovered vulnerability. Collectively known as cyber threat
intelligence (CTI), the reports are typically in an unstructured format and,
therefore, challenging to integrate seamlessly into existing intrusion
detection systems. This paper proposes a framework that uses the aggregated CTI
for analysis and defense at scale. The information is extracted and stored in a
structured format using knowledge graphs such that the semantics of the threat
intelligence can be preserved and shared at scale with other security analysts.
Specifically, we propose the first semi-supervised open-source knowledge
graph-based framework, TINKER, to capture cyber threat information and its
context. Following TINKER, we generate a Cyberthreat Intelligence Knowledge
Graph (CTI-KG) and demonstrate the usage using different use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> <span class="highlight-title">Self-Supervised</span> Learning from Images with a Joint-Embedding Predictive
  Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, <span class="highlight-author">Yann LeCun</span>, Nicolas Ballas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper demonstrates an approach for learning highly semantic image
representations without relying on hand-crafted data-augmentations. We
introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a
non-generative approach for self-supervised learning from images. The idea
behind I-JEPA is simple: from a single context block, predict the
representations of various target blocks in the same image. A core design
choice to guide I-JEPA towards producing semantic representations is the
masking strategy; specifically, it is crucial to (a) predict several target
blocks in the image, (b) sample target blocks with sufficiently large scale
(occupying 15%-20% of the image), and (c) use a sufficiently informative
(spatially distributed) context block. Empirically, when combined with Vision
Transformers, we find I-JEPA to be highly scalable. For instance, we train a
ViT-Huge/16 on ImageNet using 32 A100 GPUs in under 38 hours to achieve strong
downstream performance across a wide range of tasks requiring various levels of
abstraction, from linear classification to object counting and depth
prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Pricing and Hedging of High Dimensional American Options Using
  Recurrent Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Na, Justin Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a deep Recurrent neural network (RNN) framework for computing
prices and deltas of American options in high dimensions. Our proposed
framework uses two deep RNNs, where one network learns the price and the other
learns the delta of the option for each timestep. Our proposed framework yields
prices and deltas for the entire spacetime, not only at a given point (e.g. t =
0). The computational cost of the proposed approach is linear in time, which
improves on the quadratic time seen for feedforward networks that price
American options. The computational memory cost of our method is constant in
memory, which is an improvement over the linear memory costs seen in
feedforward networks. Our numerical simulations demonstrate these
contributions, and show that the proposed deep RNN framework is computationally
more efficient than traditional feedforward neural network frameworks in time
and memory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Score-based Causal Representation Learning with Interventions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Burak Varici, Emre Acarturk, Karthikeyan Shanmugam, Abhishek Kumar, Ali Tajer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies causal representation learning problem when the latent
causal variables are observed indirectly through an unknown linear
transformation. The objectives are: (i) recovering the unknown linear
transformation (up to scaling and ordering), and (ii) determining the directed
acyclic graph (DAG) underlying the latent variables. Since identifiable
representation learning is impossible based on only observational data, this
paper uses both observational and interventional data. The interventional data
is generated under distinct single-node randomized hard and soft interventions.
These interventions are assumed to cover all nodes in the latent space. It is
established that the latent DAG structure can be recovered under soft
randomized interventions via the following two steps. First, a set of
transformation candidates is formed by including all inverting transformations
corresponding to which the \emph{score} function of the transformed variables
has the minimal number of coordinates that change between an interventional and
the observational environment summed over all pairs. Subsequently, this set is
distilled using a simple constraint to recover the latent DAG structure. For
the special case of hard randomized interventions, with an additional
hypothesis testing step, one can also uniquely recover the linear
transformation, up to scaling and a valid causal ordering. These results
generalize the recent results that either assume deterministic hard
interventions or linear causal relationships in the latent space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion-based Conditional ECG Generation with Structured State Space
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08227v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08227v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Miguel Lopez Alcaraz, Nils Strodthoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic data generation is a promising solution to address privacy issues
with the distribution of sensitive health data. Recently, diffusion models have
set new standards for generative models for different data modalities. Also
very recently, structured state space models emerged as a powerful modeling
paradigm to capture long-term dependencies in time series. We put forward
SSSD-ECG, as the combination of these two technologies, for the generation of
synthetic 12-lead electrocardiograms conditioned on more than 70 ECG
statements. Due to a lack of reliable baselines, we also propose conditional
variants of two state-of-the-art unconditional generative models. We thoroughly
evaluate the quality of the generated samples, by evaluating pretrained
classifiers on the generated data and by evaluating the performance of a
classifier trained only on synthetic data, where SSSD-ECG clearly outperforms
its GAN-based competitors. We demonstrate the soundness of our approach through
further experiments, including conditional class interpolation and a clinical
Turing test demonstrating the high quality of the SSSD-ECG samples across a
wide range of conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tight Guarantees for Interactive Decision Making with the
  Decision-Estimation Coefficient 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan J. Foster, Noah Golowich, Yanjun Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A foundational problem in reinforcement learning and interactive decision
making is to understand what modeling assumptions lead to sample-efficient
learning guarantees, and what algorithm design principles achieve optimal
sample complexity. Recently, Foster et al. (2021) introduced the
Decision-Estimation Coefficient (DEC), a measure of statistical complexity
which leads to upper and lower bounds on the optimal sample complexity for a
general class of problems encompassing bandits and reinforcement learning with
function approximation. In this paper, we introduce a new variant of the DEC,
the Constrained Decision-Estimation Coefficient, and use it to derive new lower
bounds that improve upon prior work on three fronts:
  - They hold in expectation, with no restrictions on the class of algorithms
under consideration.
  - They hold globally, and do not rely on the notion of localization used by
Foster et al. (2021).
  - Most interestingly, they allow the reference model with respect to which
the DEC is defined to be improper, establishing that improper reference models
play a fundamental role.
  We provide upper bounds on regret that scale with the same quantity, thereby
closing all but one of the gaps between upper and lower bounds in Foster et al.
(2021). Our results apply to both the regret framework and PAC framework, and
make use of several new analysis and algorithm design techniques that we
anticipate will find broader use.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Everything is Connected: Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Petar Veličković
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many ways, graphs are the main modality of data we receive from nature.
This is due to the fact that most of the patterns we see, both in natural and
artificial systems, are elegantly representable using the language of graph
structures. Prominent examples include molecules (represented as graphs of
atoms and bonds), social networks and transportation networks. This potential
has already been seen by key scientific and industrial groups, with
already-impacted application areas including traffic forecasting, drug
discovery, social network analysis and recommender systems. Further, some of
the most successful domains of application for machine learning in previous
years -- images, text and speech processing -- can be seen as special cases of
graph representation learning, and consequently there has been significant
exchange of information between these areas. The main aim of this short survey
is to enable the reader to assimilate the key concepts in the area, and
position graph representation learning in a proper context with related fields.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Current Opinion in Structural Biology. 14 pages, 1
  figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GIPA++: A General Information Propagation Algorithm for Graph Learning <span class="chip">DASFAA2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Houyi Li, Zhihong Chen, Zhao Li, Qinkai Zheng, Peng Zhang, Shuigeng Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) have been widely used in graph-structured data
computation, showing promising performance in various applications such as node
classification, link prediction, and network recommendation. Existing works
mainly focus on node-wise correlation when doing weighted aggregation of
neighboring nodes based on attention, such as dot product by the dense vectors
of two nodes. This may cause conflicting noise in nodes to be propagated when
doing information propagation. To solve this problem, we propose a General
Information Propagation Algorithm (GIPA in short), which exploits more
fine-grained information fusion including bit-wise and feature-wise
correlations based on edge features in their propagation. Specifically, the
bit-wise correlation calculates the element-wise attention weight through a
multi-layer perceptron (MLP) based on the dense representations of two nodes
and their edge; The feature-wise correlation is based on the one-hot
representations of node attribute features for feature selection. We evaluate
the performance of GIPA on the Open Graph Benchmark proteins (OGBN-proteins for
short) dataset and the Alipay dataset of Alibaba. Experimental results reveal
that GIPA outperforms the state-of-the-art models in terms of prediction
accuracy, e.g., GIPA achieves an average ROC-AUC of $0.8901\pm 0.0011$, which
is better than that of all the existing methods listed in the OGBN-proteins
leaderboard.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by DASFAA2023. arXiv admin note: substantial text overlap
  with arXiv:2105.06035</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An SDE for Modeling SAM: Theory and Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enea Monzio Compagnoni, Antonio Orvieto, Luca Biggio, Hans Kersting, Frank Norbert Proske, Aurelien Lucchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the SAM (Sharpness-Aware Minimization) optimizer which has recently
attracted a lot of interest due to its increased performance over more
classical variants of stochastic gradient descent. Our main contribution is the
derivation of continuous-time models (in the form of SDEs) for SAM and its
unnormalized variant USAM, both for the full-batch and mini-batch settings. We
demonstrate that these SDEs are rigorous approximations of the real
discrete-time algorithms (in a weak sense, scaling linearly with the step
size). Using these models, we then offer an explanation of why SAM prefers flat
minima over sharp ones - by showing that it minimizes an implicitly regularized
loss with a Hessian-dependent noise structure. Finally, we prove that perhaps
unexpectedly SAM is attracted to saddle points under some realistic conditions.
Our theoretical results are supported by detailed experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially Private Online Bayesian Estimation With Adaptive
  Truncation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sinan Yıldırım
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel online and adaptive truncation method for differentially
private Bayesian online estimation of a static parameter regarding a
population. We assume that sensitive information from individuals is collected
sequentially and the inferential aim is to estimate, on-the-fly, a static
parameter regarding the population to which those individuals belong. We
propose sequential Monte Carlo to perform online Bayesian estimation. When
individuals provide sensitive information in response to a query, it is
necessary to perturb it with privacy-preserving noise to ensure the privacy of
those individuals. The amount of perturbation is proportional to the
sensitivity of the query, which is determined usually by the range of the
queried information. The truncation technique we propose adapts to the
previously collected observations to adjust the query range for the next
individual. The idea is that, based on previous observations, we can carefully
arrange the interval into which the next individual's information is to be
truncated before being perturbed with privacy-preserving noise. In this way, we
aim to design predictive queries with small sensitivity, hence small
privacy-preserving noise, enabling more accurate estimation while maintaining
the same level of privacy. To decide on the location and the width of the
interval, we use an exploration-exploitation approach a la Thompson sampling
with an objective function based on the Fisher information of the generated
observation. We show the merits of our methodology with numerical examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 3 figures, Code available</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building Concise Logical Patterns by Constraining Tsetlin Machine Clause
  Size 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08190v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08190v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        K. Darshana Abeyrathna, Ahmed Abdulrahem Othman Abouzeid, Bimal Bhattarai, Charul Giri, Sondre Glimsdal, Ole-Christoffer Granmo, Lei Jiao, Rupsa Saha, Jivitesh Sharma, Svein Anders Tunheim, Xuan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tsetlin machine (TM) is a logic-based machine learning approach with the
crucial advantages of being transparent and hardware-friendly. While TMs match
or surpass deep learning accuracy for an increasing number of applications,
large clause pools tend to produce clauses with many literals (long clauses).
As such, they become less interpretable. Further, longer clauses increase the
switching activity of the clause logic in hardware, consuming more power. This
paper introduces a novel variant of TM learning - Clause Size Constrained TMs
(CSC-TMs) - where one can set a soft constraint on the clause size. As soon as
a clause includes more literals than the constraint allows, it starts expelling
literals. Accordingly, oversized clauses only appear transiently. To evaluate
CSC-TM, we conduct classification, clustering, and regression experiments on
tabular data, natural language text, images, and board games. Our results show
that CSC-TM maintains accuracy with up to 80 times fewer literals. Indeed, the
accuracy increases with shorter clauses for TREC, IMDb, and BBC Sports. After
the accuracy peaks, it drops gracefully as the clause size approaches a single
literal. We finally analyze CSC-TM power consumption and derive new convergence
properties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Resolution Framework for U-Nets with Applications to
  Hierarchical VAEs <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Falck, Christopher Williams, Dominic Danks, George Deligiannidis, Christopher Yau, Chris Holmes, Arnaud Doucet, Matthew Willetts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  U-Net architectures are ubiquitous in state-of-the-art deep learning, however
their regularisation properties and relationship to wavelets are understudied.
In this paper, we formulate a multi-resolution framework which identifies
U-Nets as finite-dimensional truncations of models on an infinite-dimensional
function space. We provide theoretical results which prove that average pooling
corresponds to projection within the space of square-integrable functions and
show that U-Nets with average pooling implicitly learn a Haar wavelet basis
representation of the data. We then leverage our framework to identify
state-of-the-art hierarchical VAEs (HVAEs), which have a U-Net architecture, as
a type of two-step forward Euler discretisation of multi-resolution diffusion
processes which flow from a point mass, introducing sampling instabilities. We
also demonstrate that HVAEs learn a representation of time which allows for
improved parameter efficiency through weight-sharing. We use this observation
to achieve state-of-the-art HVAE performance with half the number of parameters
of existing models, exploiting the properties of our continuous-time
formulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022 (selected as oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Time-Warping Invariant Quantum Recurrent Neural Networks via
  Quantum-Classical Adaptive Gating 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivana Nikoloska, Osvaldo Simeone, Leonardo Banchi, Petar Velićković
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adaptive gating plays a key role in temporal data processing via classical
recurrent neural networks (RNN), as it facilitates retention of past
information necessary to predict the future, providing a mechanism that
preserves invariance to time warping transformations. This paper builds on
quantum recurrent neural networks (QRNNs), a dynamic model with quantum memory,
to introduce a novel class of temporal data processing quantum models that
preserve invariance to time-warping transformations of the (classical)
input-output sequences. The model, referred to as time warping-invariant QRNN
(TWI-QRNN), augments a QRNN with a quantum-classical adaptive gating mechanism
that chooses whether to apply a parameterized unitary transformation at each
time step as a function of the past samples of the input sequence via a
classical recurrent model. The TWI-QRNN model class is derived from first
principles, and its capacity to successfully implement time-warping
transformations is experimentally demonstrated on examples with classical or
quantum dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Vulnerability of Backdoor Defenses for Federated Learning <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pei Fang, Jinghui Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a popular distributed machine learning paradigm
that enables jointly training a global model without sharing clients' data.
However, its repetitive server-client communication gives room for backdoor
attacks with aim to mislead the global model into a targeted misprediction when
a specific trigger pattern is presented. In response to such backdoor threats
on federated learning, various defense measures have been proposed. In this
paper, we study whether the current defense mechanisms truly neutralize the
backdoor threats from federated learning in a practical setting by proposing a
new federated backdoor attack method for possible countermeasures. Different
from traditional training (on triggered data) and rescaling (the malicious
client model) based backdoor injection, the proposed backdoor attack framework
(1) directly modifies (a small proportion of) local model weights to inject the
backdoor trigger via sign flips; (2) jointly optimize the trigger pattern with
the client model, thus is more persistent and stealthy for circumventing
existing defenses. In a case study, we examine the strength and weaknesses of
recent federated backdoor defenses from three major categories and provide
suggestions to the practitioners when training federated models in practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2023 (15 pages, 12 figures, 7 tables)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Quantum Processes with Memory -- Quantum Recurrent Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dmytro Bondarenko, Robert Salzmann, Viktoria-S. Schmiesing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recurrent neural networks play an important role in both research and
industry. With the advent of quantum machine learning, the quantisation of
recurrent neural networks has become recently relevant. We propose fully
quantum recurrent neural networks, based on dissipative quantum neural
networks, capable of learning general causal quantum automata. A quantum
training algorithm is proposed and classical simulations for the case of
product outputs with the fidelity as cost function are carried out. We thereby
demonstrate the potential of these algorithms to learn complex quantum
processes with memory in terms of the exemplary delay channel, the time
evolution of quantum states governed by a time-dependent Hamiltonian, and high-
and low-frequency noise mitigation. Numerical simulations indicate that our
quantum recurrent neural networks exhibit a striking ability to generalise from
small training sets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages + 37 pages appendices, 2 + 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiME: Maximizing Mutual Information by a Difference of Matrix-Based
  Entropies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oscar Skean, Jhoan Keider Hoyos Osorio, Austin J. Brockmeier, Luis Gonzalo Sanchez Giraldo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce an information-theoretic quantity with similar properties to
mutual information that can be estimated from data without making explicit
assumptions on the underlying distribution. This quantity is based on a
recently proposed matrix-based entropy that uses the eigenvalues of a
normalized Gram matrix to compute an estimate of the eigenvalues of an
uncentered covariance operator in a reproducing kernel Hilbert space. We show
that a difference of matrix-based entropies (DiME) is well suited for problems
involving maximization of mutual information between random variables. While
many methods for such tasks can lead to trivial solutions, DiME naturally
penalizes such outcomes. We provide several examples of use cases for the
proposed quantity including a multi-view representation learning problem where
DiME is used to encourage learning a shared representation among views with
high mutual information. We also show the versatility of DiME by using it as
objective function for a variety of tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Global mapping of fragmented rocks on the Moon with a neural network:
  Implications for the failure mode of rocks on airless surfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        O. Ruesch, V. T. Bickel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has been recently recognized that the surface of sub-km asteroids in
contact with the space environment is not fine-grained regolith but consists of
centimeter to meter-scale rocks. Here we aim to understand how the rocky
morphology of minor bodies react to the well known space erosion agents on the
Moon. We deploy a neural network and map a total of ~130,000 fragmented
boulders scattered across the lunar surface and visually identify a dozen
different desintegration morphologies corresponding to different failure modes.
We find that several fragmented boulder morphologies are equivalent to
morphologies observed on asteroid Bennu, suggesting that these morphologies on
the Moon and on asteroids are likely not diagnostic of their formation
mechanism. Our findings suggest that the boulder fragmentation process is
characterized by an internal weakening period with limited morphological signs
of damage at rock scale until a sudden highly efficient impact shattering event
occurs. In addition, we identify new morphologies such as breccia boulders with
an advection-like erosion style. We publicly release the produced fractured
boulder catalog along with this paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convergence beyond the over-parameterized regime using Rayleigh
  quotients <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David A. R. Robin, Kevin Scaman, Marc Lelarge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a new strategy to prove the convergence of deep
learning architectures to a zero training (or even testing) loss by gradient
flow. Our analysis is centered on the notion of Rayleigh quotients in order to
prove Kurdyka-{\L}ojasiewicz inequalities for a broader set of neural network
architectures and loss functions. We show that Rayleigh quotients provide a
unified view for several convergence analysis techniques in the literature. Our
strategy produces a proof of convergence for various examples of parametric
learning. In particular, our analysis does not require the number of parameters
to tend to infinity, nor the number of samples to be finite, thus extending to
test loss minimization and beyond the over-parameterized regime.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at the 36th conference on Neural Information Processing
  Systems (NeurIPS 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Deep Learning with Scenario-Based Override Rules: a Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adiel Ashrov, Guy Katz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) have become a crucial instrument in the software
development toolkit, due to their ability to efficiently solve complex
problems. Nevertheless, DNNs are highly opaque, and can behave in an unexpected
manner when they encounter unfamiliar input. One promising approach for
addressing this challenge is by extending DNN-based systems with hand-crafted
override rules, which override the DNN's output when certain conditions are
met. Here, we advocate crafting such override rules using the well-studied
scenario-based modeling paradigm, which produces rules that are simple,
extensible, and powerful enough to ensure the safety of the DNN, while also
rendering the system more translucent. We report on two extensive case studies,
which demonstrate the feasibility of the approach; and through them, propose an
extension to scenario-based modeling, which facilitates its integration with
DNN components. We regard this work as a step towards creating safer and more
reliable DNN-based systems and models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A preprint of a paper with the same title, to appear at MODELSWARD
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AtMan: Understanding <span class="highlight-title">Transformer</span> Predictions Through Memory Efficient
  Attention Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mayukh Deb, Björn Deiseroth, Samuel Weinbach, Patrick Schramowski, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative transformer models have become increasingly complex, with large
numbers of parameters and the ability to process multiple input modalities.
Current methods for explaining their predictions are resource-intensive. Most
crucially, they require prohibitively large amounts of extra memory, since they
rely on backpropagation which allocates almost twice as much GPU memory as the
forward pass. This makes it difficult, if not impossible, to use them in
production. We present AtMan that provides explanations of generative
transformer models at almost no extra cost. Specifically, AtMan is a
modality-agnostic perturbation method that manipulates the attention mechanisms
of transformers to produce relevance maps for the input with respect to the
output prediction. Instead of using backpropagation, AtMan applies a
parallelizable token-based search method based on cosine similarity
neighborhood in the embedding space. Our exhaustive experiments on text and
image-text benchmarks demonstrate that AtMan outperforms current
state-of-the-art gradient-based methods on several metrics while being
computationally efficient. As such, AtMan is suitable for use in large model
inference deployments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometric path augmentation for inference of sparsely observed
  stochastic nonlinear systems <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitra Maoutsa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic evolution equations describing the dynamics of systems under the
influence of both deterministic and stochastic forces are prevalent in all
fields of science. Yet, identifying these systems from sparse-in-time
observations remains still a challenging endeavour. Existing approaches focus
either on the temporal structure of the observations by relying on conditional
expectations, discarding thereby information ingrained in the geometry of the
system's invariant density; or employ geometric approximations of the invariant
density, which are nevertheless restricted to systems with conservative forces.
Here we propose a method that reconciles these two paradigms. We introduce a
new data-driven path augmentation scheme that takes the local observation
geometry into account. By employing non-parametric inference on the augmented
paths, we can efficiently identify the deterministic driving forces of the
underlying system for systems observed at low sampling rates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12+9 pages; 3 figures; Presented at NeurIPS 2022 - Machine Learning
  and the Physical Sciences workshop; parts of text are reproduced from
  author's Ph.D. thesis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RNAS-CL: Robust Neural Architecture Search by Cross-Layer Knowledge
  Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Utkarsh Nath, Yancheng Wang, Yingzhen Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks are vulnerable to adversarial attacks. Neural
Architecture Search (NAS), one of the driving tools of deep neural networks,
demonstrates superior performance in prediction accuracy in various machine
learning applications. However, it is unclear how it performs against
adversarial attacks. Given the presence of a robust teacher, it would be
interesting to investigate if NAS would produce robust neural architecture by
inheriting robustness from the teacher. In this paper, we propose Robust Neural
Architecture Search by Cross-Layer Knowledge Distillation (RNAS-CL), a novel
NAS algorithm that improves the robustness of NAS by learning from a robust
teacher through cross-layer knowledge distillation. Unlike previous knowledge
distillation methods that encourage close student/teacher output only in the
last layer, RNAS-CL automatically searches for the best teacher layer to
supervise each student layer. Experimental result evidences the effectiveness
of RNAS-CL and shows that RNAS-CL produces small and robust neural
architecture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shapley Values with Uncertain Value Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raoul Heese, Sascha Mücke, Matthias Jakobs, Thore Gerlach, Nico Piatkowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel definition of Shapley values with uncertain value
functions based on first principles using probability theory. Such uncertain
value functions can arise in the context of explainable machine learning as a
result of non-deterministic algorithms. We show that random effects can in fact
be absorbed into a Shapley value with a noiseless but shifted value function.
Hence, Shapley values with uncertain value functions can be used in analogy to
regular Shapley values. However, their reliable evaluation typically requires
more computational effort.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figure, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On backpropagating Hessians through ODEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Axel Ciceri, Thomas Fischbacher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We discuss the problem of numerically backpropagating Hessians through
ordinary differential equations (ODEs) in various contexts and elucidate how
different approaches may be favourable in specific situations. We discuss both
theoretical and pragmatic aspects such as, respectively, bounds on
computational effort and typical impact of framework overhead.
  Focusing on the approach of hand-implemented ODE-backpropagation, we develop
the computation for the Hessian of orbit-nonclosure for a mechanical system. We
also clarify the mathematical framework for extending the
backward-ODE-evolution of the costate-equation to Hessians, in its most generic
form. Some calculations, such as that of the Hessian for orbit non-closure, are
performed in a language, defined in terms of a formal grammar, that we
introduce to facilitate the tracking of intermediate quantities.
  As pedagogical examples, we discuss the Hessian of orbit-nonclosure for the
higher dimensional harmonic oscillator and conceptually related problems in
Newtonian gravitational theory. In particular, applying our approach to the
figure-8 three-body orbit, we readily rediscover a distorted-figure-8 solution
originally described by Sim\'o.
  Possible applications may include: improvements to training of `neural ODE'-
type deep learning with second-order methods, numerical analysis of quantum
corrections around classical paths, and, more broadly, studying options for
adjusting an ODE's initial configuration such that the impact on some given
objective function is small.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 3 figures, 1500 lines of code in ancillary files (including
  tests)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Position Regression for Unsupervised Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florentin Bieder, Julia Wolleb, Robin Sandkühler, Philippe C. Cattin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, anomaly detection has become an essential field in medical
image analysis. Most current anomaly detection methods for medical images are
based on image reconstruction. In this work, we propose a novel anomaly
detection approach based on coordinate regression. Our method estimates the
position of patches within a volume, and is trained only on data of healthy
subjects. During inference, we can detect and localize anomalies by considering
the error of the position estimate of a given patch. We apply our method to 3D
CT volumes and evaluate it on patients with intracranial haemorrhages and
cranial fractures. The results show that our method performs well in detecting
these anomalies. Furthermore, we show that our method requires less memory than
comparable approaches that involve image reconstruction. This is highly
relevant for processing large 3D volumes, for instance, CT or MRI scans.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kinetic Langevin MCMC Sampling Without Gradient Lipschitz Continuity --
  the Strongly Convex Case 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08039v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08039v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Johnston, Iosif Lytras, Sotirios Sabanis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article we consider sampling from log concave distributions in
Hamiltonian setting, without assuming that the objective gradient is globally
Lipschitz. We propose two algorithms based on monotone polygonal (tamed) Euler
schemes, to sample from a target measure, and provide non-asymptotic
2-Wasserstein distance bounds between the law of the process of each algorithm
and the target measure. Finally, we apply these results to bound the excess
risk optimization error of the associated optimization problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Agent Interplay in a Competitive Survival Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Fanti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solving hard-exploration environments in an important challenge in
Reinforcement Learning. Several approaches have been proposed and studied, such
as Intrinsic Motivation, co-evolution of agents and tasks, and multi-agent
competition. In particular, the interplay between multiple agents has proven to
be capable of generating human-relevant emergent behaviour that would be
difficult or impossible to learn in single-agent settings. In this work, an
extensible competitive environment for multi-agent interplay was developed,
which features realistic physics and human-relevant semantics. Moreover,
several experiments on different variants of this environment were performed,
resulting in some simple emergent strategies and concrete directions for future
improvement. The content presented here is part of the author's thesis
"Multi-Agent Interplay in a Competitive Survival Environment" for the Master's
Degree in Artificial Intelligence and Robotics at Sapienza University of Rome,
2022.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 11 figures, part of a Master's thesis, Sapienza University
  of Rome, 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Meta-Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08028v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08028v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Beck, Risto Vuorio, Evan Zheran Liu, Zheng Xiong, Luisa Zintgraf, Chelsea Finn, Shimon Whiteson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep reinforcement learning (RL) has fueled multiple high-profile
successes in machine learning, it is held back from more widespread adoption by
its often poor data efficiency and the limited generality of the policies it
produces. A promising approach for alleviating these limitations is to cast the
development of better RL algorithms as a machine learning problem itself in a
process called meta-RL. Meta-RL is most commonly studied in a problem setting
where, given a distribution of tasks, the goal is to learn a policy that is
capable of adapting to any new task from the task distribution with as little
data as possible. In this survey, we describe the meta-RL problem setting in
detail as well as its major variations. We discuss how, at a high level,
meta-RL research can be clustered based on the presence of a task distribution
and the learning budget available for each individual task. Using these
clusters, we then survey meta-RL algorithms and applications. We conclude by
presenting the open problems on the path to making meta-RL part of the standard
toolbox for a deep RL practitioner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identification, explanation and clinical evaluation of hospital patient
  subtypes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enrico Werner, Jeffrey N. Clark, Ranjeet S. Bhamber, Michael Ambler, Christopher P. Bourdeaux, Alexander Hepburn, Christopher J. McWilliams, Raul Santos-Rodriguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a pipeline in which unsupervised machine learning techniques are
used to automatically identify subtypes of hospital patients admitted between
2017 and 2021 in a large UK teaching hospital. With the use of state-of-the-art
explainability techniques, the identified subtypes are interpreted and assigned
clinical meaning. In parallel, clinicians assessed intra-cluster similarities
and inter-cluster differences of the identified patient subtypes within the
context of their clinical knowledge. By confronting the outputs of both
automatic and clinician-based explanations, we aim to highlight the mutual
benefit of combining machine learning techniques with clinical expertise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Global Nash Equilibrium in Non-convex Multi-player Game: Theory and
  Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanpu Chen, Gehui Xu, Fengxiang He, Yiguang Hong, Leszek Rutkowski, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wide machine learning tasks can be formulated as non-convex multi-player
games, where Nash equilibrium (NE) is an acceptable solution to all players,
since no one can benefit from changing its strategy unilaterally. Attributed to
the non-convexity, obtaining the existence condition of global NE is
challenging, let alone designing theoretically guaranteed realization
algorithms. This paper takes conjugate transformation to the formulation of
non-convex multi-player games, and casts the complementary problem into a
variational inequality (VI) problem with a continuous pseudo-gradient mapping.
We then prove the existence condition of global NE: the solution to the VI
problem satisfies a duality relation. Based on this VI formulation, we design a
conjugate-based ordinary differential equation (ODE) to approach global NE,
which is proved to have an exponential convergence rate. To make the dynamics
more implementable, we further derive a discretized algorithm. We apply our
algorithm to two typical scenarios: multi-player generalized monotone game and
multi-player potential game. In the two settings, we prove that the step-size
setting is required to be $\mathcal{O}(1/k)$ and $\mathcal{O}(1/\sqrt k)$ to
yield the convergence rates of $\mathcal{O}(1/ k)$ and $\mathcal{O}(1/\sqrt
k)$, respectively. Extensive experiments in robust neural network training and
sensor localization are in full agreement with our theory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Machine Translation with Phrase Pair Injection and Corpus
  Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshay Batheja, Pushpak Bhattacharyya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we show that the combination of Phrase Pair Injection and
Corpus Filtering boosts the performance of Neural Machine Translation (NMT)
systems. We extract parallel phrases and sentences from the pseudo-parallel
corpus and augment it with the parallel corpus to train the NMT models. With
the proposed approach, we observe an improvement in the Machine Translation
(MT) system for 3 low-resource language pairs, Hindi-Marathi, English-Marathi,
and English-Pashto, and 6 translation directions by up to 2.7 BLEU points, on
the FLORES test data. These BLEU score improvements are over the models trained
using the whole pseudo-parallel corpus augmented with the parallel corpus.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuously Reliable Detection of New-Normal Misinformation: Semantic
  Masking and Contrastive Smoothing in High-Density Latent Regions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhijit Suprem, Joao Eduardo Ferreira, Calton Pu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Toxic misinformation campaigns have caused significant societal harm, e.g.,
affecting elections and COVID-19 information awareness. Unfortunately, despite
successes of (gold standard) retrospective studies of misinformation that
confirmed their harmful effects after the fact, they arrive too late for timely
intervention and reduction of such harm. By design, misinformation evades
retrospective classifiers by exploiting two properties we call new-normal: (1)
never-seen-before novelty that cause inescapable generalization challenges for
previous classifiers, and (2) massive but short campaigns that end before they
can be manually annotated for new classifier training. To tackle these
challenges, we propose UFIT, which combines two techniques: semantic masking of
strong signal keywords to reduce overfitting, and intra-proxy smoothness
regularization of high-density regions in the latent space to improve
reliability and maintain accuracy. Evaluation of UFIT on public new-normal
misinformation data shows over 30% improvement over existing approaches on
future (and unseen) campaigns. To the best of our knowledge, UFIT is the first
successful effort to achieve such high level of generalization on new-normal
misinformation data with minimal concession (1 to 5%) of accuracy compared to
oracles trained with full knowledge of all campaigns.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpotHitPy: A Study For ML-Based Song Hit Prediction Using Spotify 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ioannis Dimolitsas, Spyridon Kantarelis, Afroditi Fouka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we approached the Hit Song Prediction problem, which aims to
predict which songs will become Billboard hits. We gathered a dataset of nearly
18500 hit and non-hit songs and extracted their audio features using the
Spotify Web API. We test four machine-learning models on our dataset. We were
able to predict the Billboard success of a song with approximately 86\%
accuracy. The most succesful algorithms were Random Forest and Support Vector
Machine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Getting Away with More Network Pruning: From Sparsity to Geometry and
  Linear Regions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyang Cai, Khai-Nguyen Nguyen, Nishant Shrestha, Aidan Good, Ruisen Tu, Xin Yu, Shandian Zhe, Thiago Serra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One surprising trait of neural networks is the extent to which their
connections can be pruned with little to no effect on accuracy. But when we
cross a critical level of parameter sparsity, pruning any further leads to a
sudden drop in accuracy. This drop plausibly reflects a loss in model
complexity, which we aim to avoid. In this work, we explore how sparsity also
affects the geometry of the linear regions defined by a neural network, and
consequently reduces the expected maximum number of linear regions based on the
architecture. We observe that pruning affects accuracy similarly to how
sparsity affects the number of linear regions and our proposed bound for the
maximum number. Conversely, we find out that selecting the sparsity across
layers to maximize our bound very often improves accuracy in comparison to
pruning as much with the same sparsity in all layers, thereby providing us
guidance on where to prune.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>(Under review)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Job recommendations: benchmarking of collaborative filtering methods for
  classifieds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Kwieciński, Agata Filipowska, Tomasz Górecki, Viacheslav Dubrov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classifieds provide many challenges for recommendation methods, due to the
limited information regarding users and items. In this paper, we explore
recommendation methods for classifieds using the example of OLX Jobs.
  The goal of the paper is to benchmark different recommendation methods for
jobs classifieds in order to improve advertisements' conversion rate and user
satisfaction.
  In our research, we implemented methods that are scalable and represent
different approaches to recommendation, namely ALS, LightFM, Prod2Vec, RP3beta,
and SLIM. We performed a laboratory comparison of methods with regard to
accuracy, diversity, and scalability (memory and time consumption during
training and in prediction). Online A/B tests were also carried out by sending
millions of messages with recommendations to evaluate models in a real-world
setting.
  In addition, we have published the dataset that we created for the needs of
our research. To the best of our knowledge, this is the first dataset of this
kind. The dataset contains 65,502,201 events performed on OLX Jobs by 3,295,942
users, who interacted with (displayed, replied to, or bookmarked) 185,395 job
ads in two weeks of 2020.
  We demonstrate that RP3beta, SLIM, and ALS perform significantly better than
Prod2Vec and LightFM when tested in a laboratory setting. Online A/B tests also
demonstrated that sending messages with recommendations generated by the ALS
and RP3beta models increases the number of users contacting advertisers.
Additionally, RP3beta had a 20% greater impact on this metric than ALS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PDFormer: Propagation Delay-aware Dynamic Long-range <span class="highlight-title">Transformer</span> for
  Traffic Flow Prediction <span class="chip">AAAI2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Jiang, Chengkai Han, Wayne Xin Zhao, Jingyuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a core technology of Intelligent Transportation System, traffic flow
prediction has a wide range of applications. The fundamental challenge in
traffic flow prediction is to effectively model the complex spatial-temporal
dependencies in traffic data. Spatial-temporal Graph Neural Network (GNN)
models have emerged as one of the most promising methods to solve this problem.
However, GNN-based models have three major limitations for traffic prediction:
i) Most methods model spatial dependencies in a static manner, which limits the
ability to learn dynamic urban traffic patterns; ii) Most methods only consider
short-range spatial information and are unable to capture long-range spatial
dependencies; iii) These methods ignore the fact that the propagation of
traffic conditions between locations has a time delay in traffic systems. To
this end, we propose a novel Propagation Delay-aware dynamic long-range
transFormer, namely PDFormer, for accurate traffic flow prediction.
Specifically, we design a spatial self-attention module to capture the dynamic
spatial dependencies. Then, two graph masking matrices are introduced to
highlight spatial dependencies from short- and long-range views. Moreover, a
traffic delay-aware feature transformation module is proposed to empower
PDFormer with the capability of explicitly modeling the time delay of spatial
information propagation. Extensive experimental results on six real-world
public traffic datasets show that our method can not only achieve
state-of-the-art performance but also exhibit competitive computational
efficiency. Moreover, we visualize the learned spatial-temporal attention map
to make our model highly interpretable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures, Accepted by AAAI2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CEnt: An Entropy-based Model-agnostic Explainability Framework to
  Contrast Classifiers' Decisions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julia El Zini, Mohammad Mansour, Mariette Awad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current interpretability methods focus on explaining a particular model's
decision through present input features. Such methods do not inform the user of
the sufficient conditions that alter these decisions when they are not
desirable. Contrastive explanations circumvent this problem by providing
explanations of the form "If the feature $X>x$, the output $Y$ would be
different''. While different approaches are developed to find contrasts; these
methods do not all deal with mutability and attainability constraints.
  In this work, we present a novel approach to locally contrast the prediction
of any classifier. Our Contrastive Entropy-based explanation method, CEnt,
approximates a model locally by a decision tree to compute entropy information
of different feature splits. A graph, G, is then built where contrast nodes are
found through a one-to-many shortest path search. Contrastive examples are
generated from the shortest path to reflect feature splits that alter model
decisions while maintaining lower entropy. We perform local sampling on
manifold-like distances computed by variational auto-encoders to reflect data
density. CEnt is the first non-gradient-based contrastive method generating
diverse counterfactuals that do not necessarily exist in the training data
while satisfying immutability (ex. race) and semi-immutability (ex. age can
only change in an increasing direction). Empirical evaluation on four
real-world numerical datasets demonstrates the ability of CEnt in generating
counterfactuals that achieve better proximity rates than existing methods
without compromising latency, feasibility, and attainability. We further extend
CEnt to imagery data to derive visually appealing and useful contrasts between
class labels on MNIST and Fashion MNIST datasets. Finally, we show how CEnt can
serve as a tool to detect vulnerabilities of textual classifiers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hamiltonian Neural Networks with Automatic Symmetry Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eva Dierkes, Christian Offen, Sina Ober-Blöbaum, Kathrin Flaßkamp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Hamiltonian neural networks (HNN) have been introduced to
incorporate prior physical knowledge when learning the dynamical equations of
Hamiltonian systems. Hereby, the symplectic system structure is preserved
despite the data-driven modeling approach. However, preserving symmetries
requires additional attention. In this research, we enhance the HNN with a Lie
algebra framework to detect and embed symmetries in the neural network. This
approach allows to simultaneously learn the symmetry group action and the total
energy of the system. As illustrating examples, a pendulum on a cart and a
two-body problem from astrodynamics are considered.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interval Reachability of Nonlinear Dynamical Systems with Neural Network
  Controllers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saber Jafarpour, Akash Harapanahalli, Samuel Coogan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a computationally efficient framework, based on interval
analysis, for rigorous verification of nonlinear continuous-time dynamical
systems with neural network controllers. Given a neural network, we use an
existing verification algorithm to construct inclusion functions for its
input-output behavior. Inspired by mixed monotone theory, we embed the
closed-loop dynamics into a larger system using an inclusion function of the
neural network and a decomposition function of the open-loop system. This
embedding provides a scalable approach for safety analysis of the neural
control loop while preserving the nonlinear structure of the system.
  We show that one can efficiently compute hyper-rectangular
over-approximations of the reachable sets using a single trajectory of the
embedding system. We design an algorithm to leverage this computational
advantage through partitioning strategies, improving our reachable set
estimates while balancing its runtime with tunable parameters. We demonstrate
the performance of this algorithm through two case studies. First, we
demonstrate this method's strength in complex nonlinear environments. Then, we
show that our approach matches the performance of the state-of-the art
verification algorithm for linear discretized systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Nonstochastic Control Approach to Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Chen, Elad Hazan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tuning optimizer hyperparameters, notably the learning rate to a particular
optimization instance, is an important but nonconvex problem. Therefore
iterative optimization methods such as hypergradient descent lack global
optimality guarantees in general.
  We propose an online nonstochastic control methodology for mathematical
optimization. The choice of hyperparameters for gradient based methods,
including the learning rate, momentum parameter and preconditioner, is
described as feedback control. The optimal solution to this control problem is
shown to encompass preconditioned adaptive gradient methods with varying
acceleration and momentum parameters. Although the optimal control problem by
itself is nonconvex, we show how recent methods from online nonstochastic
control based on convex relaxation can be applied to compete with the best
offline solution. This guarantees that in episodic optimization, we converge to
the best optimization method in hindsight.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the diffusion models by conditional expectations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07882v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07882v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibin Lu, Zhongjian Wang, Guillaume Bal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provide several mathematical analyses of the diffusion model in
machine learning. The drift term of the backwards sampling process is
represented as a conditional expectation involving the data distribution and
the forward diffusion. The training process aims to find such a drift function
by minimizing the mean-squared residue related to the conditional expectation.
Using small-time approximations of the Green's function of the forward
diffusion, we show that the analytical mean drift function in DDPM and the
score function in SGM asymptotically blow up in the final stages of the
sampling process for singular data distributions such as those concentrated on
lower-dimensional manifolds, and is therefore difficult to approximate by a
network. To overcome this difficulty, we derive a new target function and
associated loss, which remains bounded even for singular data distributions. We
illustrate the theoretical findings with several numerical examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Suboptimality analysis of receding horizon quadratic control with
  unknown linear systems and its applications in learning-based control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07876v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07876v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengling Shi, Anastasios Tsiamis, Bart De Schutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For a receding-horizon controller with a known system and with an approximate
terminal value function, it is well-known that increasing the prediction
horizon can improve its control performance. However, when the prediction model
is inexact, a larger prediction horizon also causes propagation and
accumulation of the prediction error. In this work, we aim to analyze the
effect of the above trade-off between the modeling error, the terminal value
function error, and the prediction horizon on the performance of a nominal
receding-horizon linear quadratic (LQ) controller. By developing a novel
perturbation result of the Riccati difference equation, a performance upper
bound is obtained and suggests that for many cases, the prediction horizon
should be either 1 or infinity to improve the control performance, depending on
the relative difference between the modeling error and the terminal value
function error. The obtained suboptimality performance bound is also applied to
provide end-to-end performance guarantees, e.g., regret bounds, for nominal
receding-horizon LQ controllers in a learning-based setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discover governing differential equations from evolving systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07863v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07863v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanyuan Li, Kai Wu, Jing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discovering the governing equations of evolving systems from available
observations is essential and challenging. However, current methods does not
capture the situation that underlying system dynamics can be changed.Evolving
systems are changing over time, which invariably changes with system status.
Thus, finding the exact change points is critical. We propose an online
modeling method capable of handling samples one by one sequentially by modeling
streaming data instead of processing the entire dataset. The proposed method
performs well in discovering ordinary differential equations, partial
differential equations (PDEs), and high-dimensional PDEs from streaming data.
The measurement generated from a changed system is distributed dissimilarly to
before; hence, the difference can be identified by the proposed method. Our
proposal performs well in identifying the change points and discovering
governing differential equations in two evolving systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FE-TCM: Filter-Enhanced <span class="highlight-title">Transformer</span> Click Model for Web Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingfei Wang, Jianping Liu, Meng Wang, Xintao Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constructing click models and extracting implicit relevance feedback
information from the interaction between users and search engines are very
important to improve the ranking of search results. Using neural network to
model users' click behaviors has become one of the effective methods to
construct click models. In this paper, We use Transformer as the backbone
network of feature extraction, add filter layer innovatively, and propose a new
Filter-Enhanced Transformer Click Model (FE-TCM) for web search. Firstly, in
order to reduce the influence of noise on user behavior data, we use the
learnable filters to filter log noise. Secondly, following the examination
hypothesis, we model the attraction estimator and examination predictor
respectively to output the attractiveness scores and examination probabilities.
A novel transformer model is used to learn the deeper representation among
different features. Finally, we apply the combination functions to integrate
attractiveness scores and examination probabilities into the click prediction.
From our experiments on two real-world session datasets, it is proved that
FE-TCM outperforms the existing click models for the click prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From English to More Languages: Parameter-Efficient Model Reprogramming
  for Cross-Lingual Speech Recognition <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao-Han Huck Yang, Bo Li, Yu Zhang, Nanxin Chen, Rohit Prabhavalkar, Tara N. Sainath, Trevor Strohman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a new parameter-efficient learning framework based
on neural model reprogramming for cross-lingual speech recognition, which can
\textbf{re-purpose} well-trained English automatic speech recognition (ASR)
models to recognize the other languages. We design different auxiliary neural
architectures focusing on learnable pre-trained feature enhancement that, for
the first time, empowers model reprogramming on ASR. Specifically, we
investigate how to select trainable components (i.e., encoder) of a
conformer-based RNN-Transducer, as a frozen pre-trained backbone. Experiments
on a seven-language multilingual LibriSpeech speech (MLS) task show that model
reprogramming only requires 4.2% (11M out of 270M) to 6.8% (45M out of 660M) of
its original trainable parameters from a full ASR model to perform competitive
results in a range of 11.9% to 8.1% WER averaged across different languages. In
addition, we discover different setups to make large-scale pre-trained ASR
succeed in both monolingual and multilingual speech recognition. Our methods
outperform existing ASR tuning architectures and their extension with
self-supervised losses (e.g., w2v-bert) in terms of lower WER and better
training efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2023. The project was initiated in May 2022
  during a research internship at Google Research</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Concept Discovery for Fast Adapatation <span class="chip">SDM23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyu Feng, Hanghang Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advances in deep learning have enabled machine learning methods to
outperform human beings in various areas, but it remains a great challenge for
a well-trained model to quickly adapt to a new task. One promising solution to
realize this goal is through meta-learning, also known as learning to learn,
which has achieved promising results in few-shot learning. However, current
approaches are still enormously different from human beings' learning process,
especially in the ability to extract structural and transferable knowledge.
This drawback makes current meta-learning frameworks non-interpretable and hard
to extend to more complex tasks. We tackle this problem by introducing concept
discovery to the few-shot learning problem, where we achieve more effective
adaptation by meta-learning the structure among the data features, leading to a
composite representation of the data. Our proposed method Concept-Based
Model-Agnostic Meta-Learning (COMAML) has been shown to achieve consistent
improvements in the structured data for both synthesized datasets and
real-world datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SDM23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ClusterLog: Clustering Logs for Effective Log-based Anomaly Detection <span class="chip">SC22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Egersdoerfer, Dong Dai, Di Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing prevalence of scalable file systems in the context of
High Performance Computing (HPC), the importance of accurate anomaly detection
on runtime logs is increasing. But as it currently stands, many
state-of-the-art methods for log-based anomaly detection, such as DeepLog, have
encountered numerous challenges when applied to logs from many parallel file
systems (PFSes), often due to their irregularity and ambiguity in time-based
log sequences. To circumvent these problems, this study proposes ClusterLog, a
log pre-processing method that clusters the temporal sequence of log keys based
on their semantic similarity. By grouping semantically and sentimentally
similar logs, this approach aims to represent log sequences with the smallest
amount of unique log keys, intending to improve the ability of a downstream
sequence-based model to effectively learn the log patterns. The preliminary
results of ClusterLog indicate not only its effectiveness in reducing the
granularity of log sequences without the loss of important sequence information
but also its generalizability to different file systems' logs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in 12th Workshop on Fault-Tolerance for HPC at Extreme Scale
  at SC22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatio-temporal neural structural causal models for bike flow prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pan Deng, Yu Zhao, Junting Liu, Xiaofeng Jia, Mulan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a representative of public transportation, the fundamental issue of
managing bike-sharing systems is bike flow prediction. Recent methods
overemphasize the spatio-temporal correlations in the data, ignoring the
effects of contextual conditions on the transportation system and the
inter-regional timevarying causality. In addition, due to the disturbance of
incomplete observations in the data, random contextual conditions lead to
spurious correlations between data and features, making the prediction of the
model ineffective in special scenarios. To overcome this issue, we propose a
Spatio-temporal Neural Structure Causal Model(STNSCM) from the perspective of
causality. First, we build a causal graph to describe the traffic prediction,
and further analyze the causal relationship between the input data, contextual
conditions, spatiotemporal states, and prediction results. Second, we propose
to apply the frontdoor criterion to eliminate confounding biases in the feature
extraction process. Finally, we propose a counterfactual representation
reasoning module to extrapolate the spatio-temporal state under the factual
scenario to future counterfactual scenarios to improve the prediction
performance. Experiments on real-world datasets demonstrate the superior
performance of our model, especially its resistance to fluctuations caused by
the external environment. The source code and data will be released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Reinforcement Learning for Power Trading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanrong Wang, Vignesh Raja Swaminathan, Nikita P. Granger, Carlos Ros Perez, Christian Michler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Dutch power market includes a day-ahead market and an auction-like
intraday balancing market. The varying supply and demand of power and its
uncertainty induces an imbalance, which causes differing power prices in these
two markets and creates an opportunity for arbitrage. In this paper, we present
collaborative dual-agent reinforcement learning (RL) for bi-level simulation
and optimization of European power arbitrage trading. Moreover, we propose two
novel practical implementations specifically addressing the electricity power
market. Leveraging the concept of imitation learning, the RL agent's reward is
reformed by taking into account prior domain knowledge results in better
convergence during training and, moreover, improves and generalizes
performance. In addition, tranching of orders improves the bidding success rate
and significantly raises the P&L. We show that each method contributes
significantly to the overall performance uplifting, and the integrated
methodology achieves about three-fold improvement in cumulative P&L over the
original agent, as well as outperforms the highest benchmark policy by around
50% while exhibits efficient computational performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advanced Scaling Methods for VNF deployment with Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Namjin Seo, DongNyeong Heo, Heeyoul Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Network function virtualization (NFV) and software-defined network (SDN) have
become emerging network paradigms, allowing virtualized network function (VNF)
deployment at a low cost. Even though VNF deployment can be flexible, it is
still challenging to optimize VNF deployment due to its high complexity.
Several studies have approached the task as dynamic programming, e.g., integer
linear programming (ILP). However, optimizing VNF deployment for highly complex
networks remains a challenge. Alternatively, reinforcement learning (RL) based
approaches have been proposed to optimize this task, especially to employ a
scaling action-based method which can deploy VNFs within less computational
time. However, the model architecture can be improved further to generalize to
the different networking settings. In this paper, we propose an enhanced model
which can be adapted to more general network settings. We adopt the improved
GNN architecture and a few techniques to obtain a better node representation
for the VNF deployment task. Furthermore, we apply a recently proposed RL
method, phasic policy gradient (PPG), to leverage the shared representation of
the service function chain (SFC) generation model from the value function. We
evaluate the proposed method in various scenarios, achieving a better QoS with
minimum resource utilization compared to the previous methods. Finally, as a
qualitative evaluation, we analyze our proposed encoder's representation for
the nodes, which shows a more disentangled representation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum HyperNetworks: Training Binary Neural Networks in Quantum
  Superposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Carrasquilla, Mohamed Hibat-Allah, Estelle Inack, Alireza Makhzani, Kirill Neklyudov, Graham W. Taylor, Giacomo Torlai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Binary neural networks, i.e., neural networks whose parameters and
activations are constrained to only two possible values, offer a compelling
avenue for the deployment of deep learning models on energy- and memory-limited
devices. However, their training, architectural design, and hyperparameter
tuning remain challenging as these involve multiple computationally expensive
combinatorial optimization problems. Here we introduce quantum hypernetworks as
a mechanism to train binary neural networks on quantum computers, which unify
the search over parameters, hyperparameters, and architectures in a single
optimization loop. Through classical simulations, we demonstrate that of our
approach effectively finds optimal parameters, hyperparameters and
architectural choices with high probability on classification problems
including a two-dimensional Gaussian dataset and a scaled-down version of the
MNIST handwritten digits. We represent our quantum hypernetworks as variational
quantum circuits, and find that an optimal circuit depth maximizes the
probability of finding performant binary neural networks. Our unified approach
provides an immense scope for other applications in the field of machine
learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures. Minimal implementation:
  https://github.com/carrasqu/binncode</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forecasting subcritical cylinder wakes with Fourier Neural Operators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter I Renn, Cong Wang, Sahin Lale, Zongyi Li, Anima Anandkumar, Morteza Gharib
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We apply Fourier neural operators (FNOs), a state-of-the-art operator
learning technique, to forecast the temporal evolution of experimentally
measured velocity fields. FNOs are a recently developed machine learning method
capable of approximating solution operators to systems of partial differential
equations through data alone. The learned FNO solution operator can be
evaluated in milliseconds, potentially enabling faster-than-real-time modeling
for predictive flow control in physical systems. Here we use FNOs to predict
how physical fluid flows evolve in time, training with particle image
velocimetry measurements depicting cylinder wakes in the subcritical vortex
shedding regime. We train separate FNOs at Reynolds numbers ranging from Re =
240 to Re = 3060 and study how increasingly turbulent flow phenomena impact
prediction accuracy. We focus here on a short prediction horizon of ten
non-dimensionalized time-steps, as would be relevant for problems of predictive
flow control. We find that FNOs are capable of accurately predicting the
evolution of experimental velocity fields throughout the range of Reynolds
numbers tested (L2 norm error < 0.1) despite being provided with limited and
imperfect flow observations. Given these results, we conclude that this method
holds significant potential for real-time predictive flow control of physical
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating the Impact of Direct Punishment on the Emergence of
  Cooperation in Multi-Agent Reinforcement Learning Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nayana Dasgupta, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of cooperation is of fundamental importance for human societies,
with examples ranging from navigating road junctions to negotiating climate
treaties. As the use of AI becomes more pervasive within society, the need for
socially intelligent agents that are able to navigate these complex dilemmas is
becoming increasingly evident. Direct punishment is an ubiquitous social
mechanism that has been shown to benefit the emergence of cooperation within
the natural world, however no prior work has investigated its impact on
populations of learning agents. Moreover, although the use of all forms of
punishment in the natural world is strongly coupled with partner selection and
reputation, no existing work has provided a holistic analysis of their
combination within multi-agent systems. In this paper, we present a
comprehensive analysis of the behaviors and learning dynamics associated with
direct punishment in multi-agent reinforcement learning systems and how this
compares to third-party punishment, when both forms of punishment are combined
with other social mechanisms such as partner selection and reputation. We
provide an extensive and systematic evaluation of the impact of these key
mechanisms on the emergence of cooperation. Finally, we discuss the
implications of the use of these mechanisms in the design of cooperative AI
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Lost Art of Mathematical Modelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08559v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08559v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linnéa Gyllingberg, Abeba Birhane, David J. T. Sumpter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide a critique of mathematical biology in light of rapid developments
in modern machine learning. We argue that out of the three modelling activities
-- (1) formulating models; (2) analysing models; and (3) fitting or comparing
models to data -- inherent to mathematical biology, researchers currently focus
too much on activity (2) at the cost of (1). This trend, we propose, can be
reversed by realising that any given biological phenomena can be modelled in an
infinite number of different ways, through the adoption of an open/pluralistic
approach. We explain the open approach using fish locomotion as a case study
and illustrate some of the pitfalls -- universalism, creating models of models,
etc. -- that hinder mathematical biology. We then ask how we might rediscover a
lost art: that of creative mathematical modelling.
  This article is dedicated to the memory of Edmund Crampin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of the potential of Near Infrared Hyperspectral Imaging for
  monitoring the invasive brown marmorated stink bug 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Veronica Ferrari, Rosalba Calvini, Bas Boom, Camilla Menozzi, Aravind Krishnaswamy Rangarajan, Lara Maistrello, Peter Offermans, Alessandro Ulrici
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The brown marmorated stink bug (BMSB), Halyomorpha halys, is an invasive
insect pest of global importance that damages several crops, compromising
agri-food production. Field monitoring procedures are fundamental to perform
risk assessment operations, in order to promptly face crop infestations and
avoid economical losses. To improve pest management, spectral cameras mounted
on Unmanned Aerial Vehicles (UAVs) and other Internet of Things (IoT) devices,
such as smart traps or unmanned ground vehicles, could be used as an innovative
technology allowing fast, efficient and real-time monitoring of insect
infestations. The present study consists in a preliminary evaluation at the
laboratory level of Near Infrared Hyperspectral Imaging (NIR-HSI) as a possible
technology to detect BMSB specimens on different vegetal backgrounds,
overcoming the problem of BMSB mimicry. Hyperspectral images of BMSB were
acquired in the 980-1660 nm range, considering different vegetal backgrounds
selected to mimic a real field application scene. Classification models were
obtained following two different chemometric approaches. The first approach was
focused on modelling spectral information and selecting relevant spectral
regions for discrimination by means of sparse-based variable selection coupled
with Soft Partial Least Squares Discriminant Analysis (s-Soft PLS-DA)
classification algorithm. The second approach was based on modelling spatial
and spectral features contained in the hyperspectral images using Convolutional
Neural Networks (CNN). Finally, to further improve BMSB detection ability, the
two strategies were merged, considering only the spectral regions selected by
s-Soft PLS-DA for CNN modelling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted manuscript</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Models for <span class="highlight-title">Dataset</span> Drift Controls in Machine Learning With Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.02578v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.02578v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Oala, Marco Aversa, Gabriel Nobis, Kurt Willis, Yoan Neuenschwander, Michèle Buck, Christian Matek, Jerome Extermann, Enrico Pomarico, Wojciech Samek, Roderick Murray-Smith, Christoph Clausen, Bruno Sanguinetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camera images are ubiquitous in machine learning research. They also play a
central role in the delivery of important services spanning medicine and
environmental surveying. However, the application of machine learning models in
these domains has been limited because of robustness concerns. A primary
failure mode are performance drops due to differences between the training and
deployment data. While there are methods to prospectively validate the
robustness of machine learning models to such dataset drifts, existing
approaches do not account for explicit models of the primary object of
interest: the data. This limits our ability to study and understand the
relationship between data generation and downstream machine learning model
performance in a physically accurate manner. In this study, we demonstrate how
to overcome this limitation by pairing traditional machine learning with
physical optics to obtain explicit and differentiable data models. We
demonstrate how such data models can be constructed for image data and used to
control downstream machine learning model performance related to dataset drift.
The findings are distilled into three applications. First, drift synthesis
enables the controlled generation of physically faithful drift test cases to
power model selection and targeted generalization. Second, the gradient
connection between machine learning task model and data model allows advanced,
precise tolerancing of task model sensitivity to changes in the data
generation. These drift forensics can be used to precisely specify the
acceptable data environments in which a task model may be run. Third, drift
optimization opens up the possibility to create drifts that can help the task
model learn better faster, effectively optimizing the data generating process
itself. A guide to access the open code and datasets is available at
https://github.com/aiaudit-org/raw2logit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LO and MA contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dimensionality Reduction using Elastic Measures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.04933v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.04933v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        J. Derek Tucker, Matthew T. Martinez, Jose M. Laborde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the recent surge in big data analytics for hyper-dimensional data there
is a renewed interest in dimensionality reduction techniques for machine
learning applications. In order for these methods to improve performance gains
and understanding of the underlying data, a proper metric needs to be
identified. This step is often overlooked and metrics are typically chosen
without consideration of the underlying geometry of the data. In this paper, we
present a method for incorporating elastic metrics into the t-distributed
Stochastic Neighbor Embedding (t-SNE) and Uniform Manifold Approximation and
Projection (UMAP). We apply our method to functional data, which is uniquely
characterized by rotations, parameterization, and scale. If these properties
are ignored, they can lead to incorrect analysis and poor classification
performance. Through our method we demonstrate improved performance on shape
identification tasks for three benchmark data sets (MPEG-7, Car data set, and
Plane data set of Thankoor), where we achieve 0.77, 0.95, and 1.00 F1 score,
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Scalable Finite Difference Method for Deep Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07487v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07487v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Allen, John Raisbeck, Hakho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several low-bandwidth distributable black-box optimization algorithms in the
family of finite differences such as Evolution Strategies have recently been
shown to perform nearly as well as tailored Reinforcement Learning methods in
some Reinforcement Learning domains. One shortcoming of these black-box methods
is that they must collect information about the structure of the return
function at every update, and can often employ only information drawn from a
distribution centered around the current parameters. As a result, when these
algorithms are distributed across many machines, a significant portion of total
runtime may be spent with many machines idle, waiting for a final return and
then for an update to be calculated. In this work we introduce a novel method
to use older data in finite difference algorithms, which produces a scalable
algorithm that avoids significant idle time or wasted computation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nearly Minimax Optimal Reinforcement Learning with Linear Function
  Approximation <span class="chip">ICML
  2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.11489v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.11489v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pihe Hu, Yu Chen, Longbo Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study reinforcement learning with linear function approximation where the
transition probability and reward functions are linear with respect to a
feature mapping $\boldsymbol{\phi}(s,a)$. Specifically, we consider the
episodic inhomogeneous linear Markov Decision Process (MDP), and propose a
novel computation-efficient algorithm, LSVI-UCB$^+$, which achieves an
$\widetilde{O}(Hd\sqrt{T})$ regret bound where $H$ is the episode length, $d$
is the feature dimension, and $T$ is the number of steps. LSVI-UCB$^+$ builds
on weighted ridge regression and upper confidence value iteration with a
Bernstein-type exploration bonus. Our statistical results are obtained with
novel analytical tools, including a new Bernstein self-normalized bound with
conservatism on elliptical potentials, and refined analysis of the correction
term. To the best of our knowledge, this is the first minimax optimal algorithm
for linear MDPs up to logarithmic factors, which closes the $\sqrt{Hd}$ gap
between the best known upper bound of $\widetilde{O}(\sqrt{H^3d^3T})$ in
\cite{jin2020provably} and lower bound of $\Omega(Hd\sqrt{T})$ for linear MDPs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an updated version of our last version (accepted by ICML
  2022) by fixing the issue in building the over-optimistic value function</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Measuring Excess Capacity in Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.08070v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.08070v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Graf, Sebastian Zeng, Bastian Rieck, Marc Niethammer, Roland Kwitt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the excess capacity of deep networks in the context of supervised
classification. That is, given a capacity measure of the underlying hypothesis
class - in our case, empirical Rademacher complexity - to what extent can we (a
priori) constrain this class while retaining an empirical error on a par with
the unconstrained regime? To assess excess capacity in modern architectures
(such as residual networks), we extend and unify prior Rademacher complexity
bounds to accommodate function composition and addition, as well as the
structure of convolutions. The capacity-driving terms in our bounds are the
Lipschitz constants of the layers and an (2, 1) group norm distance to the
initializations of the convolution weights. Experiments on benchmark datasets
of varying task difficulty indicate that (1) there is a substantial amount of
excess capacity per task, and (2) capacity can be kept at a surprisingly
similar level across tasks. Overall, this suggests a notion of compressibility
with respect to weight norms, complementary to classic compression via weight
pruning. Source code is available at https://github.com/rkwitt/excess_capacity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated to Neurips 2022 camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Soft-labeling Strategies for Rapid Sub-Typing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.12684v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.12684v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grant Rosario, David Noever, Matt Ciolino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The challenge of labeling large example datasets for computer vision
continues to limit the availability and scope of image repositories. This
research provides a new method for automated data collection, curation,
labeling, and iterative training with minimal human intervention for the case
of overhead satellite imagery and object detection. The new operational scale
effectively scanned an entire city (68 square miles) in grid search and yielded
a prediction of car color from space observations. A partially trained yolov5
model served as an initial inference seed to output further, more refined model
predictions in iterative cycles. Soft labeling here refers to accepting label
noise as a potentially valuable augmentation to reduce overfitting and enhance
generalized predictions to previously unseen test data. The approach takes
advantage of a real-world instance where a cropped image of a car can
automatically receive sub-type information as white or colorful from pixel
values alone, thus completing an end-to-end pipeline without overdependence on
human labor.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Convenient Infinite Dimensional Framework for Generative Adversarial
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.12087v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.12087v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hayk Asatryan, Hanno Gottschalk, Marieke Lippert, Matthias Rottmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, generative adversarial networks (GANs) have demonstrated
impressive experimental results while there are only a few works that foster
statistical learning theory for GANs. In this work, we propose an infinite
dimensional theoretical framework for generative adversarial learning. We
assume that the probability density functions of the underlying measure are
uniformly bounded, $k$-times $\alpha$-H\"{o}lder differentiable
($C^{k,\alpha}$) and uniformly bounded away from zero. Under these assumptions,
we show that the Rosenblatt transformation induces an optimal generator, which
is realizable in the hypothesis space of $C^{k,\alpha}$-generators. With a
consistent definition of the hypothesis space of discriminators, we further
show that the Jensen-Shannon divergence between the distribution induced by the
generator from the adversarial learning procedure and the data generating
distribution converges to zero. Under certain regularity assumptions on the
density of the data generating process, we also provide rates of convergence
based on chaining and concentration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BO-DBA: Query-Efficient Decision-Based Adversarial Attacks via Bayesian
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.02732v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.02732v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuosheng Zhang, Shucheng Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision-based attacks (DBA), wherein attackers perturb inputs to spoof
learning algorithms by observing solely the output labels, are a type of severe
adversarial attacks against Deep Neural Networks (DNNs) requiring minimal
knowledge of attackers. State-of-the-art DBA attacks relying on zeroth-order
gradient estimation require an excessive number of queries. Recently, Bayesian
optimization (BO) has shown promising in reducing the number of queries in
score-based attacks (SBA), in which attackers need to observe real-valued
probability scores as outputs. However, extending BO to the setting of DBA is
nontrivial because in DBA only output labels instead of real-valued scores, as
needed by BO, are available to attackers. In this paper, we close this gap by
proposing an efficient DBA attack, namely BO-DBA. Different from existing
approaches, BO-DBA generates adversarial examples by searching so-called
\emph{directions of perturbations}. It then formulates the problem as a BO
problem that minimizes the real-valued distortion of perturbations. With the
optimized perturbation generation process, BO-DBA converges much faster than
the state-of-the-art DBA techniques. Experimental results on pre-trained
ImageNet classifiers show that BO-DBA converges within 200 queries while the
state-of-the-art DBA techniques need over 15,000 queries to achieve the same
level of perturbation distortion. BO-DBA also shows similar attack success
rates even as compared to BO-based SBA attacks but with less distortion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Falsification of Digital Twins 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07210v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07210v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rob Cornish, Muhammad Faaiz Taufiq, Arnaud Doucet, Chris Holmes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital twins hold substantial promise in many applications, but rigorous
procedures for assessing their accuracy are essential for their widespread
deployment in safety-critical settings. By formulating this task within the
framework of causal inference, we show it is not possible to certify that a
twin is "correct" using real-world observational data unless potentially
tenuous assumptions are made about the data-generating process. To avoid these
assumptions, we propose an assessment strategy that instead aims to find cases
where the twin is not correct, and present a general-purpose statistical
procedure for doing so that may be used across a wide variety of applications
and twin models. Our approach yields reliable and actionable information about
the twin under only the assumption of an i.i.d. dataset of real-world
observations, and in particular remains sound even in the presence of arbitrary
unmeasured confounding. We demonstrate the effectiveness of our methodology via
a large-scale case study involving sepsis modelling within the Pulse Physiology
Engine, which we assess using the MIMIC-III dataset of ICU patients.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Generalizable Models for Vehicle Routing Problems via Knowledge
  Distillation <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07686v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07686v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jieyi Bi, Yining Ma, Jiahai Wang, Zhiguang Cao, Jinbiao Chen, Yuan Sun, Yeow Meng Chee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent neural methods for vehicle routing problems always train and test the
deep models on the same instance distribution (i.e., uniform). To tackle the
consequent cross-distribution generalization concerns, we bring the knowledge
distillation to this field and propose an Adaptive Multi-Distribution Knowledge
Distillation (AMDKD) scheme for learning more generalizable deep models.
Particularly, our AMDKD leverages various knowledge from multiple teachers
trained on exemplar distributions to yield a light-weight yet generalist
student model. Meanwhile, we equip AMDKD with an adaptive strategy that allows
the student to concentrate on difficult distributions, so as to absorb
hard-to-master knowledge more effectively. Extensive experimental results show
that, compared with the baseline neural methods, our AMDKD is able to achieve
competitive results on both unseen in-distribution and out-of-distribution
instances, which are either randomly synthesized or adopted from benchmark
datasets (i.e., TSPLIB and CVRPLIB). Notably, our AMDKD is generic, and
consumes less computational resources for inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is it worth it? Comparing six deep and classical methods for
  unsupervised anomaly detection in time series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.11080v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.11080v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ferdinand Rewicki, Joachim Denzler, Julia Niebling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting anomalies in time series data is important in a variety of fields,
including system monitoring, healthcare, and cybersecurity. While the abundance
of available methods makes it difficult to choose the most appropriate method
for a given application, each method has its strengths in detecting certain
types of anomalies. In this study, we compare six unsupervised anomaly
detection methods of varying complexity to determine whether more complex
methods generally perform better and if certain methods are better suited to
certain types of anomalies. We evaluated the methods using the UCR anomaly
archive, a recent benchmark dataset for anomaly detection. We analyzed the
results on a dataset and anomaly type level after adjusting the necessary
hyperparameters for each method. Additionally, we assessed the ability of each
method to incorporate prior knowledge about anomalies and examined the
differences between point-wise and sequence-wise features. Our experiments show
that classical machine learning methods generally outperform deep learning
methods across a range of anomaly types.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 Pages, The repository to reproduce the results is available at
  https://gitlab.com/dlr-dw/is-it-worth-it-benchmark</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural network with optimal neuron activation functions based on
  additive Gaussian process regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.05567v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.05567v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sergei Manzhos, Manabu Ihara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feed-forward neural networks (NN) are a staple machine learning method widely
used in many areas of science and technology. While even a single-hidden layer
NN is a universal approximator, its expressive power is limited by the use of
simple neuron activation functions (such as sigmoid functions) that are
typically the same for all neurons. More flexible neuron activation functions
would allow using fewer neurons and layers and thereby save computational cost
and improve expressive power. We show that additive Gaussian process regression
(GPR) can be used to construct optimal neuron activation functions that are
individual to each neuron. An approach is also introduced that avoids
non-linear fitting of neural network parameters. The resulting method combines
the advantage of robustness of a linear regression with the higher expressive
power of a NN. We demonstrate the approach by fitting the potential energy
surfaces of the water molecule and formaldehyde. Without requiring any
non-linear optimization, the additive GPR based approach outperforms a
conventional NN in the high accuracy regime, where a conventional NN suffers
more from overfitting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The secret role of undesired physical effects in accurate shape sensing
  with eccentric FBGs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.16316v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.16316v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samaneh Manavi Roodsari, Sara Freund, Martin Angelmahr, Georg Rauter, Wolfgang Schade, Philippe C. Cattin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fiber optic shape sensors have enabled unique advances in various navigation
tasks, from medical tool tracking to industrial applications. Eccentric fiber
Bragg gratings (FBG) are cheap and easy-to-fabricate shape sensors that are
often interrogated with simple setups. However, using low-cost interrogation
systems for such intensity-based quasi-distributed sensors introduces further
complications to the sensor's signal. Therefore, eccentric FBGs have not been
able to accurately estimate complex multi-bend shapes. Here, we present a novel
technique to overcome these limitations and provide accurate and precise shape
estimation in eccentric FBG sensors. We investigate the most important
bending-induced effects in curved optical fibers that are usually eliminated in
intensity-based fiber sensors. These effects contain shape deformation
information with a higher spatial resolution that we are now able to extract
using deep learning techniques. We design a deep learning model based on a
convolutional neural network that is trained to predict shapes given the
sensor's spectra. We also provide a visual explanation, highlighting wavelength
elements whose intensities are more relevant in making shape predictions. These
findings imply that deep learning techniques benefit from the bending-induced
effects that impact the desired signal in a complex manner. This is the first
step toward cheap yet accurate fiber shape sensing solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 5 figures, preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Intermediate Representations of Generative Models for Phase
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.15617v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.15617v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Uelwer, Sebastian Konietzny, Stefan Harmeling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Phase retrieval is the problem of reconstructing images from magnitude-only
measurements. In many real-world applications the problem is underdetermined.
When training data is available, generative models allow optimization in a
lower-dimensional latent space, hereby constraining the solution set to those
images that can be synthesized by the generative model. However, not all
possible solutions are within the range of the generator. Instead, they are
represented with some error. To reduce this representation error in the context
of phase retrieval, we first leverage a novel variation of intermediate layer
optimization (ILO) to extend the range of the generator while still producing
images consistent with the training data. Second, we introduce new
initialization schemes that further improve the quality of the reconstruction.
With extensive experiments on the Fourier phase retrieval problem and thorough
ablation studies, we can show the benefits of our modified ILO and the new
initialization schemes. Additionally, we analyze the performance of our
approach on the Gaussian phase retrieval problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research (TMLR). First
  two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating the Robustness of Trigger Set-Based Watermarks Embedded in
  Deep Neural Networks <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.10147v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.10147v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suyoung Lee, Wonho Song, Suman Jana, Meeyoung Cha, Sooel Son
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trigger set-based watermarking schemes have gained emerging attention as they
provide a means to prove ownership for deep neural network model owners. In
this paper, we argue that state-of-the-art trigger set-based watermarking
algorithms do not achieve their designed goal of proving ownership. We posit
that this impaired capability stems from two common experimental flaws that the
existing research practice has committed when evaluating the robustness of
watermarking algorithms: (1) incomplete adversarial evaluation and (2)
overlooked adaptive attacks. We conduct a comprehensive adversarial evaluation
of 11 representative watermarking schemes against six of the existing attacks
and demonstrate that each of these watermarking schemes lacks robustness
against at least two non-adaptive attacks. We also propose novel adaptive
attacks that harness the adversary's knowledge of the underlying watermarking
algorithm of a target model. We demonstrate that the proposed attacks
effectively break all of the 11 watermarking schemes, consequently allowing
adversaries to obscure the ownership of any watermarked model. We encourage
follow-up studies to consider our guidelines when evaluating the robustness of
their watermarking schemes via conducting comprehensive adversarial evaluation
that includes our adaptive attacks to demonstrate a meaningful upper bound of
watermark robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, accepted at IEEE TDSC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TINKER: A framework for Open source Cy<span class="highlight-title">bert</span>hreat Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.05571v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.05571v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nidhi Rastogi, Sharmishtha Dutta, Mohammed J. Zaki, Alex Gittens, Charu Aggarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Threat intelligence on malware attacks and campaigns is increasingly being
shared with other security experts for a cost or for free. Other security
analysts use this intelligence to inform them of indicators of compromise,
attack techniques, and preventative actions. Security analysts prepare threat
analysis reports after investigating an attack, an emerging cyber threat, or a
recently discovered vulnerability. Collectively known as cyber threat
intelligence (CTI), the reports are typically in an unstructured format and,
therefore, challenging to integrate seamlessly into existing intrusion
detection systems. This paper proposes a framework that uses the aggregated CTI
for analysis and defense at scale. The information is extracted and stored in a
structured format using knowledge graphs such that the semantics of the threat
intelligence can be preserved and shared at scale with other security analysts.
Specifically, we propose the first semi-supervised open-source knowledge
graph-based framework, TINKER, to capture cyber threat information and its
context. Following TINKER, we generate a Cyberthreat Intelligence Knowledge
Graph (CTI-KG) and demonstrate the usage using different use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning programs by combining programs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.01614v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.01614v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Cropper, Céline Hocquette
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of inductive logic programming is to induce a logic program (a set
of logical rules) that generalises training examples. Inducing programs with
many rules and literals is a major challenge. To tackle this challenge, we
introduce an approach where we learn small non-separable programs and combine
them. We implement our approach in a constraint-driven ILP system. Our approach
can learn optimal and recursive programs and perform predicate invention. Our
experiments on multiple domains, including game playing and program synthesis,
show that our approach can drastically outperform existing approaches in terms
of predictive accuracies and learning times, sometimes reducing learning times
from over an hour to a few seconds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review. arXiv admin note: text overlap with arXiv:2109.07818</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Zero-shot Generalisation in Deep Reinforcement Learning <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.09794v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.09794v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Kirk, Amy Zhang, Edward Grefenstette, Tim Rocktäschel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of zero-shot generalisation (ZSG) in deep Reinforcement Learning
(RL) aims to produce RL algorithms whose policies generalise well to novel
unseen situations at deployment time, avoiding overfitting to their training
environments. Tackling this is vital if we are to deploy reinforcement learning
algorithms in real world scenarios, where the environment will be diverse,
dynamic and unpredictable. This survey is an overview of this nascent field. We
rely on a unifying formalism and terminology for discussing different ZSG
problems, building upon previous works. We go on to categorise existing
benchmarks for ZSG, as well as current methods for tackling these problems.
Finally, we provide a critical discussion of the current state of the field,
including recommendations for future work. Among other conclusions, we argue
that taking a purely procedural content generation approach to benchmark design
is not conducive to progress in ZSG, we suggest fast online adaptation and
tackling RL-specific problems as some areas for future work on methods for ZSG,
and we recommend building benchmarks in underexplored problem settings such as
offline RL ZSG and reward-function variation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>JAIR version. Added formal definitions of ZSPT and related concepts,
  JAIR formatting, other small rewrites;
  https://www.jair.org/index.php/jair/article/view/14174</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainability in subgraphs-enhanced Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.07926v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.07926v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michele Guerra, Indro Spinelli, Simone Scardapane, Filippo Maria Bianchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, subgraphs-enhanced Graph Neural Networks (SGNNs) have been
introduced to enhance the expressive power of Graph Neural Networks (GNNs),
which was proved to be not higher than the 1-dimensional Weisfeiler-Leman
isomorphism test. The new paradigm suggests using subgraphs extracted from the
input graph to improve the model's expressiveness, but the additional
complexity exacerbates an already challenging problem in GNNs: explaining their
predictions. In this work, we adapt PGExplainer, one of the most recent
explainers for GNNs, to SGNNs. The proposed explainer accounts for the
contribution of all the different subgraphs and can produce a meaningful
explanation that humans can interpret. The experiments that we performed both
on real and synthetic datasets show that our framework is successful in
explaining the decision process of an SGNN on graph classification tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The source code implementing our workflow is publicly available
  online at https://github.com/MicheleUIT/Explaining_SGNN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedPara: Low-Rank Hadamard Product for Communication-Efficient Federated
  Learning <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.06098v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.06098v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nam Hyeon-Woo, Moon Ye-Bin, Tae-Hyun Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a communication-efficient parameterization, FedPara,
for federated learning (FL) to overcome the burdens on frequent model uploads
and downloads. Our method re-parameterizes weight parameters of layers using
low-rank weights followed by the Hadamard product. Compared to the conventional
low-rank parameterization, our FedPara method is not restricted to low-rank
constraints, and thereby it has a far larger capacity. This property enables to
achieve comparable performance while requiring 3 to 10 times lower
communication costs than the model with the original layers, which is not
achievable by the traditional low-rank methods. The efficiency of our method
can be further improved by combining with other efficient FL optimizers. In
addition, we extend our method to a personalized FL application, pFedPara,
which separates parameters into global and local ones. We show that pFedPara
outperforms competing personalized FL methods with more than three times fewer
parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-supervised</span> Trajectory Representation Learning with Temporal
  Regularities and Travel Semantics <span class="chip">ICDE 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09510v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09510v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Jiang, Dayan Pan, Houxing Ren, Xiaohan Jiang, Chao Li, Jingyuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory Representation Learning (TRL) is a powerful tool for
spatial-temporal data analysis and management. TRL aims to convert complicated
raw trajectories into low-dimensional representation vectors, which can be
applied to various downstream tasks, such as trajectory classification,
clustering, and similarity computation. Existing TRL works usually treat
trajectories as ordinary sequence data, while some important spatial-temporal
characteristics, such as temporal regularities and travel semantics, are not
fully exploited. To fill this gap, we propose a novel Self-supervised
trajectory representation learning framework with TemporAl Regularities and
Travel semantics, namely START. The proposed method consists of two stages. The
first stage is a Trajectory Pattern-Enhanced Graph Attention Network (TPE-GAT),
which converts the road network features and travel semantics into
representation vectors of road segments. The second stage is a Time-Aware
Trajectory Encoder (TAT-Enc), which encodes representation vectors of road
segments in the same trajectory as a trajectory representation vector,
meanwhile incorporating temporal regularities with the trajectory
representation. Moreover, we also design two self-supervised tasks, i.e.,
span-masked trajectory recovery and trajectory contrastive learning, to
introduce spatial-temporal characteristics of trajectories into the training
process of our START framework. The effectiveness of the proposed method is
verified by extensive experiments on two large-scale real-world datasets for
three downstream tasks. The experiments also demonstrate that our method can be
transferred across different cities to adapt heterogeneous trajectory datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures, Accepted by ICDE 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PD-MORL: Preference-Driven Multi-Objective Reinforcement Learning
  Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.07914v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.07914v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toygun Basaklar, Suat Gumussoy, Umit Y. Ogras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-objective reinforcement learning (MORL) approaches have emerged to
tackle many real-world problems with multiple conflicting objectives by
maximizing a joint objective function weighted by a preference vector. These
approaches find fixed customized policies corresponding to preference vectors
specified during training. However, the design constraints and objectives
typically change dynamically in real-life scenarios. Furthermore, storing a
policy for each potential preference is not scalable. Hence, obtaining a set of
Pareto front solutions for the entire preference space in a given domain with a
single training is critical. To this end, we propose a novel MORL algorithm
that trains a single universal network to cover the entire preference space
scalable to continuous robotic tasks. The proposed approach, Preference-Driven
MORL (PD-MORL), utilizes the preferences as guidance to update the network
parameters. It also employs a novel parallelization approach to increase sample
efficiency. We show that PD-MORL achieves up to 25% larger hypervolume for
challenging continuous control tasks and uses an order of magnitude fewer
trainable parameters compared to prior approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 8 Figures, 9 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tighter Regret Analysis and Optimization of Online Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.06491v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.06491v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dohyeok Kwon, Jonghwan Park, Songnam Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In federated learning (FL), it is commonly assumed that all data are placed
at clients in the beginning of machine learning (ML) optimization (i.e.,
offline learning). However, in many real-world applications, it is expected to
proceed in an online fashion. To this end, online FL (OFL) has been introduced,
which aims at learning a sequence of global models from decentralized streaming
data such that the so-called cumulative regret is minimized. Combining online
gradient descent and model averaging, in this framework, FedOGD is constructed
as the counterpart of FedSGD in FL. While it can enjoy an optimal sublinear
regret, FedOGD suffers from heavy communication costs. In this paper, we
present a communication-efficient method (named OFedIQ) by means of
intermittent transmission (enabled by client subsampling and periodic
transmission) and quantization. For the first time, we derive the regret bound
that captures the impact of data-heterogeneity and the communication-efficient
techniques. Through this, we efficiently optimize the parameters of OFedIQ such
as sampling rate, transmission period, and quantization levels. Also, it is
proved that the optimized OFedIQ can asymptotically achieve the performance of
FedOGD while reducing the communication costs by 99%. Via experiments with real
datasets, we demonstrate the effectiveness of the optimized OFedIQ.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v3. Compared to the previous version, tighter regret analysis and
  parameter optimization have been included. v4. Add comments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SACDNet: Towards Early Type 2 Diabetes Prediction with Uncertainty for
  Electronic Health Records 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.04844v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.04844v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tayyab Nasir, Muhammad Kamran Malik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Type 2 diabetes mellitus (T2DM) is one of the most common diseases and a
leading cause of death. The problem of early diagnosis of T2DM is challenging
and necessary to prevent serious complications. This study proposes a novel
neural network architecture for early T2DM prediction using multi-headed
self-attention and dense layers to extract features from historic diagnoses,
patient vitals, and demographics. The proposed technique is called the
Self-Attention for Comorbid Disease Net (SACDNet), achieving an accuracy of
89.3% and an F1-Score of 89.1%, having a 1.6% increased accuracy and 1.3%
increased f1-score compared to the baseline techniques. Monte Carlo (MC)
Dropout is applied to the SACDNet to get a bayesian approximation. A T2DM
prediction framework based on the MC Dropout SACDNet is proposed to quantize
the uncertainty associated with the predictions. A T2DM prediction dataset is
also built as part of this study which is based on real-world routine
Electronic Health Record (EHR) data comprising 4,124 diabetic and 181,767
non-diabetic examples, collected from 295 different EHR systems running in
different parts of the United States of America. This dataset is further used
to evaluate 7 different machine learning and 3 deep learning-based models.
Finally, a detailed analysis of the fairness of every technique against
different patient demographic groups is performed to validate the unbiased
generalization of the techniques and the diversity of the data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Misspelled SACEDNet changed to SACDNet in Abstract. Related Work
  corrected tehcniques to techniques. Dataset rh replaced with Rh. Methodology,
  mod replaced with models, andrepresentation with representations. Removed
  bold formatting from Table 3 in Methodology. Results replaced Hence the with
  Hence, this</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Analysis of Semantically-Aligned Speech-Text Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.01235v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.01235v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Huzaifah, Ivan Kukanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embeddings play an important role in end-to-end solutions for multi-modal
language processing problems. Although there has been some effort to understand
the properties of single-modality embedding spaces, particularly that of text,
their cross-modal counterparts are less understood. In this work, we study some
intrinsic properties of a joint speech-text embedding space, constructed by
minimizing the distance between paired utterance and transcription inputs in a
teacher-student model setup, that are informative for several prominent use
cases. We found that incorporating automatic speech recognition through both
pretraining and multitask scenarios aid semantic alignment significantly,
resulting in more tightly coupled embeddings. To analyse cross-modal embeddings
we utilise a quantitative retrieval accuracy metric for semantic alignment,
zero-shot classification for generalisability, and probing of the encoders to
observe the extent of knowledge transfer from one modality to another.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is the accepted version of the paper published at IEEE Spoken
  Language Technology (SLT) Workshop 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Regression For Scale-Varying Targets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.07447v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.07447v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Khakhar, Jacob Buckman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we demonstrate that a major limitation of regression using a
mean-squared error loss is its sensitivity to the scale of its targets. This
makes learning settings consisting of target's whose values take on varying
scales challenging. A recently-proposed alternative loss function, known as
histogram loss, avoids this issue. However, its computational cost grows
linearly with the number of buckets in the histogram, which renders prediction
with real-valued targets intractable. To address this issue, we propose a novel
approach to training deep learning models on real-valued regression targets,
autoregressive regression, which learns a high-fidelity distribution by
utilizing an autoregressive target decomposition. We demonstrate that this
training objective allows us to solve regression tasks involving targets with
different scales.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReFresh: Reducing Memory Access from Exploiting Stable Historical
  Embeddings for Graph Neural Network Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07482v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07482v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kezhao Huang, Haitian Jiang, Minjie Wang, Guangxuan Xiao, David Wipf, Xiang Song, Quan Gan, Zengfeng Huang, Jidong Zhai, Zheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key performance bottleneck when training graph neural network (GNN) models
on large, real-world graphs is loading node features onto a GPU. Due to limited
GPU memory, expensive data movement is necessary to facilitate the storage of
these features on alternative devices with slower access (e.g. CPU memory).
Moreover, the irregularity of graph structures contributes to poor data
locality which further exacerbates the problem. Consequently, existing
frameworks capable of efficiently training large GNN models usually incur a
significant accuracy degradation because of the inevitable shortcuts involved.
To address these limitations, we instead propose ReFresh, a general-purpose GNN
mini-batch training framework that leverages a historical cache for storing and
reusing GNN node embeddings instead of re-computing them through fetching raw
features at every iteration. Critical to its success, the corresponding cache
policy is designed, using a combination of gradient-based and staleness
criteria, to selectively screen those embeddings which are relatively stable
and can be cached, from those that need to be re-computed to reduce estimation
errors and subsequent downstream accuracy loss. When paired with complementary
system enhancements to support this selective historical cache, ReFresh is able
to accelerate the training speed on large graph datasets such as
ogbn-papers100M and MAG240M by 4.6x up to 23.6x and reduce the memory access by
64.5% (85.7% higher than a raw feature cache), with less than 1% influence on
test accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Rank by Causal Effects Without Data to Accurately Estimate
  Causal Effects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.12532v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.12532v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Fernández-Loría, Jorge Loría
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision makers often want to identify the individuals for whom some
intervention or treatment will be most effective in order to decide who to
treat. In such cases, decision makers would ideally like to rank potential
recipients of the treatment according to their individual causal effects.
However, the available data may be completely inadequate to estimate causal
effects accurately. We formalize a new assumption -- the rank preservation
assumption (RPA) -- that defines when data are suitable to learn how to rank
individuals according to their causal effects, even if the effects themselves
cannot be accurately estimated. The RPA holds when there is data to estimate a
scoring variable that induces the same ranking of individuals as the causal
effect of interest. Some of the scoring variables we consider are confounded
estimates, proxy causal effects, and non-causal quantities. We show that such
scoring variables can work well for treatment assignment if the RPA is met, and
potentially even better than using causal effects as scores. We also show that
the RPA holds under conditions that are more general and weaker than the
typical assumptions made in observational studies. Finally, we showcase how
practitioners can apply and evaluate alternative scoring models (including
non-causal models) to maximize the causal impact of their targeting decisions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Eco-PiNN: A Physics-informed Neural Network for Eco-toll Estimation <span class="chip">SDM23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.05739v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.05739v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Li, Mingzhou Yang, Matthew Eagon, Majid Farhadloo, Yiqun Xie, William F. Northrop, Shashi Shekhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The eco-toll estimation problem quantifies the expected environmental cost
(e.g., energy consumption, exhaust emissions) for a vehicle to travel along a
path. This problem is important for societal applications such as eco-routing,
which aims to find paths with the lowest exhaust emissions or energy need. The
challenges of this problem are three-fold: (1) the dependence of a vehicle's
eco-toll on its physical parameters; (2) the lack of access to data with
eco-toll information; and (3) the influence of contextual information (i.e. the
connections of adjacent segments in the path) on the eco-toll of road segments.
Prior work on eco-toll estimation has mostly relied on pure data-driven
approaches and has high estimation errors given the limited training data. To
address these limitations, we propose a novel Eco-toll estimation
Physics-informed Neural Network framework (Eco-PiNN) using three novel ideas,
namely, (1) a physics-informed decoder that integrates the physical laws of the
vehicle engine into the network, (2) an attention-based contextual information
encoder, and (3) a physics-informed regularization to reduce overfitting.
Experiments on real-world heavy-duty truck data show that the proposed method
can greatly improve the accuracy of eco-toll estimation compared with
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Full version of the paper accepted for the SDM23 conference; Yan Li
  and Mingzhou Yang contributed equally to this paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ General Greedy De-bias Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.10572v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.10572v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinzhe Han, Shuhui Wang, Chi Su, Qingming Huang, Qi Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks often make predictions relying on the spurious correlations
from the datasets rather than the intrinsic properties of the task of interest,
facing sharp degradation on out-of-distribution (OOD) test data. Existing
de-bias learning frameworks try to capture specific dataset bias by annotations
but they fail to handle complicated OOD scenarios. Others implicitly identify
the dataset bias by special design low capability biased models or losses, but
they degrade when the training and testing data are from the same distribution.
In this paper, we propose a General Greedy De-bias learning framework (GGD),
which greedily trains the biased models and the base model. The base model is
encouraged to focus on examples that are hard to solve with biased models, thus
remaining robust against spurious correlations in the test stage. GGD largely
improves models' OOD generalization ability on various tasks, but sometimes
over-estimates the bias level and degrades on the in-distribution test. We
further re-analyze the ensemble process of GGD and introduce the Curriculum
Regularization inspired by curriculum learning, which achieves a good trade-off
between in-distribution and out-of-distribution performance. Extensive
experiments on image classification, adversarial question answering, and visual
question answering demonstrate the effectiveness of our method. GGD can learn a
more robust base model under the settings of both task-specific biased models
with prior knowledge and self-ensemble biased model without prior knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted by IEEE T-PAMI. Copyright is transferred
  without notice, after which this version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Multi-view Clustering via Ensembles: Towards Scalability,
  Superiority, and Simplicity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11572v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11572v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Huang, Chang-Dong Wang, Jian-Huang Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant progress, there remain three limitations to the previous
multi-view clustering algorithms. First, they often suffer from high
computational complexity, restricting their feasibility for large-scale
datasets. Second, they typically fuse multi-view information via one-stage
fusion, neglecting the possibilities in multi-stage fusions. Third,
dataset-specific hyperparameter-tuning is frequently required, further
undermining their practicability. In light of this, we propose a fast
multi-view clustering via ensembles (FastMICE) approach. Particularly, the
concept of random view groups is presented to capture the versatile view-wise
relationships, through which the hybrid early-late fusion strategy is designed
to enable efficient multi-stage fusions. With multiple views extended to many
view groups, three levels of diversity (w.r.t. features, anchors, and
neighbors, respectively) are jointly leveraged for constructing the
view-sharing bipartite graphs in the early-stage fusion. Then, a set of
diversified base clusterings for different view groups are obtained via fast
graph partitioning, which are further formulated into a unified bipartite graph
for final clustering in the late-stage fusion. Notably, FastMICE has almost
linear time and space complexity, and is free of dataset-specific tuning.
Experiments on 22 multi-view datasets demonstrate its advantages in scalability
(for extremely large datasets), superiority (in clustering performance), and
simplicity (to be applied) over the state-of-the-art. Code available:
https://github.com/huangdonghere/FastMICE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in IEEE Transactions on Knowledge and Data Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Threats, Vulnerabilities, and Controls of Machine Learning Based
  Systems: A <span class="highlight-title">Survey</span> and Taxonomy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07474v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07474v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Kawamoto, Kazumasa Miyake, Koichi Konishi, Yutaka Oiwa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we propose the Artificial Intelligence Security Taxonomy to
systematize the knowledge of threats, vulnerabilities, and security controls of
machine-learning-based (ML-based) systems. We first classify the damage caused
by attacks against ML-based systems, define ML-specific security, and discuss
its characteristics. Next, we enumerate all relevant assets and stakeholders
and provide a general taxonomy for ML-specific threats. Then, we collect a wide
range of security controls against ML-specific threats through an extensive
review of recent literature. Finally, we classify the vulnerabilities and
controls of an ML-based system in terms of each vulnerable asset in the
system's entire lifecycle.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Data Augmentation for Graph Machine Learning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.08871v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.08871v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Zhao, Wei Jin, Yozen Liu, Yingheng Wang, Gang Liu, Stephan Günnemann, Neil Shah, Meng Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation has recently seen increased interest in graph machine
learning given its demonstrated ability to improve model performance and
generalization by added training data. Despite this recent surge, the area is
still relatively under-explored, due to the challenges brought by complex,
non-Euclidean structure of graph data, which limits the direct analogizing of
traditional augmentation operations on other types of image, video or text
data. Our work aims to give a necessary and timely overview of existing graph
data augmentation methods; notably, we present a comprehensive and systematic
survey of graph data augmentation approaches, summarizing the literature in a
structured manner. We first introduce three different taxonomies for
categorizing graph data augmentation methods from the data, task, and learning
perspectives, respectively. Next, we introduce recent advances in graph data
augmentation, differentiated by their methodologies and applications. We
conclude by outlining currently unsolved challenges and directions for future
research. Overall, our work aims to clarify the landscape of existing
literature in graph data augmentation and motivates additional work in this
area, providing a helpful resource for researchers and practitioners in the
broader graph machine learning domain. Additionally, we provide a continuously
updated reading list at
https://github.com/zhao-tong/graph-data-augmentation-papers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Vision <span class="highlight-title">Transformer</span>s with HiLo Attention <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.13213v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.13213v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zizheng Pan, Jianfei Cai, Bohan Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers (ViTs) have triggered the most recent and significant
breakthroughs in computer vision. Their efficient designs are mostly guided by
the indirect metric of computational complexity, i.e., FLOPs, which however has
a clear gap with the direct metric such as throughput. Thus, we propose to use
the direct speed evaluation on the target platform as the design principle for
efficient ViTs. Particularly, we introduce LITv2, a simple and effective ViT
which performs favourably against the existing state-of-the-art methods across
a spectrum of different model sizes with faster speed. At the core of LITv2 is
a novel self-attention mechanism, which we dub HiLo. HiLo is inspired by the
insight that high frequencies in an image capture local fine details and low
frequencies focus on global structures, whereas a multi-head self-attention
layer neglects the characteristic of different frequencies. Therefore, we
propose to disentangle the high/low frequency patterns in an attention layer by
separating the heads into two groups, where one group encodes high frequencies
via self-attention within each local window, and another group encodes low
frequencies by performing global attention between the average-pooled
low-frequency keys and values from each window and each query position in the
input feature map. Benefiting from the efficient design for both groups, we
show that HiLo is superior to the existing attention mechanisms by
comprehensively benchmarking FLOPs, speed and memory consumption on GPUs and
CPUs. For example, HiLo is 1.4x faster than spatial reduction attention and
1.6x faster than local window attention on CPUs. Powered by HiLo, LITv2 serves
as a strong backbone for mainstream vision tasks including image
classification, dense detection and segmentation. Code is available at
https://github.com/ziplab/LITv2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022 camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Deep Double Ritz Method (D$^2$RM) for solving Partial Differential
  Equations using Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.03627v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.03627v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Uriarte, David Pardo, Ignacio Muga, Judit Muñoz-Matute
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Residual minimization is a widely used technique for solving Partial
Differential Equations in variational form. It minimizes the dual norm of the
residual, which naturally yields a saddle-point (min-max) problem over the
so-called trial and test spaces. In the context of neural networks, we can
address this min-max approach by employing one network to seek the trial
minimum, while another network seeks the test maximizers. However, the
resulting method is numerically unstable as we approach the trial solution. To
overcome this, we reformulate the residual minimization as an equivalent
minimization of a Ritz functional fed by optimal test functions computed from
another Ritz functional minimization. We call the resulting scheme the Deep
Double Ritz Method (D$^2$RM), which combines two neural networks for
approximating trial functions and optimal test functions along a nested double
Ritz minimization strategy. Numerical results on different diffusion and
convection problems support the robustness of our method, up to the
approximation properties of the networks and the training capacity of the
optimizers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WaveMix: A Resource-efficient Neural Network for Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.14375v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.14375v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Jeevan, Kavitha Viswanathan, Anandu A S, Amit Sethi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To allow image analysis in resource-constrained scenarios without
compromising generalizability, we introduce WaveMix -- a novel and flexible
neural framework that reduces the GPU RAM (memory) and compute (latency)
compared to CNNs and transformers. In addition to using convolutional layers
that exploit shift-invariant image statistics, the proposed framework uses
multi-level two-dimensional discrete wavelet transform (2D-DWT) modules to
exploit scale-invariance and edge sparseness, which gives it the following
advantages. Firstly, the fixed weights of wavelet modules do not add to the
parameter count while reorganizing information based on these image priors.
Secondly, the wavelet modules scale the spatial extents of feature maps by
integral powers of $\frac{1}{2}\times\frac{1}{2}$, which reduces the memory and
latency required for forward and backward passes. Finally, a multi-level 2D-DWT
leads to a quicker expansion of the receptive field per layer than pooling
(which we do not use) and it is a more effective spatial token mixer. WaveMix
also generalizes better than other token mixing models, such as ConvMixer,
MLP-Mixer, PoolFormer, random filters, and Fourier basis, because the wavelet
transform is much better suited for image decomposition and spatial token
mixing. WaveMix is a flexible model that can perform well on multiple image
tasks without needing architectural modifications. WaveMix achieves a semantic
segmentation mIoU of 83% on the Cityscapes validation set outperforming
transformer and CNN-based architectures. We also demonstrate the advantages of
WaveMix for classification on multiple datasets and show that WaveMix
establishes new state-of-the-results in Places-365, EMNIST, and iNAT-mini
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures. arXiv admin note: text overlap with
  arXiv:2203.03689</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NAS-Bench-360: Benchmarking Neural Architecture Search on Diverse Tasks <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.05668v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.05668v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renbo Tu, Nicholas Roberts, Mikhail Khodak, Junhong Shen, Frederic Sala, Ameet Talwalkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing neural architecture search (NAS) benchmarks and algorithms
prioritize well-studied tasks, e.g. image classification on CIFAR or ImageNet.
This makes the performance of NAS approaches in more diverse areas poorly
understood. In this paper, we present NAS-Bench-360, a benchmark suite to
evaluate methods on domains beyond those traditionally studied in architecture
search, and use it to address the following question: do state-of-the-art NAS
methods perform well on diverse tasks? To construct the benchmark, we curate
ten tasks spanning a diverse array of application domains, dataset sizes,
problem dimensionalities, and learning objectives. Each task is carefully
chosen to interoperate with modern CNN-based search methods while possibly
being far-afield from its original development domain. To speed up and reduce
the cost of NAS research, for two of the tasks we release the precomputed
performance of 15,625 architectures comprising a standard CNN search space.
Experimentally, we show the need for more robust NAS evaluation of the kind
NAS-Bench-360 enables by showing that several modern NAS procedures perform
inconsistently across the ten tasks, with many catastrophically poor results.
We also demonstrate how NAS-Bench-360 and its associated precomputed results
will enable future scientific discoveries by testing whether several recent
hypotheses promoted in the NAS literature hold on diverse tasks. NAS-Bench-360
is hosted at https://nb360.ml.cmu.edu.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022 Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Dialogue Breakdown Detection with Semi-Supervised Learning <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.00136v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.00136v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Ng, Marzyeh Ghassemi, Narendran Thangarajan, Jiacheng Pan, Qi Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building user trust in dialogue agents requires smooth and consistent
dialogue exchanges. However, agents can easily lose conversational context and
generate irrelevant utterances. These situations are called dialogue breakdown,
where agent utterances prevent users from continuing the conversation. Building
systems to detect dialogue breakdown allows agents to recover appropriately or
avoid breakdown entirely. In this paper we investigate the use of
semi-supervised learning methods to improve dialogue breakdown detection,
including continued pre-training on the Reddit dataset and a manifold-based
data augmentation method. We demonstrate the effectiveness of these methods on
the Dialogue Breakdown Detection Challenge (DBDC) English shared task. Our
submissions to the 2020 DBDC5 shared task place first, beating baselines and
other submissions by over 12\% accuracy. In ablations on DBDC4 data from 2019,
our semi-supervised learning methods improve the performance of a baseline BERT
model by 2\% accuracy. These methods are applicable generally to any dialogue
task and provide a simple way to improve model performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 1 figure, accepted at the NeurIPS Workshop on Human in the
  Loop Dialogue Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DIAMOND: Taming Sample and Communication Complexities in Decentralized
  Bilevel Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02376v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02376v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiwen Qiu, Yining Li, Zhuqing Liu, Prashant Khanduri, Jia Liu, Ness B. Shroff, Elizabeth Serena Bentley, Kurt Turck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decentralized bilevel optimization has received increasing attention recently
due to its foundational role in many emerging multi-agent learning paradigms
(e.g., multi-agent meta-learning and multi-agent reinforcement learning) over
peer-to-peer edge networks. However, to work with the limited computation and
communication capabilities of edge networks, a major challenge in developing
decentralized bilevel optimization techniques is to lower sample and
communication complexities. This motivates us to develop a new decentralized
bilevel optimization called DIAMOND (decentralized single-timescale stochastic
approximation with momentum and gradient-tracking). The contributions of this
paper are as follows: i) our DIAMOND algorithm adopts a single-loop structure
rather than following the natural double-loop structure of bilevel
optimization, which offers low computation and implementation complexity; ii)
compared to existing approaches, the DIAMOND algorithm does not require any
full gradient evaluations, which further reduces both sample and computational
complexities; iii) through a careful integration of momentum information and
gradient tracking techniques, we show that the DIAMOND algorithm enjoys
$\mathcal{O}(\epsilon^{-3/2})$ in sample and communication complexities for
achieving an $\epsilon$-stationary solution, both of which are independent of
the dataset sizes and significantly outperform existing works. Extensive
experiments also verify our theoretical findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Metalearning Approach for Physics-Informed Neural Networks (PINNs):
  Application to Parameterized PDEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.13361v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.13361v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Penwarden, Shandian Zhe, Akil Narayan, Robert M. Kirby
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-informed neural networks (PINNs) as a means of discretizing partial
differential equations (PDEs) are garnering much attention in the Computational
Science and Engineering (CS&E) world. At least two challenges exist for PINNs
at present: an understanding of accuracy and convergence characteristics with
respect to tunable parameters and identification of optimization strategies
that make PINNs as efficient as other computational science tools. The cost of
PINNs training remains a major challenge of Physics-informed Machine Learning
(PiML) - and, in fact, machine learning (ML) in general. This paper is meant to
move towards addressing the latter through the study of PINNs on new tasks, for
which parameterized PDEs provides a good testbed application as tasks can be
easily defined in this context. Following the ML world, we introduce
metalearning of PINNs with application to parameterized PDEs. By introducing
metalearning and transfer learning concepts, we can greatly accelerate the
PINNs optimization process. We present a survey of model-agnostic metalearning,
and then discuss our model-aware metalearning applied to PINNs as well as
implementation considerations and algorithmic complexity. We then test our
approach on various canonical forward parameterized PDEs that have been
presented in the emerging PINNs literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpretable bilinear attention network with domain adaptation improves
  drug-target prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.02194v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.02194v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peizhen Bai, Filip Miljković, Bino John, Haiping Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting drug-target interaction is key for drug discovery. Recent deep
learning-based methods show promising performance but two challenges remain:
(i) how to explicitly model and learn local interactions between drugs and
targets for better prediction and interpretation; (ii) how to generalize
prediction performance on novel drug-target pairs from different distribution.
In this work, we propose DrugBAN, a deep bilinear attention network (BAN)
framework with domain adaptation to explicitly learn pair-wise local
interactions between drugs and targets, and adapt on out-of-distribution data.
DrugBAN works on drug molecular graphs and target protein sequences to perform
prediction, with conditional domain adversarial learning to align learned
interaction representations across different distributions for better
generalization on novel drug-target pairs. Experiments on three benchmark
datasets under both in-domain and cross-domain settings show that DrugBAN
achieves the best overall performance against five state-of-the-art baselines.
Moreover, visualizing the learned bilinear attention map provides interpretable
insights from prediction results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Artificial Neuronal Ensembles with Learned Context Dependent Gating 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07187v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07187v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew J. Tilley, Michelle Miller, David J. Freedman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biological neural networks are capable of recruiting different sets of
neurons to encode different memories. However, when training artificial neural
networks on a set of tasks, typically, no mechanism is employed for selectively
producing anything analogous to these neuronal ensembles. Further, artificial
neural networks suffer from catastrophic forgetting, where the network's
performance rapidly deteriorates as tasks are learned sequentially. By
contrast, sequential learning is possible for a range of biological organisms.
We introduce Learned Context Dependent Gating (LXDG), a method to flexibly
allocate and recall `artificial neuronal ensembles', using a particular network
structure and a new set of regularization terms. Activities in the hidden
layers of the network are modulated by gates, which are dynamically produced
during training. The gates are outputs of networks themselves, trained with a
sigmoid output activation. The regularization terms we have introduced
correspond to properties exhibited by biological neuronal ensembles. The first
term penalizes low gate sparsity, ensuring that only a specified fraction of
the network is used. The second term ensures that previously learned gates are
recalled when the network is presented with input from previously learned
tasks. Finally, there is a regularization term responsible for ensuring that
new tasks are encoded in gates that are as orthogonal as possible from
previously used ones. We demonstrate the ability of this method to alleviate
catastrophic forgetting on continual learning benchmarks. When the new
regularization terms are included in the model along with Elastic Weight
Consolidation (EWC) it achieves better performance on the benchmark `permuted
MNIST' than with EWC alone. The benchmark `rotated MNIST' demonstrates how
similar tasks recruit similar neurons to the artificial neuronal ensemble.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectral embedding of weighted graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1910.05534v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1910.05534v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Gallagher, Andrew Jones, Anna Bertiger, Carey Priebe, Patrick Rubin-Delanchy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When analyzing weighted networks using spectral embedding, a judicious
transformation of the edge weights may produce better results. To formalize
this idea, we consider the asymptotic behavior of spectral embedding for
different edge-weight representations, under a generic low rank model. We
measure the quality of different embeddings -- which can be on entirely
different scales -- by how easy it is to distinguish communities, in an
information-theoretic sense. For common types of weighted graphs, such as count
networks or p-value networks, we find that transformations such as tempering or
thresholding can be highly beneficial, both in theory and in practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Policy Extragradient Methods for Competitive Games with Entropy
  Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.15186v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.15186v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shicong Cen, Yuting Wei, Yuejie Chi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the problem of computing the equilibrium of
competitive games, which is often modeled as a constrained saddle-point
optimization problem with probability simplex constraints. Despite recent
efforts in understanding the last-iterate convergence of extragradient methods
in the unconstrained setting, the theoretical underpinnings of these methods in
the constrained settings, especially those using multiplicative updates, remain
highly inadequate, even when the objective function is bilinear. Motivated by
the algorithmic role of entropy regularization in single-agent reinforcement
learning and game theory, we develop provably efficient extragradient methods
to find the quantal response equilibrium (QRE) -- which are solutions to
zero-sum two-player matrix games with entropy regularization -- at a linear
rate. The proposed algorithms can be implemented in a decentralized manner,
where each player executes symmetric and multiplicative updates iteratively
using its own payoff without observing the opponent's actions directly. In
addition, by controlling the knob of entropy regularization, the proposed
algorithms can locate an approximate Nash equilibrium of the unregularized
matrix game at a sublinear rate without assuming the Nash equilibrium to be
unique. Our methods also lead to efficient policy extragradient algorithms for
solving (entropy-regularized) zero-sum Markov games at similar rates. All of
our convergence rates are nearly dimension-free, which are independent of the
size of the state and action spaces up to logarithm factors, highlighting the
positive role of entropy regularization for accelerating convergence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asynchronous, Option-Based Multi-Agent Policy Gradient: A Conditional
  Reasoning Approach <span class="chip">ICRA2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.15925v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.15925v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xubo Lyu, Amin Banitalebi-Dehkordi, Mo Chen, Yong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent policy gradient methods have demonstrated success in games and
robotics but are often limited to problems with low-level action space.
However, when agents take higher-level, temporally-extended actions (i.e.
options), when and how to derive a centralized control policy, its gradient as
well as sampling options for all agents while not interrupting current option
executions, becomes a challenge. This is mostly because agents may choose and
terminate their options \textit{asynchronously}. In this work, we propose a
conditional reasoning approach to address this problem, and empirically
validate its effectiveness on representative option-based multi-agent
cooperative tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICRA2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Look, Listen, and Attack: Backdoor Attacks Against Video Action
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00986v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00986v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hasan Abed Al Kader Hammoud, Shuming Liu, Mohammed Alkhrashi, Fahad AlBalawi, Bernard Ghanem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) are vulnerable to a class of attacks called
"backdoor attacks", which create an association between a backdoor trigger and
a target label the attacker is interested in exploiting. A backdoored DNN
performs well on clean test images, yet persistently predicts an
attacker-defined label for any sample in the presence of the backdoor trigger.
Although backdoor attacks have been extensively studied in the image domain,
there are very few works that explore such attacks in the video domain, and
they tend to conclude that image backdoor attacks are less effective in the
video domain. In this work, we revisit the traditional backdoor threat model
and incorporate additional video-related aspects to that model. We show that
poisoned-label image backdoor attacks could be extended temporally in two ways,
statically and dynamically, leading to highly effective attacks in the video
domain. In addition, we explore natural video backdoors to highlight the
seriousness of this vulnerability in the video domain. And, for the first time,
we study multi-modal (audiovisual) backdoor attacks against video action
recognition models, where we show that attacking a single modality is enough
for achieving a high attack success rate.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-01-18T00:00:00Z">2023-01-18</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Close is Chat<span class="highlight-title">GPT</span> to Human Experts? Comparison Corpus, Evaluation,
  and Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, Yupeng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of ChatGPT has garnered widespread attention in both
academic and industrial communities. ChatGPT is able to respond effectively to
a wide range of human questions, providing fluent and comprehensive answers
that significantly surpass previous public chatbots in terms of security and
usefulness. On one hand, people are curious about how ChatGPT is able to
achieve such strength and how far it is from human experts. On the other hand,
people are starting to worry about the potential negative impacts that large
language models (LLMs) like ChatGPT could have on society, such as fake news,
plagiarism, and social security issues. In this work, we collected tens of
thousands of comparison responses from both human experts and ChatGPT, with
questions ranging from open-domain, financial, medical, legal, and
psychological areas. We call the collected dataset the Human ChatGPT Comparison
Corpus (HC3). Based on the HC3 dataset, we study the characteristics of
ChatGPT's responses, the differences and gaps from human experts, and future
directions for LLMs. We conducted comprehensive human evaluations and
linguistic analyses of ChatGPT-generated content compared with that of humans,
where many interesting results are revealed. After that, we conduct extensive
experiments on how to effectively detect whether a certain text is generated by
ChatGPT or humans. We build three different detection systems, explore several
key factors that influence their effectiveness, and evaluate them in different
scenarios. The dataset, code, and models are all publicly available at
https://github.com/Hello-SimpleAI/chatgpt-comparison-detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/Hello-SimpleAI/chatgpt-comparison-detection</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Holistic Understanding of Mathematical Questions with
  Contrastive <span class="highlight-title">Pre-train</span>ing <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuting Ning, Zhenya Huang, Xin Lin, Enhong Chen, Shiwei Tong, Zheng Gong, Shijin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding mathematical questions effectively is a crucial task, which can
benefit many applications, such as difficulty estimation. Researchers have
drawn much attention to designing pre-training models for question
representations due to the scarcity of human annotations (e.g., labeling
difficulty). However, unlike general free-format texts (e.g., user comments),
mathematical questions are generally designed with explicit purposes and
mathematical logic, and usually consist of more complex content, such as
formulas, and related mathematical knowledge (e.g., Function). Therefore, the
problem of holistically representing mathematical questions remains
underexplored. To this end, in this paper, we propose a novel contrastive
pre-training approach for mathematical question representations, namely QuesCo,
which attempts to bring questions with more similar purposes closer.
Specifically, we first design two-level question augmentations, including
content-level and structure-level, which generate literally diverse question
pairs with similar purposes. Then, to fully exploit hierarchical information of
knowledge concepts, we propose a knowledge hierarchy-aware rank strategy
(KHAR), which ranks the similarities between questions in a fine-grained
manner. Next, we adopt a ranking contrastive learning task to optimize our
model based on the augmented and ranked questions. We conduct extensive
experiments on two real-world mathematical datasets. The experimental results
demonstrate the effectiveness of our model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Quantitative Exploration of Natural Language Processing Applications
  for Electricity Demand Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun Bai, Simon Camal, Andrea Michiorri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The relationship between electricity demand and weather has been established
for a long time and is one of the cornerstones in load prediction for operation
and planning, along with behavioral and social aspects such as calendars or
significant events. This paper explores how and why the social information
contained in the news can be used better to understand aggregate population
behaviour in terms of energy demand. The work is done through experiments
analysing the impact of predicting features extracted from national news on
day-ahead electric demand prediction. The results are compared to a benchmark
model trained exclusively on the calendar and meteorological information.
Experimental results showed that the best-performing model reduced the official
standard errors around 4%, 11%, and 10% in terms of RMSE, MAE, and SMAPE. The
best-performing methods are: word frequency identified COVID-19-related
keywords; topic distribution that identified news on the pandemic and internal
politics; global word embeddings that identified news about international
conflicts. This study brings a new perspective to traditional electricity
demand analysis and confirms the feasibility of improving its predictions with
unstructured information contained in texts, with potential consequences in
sociology and economics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graphix-T5: Mixing <span class="highlight-title">Pre-Train</span>ed <span class="highlight-title">Transformer</span>s with Graph-Aware Layers for
  Text-to-SQL Parsing <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao Ma, Nan Huo, Fei Huang, Wenyu Du, Luo Si, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of text-to-SQL parsing, which aims at converting natural language
questions into executable SQL queries, has garnered increasing attention in
recent years, as it can assist end users in efficiently extracting vital
information from databases without the need for technical background. One of
the major challenges in text-to-SQL parsing is domain generalization, i.e., how
to generalize well to unseen databases. Recently, the pre-trained text-to-text
transformer model, namely T5, though not specialized for text-to-SQL parsing,
has achieved state-of-the-art performance on standard benchmarks targeting
domain generalization. In this work, we explore ways to further augment the
pre-trained T5 model with specialized components for text-to-SQL parsing. Such
components are expected to introduce structural inductive bias into text-to-SQL
parsers thus improving model's capacity on (potentially multi-hop) reasoning,
which is critical for generating structure-rich SQLs. To this end, we propose a
new architecture GRAPHIX-T5, a mixed model with the standard pre-trained
transformer model augmented by some specially-designed graph-aware layers.
Extensive experiments and analysis demonstrate the effectiveness of GRAPHIX-T5
across four text-to-SQL benchmarks: SPIDER, SYN, REALISTIC and DK. GRAPHIX-T5
surpass all other T5-based parsers with a significant margin, achieving new
state-of-the-art performance. Notably, GRAPHIX-T5-large reach performance
superior to the original T5-large by 5.7% on exact match (EM) accuracy and 6.6%
on execution accuracy (EX). This even outperforms the T5-3B by 1.2% on EM and
1.5% on EX.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2023 main conference (oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KILDST: Effective Knowledge-Integrated Learning for Dialogue State
  Tracking using Gazetteer and Speaker Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyungtak Choi, Hyeonmok Ko, Gurpreet Kaur, Lohith Ravuru, Kiranmayi Gandikota, Manisha Jhawar, Simma Dharani, Pranamya Patil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialogue State Tracking (DST) is core research in dialogue systems and has
received much attention. In addition, it is necessary to define a new problem
that can deal with dialogue between users as a step toward the conversational
AI that extracts and recommends information from the dialogue between users.
So, we introduce a new task - DST from dialogue between users about scheduling
an event (DST-USERS). The DST-USERS task is much more challenging since it
requires the model to understand and track dialogue states in the dialogue
between users and to understand who suggested the schedule and who agreed to
the proposed schedule. To facilitate DST-USERS research, we develop dialogue
datasets between users that plan a schedule. The annotated slot values which
need to be extracted in the dialogue are date, time, and location. Previous
approaches, such as Machine Reading Comprehension (MRC) and traditional DST
techniques, have not achieved good results in our extensive evaluations. By
adopting the knowledge-integrated learning method, we achieve exceptional
results. The proposed model architecture combines gazetteer features and
speaker information efficiently. Our evaluations of the dialogue datasets
between users that plan a schedule show that our model outperforms the baseline
model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adapting Multilingual Speech Representation Model for a New,
  Underresourced Language through Multilingual Fine-tuning and Continued
  <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karol Nowakowski, Michal Ptaszynski, Kyoko Murasaki, Jagna Nieuważny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, neural models learned through self-supervised pretraining on
large scale multilingual text or speech data have exhibited promising results
for underresourced languages, especially when a relatively large amount of data
from related language(s) is available. While the technology has a potential for
facilitating tasks carried out in language documentation projects, such as
speech transcription, pretraining a multilingual model from scratch for every
new language would be highly impractical. We investigate the possibility for
adapting an existing multilingual wav2vec 2.0 model for a new language,
focusing on actual fieldwork data from a critically endangered tongue: Ainu.
Specifically, we (i) examine the feasibility of leveraging data from similar
languages also in fine-tuning; (ii) verify whether the model's performance can
be improved by further pretraining on target language data. Our results show
that continued pretraining is the most effective method to adapt a wav2vec 2.0
model for a new language and leads to considerable reduction in error rates.
Furthermore, we find that if a model pretrained on a related speech variety or
an unrelated language with similar phonological characteristics is available,
multilingual fine-tuning using additional data from that language can have
positive impact on speech recognition performance when there is very little
labeled data in the target language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding and Detecting Hallucinations in Neural Machine Translation
  via Model Introspection <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijia Xu, Sweta Agrawal, Eleftheria Briakou, Marianna J. Martindale, Marine Carpuat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural sequence generation models are known to "hallucinate", by producing
outputs that are unrelated to the source text. These hallucinations are
potentially harmful, yet it remains unclear in what conditions they arise and
how to mitigate their impact. In this work, we first identify internal model
symptoms of hallucinations by analyzing the relative token contributions to the
generation in contrastive hallucinated vs. non-hallucinated outputs generated
via source perturbations. We then show that these symptoms are reliable
indicators of natural hallucinations, by using them to design a lightweight
hallucination detector which outperforms both model-free baselines and strong
classifiers based on quality estimation or large pre-trained models on manually
annotated English-Chinese and German-English translation test beds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at TACL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InstructPix2Pix: Learning to Follow Image Editing Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09800v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09800v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Brooks, Aleksander Holynski, Alexei A. Efros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method for editing images from human instructions: given an
input image and a written instruction that tells the model what to do, our
model follows these instructions to edit the image. To obtain training data for
this problem, we combine the knowledge of two large pretrained models -- a
language model (GPT-3) and a text-to-image model (Stable Diffusion) -- to
generate a large dataset of image editing examples. Our conditional diffusion
model, InstructPix2Pix, is trained on our generated data, and generalizes to
real images and user-written instructions at inference time. Since it performs
edits in the forward pass and does not require per example fine-tuning or
inversion, our model edits images quickly, in a matter of seconds. We show
compelling editing results for a diverse collection of input images and written
instructions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page with code:
  https://www.timothybrooks.com/instruct-pix2pix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Global Contrastive Batch Sampling via Optimization on Sample
  Permutations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.12874v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.12874v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vin Sachidananda, Ziyi Yang, Chenguang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Learning has recently achieved state-of-the-art performance in a
wide range of tasks. Many contrastive learning approaches use mined hard
negatives to make batches more informative during training but these approaches
are inefficient as they increase epoch length proportional to the number of
mined negatives and require frequent updates of nearest neighbor indices or
mining from recent batches. In this work, we provide an alternative to hard
negative mining, Global Contrastive Batch Sampling (GCBS), an efficient
approximation to the batch assignment problem that upper bounds the gap between
the global and training losses, $\mathcal{L}^{Global} - \mathcal{L}^{Train}$,
in contrastive learning settings. Through experimentation we find GCBS improves
state-of-the-art performance in sentence embedding and code-search tasks.
Additionally, GCBS is easy to implement as it requires only a few additional
lines of code, does not maintain external data structures such as nearest
neighbor indices, is more computationally efficient than the most minimal hard
negative mining approaches, and makes no changes to the model being trained.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Controllable Neural Story Plot Generation via Reward Shaping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1809.10736v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1809.10736v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pradyumna Tambwekar, Murtaza Dhuliawala, Lara J. Martin, Animesh Mehta, Brent Harrison, Mark O. Riedl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language-modeling--based approaches to story plot generation attempt to
construct a plot by sampling from a language model (LM) to predict the next
character, word, or sentence to add to the story. LM techniques lack the
ability to receive guidance from the user to achieve a specific goal, resulting
in stories that don't have a clear sense of progression and lack coherence. We
present a reward-shaping technique that analyzes a story corpus and produces
intermediate rewards that are backpropagated into a pre-trained LM in order to
guide the model towards a given goal. Automated evaluations show our technique
can create a model that generates story plots which consistently achieve a
specified goal. Human-subject studies show that the generated stories have more
plausible event ordering than baseline plot generation techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pradyumna Tambwekar & Murtaza Dhuliawala contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Out-of-Distribution Performance on Document Image Classifiers <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07448v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07448v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Larson, Gordon Lim, Yutong Ai, David Kuang, Kevin Leach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability of a document classifier to handle inputs that are drawn from a
distribution different from the training distribution is crucial for robust
deployment and generalizability. The RVL-CDIP corpus is the de facto standard
benchmark for document classification, yet to our knowledge all studies that
use this corpus do not include evaluation on out-of-distribution documents. In
this paper, we curate and release a new out-of-distribution benchmark for
evaluating out-of-distribution performance for document classifiers. Our new
out-of-distribution benchmark consists of two types of documents: those that
are not part of any of the 16 in-domain RVL-CDIP categories (RVL-CDIP-O), and
those that are one of the 16 in-domain categories yet are drawn from a
distribution different from that of the original RVL-CDIP dataset (RVL-CDIP-N).
While prior work on document classification for in-domain RVL-CDIP documents
reports high accuracy scores, we find that these models exhibit accuracy drops
of between roughly 15-30% on our new out-of-domain RVL-CDIP-N benchmark, and
further struggle to distinguish between in-domain RVL-CDIP-N and out-of-domain
RVL-CDIP-O inputs. Our new benchmark provides researchers with a valuable new
resource for analyzing out-of-distribution performance on document classifiers.
Our new out-of-distribution data can be found at
https://github.com/gxlarson/rvl-cdip-ood.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS D&B 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Programming by Example and Text-to-Code Translation for Conversational
  Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11554v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11554v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eli Whitehouse, William Gerard, Yauhen Klimovich, Marc Franco-Salvador
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialogue systems is an increasingly popular task of natural language
processing. However, the dialogue paths tend to be deterministic, restricted to
the system rails, regardless of the given request or input text. Recent
advances in program synthesis have led to systems which can synthesize programs
from very general search spaces, e.g. Programming by Example, and to systems
with very accessible interfaces for writing programs, e.g. text-to-code
translation, but have not achieved both of these qualities in the same system.
We propose Modular Programs for Text-guided Hierarchical Synthesis (MPaTHS), a
method for integrating Programming by Example and text-to-code systems which
offers an accessible natural language interface for synthesizing general
programs. We present a program representation that allows our method to be
applied to the problem of task-oriented dialogue. Finally, we demo MPaTHS using
our program representation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 2 figures, conference preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>ing Large Language Model for Machine Translation: A Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07069v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07069v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biao Zhang, Barry Haddow, Alexandra Birch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on prompting has shown excellent performance with little or even no
supervised training across many tasks. However, prompting for machine
translation is still under-explored in the literature. We fill this gap by
offering a systematic study on prompting strategies for translation, examining
various factors for prompt template and demonstration example selection. We
further explore the use of monolingual data and the feasibility of
cross-lingual, cross-domain, and sentence-to-document transfer learning in
prompting. Extensive experiments with GLM-130B (Zeng et al., 2022) as the
testbed show that 1) the number and the quality of prompt examples matter,
where using suboptimal examples degenerates translation; 2) several features of
prompt examples, such as semantic similarity, show significant Spearman
correlation with their prompting performance; yet, none of the correlations are
strong enough; 3) using pseudo parallel prompt examples constructed from
monolingual data via zero-shot prompting could improve translation; and 4)
improved performance is achievable by transferring knowledge from prompt
examples selected in other settings. We finally provide an analysis on the
model outputs and discuss several problems that prompting still suffers from.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Teacher Forcing Recovers Reward Functions for Text Generation <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.08708v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.08708v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchang Hao, Yuxin Liu, Lili Mou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has been widely used in text generation to
alleviate the exposure bias issue or to utilize non-parallel datasets. The
reward function plays an important role in making RL training successful.
However, previous reward functions are typically task-specific and sparse,
restricting the use of RL. In our work, we propose a task-agnostic approach
that derives a step-wise reward function directly from a model trained with
teacher forcing. We additionally propose a simple modification to stabilize the
RL training on non-parallel datasets with our induced reward function.
Empirical results show that our method outperforms self-training and reward
regression methods on several text generation tasks, confirming the
effectiveness of our reward function.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Good Is NLP? A Sober Look at NLP Tasks through the Lens of Social
  Impact <span class="chip">ACL 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.02359v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.02359v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijing Jin, Geeticka Chauhan, Brian Tse, Mrinmaya Sachan, Rada Mihalcea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have seen many breakthroughs in natural language processing
(NLP), transitioning it from a mostly theoretical field to one with many
real-world applications. Noting the rising number of applications of other
machine learning and AI techniques with pervasive societal impact, we
anticipate the rising importance of developing NLP technologies for social
good. Inspired by theories in moral philosophy and global priorities research,
we aim to promote a guideline for social good in the context of NLP. We lay the
foundations via the moral philosophy definition of social good, propose a
framework to evaluate the direct and indirect real-world impact of NLP tasks,
and adopt the methodology of global priorities research to identify priority
causes for NLP research. Finally, we use our theoretical framework to provide
some practical guidelines for future NLP research for social good. Our data and
code are available at http://github.com/zhijing-jin/nlp4sg_acl2021. In
addition, we curate a list of papers and resources on NLP for social good at
https://github.com/zhijing-jin/NLP4SocialGood_Papers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of ACL 2021; also accepted at the NLP for Positive Impact
  workshop@ACL 2021</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning 3D-aware Image Synthesis with Unknown Pose Distribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifan Shi, Yujun Shen, Yinghao Xu, Sida Peng, Yiyi Liao, Sheng Guo, Qifeng Chen, Dit-Yan Yeung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing methods for 3D-aware image synthesis largely depend on the 3D pose
distribution pre-estimated on the training set. An inaccurate estimation may
mislead the model into learning faulty geometry. This work proposes PoF3D that
frees generative radiance fields from the requirements of 3D pose priors. We
first equip the generator with an efficient pose learner, which is able to
infer a pose from a latent code, to approximate the underlying true pose
distribution automatically. We then assign the discriminator a task to learn
pose distribution under the supervision of the generator and to differentiate
real and synthesized images with the predicted pose as the condition. The
pose-free generator and the pose-aware discriminator are jointly trained in an
adversarial manner. Extensive results on a couple of datasets confirm that the
performance of our approach, regarding both image quality and geometry quality,
is on par with state of the art. To our best knowledge, PoF3D demonstrates the
feasibility of learning high-quality 3D-aware image synthesis without using 3D
pose priors for the first time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://vivianszf.github.io/pof3d/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention2Minority: A salient instance inference-based multiple instance
  learning for classifying small lesions in whole slide images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Su, Mostafa Rezapour, Usama Sajjad, Metin Nafi Gurcan, Muhammad Khalid Khan Niazi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple instance learning (MIL) models have achieved remarkable success in
analyzing whole slide images (WSIs) for disease classification problems.
However, with regard to gigapixel WSI classification problems, current MIL
models are often incapable of differentiating a WSI with extremely small tumor
lesions. This minute tumor-to-normal area ratio in a MIL bag inhibits the
attention mechanism from properly weighting the areas corresponding to minor
tumor lesions. To overcome this challenge, we propose salient instance
inference MIL (SiiMIL), a weakly-supervised MIL model for WSI classification.
Our method initially learns representations of normal WSIs, and it then
compares the normal WSIs representations with all the input patches to infer
the salient instances of the input WSI. Finally, it employs attention-based MIL
to perform the slide-level classification based on the selected patches of the
WSI. Our experiments imply that SiiMIL can accurately identify tumor instances,
which could only take up less than 1% of a WSI, so that the ratio of tumor to
normal instances within a bag can increase by two to four times. It is worth
mentioning that it performs equally well for large tumor lesions. As a result,
SiiMIL achieves a significant improvement in performance over the
state-of-the-art MIL methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniObject3D: Large-Vocabulary 3D Object <span class="highlight-title">Dataset</span> for Realistic
  Perception, Reconstruction and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian, Dahua Lin, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in modeling 3D objects mostly rely on synthetic datasets due
to the lack of large-scale realscanned 3D databases. To facilitate the
development of 3D perception, reconstruction, and generation in the real world,
we propose OmniObject3D, a large vocabulary 3D object dataset with massive
high-quality real-scanned 3D objects. OmniObject3D has several appealing
properties: 1) Large Vocabulary: It comprises 6,000 scanned objects in 190
daily categories, sharing common classes with popular 2D datasets (e.g.,
ImageNet and LVIS), benefiting the pursuit of generalizable 3D representations.
2) Rich Annotations: Each 3D object is captured with both 2D and 3D sensors,
providing textured meshes, point clouds, multiview rendered images, and
multiple real-captured videos. 3) Realistic Scans: The professional scanners
support highquality object scans with precise shapes and realistic appearances.
With the vast exploration space offered by OmniObject3D, we carefully set up
four evaluation tracks: a) robust 3D perception, b) novel-view synthesis, c)
neural surface reconstruction, and d) 3D object generation. Extensive studies
are performed on these four benchmarks, revealing new observations, challenges,
and opportunities for future research in realistic 3D vision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://omniobject3d.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reduced-Reference Quality Assessment of Point Clouds via
  Content-Oriented Saliency Projection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Zhou, Guanghui Yue, Ruizeng Zhang, Yipeng Qin, Hantao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many dense 3D point clouds have been exploited to represent visual objects
instead of traditional images or videos. To evaluate the perceptual quality of
various point clouds, in this letter, we propose a novel and efficient
Reduced-Reference quality metric for point clouds, which is based on
Content-oriented sAliency Projection (RR-CAP). Specifically, we make the first
attempt to simplify reference and distorted point clouds into projected
saliency maps with a downsampling operation. Through this process, we tackle
the issue of transmitting large-volume original point clouds to user-ends for
quality assessment. Then, motivated by the characteristics of the human visual
system (HVS), the objective quality scores of distorted point clouds are
produced by combining content-oriented similarity and statistical correlation
measurements. Finally, extensive experiments are conducted on SJTU-PCQA and WPC
databases. The experimental results demonstrate that our proposed algorithm
outperforms existing reduced-reference and no-reference quality metrics, and
significantly reduces the performance gap between state-of-the-art
full-reference quality assessment methods. In addition, we show the performance
variation of each proposed technical component by ablation tests.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OnePose++: Keypoint-Free One-Shot Object Pose Estimation without CAD
  Models <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, Xiaowei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new method for object pose estimation without CAD models. The
previous feature-matching-based method OnePose has shown promising results
under a one-shot setting which eliminates the need for CAD models or
object-specific training. However, OnePose relies on detecting repeatable image
keypoints and is thus prone to failure on low-textured objects. We propose a
keypoint-free pose estimation pipeline to remove the need for repeatable
keypoint detection. Built upon the detector-free feature matching method LoFTR,
we devise a new keypoint-free SfM method to reconstruct a semi-dense
point-cloud model for the object. Given a query image for object pose
estimation, a 2D-3D matching network directly establishes 2D-3D correspondences
between the query image and the reconstructed point-cloud model without first
detecting keypoints in the image. Experiments show that the proposed pipeline
outperforms existing one-shot CAD-model-free methods by a large margin and is
comparable to CAD-model-based methods on LINEMOD even for low-textured objects.
We also collect a new dataset composed of 80 sequences of 40 low-textured
objects to facilitate future research on one-shot object pose estimation. The
supplementary material, code and dataset are available on the project page:
https://zju3dv.github.io/onepose_plus_plus/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active learning for medical image segmentation with stochastic batches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mélanie Gaillochet, Christian Desrosiers, Hervé Lombaert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of learning-based algorithms improves with the amount of
labelled data used for training. Yet, manually annotating data can be tedious
and expensive, especially in medical image segmentation. To reduce manual
labelling, active learning (AL) targets the most informative samples from the
unlabelled set to annotate and add to the labelled training set. On one hand,
most active learning works have focused on the classification or limited
segmentation of natural images, despite active learning being highly desirable
in the difficult task of medical image segmentation. On the other hand,
uncertainty-based AL approaches notoriously offer sub-optimal batch-query
strategies, while diversity-based methods tend to be computationally expensive.
Over and above methodological hurdles, random sampling has proven an extremely
difficult baseline to outperform when varying learning and sampling conditions.
This work aims to take advantage of the diversity and speed offered by random
sampling to improve the selection of uncertainty-based AL methods for
segmenting medical images. More specifically, we propose to compute uncertainty
at the level of batches instead of samples through an original use of
stochastic batches during sampling in AL. Exhaustive experiments on medical
image segmentation, with an illustration on MRI prostate imaging, show that the
benefits of stochastic batches during sample selection are robust to a variety
of changes in the training and sampling procedures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Medical Image Analysis, 13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Behind the Scenes: Density Fields for Single View Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Wimbauer, Nan Yang, Christian Rupprecht, Daniel Cremers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inferring a meaningful geometric scene representation from a single image is
a fundamental problem in computer vision. Approaches based on traditional depth
map prediction can only reason about areas that are visible in the image.
Currently, neural radiance fields (NeRFs) can capture true 3D including color
but are too complex to be generated from a single image. As an alternative, we
introduce a neural network that predicts an implicit density field from a
single image. It maps every location in the frustum of the image to volumetric
density. Our network can be trained through self-supervision from only video
data. By not storing color in the implicit volume, but directly sampling color
from the available views during training, our scene representation becomes
significantly less complex compared to NeRFs, and we can train neural networks
to predict it. Thus, we can apply volume rendering to perform both depth
prediction and novel view synthesis. In our experiments, we show that our
method is able to predict meaningful geometry for regions that are occluded in
the input image. Additionally, we demonstrate the potential of our approach on
three datasets for depth prediction and novel-view synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://fwmb.github.io/bts/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DDS: Decoupled Dynamic Scene-Graph Generation Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A S M Iftekhar, Raphael Ruschel, Satish Kumar, Suya You, B. S. Manjunath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene-graph generation involves creating a structural representation of the
relationships between objects in a scene by predicting subject-object-relation
triplets from input data. However, existing methods show poor performance in
detecting triplets outside of a predefined set, primarily due to their reliance
on dependent feature learning. To address this issue we propose DDS -- a
decoupled dynamic scene-graph generation network -- that consists of two
independent branches that can disentangle extracted features. The key
innovation of the current paper is the decoupling of the features representing
the relationships from those of the objects, which enables the detection of
novel object-relationship combinations. The DDS model is evaluated on three
datasets and outperforms previous methods by a significant margin, especially
in detecting previously unseen triplets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HMDO: Markerless Multi-view Hand Manipulation Capture with Deformable
  Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Xie, Zhipeng Yu, Zimeng Zhao, Binghui Zuo, Yangang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We construct the first markerless deformable interaction dataset recording
interactive motions of the hands and deformable objects, called HMDO (Hand
Manipulation with Deformable Objects). With our built multi-view capture
system, it captures the deformable interactions with multiple perspectives,
various object shapes, and diverse interactive forms. Our motivation is the
current lack of hand and deformable object interaction datasets, as 3D hand and
deformable object reconstruction is challenging. Mainly due to mutual
occlusion, the interaction area is difficult to observe, the visual features
between the hand and the object are entangled, and the reconstruction of the
interaction area deformation is difficult. To tackle this challenge, we propose
a method to annotate our captured data. Our key idea is to collaborate with
estimated hand features to guide the object global pose estimation, and then
optimize the deformation process of the object by analyzing the relationship
between the hand and the object. Through comprehensive evaluation, the proposed
method can reconstruct interactive motions of hands and deformable objects with
high quality. HMDO currently consists of 21600 frames over 12 sequences. In the
future, this dataset could boost the research of learning-based reconstruction
of deformable interaction scenes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Facial Thermal and Blood Perfusion Patterns of Human Emotions:
  Proof-of-Concept 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor H. Aristizabal-Tique, Marcela Henao-Pérez, Diana Carolina López-Medina, Renato Zambrano-Cruz, Gloria Díaz-Londoñod
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective of the work was to realize a preliminary study of
proof-of-concept to evaluate emotions using thermographic images and blood
perfusion algorithm; the images were obtained for baseline and positive and
negative valence according to the protocol of the Geneva Affective Picture
Database. The blood perfusion algorithm is based on the heat transport
equation. The average temperature and blood perfusion in forehead, periorbital
eyes, cheeks, nose and upper lips were determined. Absolute and percentage
differences between the valences and the baseline were calculated. For negative
valence, a decrease in temperature and blood perfusion was observed in the
ROIs, and the effect was greater on the left side than on the right side. In
positive valence, the temperature and blood perfusion increased in some cases,
showing a complex pattern. The temperature and perfusion of the nose was
reduced for both valences, which is indicative of the arousal dimension. The
blood perfusion images were found to be greater contrast; the percentage
differences in the blood perfusion images are greater than those obtained in
thermographic images. Moreover, the blood perfusion images, and vasomotor
answer are consistent, therefore, they can be a better biomarker than
thermographic analysis in identifying emotions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Semantic Segmentation on Heterogeneous <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panagiotis Meletis, Gijs Dubbelman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore semantic segmentation beyond the conventional, single-dataset
homogeneous training and bring forward the problem of Heterogeneous Training of
Semantic Segmentation (HTSS). HTSS involves simultaneous training on multiple
heterogeneous datasets, i.e. datasets with conflicting label spaces and
different (weak) annotation types from the perspective of semantic
segmentation. The HTSS formulation exposes deep networks to a larger and
previously unexplored aggregation of information that can potentially enhance
semantic segmentation in three directions: i) performance: increased
segmentation metrics on seen datasets, ii) generalization: improved
segmentation metrics on unseen datasets, and iii) knowledgeability: increased
number of recognizable semantic concepts. To research these benefits of HTSS,
we propose a unified framework, that incorporates heterogeneous datasets in a
single-network training pipeline following the established FCN standard. Our
framework first curates heterogeneous datasets to bring them into a common
format and then trains a single-backbone FCN on all of them simultaneously. To
achieve this, it transforms weak annotations, which are incompatible with
semantic segmentation, to per-pixel labels, and hierarchizes their label spaces
into a universal taxonomy. The trained HTSS models demonstrate performance and
generalization gains over a wide range of datasets and extend the inference
label space entailing hundreds of semantic classes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted 2021 (under review)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A novel <span class="highlight-title">dataset</span> and a two-stage mitosis nuclei detection method based on
  hybrid anchor branch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huadeng Wang, Hao Xu, Bingbing Li, Xipeng Pan, Lingqi Zeng, Rushi Lan, Xiaonan Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitosis detection is one of the challenging problems in computational
pathology, and mitotic count is an important index of cancer grading for
pathologists. However, current counts of mitotic nuclei rely on pathologists
looking microscopically at the number of mitotic nuclei in hot spots, which is
subjective and time-consuming. In this paper, we propose a two-stage cascaded
network, named FoCasNet, for mitosis detection. In the first stage, a detection
network named M_det is proposed to detect as many mitoses as possible. In the
second stage, a classification network M_class is proposed to refine the
results of the first stage. In addition, the attention mechanism, normalization
method, and hybrid anchor branch classification subnet are introduced to
improve the overall detection performance. Our method achieves the current
highest F1-score of 0.888 on the public dataset ICPR 2012. We also evaluated
our method on the GZMH dataset released by our research team for the first time
and reached the highest F1-score of 0.563, which is also better than multiple
classic detection networks widely used at present. It confirmed the
effectiveness and generalization of our method. The code will be available at:
https://github.com/antifen/mitosis-nuclei-detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages,10 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Development, Optimization, and Deployment of Thermal Forward Vision
  Systems for Advance Vehicular Applications on Edge Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Ali Farooq, Waseem Shariff, Faisal Khan, Peter Corcoran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this research work, we have proposed a thermal tiny-YOLO multi-class
object detection (TTYMOD) system as a smart forward sensing system that should
remain effective in all weather and harsh environmental conditions using an
end-to-end YOLO deep learning framework. It provides enhanced safety and
improved awareness features for driver assistance. The system is trained on
large-scale thermal public datasets as well as newly gathered novel
open-sourced dataset comprising of more than 35,000 distinct thermal frames.
For optimal training and convergence of YOLO-v5 tiny network variant on thermal
data, we have employed different optimizers which include stochastic decent
gradient (SGD), Adam, and its variant AdamW which has an improved
implementation of weight decay. The performance of thermally tuned tiny
architecture is further evaluated on the public as well as locally gathered
test data in diversified and challenging weather and environmental conditions.
The efficacy of a thermally tuned nano network is quantified using various
qualitative metrics which include mean average precision, frames per second
rate, and average inference time. Experimental outcomes show that the network
achieved the best mAP of 56.4% with an average inference time/ frame of 4
milliseconds. The study further incorporates optimization of tiny network
variant using the TensorFlow Lite quantization tool this is beneficial for the
deployment of deep learning architectures on the edge and mobile devices. For
this study, we have used a raspberry pi 4 computing board for evaluating the
real-time feasibility performance of an optimized version of the thermal object
detection network for the automotive sensor suite. The source code, trained and
optimized models and complete validation/ testing results are publicly
available at
https://github.com/MAli-Farooq/Thermal-YOLO-And-Model-Optimization-Using-TensorFlowLite.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is accepted and in the publication phase at ICMV 2022
  Conference. Link: http://icmv.org/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Representation Learning for Text and 3D Point Cloud 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07584v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07584v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Huang, Xuran Pan, Henry Zheng, Haojun Jiang, Zhifeng Xie, Shiji Song, Gao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in vision-language pre-training (e.g. CLIP) have shown
that vision models can benefit from language supervision. While many models
using language modality have achieved great success on 2D vision tasks, the
joint representation learning of 3D point cloud with text remains
under-explored due to the difficulty of 3D-Text data pair acquisition and the
irregularity of 3D data structure. In this paper, we propose a novel Text4Point
framework to construct language-guided 3D point cloud models. The key idea is
utilizing 2D images as a bridge to connect the point cloud and the language
modalities. The proposed Text4Point follows the pre-training and fine-tuning
paradigm. During the pre-training stage, we establish the correspondence of
images and point clouds based on the readily available RGB-D data and use
contrastive learning to align the image and point cloud representations.
Together with the well-aligned image and text features achieved by CLIP, the
point cloud features are implicitly aligned with the text embeddings. Further,
we propose a Text Querying Module to integrate language information into 3D
representation learning by querying text embeddings with point cloud features.
For fine-tuning, the model learns task-specific 3D representations under
informative language guidance from the label set without 2D images. Extensive
experiments demonstrate that our model shows consistent improvement on various
downstream tasks, such as point cloud semantic segmentation, instance
segmentation, and object detection. The code will be available here:
https://github.com/LeapLabTHU/Text4Point
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Advanced Computer Vision Techniques for Sports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiago Mendes-Neves, Luís Meireles, João Mendes-Moreira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer Vision developments are enabling significant advances in many
fields, including sports. Many applications built on top of Computer Vision
technologies, such as tracking data, are nowadays essential for every top-level
analyst, coach, and even player. In this paper, we survey Computer Vision
techniques that can help many sports-related studies gather vast amounts of
data, such as Object Detection and Pose Estimation. We provide a use case for
such data: building a model for shot speed estimation with pose data obtained
using only Computer Vision models. Our model achieves a correlation of 67%. The
possibility of estimating shot speeds enables much deeper studies about
enabling the creation of new metrics and recommendation systems that will help
athletes improve their performance, in any sport. The proposed methodology is
easily replicable for many technical movements and is only limited by the
availability of video data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Blur Invariants for Image Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Flusser, Matej Lebl, Matteo Pedone, Filip Sroubek, Jitka Kostkova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blur is an image degradation that is difficult to remove. Invariants with
respect to blur offer an alternative way of a~description and recognition of
blurred images without any deblurring. In this paper, we present an original
unified theory of blur invariants. Unlike all previous attempts, the new theory
does not require any prior knowledge of the blur type. The invariants are
constructed in the Fourier domain by means of orthogonal projection operators
and moment expansion is used for efficient and stable computation. It is shown
that all blur invariants published earlier are just particular cases of this
approach. Experimental comparison to concurrent approaches shows the advantages
of the proposed theory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gated-ViGAT: Efficient Bottom-Up Event Recognition and Explanation Using
  a New Frame Selection Policy and Gating Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Gkalelis, Dimitrios Daskalakis, Vasileios Mezaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, Gated-ViGAT, an efficient approach for video event
recognition, utilizing bottom-up (object) information, a new frame sampling
policy and a gating mechanism is proposed. Specifically, the frame sampling
policy uses weighted in-degrees (WiDs), derived from the adjacency matrices of
graph attention networks (GATs), and a dissimilarity measure to select the most
salient and at the same time diverse frames representing the event in the
video. Additionally, the proposed gating mechanism fetches the selected frames
sequentially, and commits early-exiting when an adequately confident decision
is achieved. In this way, only a few frames are processed by the
computationally expensive branch of our network that is responsible for the
bottom-up information extraction. The experimental evaluation on two large,
publicly available video datasets (MiniKinetics, ActivityNet) demonstrates that
Gated-ViGAT provides a large computational complexity reduction in comparison
to our previous approach (ViGAT), while maintaining the excellent event
recognition and explainability performance. Gated-ViGAT source code is made
publicly available at https://github.com/bmezaris/Gated-ViGAT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the proceedings of IEEE Int. Symposium on
  Multimedia (ISM), Naples, Italy, Dec. 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Scale Framework for Out-of-Distribution Detection in Dermoscopic
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongzheng Huang, Tao Wang, Yuanzheng Cai, Lingyu Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automatic detection of skin diseases via dermoscopic images can improve
the efficiency in diagnosis and help doctors make more accurate judgments.
However, conventional skin disease recognition systems may produce high
confidence for out-of-distribution (OOD) data, which may become a major
security vulnerability in practical applications. In this paper, we propose a
multi-scale detection framework to detect out-of-distribution skin disease
image data to ensure the robustness of the system. Our framework extracts
features from different layers of the neural network. In the early layers,
rectified activation is used to make the output features closer to the
well-behaved distribution, and then an one-class SVM is trained to detect OOD
data; in the penultimate layer, an adapted Gram matrix is used to calculate the
features after rectified activation, and finally the layer with the best
performance is chosen to compute a normality score. Experiments show that the
proposed framework achieves superior performance when compared with other
state-of-the-art methods in the task of skin disease recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted by the 4th International Conference on Machine
  Learning for Cyber Security (ML4CS 2022), Guangzhou, China</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Curvilinear object segmentation in medical images based on ODoS filter
  and deep learning network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanyuan Peng, Lin Pan, Pengpeng Luan, Hongbin Tu, Xiong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic segmentation of curvilinear objects in medical images plays an
important role in the diagnosis and evaluation of human diseases, yet it is a
challenging uncertainty for the complex segmentation task due to different
issues like various image appearance, low contrast between curvilinear objects
and their surrounding backgrounds, thin and uneven curvilinear structures, and
improper background illumination. To overcome these challenges, we present a
unique curvilinear structure segmentation framework based on oriented
derivative of stick (ODoS) filter and deep learning network for curvilinear
object segmentation in medical images. Currently, a large number of deep
learning models emphasis on developing deep architectures and ignore capturing
the structural features of curvature objects, which may lead to unsatisfactory
results. In consequence, a new approach that incorporates the ODoS filter as
part of a deep learning network is presented to improve the spatial attention
of curvilinear objects. In which, the original image is considered as principal
part to describe various image appearance and complex background illumination,
the multi-step strategy is used to enhance contrast between curvilinear objects
and their surrounding backgrounds, and the vector field is applied to
discriminate thin and uneven curvilinear structures. Subsequently, a deep
learning framework is employed to extract varvious structural features for
curvilinear object segmentation in medical images. The performance of the
computational model was validated in experiments with publicly available DRIVE,
STARE and CHASEDB1 datasets. Experimental results indicate that the presented
model has yielded surprising results compared with some state-of-the-art
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model-based inexact graph matching on top of CNNs for semantic scene
  understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jérémy Chopin, Jean-Baptiste Fasquel, Harold Mouchère, Rozenn Dahyot, Isabelle Bloch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning based pipelines for semantic segmentation often ignore
structural information available on annotated images used for training. We
propose a novel post-processing module enforcing structural knowledge about the
objects of interest to improve segmentation results provided by deep learning.
This module corresponds to a "many-to-one-or-none" inexact graph matching
approach, and is formulated as a quadratic assignment problem. Our approach is
compared to a CNN-based segmentation (for various CNN backbones) on two public
datasets, one for face segmentation from 2D RGB images (FASSEG), and the other
for brain segmentation from 3D MRIs (IBSR). Evaluations are performed using two
types of structural information (distances and directional relations, , this
choice being a hyper-parameter of our generic framework). On FASSEG data,
results show that our module improves accuracy of the CNN by about 6.3% (the
Hausdorff distance decreases from 22.11 to 20.71). On IBSR data, the
improvement is of 51% (the Hausdorff distance decreases from 11.01 to 5.4). In
addition, our approach is shown to be resilient to small training datasets that
often limit the performance of deep learning methods: the improvement increases
as the size of the training dataset decreases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 10 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLIPTER: Looking at the Bigger Picture in Scene Text Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aviad Aberdam, David Bensaïd, Alona Golts, Roy Ganz, Oren Nuriel, Royee Tichauer, Shai Mazor, Ron Litman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the scene is often essential for reading text in real-world
scenarios. However, current scene text recognizers operate on cropped text
images, unaware of the bigger picture. In this work, we harness the
representative power of recent vision-language models, such as CLIP, to provide
the crop-based recognizer with scene, image-level information. Specifically, we
obtain a rich representation of the entire image and fuse it with the
recognizer word-level features via cross-attention. Moreover, a gated mechanism
is introduced that gradually shifts to the context-enriched representation,
enabling simply fine-tuning a pretrained recognizer. We implement our
model-agnostic framework, named CLIPTER - CLIP Text Recognition, on several
leading text recognizers and demonstrate consistent performance gains,
achieving state-of-the-art results over multiple benchmarks. Furthermore, an
in-depth analysis reveals improved robustness to out-of-vocabulary words and
enhanced generalization in low-data regimes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Perceiving Video-Language <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07463v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07463v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Ma, Xiaojie Jin, Heng Wang, Jingjia Huang, Linchao Zhu, Jiashi Feng, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-Language Pre-training models have recently significantly improved
various multi-modal downstream tasks. Previous dominant works mainly adopt
contrastive learning to achieve global feature alignment across modalities.
However, the local associations between videos and texts are not modeled,
restricting the pre-training models' generality, especially for tasks requiring
the temporal video boundary for certain query texts. This work introduces a
novel text-video localization pre-text task to enable fine-grained temporal and
semantic alignment such that the trained model can accurately perceive temporal
boundaries in videos given the text description. Specifically, text-video
localization consists of moment retrieval, which predicts start and end
boundaries in videos given the text description, and text localization which
matches the subset of texts with the video features. To produce temporal
boundaries, frame features in several videos are manually merged into a long
video sequence that interacts with a text sequence. With the localization task,
our method connects the fine-grained frame representations with the word
representations and implicitly distinguishes representations of different
instances in the single modality. Notably, comprehensive experimental results
show that our method significantly improves the state-of-the-art performance on
various benchmarks, covering text-to-video retrieval, video question answering,
video captioning, temporal action localization and temporal moment retrieval.
The code will be released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sharp Eyes: A Salient Object Detector Working The Same Way as Human
  Visual Characteristics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Zhu, Jinbao Li, Yahong Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current methods aggregate multi-level features or introduce edge and skeleton
to get more refined saliency maps. However, little attention is paid to how to
obtain the complete salient object in cluttered background, where the targets
are usually similar in color and texture to the background. To handle this
complex scene, we propose a sharp eyes network (SENet) that first seperates the
object from scene, and then finely segments it, which is in line with human
visual characteristics, i.e., to look first and then focus. Different from
previous methods which directly integrate edge or skeleton to supplement the
defects of objects, the proposed method aims to utilize the expanded objects to
guide the network obtain complete prediction. Specifically, SENet mainly
consists of target separation (TS) brach and object segmentation (OS) branch
trained by minimizing a new hierarchical difference aware (HDA) loss. In the TS
branch, we construct a fractal structure to produce saliency features with
expanded boundary via the supervision of expanded ground truth, which can
enlarge the detail difference between foreground and background. In the OS
branch, we first aggregate multi-level features to adaptively select
complementary components, and then feed the saliency features with expanded
boundary into aggregated features to guide the network obtain complete
prediction. Moreover, we propose the HDA loss to further improve the structural
integrity and local details of the salient objects, which assigns weight to
each pixel according to its distance from the boundary hierarchically. Hard
pixels with similar appearance in border region will be given more attention
hierarchically to emphasize their importance in completeness prediction.
Comprehensive experimental results on five datasets demonstrate that the
proposed approach outperforms the state-of-the-art methods both quantitatively
and qualitatively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representing Noisy Image Without Denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuren Qi, Yushu Zhang, Chao Wang, Tao Xiang, Xiaochun Cao, Yong Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A long-standing topic in artificial intelligence is the effective recognition
of patterns from noisy images. In this regard, the recent data-driven paradigm
considers 1) improving the representation robustness by adding noisy samples in
training phase (i.e., data augmentation) or 2) pre-processing the noisy image
by learning to solve the inverse problem (i.e., image denoising). However, such
methods generally exhibit inefficient process and unstable result, limiting
their practical applications. In this paper, we explore a non-learning paradigm
that aims to derive robust representation directly from noisy images, without
the denoising as pre-processing. Here, the noise-robust representation is
designed as Fractional-order Moments in Radon space (FMR), with also beneficial
properties of orthogonality and rotation invariance. Unlike earlier
integer-order methods, our work is a more generic design taking such classical
methods as special cases, and the introduced fractional-order parameter offers
time-frequency analysis capability that is not available in classical methods.
Formally, both implicit and explicit paths for constructing the FMR are
discussed in detail. Extensive simulation experiments and an image security
application are provided to demonstrate the uniqueness and usefulness of our
FMR, especially for noise robustness, rotation invariance, and time-frequency
discriminability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TAME: Attention Mechanism Based Feature Fusion for Generating
  Explanation Maps of Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mariano Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The apparent ``black box'' nature of neural networks is a barrier to adoption
in applications where explainability is essential. This paper presents TAME
(Trainable Attention Mechanism for Explanations), a method for generating
explanation maps with a multi-branch hierarchical attention mechanism. TAME
combines a target model's feature maps from multiple layers using an attention
mechanism, transforming them into an explanation map. TAME can easily be
applied to any convolutional neural network (CNN) by streamlining the
optimization of the attention mechanism's training method and the selection of
target model's feature maps. After training, explanation maps can be computed
in a single forward pass. We apply TAME to two widely used models, i.e. VGG-16
and ResNet-50, trained on ImageNet and show improvements over previous
top-performing methods. We also provide a comprehensive ablation study
comparing the performance of different variations of TAME's architecture. TAME
source code is made publicly available at https://github.com/bmezaris/TAME
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the proceedings of IEEE Int. Symposium on
  Multimedia (ISM), Naples, Italy, Dec. 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HiDAnet: RGB-D Salient Object Detection via Hierarchical Depth Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongwei Wu, Guillaume Allibert, Fabrice Meriaudeau, Chao Ma, Cédric Demonceaux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  RGB-D saliency detection aims to fuse multi-modal cues to accurately localize
salient regions. Existing works often adopt attention modules for feature
modeling, with few methods explicitly leveraging fine-grained details to merge
with semantic cues. Thus, despite the auxiliary depth information, it is still
challenging for existing models to distinguish objects with similar appearances
but at distinct camera distances. In this paper, from a new perspective, we
propose a novel Hierarchical Depth Awareness network (HiDAnet) for RGB-D
saliency detection. Our motivation comes from the observation that the
multi-granularity properties of geometric priors correlate well with the neural
network hierarchies. To realize multi-modal and multi-level fusion, we first
use a granularity-based attention scheme to strengthen the discriminatory power
of RGB and depth features separately. Then we introduce a unified cross
dual-attention module for multi-modal and multi-level fusion in a
coarse-to-fine manner. The encoded multi-modal features are gradually
aggregated into a shared decoder. Further, we exploit a multi-scale loss to
take full advantage of the hierarchical information. Extensive experiments on
challenging benchmark datasets demonstrate that our HiDAnet performs favorably
over the state-of-the-art methods by large margins.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Models that Can See and Read 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roy Ganz, Oren Nuriel, Aviad Aberdam, Yair Kittenplon, Shai Mazor, Ron Litman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Question Answering (VQA) and Image Captioning (CAP), which are among
the most popular vision-language tasks, have analogous scene-text versions that
require reasoning from the text in the image. Despite the obvious resemblance
between them, the two are treated independently, yielding task-specific methods
that can either see or read, but not both. In this work, we conduct an in-depth
analysis of this phenomenon and propose UniTNT, a Unified Text-Non-Text
approach, which grants existing multimodal architectures scene-text
understanding capabilities. Specifically, we treat scene-text information as an
additional modality, fusing it with any pretrained encoder-decoder-based
architecture via designated modules. Thorough experiments reveal that UniTNT
leads to the first single model that successfully handles both task types.
Moreover, we show that scene-text understanding capabilities can boost
vision-language models' performance on VQA and CAP by up to 3.49% and 0.7
CIDEr, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Three-dimensional reconstruction and characterization of bladder
  deformations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Augustin C. Ogier, Stanislas Rapacchi, Marc-Emmanuel Bellemare
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background and Objective: Pelvic floor disorders are prevalent diseases and
patient care remains difficult as the dynamics of the pelvic floor remains
poorly known. So far, only 2D dynamic observations of straining exercises at
excretion are available in the clinics and the understanding of
three-dimensional pelvic organs mechanical defects is not yet achievable. In
this context, we proposed a complete methodology for the 3D representation of
the non-reversible bladder deformations during exercises, directly combined
with synthesized 3D representation of the location of the highest strain areas
on the organ surface. Methods: Novel image segmentation and registration
approaches have been combined with three geometrical configurations of
up-to-date rapid dynamic multi-slices MRI acquisition for the reconstruction of
real-time dynamic bladder volumes. Results: For the first time, we proposed
real-time 3D deformation fields of the bladder under strain from in-bore forced
breathing exercises. The potential of our method was assessed on eight control
subjects undergoing forced breathing exercises. We obtained average volume
deviation of the reconstructed dynamic volume of bladders around 2.5\% and high
registration accuracy with mean distance values of 0.4 $\pm$ 0.3 mm and
Hausdorff distance values of 2.2 $\pm$ 1.1 mm. Conclusions: Immediately
transferable to the clinics with rapid acquisitions, the proposed framework
represents a real advance in the field of pelvic floor disorders as it
provides, for the first time, a proper 3D+t spatial tracking of bladder
non-reversible deformations. This work is intended to be extended to patients
with cavities filling and excretion to better characterize the degree of
severity of pelvic floor pathologies for diagnostic assistance or in
preoperative surgical planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures, full article paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViT-AE++: Improving Vision <span class="highlight-title">Transformer</span> Autoencoder for <span class="highlight-title">Self-supervised</span>
  Medical Image Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07382v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07382v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinmay Prabhakar, Hongwei Bran Li, Jiancheng Yang, Suprosana Shit, Benedikt Wiestler, Bjoern Menze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning has attracted increasing attention as it learns
data-driven representation from data without annotations. Vision
transformer-based autoencoder (ViT-AE) by He et al. (2021) is a recent
self-supervised learning technique that employs a patch-masking strategy to
learn a meaningful latent space. In this paper, we focus on improving ViT-AE
(nicknamed ViT-AE++) for a more effective representation of both 2D and 3D
medical images. We propose two new loss functions to enhance the representation
during the training stage. The first loss term aims to improve
self-reconstruction by considering the structured dependencies and hence
indirectly improving the representation. The second loss term leverages
contrastive loss to directly optimize the representation from two randomly
masked views. As an independent contribution, we extended ViT-AE++ to a 3D
fashion for volumetric medical images. We extensively evaluate ViT-AE++ on both
natural images and medical images, demonstrating consistent improvement over
vanilla ViT-AE and its superiority over other contrastive learning approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review. C. Prabhakar and H. B. Li contribute equally. Codes
  will be available soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MADAv2: Advanced Multi-Anchor Based Active Domain Adaptation
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Munan Ning, Donghuan Lu, Yujia Xie, Dongdong Chen, Dong Wei, Yefeng Zheng, Yonghong Tian, Shuicheng Yan, Li Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised domain adaption has been widely adopted in tasks with scarce
annotated data. Unfortunately, mapping the target-domain distribution to the
source-domain unconditionally may distort the essential structural information
of the target-domain data, leading to inferior performance. To address this
issue, we firstly propose to introduce active sample selection to assist domain
adaptation regarding the semantic segmentation task. By innovatively adopting
multiple anchors instead of a single centroid, both source and target domains
can be better characterized as multimodal distributions, in which way more
complementary and informative samples are selected from the target domain. With
only a little workload to manually annotate these active samples, the
distortion of the target-domain distribution can be effectively alleviated,
achieving a large performance gain. In addition, a powerful semi-supervised
domain adaptation strategy is proposed to alleviate the long-tail distribution
problem and further improve the segmentation performance. Extensive experiments
are conducted on public datasets, and the results demonstrate that the proposed
approach outperforms state-of-the-art methods by large margins and achieves
similar performance to the fully-supervised upperbound, i.e., 71.4% mIoU on
GTA5 and 71.8% mIoU on SYNTHIA. The effectiveness of each component is also
verified by thorough ablation studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2108.08012</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Supervised Semantic Segmentation via Gentle Teaching Assistant <span class="chip">NeurIPS2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Jin, Jiaqi Wang, Dahua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-Supervised Semantic Segmentation aims at training the segmentation model
with limited labeled data and a large amount of unlabeled data. To effectively
leverage the unlabeled data, pseudo labeling, along with the teacher-student
framework, is widely adopted in semi-supervised semantic segmentation. Though
proved to be effective, this paradigm suffers from incorrect pseudo labels
which inevitably exist and are taken as auxiliary training data. To alleviate
the negative impact of incorrect pseudo labels, we delve into the current
Semi-Supervised Semantic Segmentation frameworks. We argue that the unlabeled
data with pseudo labels can facilitate the learning of representative features
in the feature extractor, but it is unreliable to supervise the mask predictor.
Motivated by this consideration, we propose a novel framework, Gentle Teaching
Assistant (GTA-Seg) to disentangle the effects of pseudo labels on feature
extractor and mask predictor of the student model. Specifically, in addition to
the original teacher-student framework, our method introduces a teaching
assistant network which directly learns from pseudo labels generated by the
teacher network. The gentle teaching assistant (GTA) is coined gentle since it
only transfers the beneficial feature representation knowledge in the feature
extractor to the student model in an Exponential Moving Average (EMA) manner,
protecting the student model from the negative influences caused by unreliable
pseudo labels in the mask predictor. The student model is also supervised by
reliable labeled data to train an accurate mask predictor, further facilitating
feature representation. Extensive experiment results on benchmark datasets
validate that our method shows competitive performance against previous
methods. Code is available at https://github.com/Jin-Ying/GTA-Seg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS2022 camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Class Enhancement Losses with Pseudo Labels for Zero-shot Semantic
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Son Duy Dao, Hengcan Shi, Dinh Phung, Jianfei Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent mask proposal models have significantly improved the performance of
zero-shot semantic segmentation. However, the use of a `background' embedding
during training in these methods is problematic as the resulting model tends to
over-learn and assign all unseen classes as the background class instead of
their correct labels. Furthermore, they ignore the semantic relationship of
text embeddings, which arguably can be highly informative for zero-shot
prediction as seen classes may have close relationship with unseen classes. To
this end, this paper proposes novel class enhancement losses to bypass the use
of the background embbedding during training, and simultaneously exploit the
semantic relationship between text embeddings and mask proposals by ranking the
similarity scores. To further capture the relationship between seen and unseen
classes, we propose an effective pseudo label generation pipeline using
pretrained vision-language model. Extensive experiments on several benchmark
datasets show that our method achieves overall the best performance for
zero-shot semantic segmentation. Our method is flexible, and can also be
applied to the challenging open-vocabulary semantic segmentation problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FPANet: Frequency-based Video Demoireing using Frame-level Post
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyeongrok Oh, Heon Gu, Sangpil Kim, Jinkyu Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interference between overlapping gird patterns creates moire patterns,
degrading the visual quality of an image that captures a screen of a digital
display device by an ordinary digital camera. Removing such moire patterns is
challenging due to their complex patterns of diverse sizes and color
distortions. Existing approaches mainly focus on filtering out in the spatial
domain, failing to remove a large-scale moire pattern. In this paper, we
propose a novel model called FPANet that learns filters in both frequency and
spatial domains, improving the restoration quality by removing various sizes of
moire patterns. To further enhance, our model takes multiple consecutive
frames, learning to extract frame-invariant content features and outputting
better quality temporally consistent images. We demonstrate the effectiveness
of our proposed method with a publicly available large-scale dataset, observing
that ours outperforms the state-of-the-art approaches, including ESDNet,
VDmoire, MBCNN, WDNet, UNet, and DMCNN, in terms of the image and video quality
metrics, such as PSNR, SSIM, LPIPS, FVD, and FSIM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Dynamic Scene Deblurring from Optical Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Zhang, Jinshan Pan, Daoye Wang, Shangchen Zhou, Xing Wei, Furong Zhao, Jianbo Liu, Jimmy Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deblurring can not only provide visually more pleasant pictures and make
photography more convenient, but also can improve the performance of objection
detection as well as tracking. However, removing dynamic scene blur from images
is a non-trivial task as it is difficult to model the non-uniform blur
mathematically. Several methods first use single or multiple images to estimate
optical flow (which is treated as an approximation of blur kernels) and then
adopt non-blind deblurring algorithms to reconstruct the sharp images. However,
these methods cannot be trained in an end-to-end manner and are usually
computationally expensive. In this paper, we explore optical flow to remove
dynamic scene blur by using the multi-scale spatially variant recurrent neural
network (RNN). We utilize FlowNets to estimate optical flow from two
consecutive images in different scales. The estimated optical flow provides the
RNN weights in different scales so that the weights can better help RNNs to
remove blur in the feature spaces. Finally, we develop a convolutional neural
network (CNN) to restore the sharp images from the deblurred features. Both
quantitative and qualitative evaluations on the benchmark datasets demonstrate
that the proposed method performs favorably against state-of-the-art algorithms
in terms of accuracy, speed, and model size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by tcsvt</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HSTFormer: Hierarchical Spatial-Temporal <span class="highlight-title">Transformer</span>s for 3D Human Pose
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoye Qian, Youbao Tang, Ning Zhang, Mei Han, Jing Xiao, Ming-Chun Huang, Ruei-Sung Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based approaches have been successfully proposed for 3D human
pose estimation (HPE) from 2D pose sequence and achieved state-of-the-art
(SOTA) performance. However, current SOTAs have difficulties in modeling
spatial-temporal correlations of joints at different levels simultaneously.
This is due to the poses' spatial-temporal complexity. Poses move at various
speeds temporarily with various joints and body-parts movement spatially.
Hence, a cookie-cutter transformer is non-adaptable and can hardly meet the
"in-the-wild" requirement. To mitigate this issue, we propose Hierarchical
Spatial-Temporal transFormers (HSTFormer) to capture multi-level joints'
spatial-temporal correlations from local to global gradually for accurate 3D
HPE. HSTFormer consists of four transformer encoders (TEs) and a fusion module.
To the best of our knowledge, HSTFormer is the first to study hierarchical TEs
with multi-level fusion. Extensive experiments on three datasets (i.e.,
Human3.6M, MPI-INF-3DHP, and HumanEva) demonstrate that HSTFormer achieves
competitive and consistent performance on benchmarks with various scales and
difficulties. Specifically, it surpasses recent SOTAs on the challenging
MPI-INF-3DHP dataset and small-scale HumanEva dataset, with a highly
generalized systematic approach. The code is available at:
https://github.com/qianxiaoye825/HSTFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors have equal contribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Knowledge Adaptation for Federated Unsupervised Person ReID 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianfeng Weng, Kun Hu, Tingting Yao, Jingya Wang, Zhiyong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Person Re-identification (ReID) has been extensively studied in recent years
due to the increasing demand in public security. However, collecting and
dealing with sensitive personal data raises privacy concerns. Therefore,
federated learning has been explored for Person ReID, which aims to share
minimal sensitive data between different parties (clients). However, existing
federated learning based person ReID methods generally rely on laborious and
time-consuming data annotations and it is difficult to guarantee cross-domain
consistency. Thus, in this work, a federated unsupervised cluster-contrastive
(FedUCC) learning method is proposed for Person ReID. FedUCC introduces a
three-stage modelling strategy following a coarse-to-fine manner. In detail,
generic knowledge, specialized knowledge and patch knowledge are discovered
using a deep neural network. This enables the sharing of mutual knowledge among
clients while retaining local domain-specific knowledge based on the kinds of
network layers and their parameters. Comprehensive experiments on 8 public
benchmark datasets demonstrate the state-of-the-art performance of our proposed
method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptively Integrated Knowledge Distillation and Prediction Uncertainty
  for Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kanghao Chen, Sijia Liu, Ruixuan Wang, Wei-Shi Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current deep learning models often suffer from catastrophic forgetting of old
knowledge when continually learning new knowledge. Existing strategies to
alleviate this issue often fix the trade-off between keeping old knowledge
(stability) and learning new knowledge (plasticity). However, the
stability-plasticity trade-off during continual learning may need to be
dynamically changed for better model performance. In this paper, we propose two
novel ways to adaptively balance model stability and plasticity. The first one
is to adaptively integrate multiple levels of old knowledge and transfer it to
each block level in the new model. The second one uses prediction uncertainty
of old knowledge to naturally tune the importance of learning new knowledge
during model training. To our best knowledge, this is the first time to connect
model prediction uncertainty and knowledge distillation for continual learning.
In addition, this paper applies a modified CutMix particularly to augment the
data for old knowledge, further alleviating the catastrophic forgetting issue.
Extensive evaluations on the CIFAR100 and the ImageNet datasets confirmed the
effectiveness of the proposed method for continual learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Face Recognition in the age of CLIP & Billion image <span class="highlight-title">dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaditya Bhat, Shrey Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CLIP (Contrastive Language-Image Pre-training) models developed by OpenAI
have achieved outstanding results on various image recognition and retrieval
tasks, displaying strong zero-shot performance. This means that they are able
to perform effectively on tasks for which they have not been explicitly
trained. Inspired by the success of OpenAI CLIP, a new publicly available
dataset called LAION-5B was collected which resulted in the development of open
ViT-H/14, ViT-G/14 models that outperform the OpenAI L/14 model. The LAION-5B
dataset also released an approximate nearest neighbor index, with a web
interface for search & subset creation.
  In this paper, we evaluate the performance of various CLIP models as
zero-shot face recognizers. Our findings show that CLIP models perform well on
face recognition tasks, but increasing the size of the CLIP model does not
necessarily lead to improved accuracy. Additionally, we investigate the
robustness of CLIP models against data poisoning attacks by testing their
performance on poisoned data. Through this analysis, we aim to understand the
potential consequences and misuse of search engines built using CLIP models,
which could potentially function as unintentional face recognition engines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improve Noise Tolerance of Robust Loss via Noise-Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kehui Ding, Jun Shu, Deyu Meng, Zongben Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust loss minimization is an important strategy for handling robust
learning issue on noisy labels. Current robust losses, however, inevitably
involve hyperparameters to be tuned for different datasets with noisy labels,
manually or heuristically through cross validation, which makes them fairly
hard to be generally applied in practice. Existing robust loss methods usually
assume that all training samples share common hyperparameters, which are
independent of instances. This limits the ability of these methods on
distinguishing individual noise properties of different samples, making them
hardly adapt to different noise structures. To address above issues, we propose
to assemble robust loss with instance-dependent hyperparameters to improve
their noise-tolerance with theoretical guarantee. To achieve setting such
instance-dependent hyperparameters for robust loss, we propose a meta-learning
method capable of adaptively learning a hyperparameter prediction function,
called Noise-Aware-Robust-Loss-Adjuster (NARL-Adjuster). Specifically, through
mutual amelioration between hyperparameter prediction function and classifier
parameters in our method, both of them can be simultaneously finely ameliorated
and coordinated to attain solutions with good generalization capability. Four
kinds of SOTA robust losses are attempted to be integrated with our algorithm,
and experiments substantiate the general availability and effectiveness of the
proposed method in both its noise tolerance and generalization performance.
Meanwhile, the explicit parameterized structure makes the meta-learned
prediction function capable of being readily transferrable and plug-and-play to
unseen datasets with noisy labels. Specifically, we transfer our meta-learned
NARL-Adjuster to unseen tasks, including several real noisy datasets, and
achieve better performance compared with conventional hyperparameter tuning
strategy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2002.06482</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PTA-Det: Point <span class="highlight-title">Transformer</span> Associating Point cloud and Image for 3D
  Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Wan, Tianyun Zhao, Wei Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In autonomous driving, 3D object detection based on multi-modal data has
become an indispensable approach when facing complex environments around the
vehicle. During multi-modal detection, LiDAR and camera are simultaneously
applied for capturing and modeling. However, due to the intrinsic discrepancies
between the LiDAR point and camera image, the fusion of the data for object
detection encounters a series of problems. Most multi-modal detection methods
perform even worse than LiDAR-only methods. In this investigation, we propose a
method named PTA-Det to improve the performance of multi-modal detection.
Accompanied by PTA-Det, a Pseudo Point Cloud Generation Network is proposed,
which can convert image information including texture and semantic features by
pseudo points. Thereafter, through a transformer-based Point Fusion Transition
(PFT) module, the features of LiDAR points and pseudo points from image can be
deeply fused under a unified point-based representation. The combination of
these modules can conquer the major obstacle in feature fusion across
modalities and realizes a complementary and discriminative representation for
proposal generation. Extensive experiments on the KITTI dataset show the
PTA-Det achieves a competitive result and support its effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Self-Training Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aswathnarayan Radhakrishnan, Jim Davis, Zachary Rabin, Benjamin Lewis, Matthew Scherreik, Roman Ilin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning approaches train on small sets of labeled data along
with large sets of unlabeled data. Self-training is a semi-supervised
teacher-student approach that often suffers from the problem of "confirmation
bias" that occurs when the student model repeatedly overfits to incorrect
pseudo-labels given by the teacher model for the unlabeled data. This bias
impedes improvements in pseudo-label accuracy across self-training iterations,
leading to unwanted saturation in model performance after just a few
iterations. In this work, we describe multiple enhancements to improve the
self-training pipeline to mitigate the effect of confirmation bias. We evaluate
our enhancements over multiple datasets showing performance gains over existing
self-training design choices. Finally, we also study the extendability of our
enhanced approach to Open Set unlabeled data (containing classes not seen in
labeled data).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reslicing Ultrasound Images for Data Augmentation and Vessel
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cecilia Morales, Jason Yao, Tejas Rane, Robert Edman, Howie Choset, Artur Dubrawski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot-guided catheter insertion has the potential to deliver urgent medical
care in situations where medical personnel are unavailable. However, this
technique requires accurate and reliable segmentation of anatomical landmarks
in the body. For the ultrasound imaging modality, obtaining large amounts of
training data for a segmentation model is time-consuming and expensive. This
paper introduces RESUS (RESlicing of UltraSound Images), a weak supervision
data augmentation technique for ultrasound images based on slicing
reconstructed 3D volumes from tracked 2D images. This technique allows us to
generate views which cannot be easily obtained in vivo due to physical
constraints of ultrasound imaging, and use these augmented ultrasound images to
train a semantic segmentation model. We demonstrate that RESUS achieves
statistically significant improvement over training with non-augmented images
and highlight qualitative improvements through vessel reconstruction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Learning for <span class="highlight-title">Self-Supervised</span> <span class="highlight-title">Pre-Train</span>ing of Point Cloud
  Segmentation Networks With Image Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrej Janda, Brandon Wagstaff, Edwin G. Ng, Jonathan Kelly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reducing the quantity of annotations required for supervised training is
vital when labels are scarce and costly. This reduction is particularly
important for semantic segmentation tasks involving 3D datasets, which are
often significantly smaller and more challenging to annotate than their
image-based counterparts. Self-supervised pre-training on unlabelled data is
one way to reduce the amount of manual annotations needed. Previous work has
focused on pre-training with point clouds exclusively. While useful, this
approach often requires two or more registered views. In the present work, we
combine image and point cloud modalities by first learning self-supervised
image features and then using these features to train a 3D model. By
incorporating image data, which is often included in many 3D datasets, our
pre-training method only requires a single scan of a scene and can be applied
to cases where localization information is unavailable. We demonstrate that our
pre-training approach, despite using single scans, achieves comparable
performance to other multi-scan, point cloud-only methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the IEEE Conference on Computer and Robot Vision
  (CRV'23), Montreal, Canada, June 6-8, 2023. arXiv admin note: text overlap
  with arXiv:2211.11801</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SensorX2car: Sensors-to-car calibration for autonomous driving in road
  scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guohang Yan, Zhaotong Luo, Zhuochun Liu, Yikang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of sensors in the autonomous driving system is fundamentally
limited by the quality of sensor calibration. Sensors must be well-located with
respect to the car-body frame before they can provide meaningful localization
and environmental perception. However, while many online methods are proposed
to calibrate the extrinsic parameters between sensors, few studies focus on the
calibration between sensor and vehicle coordinate system. To this end, we
present SensorX2car, a calibration toolbox for the online calibration of
sensor-to-car coordinate systems in road scenes. It contains four commonly used
sensors: IMU (Inertial Measurement Unit), GNSS (Global Navigation Satellite
System), LiDAR (Light Detection and Ranging), Camera, and millimeter-wave
Radar. We design a method for each sensor respectively and mainly calibrate its
rotation to the car-body. Real-world and simulated experiments demonstrate the
accuracy and generalization capabilities of the proposed method. Meanwhile, the
related codes have been open-sourced to benefit the community. To the best of
our knowledge, SensorX2car is the first open-source sensor-to-car calibration
toolbox. The code is available at https://github.com/OpenCalib/SensorX2car.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ACQ: Improving Generative Data-free Quantization Via Attention
  Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jixing Li, Xiaozhou Guo, Benzhe Dai, Guoliang Gong, Min Jin, Gang Chen, Wenyu Mao, Huaxiang Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-free quantization aims to achieve model quantization without accessing
any authentic sample. It is significant in an application-oriented context
involving data privacy. Converting noise vectors into synthetic samples through
a generator is a popular data-free quantization method, which is called
generative data-free quantization. However, there is a difference in attention
between synthetic samples and authentic samples. This is always ignored and
restricts the quantization performance. First, since synthetic samples of the
same class are prone to have homogenous attention, the quantized network can
only learn limited modes of attention. Second, synthetic samples in eval mode
and training mode exhibit different attention. Hence, the batch-normalization
statistics matching tends to be inaccurate. ACQ is proposed in this paper to
fix the attention of synthetic samples. An attention center position-condition
generator is established regarding the homogenization of intra-class attention.
Restricted by the attention center matching loss, the attention center position
is treated as the generator's condition input to guide synthetic samples in
obtaining diverse attention. Moreover, we design adversarial loss of paired
synthetic samples under the same condition to prevent the generator from paying
overmuch attention to the condition, which may result in mode collapse. To
improve the attention similarity of synthetic samples in different network
modes, we introduce a consistency penalty to guarantee accurate BN statistics
matching. The experimental results demonstrate that ACQ effectively improves
the attention problems of synthetic samples. Under various training settings,
ACQ achieves the best quantization performance. For the 4-bit quantization of
Resnet18 and Resnet50, ACQ reaches 67.55% and 72.23% accuracy, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tailor: Altering Skip Connections for Resource-Efficient Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivia Weng, Gabriel Marcano, Vladimir Loncar, Alireza Khodamoradi, Nojan Sheybani, Farinaz Koushanfar, Kristof Denolf, Javier Mauricio Duarte, Ryan Kastner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks use skip connections to improve training convergence.
However, these skip connections are costly in hardware, requiring extra buffers
and increasing on- and off-chip memory utilization and bandwidth requirements.
In this paper, we show that skip connections can be optimized for hardware when
tackled with a hardware-software codesign approach. We argue that while a
network's skip connections are needed for the network to learn, they can later
be removed or shortened to provide a more hardware efficient implementation
with minimal to no accuracy loss. We introduce Tailor, a codesign tool whose
hardware-aware training algorithm gradually removes or shortens a fully trained
network's skip connections to lower their hardware cost. The optimized hardware
designs improve resource utilization by up to 34% for BRAMs, 13% for FFs, and
16% for LUTs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Effective End-to-End Vision Language <span class="highlight-title">Pretrain</span>ing with Semantic Visual
  Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Yang, Fayao Liu, Guosheng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current vision language pretraining models are dominated by methods using
region visual features extracted from object detectors. Given their good
performance, the extract-then-process pipeline significantly restricts the
inference speed and therefore limits their real-world use cases. However,
training vision language models from raw image pixels is difficult, as the raw
image pixels give much less prior knowledge than region features. In this
paper, we systematically study how to leverage auxiliary visual pretraining
tasks to help training end-to-end vision language models. We introduce three
types of visual losses that enable much faster convergence and better
finetuning accuracy. Compared with region feature models, our end-to-end models
could achieve similar or better performance on downstream tasks and run more
than 10 times faster during inference. Compared with other end-to-end models,
our proposed method could achieve similar or better performance when pretrained
for only 10% of the pretraining GPU hours.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Unsupervised Phase-based 3D Incompressible Motion Estimation in
  Tagged-MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07234v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07234v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangxing Bian, Fangxu Xing, Jinglun Yu, Muhan Shao, Yihao Liu, Aaron Carass, Jonghye Woo, Jerry L. Prince
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tagged magnetic resonance imaging (MRI) has been used for decades to observe
and quantify the detailed motion of deforming tissue. However, this technique
faces several challenges such as tag fading, large motion, long computation
times, and difficulties in obtaining diffeomorphic incompressible flow fields.
To address these issues, this paper presents a novel unsupervised phase-based
3D motion estimation technique for tagged MRI. We introduce two key
innovations. First, we apply a sinusoidal transformation to the harmonic phase
input, which enables end-to-end training and avoids the need for phase
interpolation. Second, we propose a Jacobian determinant-based learning
objective to encourage incompressible flow fields for deforming biological
tissues. Our method efficiently estimates 3D motion fields that are accurate,
dense, and approximately diffeomorphic and incompressible. The efficacy of the
method is assessed using human tongue motion during speech, and includes both
healthy controls and patients that have undergone glossectomy. We show that the
method outperforms existing approaches, and also exhibits improvements in
speed, robustness to tag fading, and large tongue motion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measuring uncertainty in human visual segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Vacher, Claire Launay, Pascal Mamassian, Ruben Coen-Cagli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmenting visual stimuli into distinct groups of features and visual objects
is central to visual function. Classical psychophysical methods have helped
uncover many rules of human perceptual segmentation, and recent progress in
machine learning has produced successful algorithms. Yet, the computational
logic of human segmentation remains unclear, partially because we lack
well-controlled paradigms to measure perceptual segmentation maps and compare
models quantitatively. Here we propose a new, integrated approach: given an
image, we measure multiple pixel-based same-different judgments and perform
model--based reconstruction of the underlying segmentation map. The
reconstruction is robust to several experimental manipulations and captures the
variability of individual participants. We demonstrate the validity of the
approach on human segmentation of natural images and composite textures. We
show that image uncertainty affects measured human variability, and it
influences how participants weigh different visual features. Because any
putative segmentation algorithm can be inserted to perform the reconstruction,
our paradigm affords quantitative tests of theories of perception as well as
new benchmarks for segmentation algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 8 figures, 4 appendix, 4 figures in appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-target multi-camera vehicle tracking using <span class="highlight-title">transformer</span>-based
  camera link model and spatial-temporal information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hsiang-Wei Huang, Jenq-Neng Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-target multi-camera tracking (MTMCT) of vehicles, i.e. tracking
vehicles across multiple cameras, is a crucial application for the development
of smart city and intelligent traffic system. The main challenges of MTMCT of
vehicles include the intra-class variability of the same vehicle and
inter-class similarity between different vehicles and how to associate the same
vehicle accurately across different cameras under large search space. Previous
methods for MTMCT usually use hierarchical clustering of trajectories to
conduct cross camera association. However, the search space can be large and
does not take spatial and temporal information into consideration. In this
paper, we proposed a transformer-based camera link model with spatial and
temporal filtering to conduct cross camera tracking. Achieving 73.68% IDF1 on
the Nvidia Cityflow V2 dataset test set, showing the effectiveness of our
camera link model on multi-target multi-camera tracking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Domain-Agnostic Approach for Characterization of Lifelong Learning
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Megan M. Baker, Alexander New, Mario Aguilar-Simon, Ziad Al-Halah, Sébastien M. R. Arnold, Ese Ben-Iwhiwhu, Andrew P. Brna, Ethan Brooks, Ryan C. Brown, Zachary Daniels, Anurag Daram, Fabien Delattre, Ryan Dellana, Eric Eaton, Haotian Fu, Kristen Grauman, Jesse Hostetler, Shariq Iqbal, Cassandra Kent, Nicholas Ketz, Soheil Kolouri, George Konidaris, Dhireesha Kudithipudi, Erik Learned-Miller, Seungwon Lee, Michael L. Littman, Sandeep Madireddy, Jorge A. Mendez, Eric Q. Nguyen, Christine D. Piatko, Praveen K. Pilly, Aswin Raghavan, Abrar Rahman, Santhosh Kumar Ramakrishnan, Neale Ratzlaff, Andrea Soltoggio, Peter Stone, Indranil Sur, Zhipeng Tang, Saket Tiwari, Kyle Vedder, Felix Wang, Zifan Xu, Angel Yanguas-Gil, Harel Yedidsion, Shangqun Yu, Gautam K. Vallabha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the advancement of machine learning techniques in recent years,
state-of-the-art systems lack robustness to "real world" events, where the
input distributions and tasks encountered by the deployed systems will not be
limited to the original training context, and systems will instead need to
adapt to novel distributions and tasks while deployed. This critical gap may be
addressed through the development of "Lifelong Learning" systems that are
capable of 1) Continuous Learning, 2) Transfer and Adaptation, and 3)
Scalability. Unfortunately, efforts to improve these capabilities are typically
treated as distinct areas of research that are assessed independently, without
regard to the impact of each separate capability on other aspects of the
system. We instead propose a holistic approach, using a suite of metrics and an
evaluation framework to assess Lifelong Learning in a principled way that is
agnostic to specific domains or system techniques. Through five case studies,
we show that this suite of metrics can inform the development of varied and
complex Lifelong Learning systems. We highlight how the proposed suite of
metrics quantifies performance trade-offs present during Lifelong Learning
system development - both the widely discussed Stability-Plasticity dilemma and
the newly proposed relationship between Sample Efficient and Robust Learning.
Further, we make recommendations for the formulation and use of metrics to
guide the continuing development of Lifelong Learning systems and assess their
progress in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Neural Networks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via
  Novel-View Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Allan Zhou, Moo Jin Kim, Lirui Wang, Pete Florence, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expert demonstrations are a rich source of supervision for training visual
robotic manipulation policies, but imitation learning methods often require
either a large number of demonstrations or expensive online expert supervision
to learn reactive closed-loop behaviors. In this work, we introduce SPARTN
(Synthetic Perturbations for Augmenting Robot Trajectories via NeRF): a
fully-offline data augmentation scheme for improving robot policies that use
eye-in-hand cameras. Our approach leverages neural radiance fields (NeRFs) to
synthetically inject corrective noise into visual demonstrations, using NeRFs
to generate perturbed viewpoints while simultaneously calculating the
corrective actions. This requires no additional expert supervision or
environment interaction, and distills the geometric information in NeRFs into a
real-time reactive RGB-only policy. In a simulated 6-DoF visual grasping
benchmark, SPARTN improves success rates by 2.8$\times$ over imitation learning
without the corrective augmentations and even outperforms some methods that use
online supervision. It additionally closes the gap between RGB-only and RGB-D
success rates, eliminating the previous need for depth sensors. In real-world
6-DoF robotic grasping experiments from limited human demonstrations, our
method improves absolute success rates by $22.5\%$ on average, including
objects that are traditionally challenging for depth-based methods. See video
results at \url{https://bland.website/spartn}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning image representations for anomaly detection: application to
  discovery of histological alterations in drug development 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07675v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07675v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Igor Zingman, Birgit Stierstorfer, Charlotte Lempp, Fabian Heinemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a system for anomaly detection in histopathological images. In
histology, normal samples are usually abundant, whereas anomalous
(pathological) cases are scarce or not available. Under such settings,
one-class classifiers trained on healthy data can detect out-of-distribution
anomalous samples. Such approaches combined with pre-trained Convolutional
Neural Network (CNN) representations of images were previously employed for
anomaly detection (AD). However, pre-trained off-the-shelf CNN representations
may not be sensitive to abnormal conditions in tissues, while natural
variations of healthy tissue may result in distant representations. To adapt
representations to relevant details in healthy tissue we propose training a CNN
on an auxiliary task that discriminates healthy tissue of different species,
organs, and staining reagents. Almost no additional labeling workload is
required, since healthy samples come automatically with aforementioned labels.
During training we enforce compact image representations with a center-loss
term, which further improves representations for AD. The proposed system
outperforms established AD methods on a published dataset of liver anomalies.
Moreover, it provided comparable results to conventional methods specifically
tailored for quantification of liver anomalies. We show that our approach can
be used for toxicity assessment of candidate drugs at early development stages
and thereby may reduce expensive late-stage drug attrition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InstructPix2Pix: Learning to Follow Image Editing Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09800v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09800v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Brooks, Aleksander Holynski, Alexei A. Efros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method for editing images from human instructions: given an
input image and a written instruction that tells the model what to do, our
model follows these instructions to edit the image. To obtain training data for
this problem, we combine the knowledge of two large pretrained models -- a
language model (GPT-3) and a text-to-image model (Stable Diffusion) -- to
generate a large dataset of image editing examples. Our conditional diffusion
model, InstructPix2Pix, is trained on our generated data, and generalizes to
real images and user-written instructions at inference time. Since it performs
edits in the forward pass and does not require per example fine-tuning or
inversion, our model edits images quickly, in a matter of seconds. We show
compelling editing results for a diverse collection of input images and written
instructions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page with code:
  https://www.timothybrooks.com/instruct-pix2pix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ YoloCurvSeg: You Only Label One Noisy Skeleton for Vessel-style
  Curvilinear Structure Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05566v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05566v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Lin, Linkai Peng, Huaqing He, Pujin Cheng, Jiewei Wu, Kenneth K. Y. Wong, Xiaoying Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly-supervised learning (WSL) has been proposed to alleviate the conflict
between data annotation cost and model performance through employing
sparsely-grained (i.e., point-, box-, scribble-wise) supervision and has shown
promising performance, particularly in the image segmentation field. However,
it is still a very challenging task due to the limited supervision, especially
when only a small number of labeled samples are available. Additionally, almost
all existing WSL segmentation methods are designed for star-convex structures
which are very different from curvilinear structures such as vessels and
nerves. In this paper, we propose a novel sparsely annotated segmentation
framework for curvilinear structures, named YoloCurvSeg. A very essential
component of YoloCurvSeg is image synthesis. Specifically, a background
generator delivers image backgrounds that closely match the real distributions
through inpainting dilated skeletons. The extracted backgrounds are then
combined with randomly emulated curves generated by a Space Colonization
Algorithm-based foreground generator and through a multilayer patch-wise
contrastive learning synthesizer. In this way, a synthetic dataset with both
images and curve segmentation labels is obtained, at the cost of only one or a
few noisy skeleton annotations. Finally, a segmenter is trained with the
generated dataset and possibly an unlabeled dataset. The proposed YoloCurvSeg
is evaluated on four publicly available datasets (OCTA500, CORN, DRIVE and
CHASEDB1) and the results show that YoloCurvSeg outperforms state-of-the-art
WSL segmentation methods by large margins. With only one noisy skeleton
annotation (respectively 0.14%, 0.03%, 1.40%, and 0.65% of the full
annotation), YoloCurvSeg achieves more than 97% of the fully-supervised
performance on each dataset. Code and datasets will be released at
https://github.com/llmir/YoloCurvSeg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 10 figures, submitted to MEDIA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Out-of-Distribution Performance on Document Image Classifiers <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07448v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07448v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Larson, Gordon Lim, Yutong Ai, David Kuang, Kevin Leach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability of a document classifier to handle inputs that are drawn from a
distribution different from the training distribution is crucial for robust
deployment and generalizability. The RVL-CDIP corpus is the de facto standard
benchmark for document classification, yet to our knowledge all studies that
use this corpus do not include evaluation on out-of-distribution documents. In
this paper, we curate and release a new out-of-distribution benchmark for
evaluating out-of-distribution performance for document classifiers. Our new
out-of-distribution benchmark consists of two types of documents: those that
are not part of any of the 16 in-domain RVL-CDIP categories (RVL-CDIP-O), and
those that are one of the 16 in-domain categories yet are drawn from a
distribution different from that of the original RVL-CDIP dataset (RVL-CDIP-N).
While prior work on document classification for in-domain RVL-CDIP documents
reports high accuracy scores, we find that these models exhibit accuracy drops
of between roughly 15-30% on our new out-of-domain RVL-CDIP-N benchmark, and
further struggle to distinguish between in-domain RVL-CDIP-N and out-of-domain
RVL-CDIP-O inputs. Our new benchmark provides researchers with a valuable new
resource for analyzing out-of-distribution performance on document classifiers.
Our new out-of-distribution data can be found at
https://github.com/gxlarson/rvl-cdip-ood.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS D&B 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Theseus: A Library for Differentiable Nonlinear Optimization <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09442v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09442v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Pineda, Taosha Fan, Maurizio Monge, Shobha Venkataraman, Paloma Sodhi, Ricky T. Q. Chen, Joseph Ortiz, Daniel DeTone, Austin Wang, Stuart Anderson, Jing Dong, Brandon Amos, Mustafa Mukadam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Theseus, an efficient application-agnostic open source library for
differentiable nonlinear least squares (DNLS) optimization built on PyTorch,
providing a common framework for end-to-end structured learning in robotics and
vision. Existing DNLS implementations are application specific and do not
always incorporate many ingredients important for efficiency. Theseus is
application-agnostic, as we illustrate with several example applications that
are built using the same underlying differentiable components, such as
second-order optimizers, standard costs functions, and Lie groups. For
efficiency, Theseus incorporates support for sparse solvers, automatic
vectorization, batching, GPU acceleration, and gradient computation with
implicit differentiation and direct loss minimization. We do extensive
performance evaluation in a set of applications, demonstrating significant
efficiency gains and better scalability when these features are incorporated.
Project page: https://sites.google.com/view/theseus-ai
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Advances in Neural Information Processing Systems (NeurIPS), 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Category-Independent Articulated Object Tracking with Factor Graphs <span class="chip">IROS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.03721v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.03721v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Heppert, Toki Migimatsu, Brent Yi, Claire Chen, Jeannette Bohg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots deployed in human-centric environments may need to manipulate a
diverse range of articulated objects, such as doors, dishwashers, and cabinets.
Articulated objects often come with unexpected articulation mechanisms that are
inconsistent with categorical priors: for example, a drawer might rotate about
a hinge joint instead of sliding open. We propose a category-independent
framework for predicting the articulation models of unknown objects from
sequences of RGB-D images. The prediction is performed by a two-step process:
first, a visual perception module tracks object part poses from raw images, and
second, a factor graph takes these poses and infers the articulation model
including the current configuration between the parts as a 6D twist. We also
propose a manipulation-oriented metric to evaluate predicted joint twists in
terms of how well a compliant robot controller would be able to manipulate the
articulated object given the predicted twist. We demonstrate that our visual
perception and factor graph modules outperform baselines on simulated data and
show the applicability of our factor graph on real world data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>V2: Camera-ready IROS 2022 version 11 pages, 10 figures, IROS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Initial Orbit Determination from Only Heading Measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.10120v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.10120v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John A. Christian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces the problem of initial orbit determination (IOD) from
only heading measurements. Such a problem occurs in practice when estimating
the orbit of a spacecraft using visual odometry measurements from an optical
camera. After reviewing the problem geometry, a simple solution is developed in
the form of an iterative scheme on the parameters describing the orbital
hodograph. Numerical results are presented for an example spacecraft in low
lunar orbit. The principal intent of this brief study is to communicate the
existence of a new class of IOD problem to the community and to encourage the
broader study of hodographs and heading-only IOD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exposing Fine-Grained Adversarial Vulnerability of Face Anti-Spoofing
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.14851v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.14851v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songlin Yang, Wei Wang, Chenye Xu, Ziwen He, Bo Peng, Jing Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face anti-spoofing aims to discriminate the spoofing face images (e.g.,
printed photos) from live ones. However, adversarial examples greatly challenge
its credibility, where adding some perturbation noise can easily change the
predictions. Previous works conducted adversarial attack methods to evaluate
the face anti-spoofing performance without any fine-grained analysis that which
model architecture or auxiliary feature is vulnerable to the adversary. To
handle this problem, we propose a novel framework to expose the fine-grained
adversarial vulnerability of the face anti-spoofing models, which consists of a
multitask module and a semantic feature augmentation (SFA) module. The
multitask module can obtain different semantic features for further evaluation,
but only attacking these semantic features fails to reflect the
discrimination-related vulnerability. We then design the SFA module to
introduce the data distribution prior for more discrimination-related gradient
directions for generating adversarial examples. Comprehensive experiments show
that SFA module increases the attack success rate by nearly 40$\%$ on average.
We conduct this fine-grained adversarial analysis on different annotations,
geometric maps, and backbone networks (e.g., Resnet network). These
fine-grained adversarial examples can be used for selecting robust backbone
networks and auxiliary features. They also can be used for adversarial
training, which makes it practical to further improve the accuracy and
robustness of the face anti-spoofing models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instance Segmentation Based Graph Extraction for Handwritten Circuit
  Diagram Images <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.03155v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.03155v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Bayer, Amit Kumar Roy, Andreas Dengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Handwritten circuit diagrams from educational scenarios or historic sources
usually exist on analogue media. For deriving their functional principles or
flaws automatically, they need to be digitized, extracting their electrical
graph. Recently, the base technologies for automated pipelines facilitating
this process shifted from computer vision to machine learning. This paper
describes an approach for extracting both the electrical components (including
their terminals and describing texts) as well their interconnections (including
junctions and wire hops) by the means of instance segmentation and keypoint
extraction. Consequently, the resulting graph extraction process consists of a
simple two-step process of model inference and trivial geometric keypoint
matching. The dataset itself, its preparation, model training and
post-processing are described and publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>As submitted to ICPRAM23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Object Detection in 20 Years: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1905.05055v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1905.05055v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengxia Zou, Keyan Chen, Zhenwei Shi, Yuhong Guo, Jieping Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection, as of one the most fundamental and challenging problems in
computer vision, has received great attention in recent years. Over the past
two decades, we have seen a rapid technological evolution of object detection
and its profound impact on the entire computer vision field. If we consider
today's object detection technique as a revolution driven by deep learning,
then back in the 1990s, we would see the ingenious thinking and long-term
perspective design of early computer vision. This paper extensively reviews
this fast-moving research field in the light of technical evolution, spanning
over a quarter-century's time (from the 1990s to 2022). A number of topics have
been covered in this paper, including the milestone detectors in history,
detection datasets, metrics, fundamental building blocks of the detection
system, speed-up techniques, and the recent state-of-the-art detection methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Proceedings of the IEEE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Object Detection performance variation on compressed satellite image
  <span class="highlight-title">dataset</span>s with iquaflow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.05892v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.05892v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pau Gallés, Katalin Takats, Javier Marin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A lot of work has been done to reach the best possible performance of
predictive models on images. There are fewer studies about the resilience of
these models when they are trained on image datasets that suffer modifications
altering their original quality. Yet this is a common problem that is often
encountered in the industry. A good example of that is with earth observation
satellites that are capturing many images. The energy and time of connection to
the earth of an orbiting satellite are limited and must be carefully used. An
approach to mitigate that is to compress the images on board before
downloading. The compression can be regulated depending on the intended usage
of the image and the requirements of this application. We present a new
software tool with the name iquaflow that is designed to study image quality
and model performance variation given an alteration of the image dataset.
Furthermore, we do a showcase study about oriented object detection models
adoption on a public image dataset DOTA Xia_2018_CVPR given different
compression levels. The optimal compression point is found and the usefulness
of iquaflow becomes evident.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ T2M-<span class="highlight-title">GPT</span>: Generating Human Motion from Textual Descriptions with Discrete
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06052v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06052v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang, Yong Zhang, Hongwei Zhao, Hongtao Lu, Xi Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we investigate a simple and must-known conditional generative
framework based on Vector Quantised-Variational AutoEncoder (VQ-VAE) and
Generative Pre-trained Transformer (GPT) for human motion generation from
textural descriptions. We show that a simple CNN-based VQ-VAE with commonly
used training recipes (EMA and Code Reset) allows us to obtain high-quality
discrete representations. For GPT, we incorporate a simple corruption strategy
during the training to alleviate training-testing discrepancy. Despite its
simplicity, our T2M-GPT shows better performance than competitive approaches,
including recent diffusion-based approaches. For example, on HumanML3D, which
is currently the largest dataset, we achieve comparable performance on the
consistency between text and generated motion (R-Precision), but with FID 0.116
largely outperforming MotionDiffuse of 0.630. Additionally, we conduct analyses
on HumanML3D and observe that the dataset size is a limitation of our approach.
Our work suggests that VQ-VAE still remains a competitive approach for human
motion generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages. Project page: https://mael-zys.github.io/T2M-GPT/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Achieving Domain Generalization in Underwater Object Detection by Domain
  Mixup and Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.02230v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.02230v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Chen, Pinhao Song, Hong Liu, Linhui Dai, Xiaochuan Zhang, Runwei Ding, Shengquan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of existing underwater object detection methods degrades
seriously when facing domain shift caused by complicated underwater
environments. Due to the limitation of the number of domains in the dataset,
deep detectors easily memorize a few seen domains, which leads to low
generalization ability. There are two common ideas to improve the domain
generalization performance. First, it can be inferred that the detector trained
on as many domains as possible is domain-invariant. Second, for the images with
the same semantic content in different domains, their hidden features should be
equivalent. This paper further excavates these two ideas and proposes a domain
generalization framework (named DMC) that learns how to generalize across
domains from Domain Mixup and Contrastive Learning. First, based on the
formation of underwater images, an image in an underwater environment is the
linear transformation of another underwater environment. Thus, a style transfer
model, which outputs a linear transformation matrix instead of the whole image,
is proposed to transform images from one source domain to another, enriching
the domain diversity of the training data. Second, mixup operation interpolates
different domains on the feature level, sampling new domains on the domain
manifold. Third, contrastive loss is selectively applied to features from
different domains to force the model to learn domain invariant features but
retain the discriminative capacity. With our method, detectors will be robust
to domain shift. Also, a domain generalization benchmark S-UODAC2020 for
detection is set up to measure the performance of our method. Comprehensive
experiments on S-UODAC2020 and two object recognition benchmarks (PACS and
VLCS) demonstrate that the proposed method is able to learn domain-invariant
representations, and outperforms other domain generalization methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Auxiliary Cross-Modal Representation Learning with Triplet Loss
  Functions for Online Handwriting Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.07901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.07901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Ott, David Rügamer, Lucas Heublein, Bernd Bischl, Christopher Mutschler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-modal representation learning learns a shared embedding between two or
more modalities to improve performance in a given task compared to using only
one of the modalities. Cross-modal representation learning from different data
types -- such as images and time-series data (e.g., audio or text data) --
requires a deep metric learning loss that minimizes the distance between the
modality embeddings. In this paper, we propose to use the triplet loss, which
uses positive and negative identities to create sample pairs with different
labels, for cross-modal representation learning between image and time-series
modalities (CMR-IS). By adapting the triplet loss for cross-modal
representation learning, higher accuracy in the main (time-series
classification) task can be achieved by exploiting additional information of
the auxiliary (image classification) task. Our experiments on synthetic data
and handwriting recognition data from sensor-enhanced pens show improved
classification accuracy, faster convergence, and better generalizability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamically Mitigating Data Discrepancy with Balanced Focal Loss for
  Replay Attack Detection <span class="chip">ICPR2020</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2006.14563v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2006.14563v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongqiang Dou, Haocheng Yang, Maolin Yang, Yanyan Xu, Dengfeng Ke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It becomes urgent to design effective anti-spoofing algorithms for vulnerable
automatic speaker verification systems due to the advancement of high-quality
playback devices. Current studies mainly treat anti-spoofing as a binary
classification problem between bonafide and spoofed utterances, while lack of
indistinguishable samples makes it difficult to train a robust spoofing
detector. In this paper, we argue that for anti-spoofing, it needs more
attention for indistinguishable samples over easily-classified ones in the
modeling process, to make correct discrimination a top priority. Therefore, to
mitigate the data discrepancy between training and inference, we propose D3M,
to leverage a balanced focal loss function as the training objective to
dynamically scale the loss based on the traits of the sample itself. Besides,
in the experiments, we select three kinds of features that contain both
magnitude-based and phase-based information to form complementary and
informative features. Experimental results on the ASVspoof2019 dataset
demonstrate the superiority of the proposed methods by comparison between our
systems and top-performing ones. Systems trained with the balanced focal loss
perform significantly better than conventional cross-entropy loss. With
complementary features, our fusion system with only three kinds of features
outperforms other systems containing five or more complex single models by
22.5% for min-tDCF and 7% for EER, achieving a min-tDCF and an EER of 0.0124
and 0.55% respectively. Furthermore, we present and discuss the evaluation
results on real replay data apart from the simulated ASVspoof2019 data,
indicating that research for anti-spoofing still has a long way to go. Source
code, analysis data, and other details are publicly available at
https://github.com/asvspoof/D3M.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 25th International Conference on Pattern Recognition (ICPR2020)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comprehensive Literature <span class="highlight-title">Survey</span> on Deep Learning used in Image
  Memorability Prediction and Modification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06080v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06080v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ananya Sadana, Nikita Thakur, Nikita Poria, Astika Anand, Seeja K. R
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As humans, we can remember certain visuals in great detail, and sometimes
even after viewing them once. What is even more interesting is that humans tend
to remember and forget the same things, suggesting that there might be some
general internal characteristics of an image to encode and discard similar
types of information. Research suggests that some pictures tend to be memorized
more than others. The ability of an image to be remembered by different viewers
is one of its intrinsic properties. In visualization and photography, creating
memorable images is a difficult task. Hence, to solve the problem, various
techniques predict visual memorability and manipulate images' memorability. We
present a comprehensive literature survey to assess the deep learning
techniques used to predict and modify memorability. In particular, we analyze
the use of Convolutional Neural Networks, Recurrent Neural Networks, and
Generative Adversarial Networks for image memorability prediction and
modification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with
  Multimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06267v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06267v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiu Lin, Samuel Yu, Zhiyi Kuang, Deepak Pathak, Deva Ramanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to quickly learn a new task with minimal instruction - known as
few-shot learning - is a central aspect of intelligent agents. Classical
few-shot benchmarks make use of few-shot samples from a single modality, but
such samples may not be sufficient to characterize an entire concept class. In
contrast, humans use cross-modal information to learn new concepts efficiently.
In this work, we demonstrate that one can indeed build a better ${\bf visual}$
dog classifier by ${\bf read}$ing about dogs and ${\bf listen}$ing to them
bark. To do so, we exploit the fact that recent multimodal foundation models
such as CLIP are inherently cross-modal, mapping different modalities to the
same representation space. Specifically, we propose a simple cross-modal
adaptation approach that learns from few-shot examples spanning different
modalities. By repurposing class names as additional one-shot training samples,
we achieve SOTA results with an embarrassingly simple linear classifier for
vision-language adaptation. Furthermore, we show that our approach can benefit
existing methods such as prefix tuning, adapters, and classifier ensembling.
Finally, to explore other modalities beyond vision and language, we construct
the first (to our knowledge) audiovisual few-shot benchmark and use cross-modal
training to improve the performance of both image and audio classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://linzhiqiu.github.io/papers/cross_modal/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy-Protecting Behaviours of Risk Detection in People with Dementia
  using Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10682v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10682v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pratik K. Mishra, Andrea Iaboni, Bing Ye, Kristine Newman, Alex Mihailidis, Shehroz S. Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  People living with dementia often exhibit behavioural and psychological
symptoms of dementia that can put their and others' safety at risk. Existing
video surveillance systems in long-term care facilities can be used to monitor
such behaviours of risk to alert the staff to prevent potential injuries or
death in some cases. However, these behaviours of risk events are heterogeneous
and infrequent in comparison to normal events. Moreover, analyzing raw videos
can also raise privacy concerns. In this paper, we present two novel
privacy-protecting video-based anomaly detection approaches to detect
behaviours of risks in people with dementia. We either extracted body pose
information as skeletons or used semantic segmentation masks to replace
multiple humans in the scene with their semantic boundaries. Our work differs
from most existing approaches for video anomaly detection that focus on
appearance-based features, which can put the privacy of a person at risk and is
also susceptible to pixel-based noise, including illumination and viewing
direction. We used anonymized videos of normal activities to train customized
spatio-temporal convolutional autoencoders and identify behaviours of risk as
anomalies. We showed our results on a real-world study conducted in a dementia
care unit with patients with dementia, containing approximately 21 hours of
normal activities data for training and 9 hours of data containing normal and
behaviours of risk events for testing. We compared our approaches with the
original RGB videos and obtained a similar area under the receiver operating
characteristic curve performance of 0.807 for the skeleton-based approach and
0.823 for the segmentation mask-based approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From One Hand to Multiple Hands: Imitation Learning for Dexterous
  Manipulation from Single-Camera Teleoperation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.12490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.12490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhe Qin, Hao Su, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose to perform imitation learning for dexterous manipulation with
multi-finger robot hand from human demonstrations, and transfer the policy to
the real robot hand. We introduce a novel single-camera teleoperation system to
collect the 3D demonstrations efficiently with only an iPad and a computer. One
key contribution of our system is that we construct a customized robot hand for
each user in the physical simulator, which is a manipulator resembling the same
kinematics structure and shape of the operator's hand. This provides an
intuitive interface and avoid unstable human-robot hand retargeting for data
collection, leading to large-scale and high quality data. Once the data is
collected, the customized robot hand trajectories can be converted to different
specified robot hands (models that are manufactured) to generate training
demonstrations. With imitation learning using our data, we show large
improvement over baselines with multiple complex manipulation tasks.
Importantly, we show our learned policy is significantly more robust when
transferring to the real robot. More videos can be found in the
https://yzqin.github.io/dex-teleop-imitation .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://yzqin.github.io/dex-teleop-imitation/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Omni-Seg: A Scale-aware Dynamic Network for Renal Pathological Image
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.13632v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.13632v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruining Deng, Quan Liu, Can Cui, Tianyuan Yao, Jun Long, Zuhayr Asad, R. Michael Womick, Zheyu Zhu, Agnes B. Fogo, Shilin Zhao, Haichun Yang, Yuankai Huo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Comprehensive semantic segmentation on renal pathological images is
challenging due to the heterogeneous scales of the objects. For example, on a
whole slide image (WSI), the cross-sectional areas of glomeruli can be 64 times
larger than that of the peritubular capillaries, making it impractical to
segment both objects on the same patch, at the same scale. To handle this
scaling issue, prior studies have typically trained multiple segmentation
networks in order to match the optimal pixel resolution of heterogeneous tissue
types. This multi-network solution is resource-intensive and fails to model the
spatial relationship between tissue types. In this paper, we propose the
Omni-Seg+ network, a scale-aware dynamic neural network that achieves
multi-object (six tissue types) and multi-scale (5X to 40X scale) pathological
image segmentation via a single neural network. The contribution of this paper
is three-fold: (1) a novel scale-aware controller is proposed to generalize the
dynamic neural network from single-scale to multi-scale; (2) semi-supervised
consistency regularization of pseudo-labels is introduced to model the
inter-scale correlation of unannotated tissue types into a single end-to-end
learning paradigm; and (3) superior scale-aware generalization is evidenced by
directly applying a model trained on human kidney images to mouse kidney
images, without retraining. By learning from ~150,000 human pathological image
patches from six tissue types at three different resolutions, our approach
achieved superior segmentation performance according to human visual assessment
and evaluation of image-omics (i.e., spatial transcriptomics). The official
implementation is available at https://github.com/ddrrnn123/Omni-Seg.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automated Reconstruction of 3D Open Surfaces from Sparse Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.15059v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.15059v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Samiul Arshad, William J. Beksi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world 3D data may contain intricate details defined by salient surface
gaps. Automated reconstruction of these open surfaces (e.g., non-watertight
meshes) is a challenging problem for environment synthesis in mixed reality
applications. Current learning-based implicit techniques can achieve high
fidelity on closed-surface reconstruction. However, their dependence on the
distinction between the inside and outside of a surface makes them incapable of
reconstructing open surfaces. Recently, a new class of implicit functions have
shown promise in reconstructing open surfaces by regressing an unsigned
distance field. Yet, these methods rely on a discretized representation of the
raw data, which loses important surface details and can lead to outliers in the
reconstruction. We propose IPVNet, a learning-based implicit model that
predicts the unsigned distance between a surface and a query point in 3D space
by leveraging both raw point cloud data and its discretized voxel counterpart.
Experiments on synthetic and real-world public datasets demonstrates that
IPVNet outperforms the state of the art while producing far fewer outliers in
the reconstruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be presented at the 2022 IEEE International Symposium on Mixed and
  Augmented Reality (ISMAR) Workshop on Photorealistic Image and Environment
  Synthesis for Mixed Reality (PIES-MR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Salient Sign Detection In Safe Autonomous Driving: AI Which Reasons Over
  Full Visual Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.05804v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.05804v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ross Greer, Akshay Gopalkrishnan, Nachiket Deo, Akshay Rangesh, Mohan Trivedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting road traffic signs and accurately determining how they can affect
the driver's future actions is a critical task for safe autonomous driving
systems. However, various traffic signs in a driving scene have an unequal
impact on the driver's decisions, making detecting the salient traffic signs a
more important task. Our research addresses this issue, constructing a traffic
sign detection model which emphasizes performance on salient signs, or signs
that influence the decisions of a driver. We define a traffic sign salience
property and use it to construct the LAVA Salient Signs Dataset, the first
traffic sign dataset that includes an annotated salience property. Next, we use
a custom salience loss function, Salience-Sensitive Focal Loss, to train a
Deformable DETR object detection model in order to emphasize stronger
performance on salient signs. Results show that a model trained with
Salience-Sensitive Focal Loss outperforms a model trained without, with regards
to recall of both salient signs and all signs combined. Further, the
performance margin on salient signs compared to all signs is largest for the
model trained with Salience-Sensitive Focal Loss.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe Control Transitions: Machine Vision Based Observable Readiness
  Index and Data-Driven Takeover Time Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.05805v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.05805v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ross Greer, Nachiket Deo, Akshay Rangesh, Pujitha Gunaratne, Mohan Trivedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To make safe transitions from autonomous to manual control, a vehicle must
have a representation of the awareness of driver state; two metrics which
quantify this state are the Observable Readiness Index and Takeover Time. In
this work, we show that machine learning models which predict these two metrics
are robust to multiple camera views, expanding from the limited view angles in
prior research. Importantly, these models take as input feature vectors
corresponding to hand location and activity as well as gaze location, and we
explore the tradeoffs of different views in generating these feature vectors.
Further, we introduce two metrics to evaluate the quality of control
transitions following the takeover event (the maximal lateral deviation and
velocity deviation) and compute correlations of these post-takeover metrics to
the pre-takeover predictive metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ (Safe) SMART Hands: Hand Activity Analysis and Distraction Alerts Using
  a Multi-Camera Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.05838v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.05838v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ross Greer, Lulua Rakla, Anish Gopalan, Mohan Trivedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manual (hand-related) activity is a significant source of crash risk while
driving. Accordingly, analysis of hand position and hand activity occupation is
a useful component to understanding a driver's readiness to take control of a
vehicle. Visual sensing through cameras provides a passive means of observing
the hands, but its effectiveness varies depending on camera location. We
introduce an algorithmic framework, SMART Hands, for accurate hand
classification with an ensemble of camera views using machine learning. We
illustrate the effectiveness of this framework in a 4-camera setup, reaching
98% classification accuracy on a variety of locations and held objects for both
of the driver's hands. We conclude that this multi-camera framework can be
extended to additional tasks such as gaze and pose analysis, with further
applications in driver and passenger safety.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CHAMP: Crowdsourced, History-Based Advisory of Mapped Pedestrians for
  Safer Driver Assistance Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.05842v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.05842v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ross Greer, Lulua Rakla, Samveed Desai, Afnan Alofi, Akshay Gopalkrishnan, Mohan Trivedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicles are constantly approaching and sharing the road with pedestrians,
and as a result it is critical for vehicles to prevent any collisions with
pedestrians. Current methods for pedestrian collision prevention focus on
integrating visual pedestrian detectors with Automatic Emergency Braking (AEB)
systems which can trigger warnings and apply brakes as a pedestrian enters a
vehicle's path. Unfortunately, pedestrian-detection-based systems can be
hindered in certain situations such as nighttime or when pedestrians are
occluded. Our system, CHAMP (Crowdsourced, History-based Advisories of Mapped
Pedestrians), addresses such issues using an online, map-based pedestrian
detection system where pedestrian locations are aggregated into a dataset after
repeated passes of locations. Using this dataset, we are able to learn
pedestrian zones and generate advisory notices when a vehicle is approaching a
pedestrian despite challenges like dark lighting or pedestrian occlusion. We
collected and carefully annotated pedestrian data in La Jolla, CA to construct
training and test sets of pedestrian locations. Moreover, we use the number of
correct advisories, false advisories, and missed advisories to define precision
and recall performance metrics to evaluate CHAMP. This approach can be tuned
such that we achieve a maximum of 100% precision and 75% recall on the
experimental dataset, with performance enhancement options through further data
collection.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Analysis of Bias Amplification in Graph Neural Network
  Approaches for Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikzad Chizari, Niloufar Shoeibi, María N. Moreno-García
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender Systems (RSs) are used to provide users with personalized item
recommendations and help them overcome the problem of information overload.
Currently, recommendation methods based on deep learning are gaining ground
over traditional methods such as matrix factorization due to their ability to
represent the complex relationships between users and items and to incorporate
additional information. The fact that these data have a graph structure and the
greater capability of Graph Neural Networks (GNNs) to learn from these
structures has led to their successful incorporation into recommender systems.
However, the bias amplification issue needs to be investigated while using
these algorithms. Bias results in unfair decisions, which can negatively affect
the company reputation and financial status due to societal disappointment and
environmental harm. In this paper, we aim to comprehensively study this problem
through a literature review and an analysis of the behavior against biases of
different GNN-based algorithms compared to state-of-the-art methods. We also
intend to explore appropriate solutions to tackle this issue with the least
possible impact on the model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Biases in Scholarly Recommender Systems: Impact, Prevalence, and
  Mitigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Färber, Melissa Coutinho, Shuzhou Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the remarkable increase in the number of scientific entities such as
publications, researchers, and scientific topics, and the associated
information overload in science, academic recommender systems have become
increasingly important for millions of researchers and science enthusiasts.
However, it is often overlooked that these systems are subject to various
biases. In this article, we first break down the biases of academic recommender
systems and characterize them according to their impact and prevalence. In
doing so, we distinguish between biases originally caused by humans and biases
induced by the recommender system. Second, we provide an overview of methods
that have been used to mitigate these biases in the scholarly domain. Based on
this, third, we present a framework that can be used by researchers and
developers to mitigate biases in scholarly recommender systems and to evaluate
recommender systems fairly. Finally, we discuss open challenges and possible
research directions related to scholarly biases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages, 6 figures. To be published in Scientometrics</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An investigation of the reconstruction capacity of stacked convolutional
  autoencoders for log-mel-spectrograms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasia Natsiou, Luca Longo, Sean O'Leary
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In audio processing applications, the generation of expressive sounds based
on high-level representations demonstrates a high demand. These representations
can be used to manipulate the timbre and influence the synthesis of creative
instrumental notes. Modern algorithms, such as neural networks, have inspired
the development of expressive synthesizers based on musical instrument timbre
compression. Unsupervised deep learning methods can achieve audio compression
by training the network to learn a mapping from waveforms or spectrograms to
low-dimensional representations. This study investigates the use of stacked
convolutional autoencoders for the compression of time-frequency audio
representations for a variety of instruments for a single pitch. Further
exploration of hyper-parameters and regularization techniques is demonstrated
to enhance the performance of the initial design. In an unsupervised manner,
the network is able to reconstruct a monophonic and harmonic sound based on
latent representations. In addition, we introduce an evaluation metric to
measure the similarity between the original and reconstructed samples.
Evaluating a deep generative model for the synthesis of sound is a challenging
task. Our approach is based on the accuracy of the generated frequencies as it
presents a significant metric for the perception of harmonic sounds. This work
is expected to accelerate future experiments on audio compression using neural
autoencoders.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-parametric identifiability and sensitivity analysis of synthetic
  control models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakob Zeitler, Athanasios Vlontzos, Ciaran M. Gilligan-Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantifying cause and effect relationships is an important problem in many
domains. The gold standard solution is to conduct a randomised controlled
trial. However, in many situations such trials cannot be performed. In the
absence of such trials, many methods have been devised to quantify the causal
impact of an intervention from observational data given certain assumptions.
One widely used method are synthetic control models. While identifiability of
the causal estimand in such models has been obtained from a range of
assumptions, it is widely and implicitly assumed that the underlying
assumptions are satisfied for all time periods both pre- and post-intervention.
This is a strong assumption, as synthetic control models can only be learned in
pre-intervention period. In this paper we address this challenge, and prove
identifiability can be obtained without the need for this assumption, by
showing it follows from the principle of invariant causal mechanisms. Moreover,
for the first time, we formulate and study synthetic control models in Pearl's
structural causal model framework. Importantly, we provide a general framework
for sensitivity analysis of synthetic control causal inference to violations of
the assumptions underlying non-parametric identifiability. We end by providing
an empirical demonstration of our sensitivity analysis framework on simulated
and real data in the widely-used linear synthetic control framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Causal Learning and Reasoning Conference (CLeaR) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Analysis of Loss Functions for Binary Classification and Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeffrey Buzas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores connections between margin-based loss functions and
consistency in binary classification and regression applications. It is shown
that a large class of margin-based loss functions for binary
classification/regression result in estimating scores equivalent to
log-likelihood scores weighted by an even function. A simple characterization
for conformable (consistent) loss functions is given, which allows for
straightforward comparison of different losses, including exponential loss,
logistic loss, and others. The characterization is used to construct a new
Huber-type loss function for the logistic model. A simple relation between the
margin and standardized logistic regression residuals is derived, demonstrating
that all margin-based loss can be viewed as loss functions of squared
standardized logistic regression residuals. The relation provides new,
straightforward interpretations for exponential and logistic loss, and aids in
understanding why exponential loss is sensitive to outliers. In particular, it
is shown that minimizing empirical exponential loss is equivalent to minimizing
the sum of squared standardized logistic regression residuals. The relation
also provides new insight into the AdaBoost algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local Learning with Neuron Groups 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adeetya Patel, Michael Eickenberg, Eugene Belilovsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional deep network training methods optimize a monolithic objective
function jointly for all the components. This can lead to various
inefficiencies in terms of potential parallelization. Local learning is an
approach to model-parallelism that removes the standard end-to-end learning
setup and utilizes local objective functions to permit parallel learning
amongst model components in a deep network. Recent works have demonstrated that
variants of local learning can lead to efficient training of modern deep
networks. However, in terms of how much computation can be distributed, these
approaches are typically limited by the number of layers in a network. In this
work we propose to study how local learning can be applied at the level of
splitting layers or modules into sub-components, adding a notion of width-wise
modularity to the existing depth-wise modularity associated with local
learning. We investigate local-learning penalties that permit such models to be
trained efficiently. Our experiments on the CIFAR-10, CIFAR-100, and Imagenet32
datasets demonstrate that introducing width-level modularity can lead to
computational advantages over existing methods based on local learning and
opens new opportunities for improved model-parallel distributed training. Code
is available at: https://github.com/adeetyapatel12/GN-DGL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Semantic Segmentation on Heterogeneous <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panagiotis Meletis, Gijs Dubbelman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore semantic segmentation beyond the conventional, single-dataset
homogeneous training and bring forward the problem of Heterogeneous Training of
Semantic Segmentation (HTSS). HTSS involves simultaneous training on multiple
heterogeneous datasets, i.e. datasets with conflicting label spaces and
different (weak) annotation types from the perspective of semantic
segmentation. The HTSS formulation exposes deep networks to a larger and
previously unexplored aggregation of information that can potentially enhance
semantic segmentation in three directions: i) performance: increased
segmentation metrics on seen datasets, ii) generalization: improved
segmentation metrics on unseen datasets, and iii) knowledgeability: increased
number of recognizable semantic concepts. To research these benefits of HTSS,
we propose a unified framework, that incorporates heterogeneous datasets in a
single-network training pipeline following the established FCN standard. Our
framework first curates heterogeneous datasets to bring them into a common
format and then trains a single-backbone FCN on all of them simultaneously. To
achieve this, it transforms weak annotations, which are incompatible with
semantic segmentation, to per-pixel labels, and hierarchizes their label spaces
into a universal taxonomy. The trained HTSS models demonstrate performance and
generalization gains over a wide range of datasets and extend the inference
label space entailing hundreds of semantic classes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted 2021 (under review)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal Neural-Cracking-Machines: Self-Configurable Password Models
  from Auxiliary Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dario Pasquini, Giuseppe Ateniese, Carmela Troncoso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop the first universal password model -- a password model that, once
pre-trained, can automatically adapt to any password distribution. To achieve
this result, the model does not need to access any plaintext passwords from the
target set. Instead, it exploits users' auxiliary information, such as email
addresses, as a proxy signal to predict the underlying target password
distribution. The model uses deep learning to capture the correlation between
the auxiliary data of a group of users (e.g., users of a web application) and
their passwords. It then exploits those patterns to create a tailored password
model for the target community at inference time. No further training steps,
targeted data collection, or prior knowledge of the community's password
distribution is required. Besides defining a new state-of-the-art for password
strength estimation, our model enables any end-user (e.g., system
administrators) to autonomously generate tailored password models for their
systems without the often unworkable requirement of collecting suitable
training data and fitting the underlying password model. Ultimately, our
framework enables the democratization of well-calibrated password models to the
community, addressing a major challenge in the deployment of password security
solutions on a large scale.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v0.01</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performance-Preserving Event Log Sampling for Predictive Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadreza Fani Sani, Mozhgan Vazifehdoostirani, Gyunam Park, Marco Pegoraro, Sebastiaan J. van Zelst, Wil M. P. van der Aalst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predictive process monitoring is a subfield of process mining that aims to
estimate case or event features for running process instances. Such predictions
are of significant interest to the process stakeholders. However, most of the
state-of-the-art methods for predictive monitoring require the training of
complex machine learning models, which is often inefficient. Moreover, most of
these methods require a hyper-parameter optimization that requires several
repetitions of the training process which is not feasible in many real-life
applications. In this paper, we propose an instance selection procedure that
allows sampling training process instances for prediction models. We show that
our instance selection procedure allows for a significant increase of training
speed for next activity and remaining time prediction methods while maintaining
reliable levels of prediction accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 1 figure, 13 tables, 47 references. arXiv admin note:
  substantial text overlap with arXiv:2204.01470</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-informed Information Field Theory for Modeling Physical Systems
  with Uncertainty Quantification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Alberts, Ilias Bilionis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven approaches coupled with physical knowledge are powerful
techniques to model systems. The goal of such models is to efficiently solve
for the underlying field by combining measurements with known physical laws. As
many systems contain unknown elements, such as missing parameters, noisy data,
or incomplete physical laws, this is widely approached as an uncertainty
quantification problem. The common techniques to handle all the variables
typically depend on the numerical scheme used to approximate the posterior, and
it is desirable to have a method which is independent of any such
discretization. Information field theory (IFT) provides the tools necessary to
perform statistics over fields that are not necessarily Gaussian. We extend IFT
to physics-informed IFT (PIFT) by encoding the functional priors with
information about the physical laws which describe the field. The posteriors
derived from this PIFT remain independent of any numerical scheme and can
capture multiple modes, allowing for the solution of problems which are
ill-posed. We demonstrate our approach through an analytical example involving
the Klein-Gordon equation. We then develop a variant of stochastic gradient
Langevin dynamics to draw samples from the joint posterior over the field and
model parameters. We apply our method to numerical examples with various
degrees of model-form error and to inverse problems involving nonlinear
differential equations. As an addendum, the method is equipped with a metric
which allows the posterior to automatically quantify model-form uncertainty.
Because of this, our numerical experiments show that the method remains robust
to even an incorrect representation of the physics given sufficient data. We
numerically demonstrate that the method correctly identifies when the physics
cannot be trusted, in which case it automatically treats learning the field as
a regression problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-Timescale Adaptation in an Open-Ended Task Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Adaptive Agent Team, Jakob Bauer, Kate Baumli, Satinder Baveja, Feryal Behbahani, Avishkar Bhoopchand, Nathalie Bradley-Schmieg, Michael Chang, Natalie Clay, Adrian Collister, Vibhavari Dasagi, Lucy Gonzalez, Karol Gregor, Edward Hughes, Sheleem Kashem, Maria Loks-Thompson, Hannah Openshaw, Jack Parker-Holder, Shreya Pathak, Nicolas Perez-Nieves, Nemanja Rakicevic, Tim Rocktäschel, Yannick Schroecker, Jakub Sygnowski, Karl Tuyls, Sarah York, Alexander Zacherl, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models have shown impressive adaptation and scalability in
supervised and self-supervised learning problems, but so far these successes
have not fully translated to reinforcement learning (RL). In this work, we
demonstrate that training an RL agent at scale leads to a general in-context
learning algorithm that can adapt to open-ended novel embodied 3D problems as
quickly as humans. In a vast space of held-out environment dynamics, our
adaptive agent (AdA) displays on-the-fly hypothesis-driven exploration,
efficient exploitation of acquired knowledge, and can successfully be prompted
with first-person demonstrations. Adaptation emerges from three ingredients:
(1) meta-reinforcement learning across a vast, smooth and diverse task
distribution, (2) a policy parameterised as a large-scale attention-based
memory architecture, and (3) an effective automated curriculum that prioritises
tasks at the frontier of an agent's capabilities. We demonstrate characteristic
scaling laws with respect to network size, memory length, and richness of the
training task distribution. We believe our results lay the foundation for
increasingly general and adaptive RL agents that perform well across
ever-larger open-ended domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Strong inductive biases provably prevent harmless interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Aerni, Marco Milanta, Konstantin Donhauser, Fanny Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classical wisdom suggests that estimators should avoid fitting noise to
achieve good generalization. In contrast, modern overparameterized models can
yield small test error despite interpolating noise -- a phenomenon often called
"benign overfitting" or "harmless interpolation". This paper argues that the
degree to which interpolation is harmless hinges upon the strength of an
estimator's inductive bias, i.e., how heavily the estimator favors solutions
with a certain structure: while strong inductive biases prevent harmless
interpolation, weak inductive biases can even require fitting noise to
generalize well. Our main theoretical result establishes tight non-asymptotic
bounds for high-dimensional kernel regression that reflect this phenomenon for
convolutional kernels, where the filter size regulates the strength of the
inductive bias. We further provide empirical evidence of the same behavior for
deep neural networks with varying filter sizes and rotational invariance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Representation Learning for Text and 3D Point Cloud 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07584v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07584v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Huang, Xuran Pan, Henry Zheng, Haojun Jiang, Zhifeng Xie, Shiji Song, Gao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in vision-language pre-training (e.g. CLIP) have shown
that vision models can benefit from language supervision. While many models
using language modality have achieved great success on 2D vision tasks, the
joint representation learning of 3D point cloud with text remains
under-explored due to the difficulty of 3D-Text data pair acquisition and the
irregularity of 3D data structure. In this paper, we propose a novel Text4Point
framework to construct language-guided 3D point cloud models. The key idea is
utilizing 2D images as a bridge to connect the point cloud and the language
modalities. The proposed Text4Point follows the pre-training and fine-tuning
paradigm. During the pre-training stage, we establish the correspondence of
images and point clouds based on the readily available RGB-D data and use
contrastive learning to align the image and point cloud representations.
Together with the well-aligned image and text features achieved by CLIP, the
point cloud features are implicitly aligned with the text embeddings. Further,
we propose a Text Querying Module to integrate language information into 3D
representation learning by querying text embeddings with point cloud features.
For fine-tuning, the model learns task-specific 3D representations under
informative language guidance from the label set without 2D images. Extensive
experiments demonstrate that our model shows consistent improvement on various
downstream tasks, such as point cloud semantic segmentation, instance
segmentation, and object detection. The code will be available here:
https://github.com/LeapLabTHU/Text4Point
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Advanced Computer Vision Techniques for Sports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiago Mendes-Neves, Luís Meireles, João Mendes-Moreira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer Vision developments are enabling significant advances in many
fields, including sports. Many applications built on top of Computer Vision
technologies, such as tracking data, are nowadays essential for every top-level
analyst, coach, and even player. In this paper, we survey Computer Vision
techniques that can help many sports-related studies gather vast amounts of
data, such as Object Detection and Pose Estimation. We provide a use case for
such data: building a model for shot speed estimation with pose data obtained
using only Computer Vision models. Our model achieves a correlation of 67%. The
possibility of estimating shot speeds enables much deeper studies about
enabling the creation of new metrics and recommendation systems that will help
athletes improve their performance, in any sport. The proposed methodology is
easily replicable for many technical movements and is only limited by the
availability of video data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthcity: facilitating innovative use cases of synthetic data in
  different data modalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaozhi Qian, Bogdan-Constantin Cebere, Mihaela van der Schaar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthcity is an open-source software package for innovative use cases of
synthetic data in ML fairness, privacy and augmentation across diverse tabular
data modalities, including static data, regular and irregular time series, data
with censoring, multi-source data, composite data, and more. Synthcity provides
the practitioners with a single access point to cutting edge research and tools
in synthetic data. It also offers the community a playground for rapid
experimentation and prototyping, a one-stop-shop for SOTA benchmarks, and an
opportunity for extending research impact. The library can be accessed on
GitHub (https://github.com/vanderschaarlab/synthcity) and pip
(https://pypi.org/project/synthcity/). We warmly invite the community to join
the development effort by providing feedback, reporting bugs, and contributing
code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beating the Best: Improving on AlphaFold2 at Protein Structure
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abbi Abdel-Rehim, Oghenejokpeme Orhobor, Hang Lou, Hao Ni, Ross D. King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of Protein Structure Prediction (PSP) problem is to predict a
protein's 3D structure (confirmation) from its amino acid sequence. The problem
has been a 'holy grail' of science since the Noble prize-winning work of
Anfinsen demonstrated that protein conformation was determined by sequence. A
recent and important step towards this goal was the development of AlphaFold2,
currently the best PSP method. AlphaFold2 is probably the highest profile
application of AI to science. Both AlphaFold2 and RoseTTAFold (another
impressive PSP method) have been published and placed in the public domain
(code & models). Stacking is a form of ensemble machine learning ML in which
multiple baseline models are first learnt, then a meta-model is learnt using
the outputs of the baseline level model to form a model that outperforms the
base models. Stacking has been successful in many applications. We developed
the ARStack PSP method by stacking AlphaFold2 and RoseTTAFold. ARStack
significantly outperforms AlphaFold2. We rigorously demonstrate this using two
sets of non-homologous proteins, and a test set of protein structures published
after that of AlphaFold2 and RoseTTAFold. As more high quality prediction
methods are published it is likely that ensemble methods will increasingly
outperform any single method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Targeted Image Reconstruction by Sampling <span class="highlight-title">Pre-train</span>ed Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiageng Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A trained neural network model contains information on the training data.
Given such a model, malicious parties can leverage the "knowledge" in this
model and design ways to print out any usable information (known as model
inversion attack). Therefore, it is valuable to explore the ways to conduct a
such attack and demonstrate its severity. In this work, we proposed ways to
generate a data point of the target class without prior knowledge of the exact
target distribution by using a pre-trained diffusion model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimistic Dynamic Regret Bounds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Haddouche, Benjamin Guedj, Olivier Wintenberger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online Learning (OL) algorithms have originally been developed to guarantee
good performances when comparing their output to the best fixed strategy. The
question of performance with respect to dynamic strategies remains an active
research topic. We develop in this work dynamic adaptations of classical OL
algorithms based on the use of experts' advice and the notion of optimism. We
also propose a constructivist method to generate those advices and eventually
provide both theoretical and experimental guarantees for our procedures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReFresh: Reducing Memory Access from Exploiting Stable Historical
  Embeddings for Graph Neural Network Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kezhao Huang, Haitian Jiang, Minjie Wang, Guangxuan Xiao, David Wipf, Xiang Song, Quan Gan, Zengfeng Huang, Jidong Zhai, Zheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key performance bottleneck when training graph neural network (GNN) models
on large, real-world graphs is loading node features onto a GPU. Due to limited
GPU memory, expensive data movement is necessary to facilitate the storage of
these features on alternative devices with slower access (e.g. CPU memory).
Moreover, the irregularity of graph structures contributes to poor data
locality which further exacerbates the problem. Consequently, existing
frameworks capable of efficiently training large GNN models usually incur a
significant accuracy degradation because of the inevitable shortcuts involved.
To address these limitations, we instead propose ReFresh, a general-purpose GNN
mini-batch training framework that leverages a historical cache for storing and
reusing GNN node embeddings instead of re-computing them through fetching raw
features at every iteration. Critical to its success, the corresponding cache
policy is designed, using a combination of gradient-based and staleness
criteria, to selectively screen those embeddings which are relatively stable
and can be cached, from those that need to be re-computed to reduce estimation
errors and subsequent downstream accuracy loss. When paired with complementary
system enhancements to support this selective historical cache, ReFresh is able
to accelerate the training speed on large graph datasets such as
ogbn-papers100M and MAG240M by 4.6x up to 23.6x and reduce the memory access by
64.5% (85.7% higher than a raw feature cache), with less than 1% influence on
test accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Curvilinear object segmentation in medical images based on ODoS filter
  and deep learning network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanyuan Peng, Lin Pan, Pengpeng Luan, Hongbin Tu, Xiong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic segmentation of curvilinear objects in medical images plays an
important role in the diagnosis and evaluation of human diseases, yet it is a
challenging uncertainty for the complex segmentation task due to different
issues like various image appearance, low contrast between curvilinear objects
and their surrounding backgrounds, thin and uneven curvilinear structures, and
improper background illumination. To overcome these challenges, we present a
unique curvilinear structure segmentation framework based on oriented
derivative of stick (ODoS) filter and deep learning network for curvilinear
object segmentation in medical images. Currently, a large number of deep
learning models emphasis on developing deep architectures and ignore capturing
the structural features of curvature objects, which may lead to unsatisfactory
results. In consequence, a new approach that incorporates the ODoS filter as
part of a deep learning network is presented to improve the spatial attention
of curvilinear objects. In which, the original image is considered as principal
part to describe various image appearance and complex background illumination,
the multi-step strategy is used to enhance contrast between curvilinear objects
and their surrounding backgrounds, and the vector field is applied to
discriminate thin and uneven curvilinear structures. Subsequently, a deep
learning framework is employed to extract varvious structural features for
curvilinear object segmentation in medical images. The performance of the
computational model was validated in experiments with publicly available DRIVE,
STARE and CHASEDB1 datasets. Experimental results indicate that the presented
model has yielded surprising results compared with some state-of-the-art
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Threats, Vulnerabilities, and Controls of Machine Learning Based
  Systems: A <span class="highlight-title">Survey</span> and Taxonomy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Kawamoto, Kazumasa Miyake, Koich Konishi, Yutaka Oiwa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we propose the Artificial Intelligence Security Taxonomy to
systematize the knowledge of threats, vulnerabilities, and security controls of
ML-based systems. We first classify the damage caused by attacks against
ML-based systems, define ML-specific security, and discuss its characteristics.
Next, we enumerate all relevant assets and stakeholders and provide a general
taxonomy for ML-specific threats. Then, we collect a wide range of security
controls against ML-specific threats through an extensive review of recent
literature. Finally, we classify the vulnerabilities and controls of an
ML-based system in terms of each vulnerable asset in the system's entire
lifecycle.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discrete Latent Structure in Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vlad Niculae, Caio F. Corro, Nikita Nangia, Tsvetomila Mihaylova, André F. T. Martins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many types of data from fields including natural language processing,
computer vision, and bioinformatics, are well represented by discrete,
compositional structures such as trees, sequences, or matchings. Latent
structure models are a powerful tool for learning to extract such
representations, offering a way to incorporate structural bias, discover
insight about the data, and interpret decisions. However, effective training is
challenging, as neural networks are typically designed for continuous
computation.
  This text explores three broad strategies for learning with discrete latent
structure: continuous relaxation, surrogate gradients, and probabilistic
estimation. Our presentation relies on consistent notations for a wide range of
models. As such, we reveal many new connections between latent structure
learning strategies, showing how most consist of the same small set of
fundamental building blocks, but use them differently, leading to substantially
different applicability and properties.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLIPTER: Looking at the Bigger Picture in Scene Text Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aviad Aberdam, David Bensaïd, Alona Golts, Roy Ganz, Oren Nuriel, Royee Tichauer, Shai Mazor, Ron Litman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the scene is often essential for reading text in real-world
scenarios. However, current scene text recognizers operate on cropped text
images, unaware of the bigger picture. In this work, we harness the
representative power of recent vision-language models, such as CLIP, to provide
the crop-based recognizer with scene, image-level information. Specifically, we
obtain a rich representation of the entire image and fuse it with the
recognizer word-level features via cross-attention. Moreover, a gated mechanism
is introduced that gradually shifts to the context-enriched representation,
enabling simply fine-tuning a pretrained recognizer. We implement our
model-agnostic framework, named CLIPTER - CLIP Text Recognition, on several
leading text recognizers and demonstrate consistent performance gains,
achieving state-of-the-art results over multiple benchmarks. Furthermore, an
in-depth analysis reveals improved robustness to out-of-vocabulary words and
enhanced generalization in low-data regimes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DDPEN: Trajectory Optimisation With Sub Goal Generation Model <span class="chip">IROS2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksander Gamayunov, Aleksey Postnikov, Gonzalo Ferrer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differential dynamic programming (DDP) is a widely used and powerful
trajectory optimization technique, however, due to its internal structure, it
is not exempt from local minima. In this paper, we present Differential Dynamic
Programming with Escape Network (DDPEN) - a novel approach to avoid DDP local
minima by utilising an additional term used in the optimization criteria
pointing towards the direction where robot should move in order to escape local
minima. In order to produce the aforementioned directions, we propose to
utilize a deep model that takes as an input the map of the environment in the
form of a costmap together with the desired goal position. The Model produces
possible future directions that will lead to the goal, avoiding local minima
which is possible to run in real time conditions. The model is trained on a
synthetic dataset and overall the system is evaluated at the Gazebo simulator.
In this work we show that our proposed method allows avoiding local minima of
trajectory optimization algorithm and successfully execute a trajectory 278 m
long with various convex and nonconvex obstacles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 6 figures, IROS2022 Workshop: Artificial Intelligence for
  Social Robots Interacting with Humans in the Real World [intellect4hri]</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Autonomous Slalom Maneuver Based on Expert Drivers' Behavior Using
  Convolutional Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shafagh A. Pashaki, Ali Nahvi, Ahmad Ahmadi, Sajad Tavakoli, Shahin Naeemi, Salar H. Shamchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lane changing and obstacle avoidance are one of the most important tasks in
automated cars. To date, many algorithms have been suggested that are generally
based on path trajectory or reinforcement learning approaches. Although these
methods have been efficient, they are not able to accurately imitate a smooth
path traveled by an expert driver. In this paper, a method is presented to
mimic drivers' behavior using a convolutional neural network (CNN). First,
seven features are extracted from a dataset gathered from four expert drivers
in a driving simulator. Then, these features are converted from 1D arrays to 2D
arrays and injected into a CNN. The CNN model computes the desired steering
wheel angle and sends it to an adaptive PD controller. Finally, the control
unit applies proper torque to the steering wheel. Results show that the CNN
model can mimic the drivers' behavior with an R2-squared of 0.83. Also, the
performance of the presented method was evaluated in the driving simulator for
17 trials, which avoided all traffic cones successfully. In some trials, the
presented method performed a smoother maneuver compared to the expert drivers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DIRECT: Learning from Sparse and Shifting Rewards using Discriminative
  Reward Co-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Altmann, Thomy Phan, Fabian Ritz, Thomas Gabor, Claudia Linnhoff-Popien
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose discriminative reward co-training (DIRECT) as an extension to deep
reinforcement learning algorithms. Building upon the concept of self-imitation
learning (SIL), we introduce an imitation buffer to store beneficial
trajectories generated by the policy determined by their return. A
discriminator network is trained concurrently to the policy to distinguish
between trajectories generated by the current policy and beneficial
trajectories generated by previous policies. The discriminator's verdict is
used to construct a reward signal for optimizing the policy. By interpolating
prior experience, DIRECT is able to act as a surrogate, steering policy
optimization towards more valuable regions of the reward landscape thus
learning an optimal policy. Our results show that DIRECT outperforms
state-of-the-art algorithms in sparse- and shifting-reward environments being
able to provide a surrogate reward to the policy and direct the optimization
towards valuable areas.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 10 figures, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compression of GPS Trajectories using Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Kölle, Steffen Illium, Carsten Hahn, Lorenz Schauer, Johannes Hutter, Claudia Linnhoff-Popien
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ubiquitous availability of mobile devices capable of location tracking
led to a significant rise in the collection of GPS data. Several compression
methods have been developed in order to reduce the amount of storage needed
while keeping the important information. In this paper, we present an
lstm-autoencoder based approach in order to compress and reconstruct GPS
trajectories, which is evaluated on both a gaming and real-world dataset. We
consider various compression ratios and trajectory lengths. The performance is
compared to other trajectory compression algorithms, i.e., Douglas-Peucker.
Overall, the results indicate that our approach outperforms Douglas-Peucker
significantly in terms of the discrete Fr\'echet distance and dynamic time
warping. Furthermore, by reconstructing every point lossy, the proposed
methodology offers multiple advantages over traditional methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICAART 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Relativistic Digital Twin: Bringing the IoT to the Future 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07390v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07390v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Sciullo, Alberto De Marchi, Angelo Trotta, Federico Montori, Luciano Bononi, Marco Di Felice
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complex IoT ecosystems often require the usage of Digital Twins (DTs) of
their physical assets in order to perform predictive analytics and simulate
what-if scenarios. DTs are able to replicate IoT devices and adapt over time to
their behavioral changes. However, DTs in IoT are typically tailored to a
specific use case, without the possibility to seamlessly adapt to different
scenarios. Further, the fragmentation of IoT poses additional challenges on how
to deploy DTs in heterogeneous scenarios characterized by the usage of multiple
data formats and IoT network protocols. In this paper, we propose the
Relativistic Digital Twin (RDT) framework, through which we automatically
generate general purpose DTs of IoT entities and tune their behavioral models
over time by constantly observing their real counterparts. The framework relies
on the object representation via the Web of Things (WoT), to offer a
standardized interface to each of the IoT devices as well as to their DTs. To
this purpose, we extended the W3C WoT standard in order to encompass the
concept of behavioral model and define it in the Thing Description (TD) through
a new vocabulary. Finally, we evaluated the RDT framework over two disjoint use
cases to assess its correctness and learning performance, i.e. the DT of a
simulated smart home scenario with the capability of forecasting the indoor
temperature, and the DT of a real-world drone with the capability of
forecasting its trajectory in an outdoor scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 10 figures, 3 tables, 6 listings. This work has been
  submitted to the IEEE for possible publication. Copyright may be transferred
  without notice, after which this version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Models that Can See and Read 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roy Ganz, Oren Nuriel, Aviad Aberdam, Yair Kittenplon, Shai Mazor, Ron Litman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Question Answering (VQA) and Image Captioning (CAP), which are among
the most popular vision-language tasks, have analogous scene-text versions that
require reasoning from the text in the image. Despite the obvious resemblance
between them, the two are treated independently, yielding task-specific methods
that can either see or read, but not both. In this work, we conduct an in-depth
analysis of this phenomenon and propose UniTNT, a Unified Text-Non-Text
approach, which grants existing multimodal architectures scene-text
understanding capabilities. Specifically, we treat scene-text information as an
additional modality, fusing it with any pretrained encoder-decoder-based
architecture via designated modules. Thorough experiments reveal that UniTNT
leads to the first single model that successfully handles both task types.
Moreover, we show that scene-text understanding capabilities can boost
vision-language models' performance on VQA and CAP by up to 3.49% and 0.7
CIDEr, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Deformation Trajectories of Boltzmann Densities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bálint Máté, François Fleuret
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a training objective for continuous normalizing flows that can
be used in the absence of samples but in the presence of an energy function.
Our method relies on either a prescribed or a learnt interpolation $f_t$ of
energy functions between the target energy $f_1$ and the energy function of a
generalized Gaussian $f_0(x) = (|x|/\sigma)^p$. This then induces an
interpolation of Boltzmann densities $p_t \propto e^{-f_t}$ and we aim to find
a time-dependent vector field $V_t$ that transports samples along this family
of densities. Concretely, this condition can be translated to a PDE between
$V_t$ and $f_t$ and we minimize the amount by which this PDE fails to hold. We
compare this objective to the reverse KL-divergence on Gaussian mixtures and on
the $\phi^4$ lattice field theory on a circle.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dirichlet-Neumann learning algorithm for solving elliptic interface
  problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Sun, Xuejun Xu, Haotian Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-overlapping domain decomposition methods are natural for solving
interface problems arising from various disciplines, however, the numerical
simulation requires technical analysis and is often available only with the use
of high-quality grids, thereby impeding their use in more complicated
situations. To remove the burden of mesh generation and to effectively tackle
with the interface jump conditions, a novel mesh-free scheme, i.e.,
Dirichlet-Neumann learning algorithm, is proposed in this work to solve the
benchmark elliptic interface problem with high-contrast coefficients as well as
irregular interfaces. By resorting to the variational principle, we carry out a
rigorous error analysis to evaluate the discrepancy caused by the boundary
penalty treatment for each decomposed subproblem, which paves the way for
realizing the Dirichlet-Neumann algorithm using neural network extension
operators. The effectiveness and robustness of our proposed methods are
demonstrated experimentally through a series of elliptic interface problems,
achieving better performance over other alternatives especially in the presence
of erroneous flux prediction at interface.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptively Integrated Knowledge Distillation and Prediction Uncertainty
  for Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kanghao Chen, Sijia Liu, Ruixuan Wang, Wei-Shi Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current deep learning models often suffer from catastrophic forgetting of old
knowledge when continually learning new knowledge. Existing strategies to
alleviate this issue often fix the trade-off between keeping old knowledge
(stability) and learning new knowledge (plasticity). However, the
stability-plasticity trade-off during continual learning may need to be
dynamically changed for better model performance. In this paper, we propose two
novel ways to adaptively balance model stability and plasticity. The first one
is to adaptively integrate multiple levels of old knowledge and transfer it to
each block level in the new model. The second one uses prediction uncertainty
of old knowledge to naturally tune the importance of learning new knowledge
during model training. To our best knowledge, this is the first time to connect
model prediction uncertainty and knowledge distillation for continual learning.
In addition, this paper applies a modified CutMix particularly to augment the
data for old knowledge, further alleviating the catastrophic forgetting issue.
Extensive evaluations on the CIFAR100 and the ImageNet datasets confirmed the
effectiveness of the proposed method for continual learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improve Noise Tolerance of Robust Loss via Noise-Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kehui Ding, Jun Shu, Deyu Meng, Zongben Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust loss minimization is an important strategy for handling robust
learning issue on noisy labels. Current robust losses, however, inevitably
involve hyperparameters to be tuned for different datasets with noisy labels,
manually or heuristically through cross validation, which makes them fairly
hard to be generally applied in practice. Existing robust loss methods usually
assume that all training samples share common hyperparameters, which are
independent of instances. This limits the ability of these methods on
distinguishing individual noise properties of different samples, making them
hardly adapt to different noise structures. To address above issues, we propose
to assemble robust loss with instance-dependent hyperparameters to improve
their noise-tolerance with theoretical guarantee. To achieve setting such
instance-dependent hyperparameters for robust loss, we propose a meta-learning
method capable of adaptively learning a hyperparameter prediction function,
called Noise-Aware-Robust-Loss-Adjuster (NARL-Adjuster). Specifically, through
mutual amelioration between hyperparameter prediction function and classifier
parameters in our method, both of them can be simultaneously finely ameliorated
and coordinated to attain solutions with good generalization capability. Four
kinds of SOTA robust losses are attempted to be integrated with our algorithm,
and experiments substantiate the general availability and effectiveness of the
proposed method in both its noise tolerance and generalization performance.
Meanwhile, the explicit parameterized structure makes the meta-learned
prediction function capable of being readily transferrable and plug-and-play to
unseen datasets with noisy labels. Specifically, we transfer our meta-learned
NARL-Adjuster to unseen tasks, including several real noisy datasets, and
achieve better performance compared with conventional hyperparameter tuning
strategy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2002.06482</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PIRLNav: <span class="highlight-title">Pretrain</span>ing with Imitation and RL Finetuning for ObjectNav 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ram Ramrakhya, Dhruv Batra, Erik Wijmans, Abhishek Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study ObjectGoal Navigation - where a virtual robot situated in a new
environment is asked to navigate to an object. Prior work has shown that
imitation learning (IL) on a dataset of human demonstrations achieves promising
results. However, this has limitations $-$ 1) IL policies generalize poorly to
new states, since the training mimics actions not their consequences, and 2)
collecting demonstrations is expensive. On the other hand, reinforcement
learning (RL) is trivially scalable, but requires careful reward engineering to
achieve desirable behavior. We present a two-stage learning scheme for IL
pretraining on human demonstrations followed by RL-finetuning. This leads to a
PIRLNav policy that advances the state-of-the-art on ObjectNav from $60.0\%$
success rate to $65.0\%$ ($+5.0\%$ absolute). Using this IL$\rightarrow$RL
training recipe, we present a rigorous empirical analysis of design choices.
First, we investigate whether human demonstrations can be replaced with `free'
(automatically generated) sources of demonstrations, e.g. shortest paths (SP)
or task-agnostic frontier exploration (FE) trajectories. We find that
IL$\rightarrow$RL on human demonstrations outperforms IL$\rightarrow$RL on SP
and FE trajectories, even when controlled for the same IL-pretraining success
on TRAIN, and even on a subset of VAL episodes where IL-pretraining success
favors the SP or FE policies. Next, we study how RL-finetuning performance
scales with the size of the IL pretraining dataset. We find that as we increase
the size of the IL-pretraining dataset and get to high IL accuracies, the
improvements from RL-finetuning are smaller, and that $90\%$ of the performance
of our best IL$\rightarrow$RL policy can be achieved with less than half the
number of IL demonstrations. Finally, we analyze failure modes of our ObjectNav
policies, and present guidelines for further improving them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages + supplement</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PTA-Det: Point <span class="highlight-title">Transformer</span> Associating Point cloud and Image for 3D
  Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Wan, Tianyun Zhao, Wei Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In autonomous driving, 3D object detection based on multi-modal data has
become an indispensable approach when facing complex environments around the
vehicle. During multi-modal detection, LiDAR and camera are simultaneously
applied for capturing and modeling. However, due to the intrinsic discrepancies
between the LiDAR point and camera image, the fusion of the data for object
detection encounters a series of problems. Most multi-modal detection methods
perform even worse than LiDAR-only methods. In this investigation, we propose a
method named PTA-Det to improve the performance of multi-modal detection.
Accompanied by PTA-Det, a Pseudo Point Cloud Generation Network is proposed,
which can convert image information including texture and semantic features by
pseudo points. Thereafter, through a transformer-based Point Fusion Transition
(PFT) module, the features of LiDAR points and pseudo points from image can be
deeply fused under a unified point-based representation. The combination of
these modules can conquer the major obstacle in feature fusion across
modalities and realizes a complementary and discriminative representation for
proposal generation. Extensive experiments on the KITTI dataset show the
PTA-Det achieves a competitive result and support its effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adapting Multilingual Speech Representation Model for a New,
  Underresourced Language through Multilingual Fine-tuning and Continued
  <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karol Nowakowski, Michal Ptaszynski, Kyoko Murasaki, Jagna Nieuważny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, neural models learned through self-supervised pretraining on
large scale multilingual text or speech data have exhibited promising results
for underresourced languages, especially when a relatively large amount of data
from related language(s) is available. While the technology has a potential for
facilitating tasks carried out in language documentation projects, such as
speech transcription, pretraining a multilingual model from scratch for every
new language would be highly impractical. We investigate the possibility for
adapting an existing multilingual wav2vec 2.0 model for a new language,
focusing on actual fieldwork data from a critically endangered tongue: Ainu.
Specifically, we (i) examine the feasibility of leveraging data from similar
languages also in fine-tuning; (ii) verify whether the model's performance can
be improved by further pretraining on target language data. Our results show
that continued pretraining is the most effective method to adapt a wav2vec 2.0
model for a new language and leads to considerable reduction in error rates.
Furthermore, we find that if a model pretrained on a related speech variety or
an unrelated language with similar phonological characteristics is available,
multilingual fine-tuning using additional data from that language can have
positive impact on speech recognition performance when there is very little
labeled data in the target language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Self-Training Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aswathnarayan Radhakrishnan, Jim Davis, Zachary Rabin, Benjamin Lewis, Matthew Scherreik, Roman Ilin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning approaches train on small sets of labeled data along
with large sets of unlabeled data. Self-training is a semi-supervised
teacher-student approach that often suffers from the problem of "confirmation
bias" that occurs when the student model repeatedly overfits to incorrect
pseudo-labels given by the teacher model for the unlabeled data. This bias
impedes improvements in pseudo-label accuracy across self-training iterations,
leading to unwanted saturation in model performance after just a few
iterations. In this work, we describe multiple enhancements to improve the
self-training pipeline to mitigate the effect of confirmation bias. We evaluate
our enhancements over multiple datasets showing performance gains over existing
self-training design choices. Finally, we also study the extendability of our
enhanced approach to Open Set unlabeled data (containing classes not seen in
labeled data).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reslicing Ultrasound Images for Data Augmentation and Vessel
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cecilia Morales, Jason Yao, Tejas Rane, Robert Edman, Howie Choset, Artur Dubrawski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot-guided catheter insertion has the potential to deliver urgent medical
care in situations where medical personnel are unavailable. However, this
technique requires accurate and reliable segmentation of anatomical landmarks
in the body. For the ultrasound imaging modality, obtaining large amounts of
training data for a segmentation model is time-consuming and expensive. This
paper introduces RESUS (RESlicing of UltraSound Images), a weak supervision
data augmentation technique for ultrasound images based on slicing
reconstructed 3D volumes from tracked 2D images. This technique allows us to
generate views which cannot be easily obtained in vivo due to physical
constraints of ultrasound imaging, and use these augmented ultrasound images to
train a semantic segmentation model. We demonstrate that RESUS achieves
statistically significant improvement over training with non-augmented images
and highlight qualitative improvements through vessel reconstruction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel, Scale-Invariant, Differentiable, Efficient, Scalable
  Regularizer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hovig Tigran Bayandorian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  $L_{p}$-norm regularization schemes such as $L_{0}$, $L_{1}$, and
$L_{2}$-norm regularization and $L_{p}$-norm-based regularization techniques
such as weight decay and group LASSO compute a quantity which de pends on model
weights considered in isolation from one another. This paper describes a novel
regularizer which is not based on an $L_{p}$-norm. In contrast with
$L_{p}$-norm-based regularization, this regularizer is concerned with the
spatial arrangement of weights within a weight matrix. This regularizer is an
additive term for the loss function and is differentiable, simple and fast to
compute, scale-invariant, requires a trivial amount of additional memory, and
can easily be parallelized. Empirically this method yields approximately a one
order-of-magnitude improvement in the number of nonzero model parameters at a
given level of accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Label Inference Attack against Split Learning under Regression Setting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangyu Xie, Xin Yang, Yuanshun Yao, Tianyi Liu, Taiqing Wang, Jiankai Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a crucial building block in vertical Federated Learning (vFL), Split
Learning (SL) has demonstrated its practice in the two-party model training
collaboration, where one party holds the features of data samples and another
party holds the corresponding labels. Such method is claimed to be private
considering the shared information is only the embedding vectors and gradients
instead of private raw data and labels. However, some recent works have shown
that the private labels could be leaked by the gradients. These existing attack
only works under the classification setting where the private labels are
discrete. In this work, we step further to study the leakage in the scenario of
the regression model, where the private labels are continuous numbers (instead
of discrete labels in classification). This makes previous attacks harder to
infer the continuous labels due to the unbounded output range. To address the
limitation, we propose a novel learning-based attack that integrates gradient
information and extra learning regularization objectives in aspects of model
training properties, which can infer the labels under regression settings
effectively. The comprehensive experiments on various datasets and models have
demonstrated the effectiveness of our proposed attack. We hope our work can
pave the way for future analyses that make the vFL framework more secure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting and Ranking Causal Anomalies in End-to-End Complex System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ching Chang, Cheng-Han Yeh, Wen-Chih Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of technology, the automated monitoring systems of
large-scale factories are becoming more and more important. By collecting a
large amount of machine sensor data, we can have many ways to find anomalies.
We believe that the real core value of an automated monitoring system is to
identify and track the cause of the problem. The most famous method for finding
causal anomalies is RCA, but there are many problems that cannot be ignored.
They used the AutoRegressive eXogenous (ARX) model to create a time-invariant
correlation network as a machine profile, and then use this profile to track
the causal anomalies by means of a method called fault propagation. There are
two major problems in describing the behavior of a machine by using the
correlation network established by ARX: (1) It does not take into account the
diversity of states (2) It does not separately consider the correlations with
different time-lag. Based on these problems, we propose a framework called
Ranking Causal Anomalies in End-to-End System (RCAE2E), which completely solves
the problems mentioned above. In the experimental part, we use synthetic data
and real-world large-scale photoelectric factory data to verify the correctness
and existence of our method hypothesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A variational autoencoder-based nonnegative matrix factorisation model
  for deep dictionary learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong-Bo Xie, Caoyuan Li, Shuliang Wang, Richard Yi Da Xu, Kerrie Mengersen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Construction of dictionaries using nonnegative matrix factorisation (NMF) has
extensive applications in signal processing and machine learning. With the
advances in deep learning, training compact and robust dictionaries using deep
neural networks, i.e., dictionaries of deep features, has been proposed. In
this study, we propose a probabilistic generative model which employs a
variational autoencoder (VAE) to perform nonnegative dictionary learning. In
contrast to the existing VAE models, we cast the model under a statistical
framework with latent variables obeying a Gamma distribution and design a new
loss function to guarantee the nonnegative dictionaries. We adopt an
acceptance-rejection sampling reparameterization trick to update the latent
variables iteratively. We apply the dictionaries learned from VAE-NMF to two
signal processing tasks, i.e., enhancement of speech and extraction of muscle
synergies. Experimental results demonstrate that VAE-NMF performs better in
learning the latent nonnegative dictionaries in comparison with
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tailor: Altering Skip Connections for Resource-Efficient Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivia Weng, Gabriel Marcano, Vladimir Loncar, Alireza Khodamoradi, Nojan Sheybani, Farinaz Koushanfar, Kristof Denolf, Javier Mauricio Duarte, Ryan Kastner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks use skip connections to improve training convergence.
However, these skip connections are costly in hardware, requiring extra buffers
and increasing on- and off-chip memory utilization and bandwidth requirements.
In this paper, we show that skip connections can be optimized for hardware when
tackled with a hardware-software codesign approach. We argue that while a
network's skip connections are needed for the network to learn, they can later
be removed or shortened to provide a more hardware efficient implementation
with minimal to no accuracy loss. We introduce Tailor, a codesign tool whose
hardware-aware training algorithm gradually removes or shortens a fully trained
network's skip connections to lower their hardware cost. The optimized hardware
designs improve resource utilization by up to 34% for BRAMs, 13% for FFs, and
16% for LUTs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient correlation-based discretization of continuous variables for
  annealing machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuki Furue, Makiko Konoshima, Hirotaka Tamura, Jun Ohkubo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Annealing machines specialized for combinatorial optimization problems have
been developed, and some companies offer services to use those machines. Such
specialized machines can only handle binary variables, and their input format
is the quadratic unconstrained binary optimization (QUBO) formulation.
Therefore, discretization is necessary to solve problems with continuous
variables. However, there is a severe constraint on the number of binary
variables with such machines. Although the simple binary expansion in the
previous research requires many binary variables, we need to reduce the number
of such variables in the QUBO formulation due to the constraint. We propose a
discretization method that involves using correlations of continuous variables.
We numerically show that the proposed method reduces the number of necessary
binary variables in the QUBO formulation without a significant loss in
prediction accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Complexity Analysis of a Countable-armed Bandit Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anand Kalvit, Assaf Zeevi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a stochastic multi-armed bandit (MAB) problem motivated by
``large'' action spaces, and endowed with a population of arms containing
exactly $K$ arm-types, each characterized by a distinct mean reward. The
decision maker is oblivious to the statistical properties of reward
distributions as well as the population-level distribution of different
arm-types, and is precluded also from observing the type of an arm after play.
We study the classical problem of minimizing the expected cumulative regret
over a horizon of play $n$, and propose algorithms that achieve a rate-optimal
finite-time instance-dependent regret of $\mathcal{O}\left( \log n \right)$. We
also show that the instance-independent (minimax) regret is
$\tilde{\mathcal{O}}\left( \sqrt{n} \right)$ when $K=2$. While the order of
regret and complexity of the problem suggests a great degree of similarity to
the classical MAB problem, properties of the performance bounds and salient
aspects of algorithm design are quite distinct from the latter, as are the key
primitives that determine complexity along with the analysis tools needed to
study them.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Augmenting a Physics-Informed Neural Network for the 2D Burgers Equation
  by Addition of Solution Data Points 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marlon Sproesser Mathias, Wesley Pereira de Almeida, Marcel Rodrigues de Barros, Jefferson Fialho Coelho, Lucas Palmiro de Freitas, Felipe Marino Moreno, Caio Fabricio Deberaldini Netto, Fabio Gagliardi Cozman, Anna Helena Reali Costa, Eduardo Aoun Tannuri, Edson Satoshi Gomi, Marcelo Dottori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We implement a Physics-Informed Neural Network (PINN) for solving the
two-dimensional Burgers equations. This type of model can be trained with no
previous knowledge of the solution; instead, it relies on evaluating the
governing equations of the system in points of the physical domain. It is also
possible to use points with a known solution during training. In this paper, we
compare PINNs trained with different amounts of governing equation evaluation
points and known solution points. Comparing models that were trained purely
with known solution points to those that have also used the governing
equations, we observe an improvement in the overall observance of the
underlying physics in the latter. We also investigate how changing the number
of each type of point affects the resulting models differently. Finally, we
argue that the addition of the governing equations during training may provide
a way to improve the overall performance of the model without relying on
additional data, which is especially important for situations where the number
of known solution points is limited.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint has not undergone peer review or any post-submission
  improvements or corrections. The Version of Record of this contribution is
  published in the Lecture Notes in Computer Science book series (LNAI,volume
  13654), and is available online at
  https://doi.org/10.1007/978-3-031-21689-3_28</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emergence of the SVD as an interpretable factorization in deep learning
  for inverse problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashank Sule, Richard G. Spencer, Wojciech Czaja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate the emergence of weight matrix singular value decomposition
(SVD) in interpreting neural networks (NNs) for parameter estimation from noisy
signals. The SVD appears naturally as a consequence of initial application of a
descrambling transform - a recently-developed technique for addressing
interpretability in NNs \cite{amey2021neural}. We find that within the class of
noisy parameter estimation problems, the SVD may be the means by which networks
memorize the signal model. We substantiate our theoretical findings with
empirical evidence from both linear and non-linear settings. Our results also
illuminate the connections between a mathematical theory of semantic
development \cite{saxe2019mathematical} and neural network interpretability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Automatic Differentiation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keith Rush, Zachary Charles, Zachary Garrett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is a general framework for learning across
heterogeneous clients while preserving data privacy, under the orchestration of
a central server. FL methods often compute gradients of loss functions purely
locally (ie. entirely at each client, or entirely at the server), typically
using automatic differentiation (AD) techniques. We propose a federated
automatic differentiation (FAD) framework that 1) enables computing derivatives
of functions involving client and server computation as well as communication
between them and 2) operates in a manner compatible with existing federated
technology. In other words, FAD computes derivatives across communication
boundaries. We show, in analogy with traditional AD, that FAD may be
implemented using various accumulation modes, which introduce distinct
computation-communication trade-offs and systems requirements. Further, we show
that a broad class of federated computations is closed under these various
modes of FAD, implying in particular that if the original computation can be
implemented using privacy-preserving primitives, its derivative may be computed
using only these same primitives. We then show how FAD can be used to create
algorithms that dynamically learn components of the algorithm itself. In
particular, we show that FedAvg-style algorithms can exhibit significantly
improved performance by using FAD to adjust the server optimization step
automatically, or by using FAD to learn weighting schemes for computing
weighted averages across clients.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Domain-Agnostic Approach for Characterization of Lifelong Learning
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Megan M. Baker, Alexander New, Mario Aguilar-Simon, Ziad Al-Halah, Sébastien M. R. Arnold, Ese Ben-Iwhiwhu, Andrew P. Brna, Ethan Brooks, Ryan C. Brown, Zachary Daniels, Anurag Daram, Fabien Delattre, Ryan Dellana, Eric Eaton, Haotian Fu, Kristen Grauman, Jesse Hostetler, Shariq Iqbal, Cassandra Kent, Nicholas Ketz, Soheil Kolouri, George Konidaris, Dhireesha Kudithipudi, Erik Learned-Miller, Seungwon Lee, Michael L. Littman, Sandeep Madireddy, Jorge A. Mendez, Eric Q. Nguyen, Christine D. Piatko, Praveen K. Pilly, Aswin Raghavan, Abrar Rahman, Santhosh Kumar Ramakrishnan, Neale Ratzlaff, Andrea Soltoggio, Peter Stone, Indranil Sur, Zhipeng Tang, Saket Tiwari, Kyle Vedder, Felix Wang, Zifan Xu, Angel Yanguas-Gil, Harel Yedidsion, Shangqun Yu, Gautam K. Vallabha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the advancement of machine learning techniques in recent years,
state-of-the-art systems lack robustness to "real world" events, where the
input distributions and tasks encountered by the deployed systems will not be
limited to the original training context, and systems will instead need to
adapt to novel distributions and tasks while deployed. This critical gap may be
addressed through the development of "Lifelong Learning" systems that are
capable of 1) Continuous Learning, 2) Transfer and Adaptation, and 3)
Scalability. Unfortunately, efforts to improve these capabilities are typically
treated as distinct areas of research that are assessed independently, without
regard to the impact of each separate capability on other aspects of the
system. We instead propose a holistic approach, using a suite of metrics and an
evaluation framework to assess Lifelong Learning in a principled way that is
agnostic to specific domains or system techniques. Through five case studies,
we show that this suite of metrics can inform the development of varied and
complex Lifelong Learning systems. We highlight how the proposed suite of
metrics quantifies performance trade-offs present during Lifelong Learning
system development - both the widely discussed Stability-Plasticity dilemma and
the newly proposed relationship between Sample Efficient and Robust Learning.
Further, we make recommendations for the formulation and use of metrics to
guide the continuing development of Lifelong Learning systems and assess their
progress in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Neural Networks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HCE: Improving Performance and Efficiency with Heterogeneously
  Compressed Neural Network Ensemble 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingchi Zhang, Huanrui Yang, Hai Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensemble learning has gain attention in resent deep learning research as a
way to further boost the accuracy and generalizability of deep neural network
(DNN) models. Recent ensemble training method explores different training
algorithms or settings on multiple sub-models with the same model architecture,
which lead to significant burden on memory and computation cost of the ensemble
model. Meanwhile, the heurtsically induced diversity may not lead to
significant performance gain. We propose a new prespective on exploring the
intrinsic diversity within a model architecture to build efficient DNN
ensemble. We make an intriguing observation that pruning and quantization,
while both leading to efficient model architecture at the cost of small
accuracy drop, leads to distinct behavior in the decision boundary. To this
end, we propose Heterogeneously Compressed Ensemble (HCE), where we build an
efficient ensemble with the pruned and quantized variants from a pretrained DNN
model. An diversity-aware training objective is proposed to further boost the
performance of the HCE ensemble. Experiemnt result shows that HCE achieves
significant improvement in the efficiency-accuracy tradeoff comparing to both
traditional DNN ensemble training methods and previous model compression
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sample-Efficient Multi-Objective Learning via Generalized Policy
  Improvement Prioritization <span class="chip">AAMAS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas N. Alegre, Ana L. C. Bazzan, Diederik M. Roijers, Ann Nowé, Bruno C. da Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-objective reinforcement learning (MORL) algorithms tackle sequential
decision problems where agents may have different preferences over (possibly
conflicting) reward functions. Such algorithms often learn a set of policies
(each optimized for a particular agent preference) that can later be used to
solve problems with novel preferences. We introduce a novel algorithm that uses
Generalized Policy Improvement (GPI) to define principled, formally-derived
prioritization schemes that improve sample-efficient learning. They implement
active-learning strategies by which the agent can (i) identify the most
promising preferences/objectives to train on at each moment, to more rapidly
solve a given MORL problem; and (ii) identify which previous experiences are
most relevant when learning a policy for a particular agent preference, via a
novel Dyna-style MORL method. We prove our algorithm is guaranteed to always
converge to an optimal solution in a finite number of steps, or an
$\epsilon$-optimal solution (for a bounded $\epsilon$) if the agent is limited
and can only identify possibly sub-optimal policies. We also prove that our
method monotonically improves the quality of its partial solutions while
learning. Finally, we introduce a bound that characterizes the maximum utility
loss (with respect to the optimal solution) incurred by the partial solutions
computed by our method throughout learning. We empirically show that our method
outperforms state-of-the-art MORL algorithms in challenging multi-objective
tasks, both with discrete and continuous state spaces.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAMAS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated deep reinforcement learning for real-time scheduling strategy
  of multi-energy system integrated with post-carbon and direct-air carbon
  captured system 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobi Michael Alabi, Nathan P. Lawrence, Lin Lu, Zaiyue Yang, R. Bhushan Gopaluni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The carbon-capturing process with the aid of CO2 removal technology (CDRT)
has been recognised as an alternative and a prominent approach to deep
decarbonisation. However, the main hindrance is the enormous energy demand and
the economic implication of CDRT if not effectively managed. Hence, a novel
deep reinforcement learning agent (DRL), integrated with an automated
hyperparameter selection feature, is proposed in this study for the real-time
scheduling of a multi-energy system coupled with CDRT. Post-carbon capture
systems (PCCS) and direct-air capture systems (DACS) are considered CDRT.
Various possible configurations are evaluated using real-time multi-energy data
of a district in Arizona and CDRT parameters from manufacturers' catalogues and
pilot project documentation. The simulation results validate that an optimised
soft-actor critic (SAC) algorithm outperformed the TD3 algorithm due to its
maximum entropy feature. We then trained four (4) SAC agents, equivalent to the
number of considered case studies, using optimised hyperparameter values and
deployed them in real time for evaluation. The results show that the proposed
DRL agent can meet the prosumers' multi-energy demand and schedule the CDRT
energy demand economically without specified constraints violation. Also, the
proposed DRL agent outperformed rule-based scheduling by 23.65%. However, the
configuration with PCCS and solid-sorbent DACS is considered the most suitable
configuration with a high CO2 captured-released ratio of 38.54, low CO2
released indicator value of 2.53, and a 36.5% reduction in CDR cost due to
waste heat utilisation and high absorption capacity of the selected sorbent.
However, the adoption of CDRT is not economically viable at the current carbon
price. Finally, we showed that CDRT would be attractive at a carbon price of
400-450USD/ton with the provision of tax incentives by the policymakers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages; postprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using CycleGANs to Generate Realistic STEM Images for Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abid Khan, Chia-Hao Lee, Pinshane Y. Huang, Bryan K. Clark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of automation and machine learning (ML) in electron microscopy has
the potential to revolutionize materials research by enabling the autonomous
collection and processing of vast amounts of atomic resolution data. However, a
major challenge is developing ML models that can reliably and rapidly
generalize to large data sets with varying experimental conditions. To overcome
this challenge, we develop a cycle generative adversarial network (CycleGAN)
that introduces a novel reciprocal space discriminator to augment simulated
data with realistic, complex spatial frequency information learned from
experimental data. This enables the CycleGAN to generate nearly
indistinguishable images from real experimental data, while also providing
labels for further ML applications. We demonstrate the effectiveness of this
approach by training a fully convolutional network (FCN) to identify single
atom defects in a large data set of 4.5 million atoms, which we collected using
automated acquisition in an aberration-corrected scanning transmission electron
microscope (STEM). Our approach yields highly adaptable FCNs that can adjust to
dynamically changing experimental variables, such as lens aberrations, noise,
and local contamination, with minimal manual intervention. This represents a
significant step towards building fully autonomous approaches for harnessing
microscopy big data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 6 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Catapult Dynamics and Phase Transitions in Quadratic Nets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Meltzer, Junyu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks trained with gradient descent can undergo non-trivial phase
transitions as a function of the learning rate. In (Lewkowycz et al., 2020) it
was discovered that wide neural nets can exhibit a catapult phase for
super-critical learning rates, where the training loss grows exponentially
quickly at early times before rapidly decreasing to a small value. During this
phase the top eigenvalue of the neural tangent kernel (NTK) also undergoes
significant evolution. In this work, we will prove that the catapult phase
exists in a large class of models, including quadratic models and two-layer,
homogenous neural nets. To do this, we show that for a certain range of
learning rates the weight norm decreases whenever the loss becomes large. We
also empirically study learning rates beyond this theoretically derived range
and show that the activation map of ReLU nets trained with super-critical
learning rates becomes increasingly sparse as we increase the learning rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages, 16 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning-Rate-Free Learning by D-Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron Defazio, Konstantin Mishchenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The speed of gradient descent for convex Lipschitz functions is highly
dependent on the choice of learning rate. Setting the learning rate to achieve
the optimal convergence rate requires knowing the distance D from the initial
point to the solution set. In this work, we describe a single-loop method, with
no back-tracking or line searches, which does not require knowledge of $D$ yet
asymptotically achieves the optimal rate of convergence for the complexity
class of convex Lipschitz functions. Our approach is the first parameter-free
method for this class without additional multiplicative log factors in the
convergence rate. We present extensive experiments for SGD and Adam variants of
our method, where the method automatically matches hand-tuned learning rates
across more than a dozen diverse machine learning problems, including
large-scale vision and language problems. Our method is practical, efficient
and requires no additional function value or gradient evaluations each step. An
open-source implementation is available
(https://github.com/facebookresearch/dadaptation).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning image representations for anomaly detection: application to
  discovery of histological alterations in drug development 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07675v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07675v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Igor Zingman, Birgit Stierstorfer, Charlotte Lempp, Fabian Heinemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a system for anomaly detection in histopathological images. In
histology, normal samples are usually abundant, whereas anomalous
(pathological) cases are scarce or not available. Under such settings,
one-class classifiers trained on healthy data can detect out-of-distribution
anomalous samples. Such approaches combined with pre-trained Convolutional
Neural Network (CNN) representations of images were previously employed for
anomaly detection (AD). However, pre-trained off-the-shelf CNN representations
may not be sensitive to abnormal conditions in tissues, while natural
variations of healthy tissue may result in distant representations. To adapt
representations to relevant details in healthy tissue we propose training a CNN
on an auxiliary task that discriminates healthy tissue of different species,
organs, and staining reagents. Almost no additional labeling workload is
required, since healthy samples come automatically with aforementioned labels.
During training we enforce compact image representations with a center-loss
term, which further improves representations for AD. The proposed system
outperforms established AD methods on a published dataset of liver anomalies.
Moreover, it provided comparable results to conventional methods specifically
tailored for quantification of liver anomalies. We show that our approach can
be used for toxicity assessment of candidate drugs at early development stages
and thereby may reduce expensive late-stage drug attrition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reliable amortized variational inference with physics-based latent
  distribution correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.11640v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.11640v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Siahkoohi, Gabrio Rizzuti, Rafael Orozco, Felix J. Herrmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian inference for high-dimensional inverse problems is computationally
costly and requires selecting a suitable prior distribution. Amortized
variational inference addresses these challenges via a neural network that
approximates the posterior distribution not only for one instance of data, but
a distribution of data pertaining to a specific inverse problem. During
inference, the neural network -- in our case a conditional normalizing flow --
provides posterior samples at virtually no cost. However, the accuracy of
amortized variational inference relies on the availability of high-fidelity
training data, which seldom exists in geophysical inverse problems due to the
Earth's heterogeneity. In addition, the network is prone to errors if evaluated
over out-of-distribution data. As such, we propose to increase the resilience
of amortized variational inference in the presence of moderate data
distribution shifts. We achieve this via a correction to the latent
distribution that improves the posterior distribution approximation for the
data at hand. The correction involves relaxing the standard Gaussian assumption
on the latent distribution and parameterizing it via a Gaussian distribution
with an unknown mean and (diagonal) covariance. These unknowns are then
estimated by minimizing the Kullback-Leibler divergence between the corrected
and the (physics-based) true posterior distributions. While generic and
applicable to other inverse problems, by means of a linearized seismic imaging
example, we show that our correction step improves the robustness of amortized
variational inference with respect to changes in the number of seismic sources,
noise variance, and shifts in the prior distribution. This approach provides a
seismic image with limited artifacts and an assessment of its uncertainty at
approximately the same cost as five reverse-time migrations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural DAEs: Constrained neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.14302v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.14302v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tue Boesen, Eldad Haber, Uri M. Ascher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article we investigate the effect of explicitly adding auxiliary
trajectory information to neural networks for dynamical systems. We draw
inspiration from the field of differential-algebraic equations and differential
equations on manifolds and implement similar methods in residual neural
networks. We discuss constraints through stabilization as well as projection
methods, and show when to use which method based on experiments involving
simulations of multi-body pendulums and molecular dynamics scenarios. Several
of our methods are easy to implement in existing code and have limited impact
on training performance while giving significant boosts in terms of inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated the experiments and preparing for submission to JCP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Balancing Weights via Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.07533v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.07533v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoshiaki Kitazawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present generalized balancing weights, Neural Balancing Weights (NBW), to
estimate the causal effects for an arbitrary mixture of discrete and continuous
interventions. The weights were obtained by directly estimating the density
ratio between the source and balanced distributions by optimizing the
variational representation of $f$-divergence. For this, we selected
$\alpha$-divergence since it has good properties for optimization: It has a
$\sqrt{N}$-consistency estimator and unbiased mini-batch gradients and is
advantageous for the vanishing gradient problem. In addition, we provide a
method for checking the balance of the distribution changed by the weights. If
the balancing is imperfect, the weights can be improved by adding new balancing
weights. Our method can be conveniently implemented with any present
deep-learning libraries, and weights can be used in most state-of-the-art
supervised algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The main theorem and algorithms are partially changed, because errors
  were found in the proof of the theorem</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What relations are reliably embeddable in Euclidean space? <span class="chip">ALT 2020</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1903.05347v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1903.05347v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robi Bhattacharjee, Sanjoy Dasgupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of embedding a relation, represented as a directed
graph, into Euclidean space. For three types of embeddings motivated by the
recent literature on knowledge graphs, we obtain characterizations of which
relations they are able to capture, as well as bounds on the minimal
dimensionality and precision needed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ALT 2020</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sample Complexity of Adversarially Robust Linear Classification on
  Separated Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.10794v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.10794v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robi Bhattacharjee, Somesh Jha, Kamalika Chaudhuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the sample complexity of learning with adversarial robustness.
Most prior theoretical results for this problem have considered a setting where
different classes in the data are close together or overlapping. Motivated by
some real applications, we consider, in contrast, the well-separated case where
there exists a classifier with perfect accuracy and robustness, and show that
the sample complexity narrates an entirely different story. Specifically, for
linear classifiers, we show a large class of well-separated distributions where
the expected robust loss of any algorithm is at least $\Omega(\frac{d}{n})$,
whereas the max margin algorithm has expected standard loss $O(\frac{1}{n})$.
This shows a gap in the standard and robust losses that cannot be obtained via
prior techniques. Additionally, we present an algorithm that, given an instance
where the robustness radius is much smaller than the gap between the classes,
gives a solution with expected robust loss is $O(\frac{1}{n})$. This shows that
for very well-separated data, convergence rates of $O(\frac{1}{n})$ are
achievable, which is not the case otherwise. Our results apply to robustness
measured in any $\ell_p$ norm with $p > 1$ (including $p = \infty$).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybrid quantum-classical convolutional neural networks to improve
  molecular protein binding affinity predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        L. Domingo, M. Djukic, C. Johnson, F. Borondo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the main challenges in drug discovery is to find molecules that bind
specifically and strongly to their target protein while having minimal binding
to other proteins. By predicting binding affinity, it is possible to identify
the most promising candidates from a large pool of potential compounds,
reducing the number of compounds that need to be tested experimentally.
Recently, deep learning methods have shown superior performance than
traditional computational methods for making accurate predictions on large
datasets. However, the complexity and time-consuming nature of these methods
have limited their usage and development. Quantum machine learning is an
emerging technology that has the potential to improve many classical machine
learning algorithms. In this work we present a hybrid quantum-classical
convolutional neural network, which is able to reduce by 20% the complexity of
the classical network while maintaining optimal performance in the predictions.
Additionally, it results in a significant time savings of up to 40% in the
training process, which means a meaningful speed up of the drug discovery
process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Consistent Non-Parametric Methods for Maximizing Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.09086v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.09086v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robi Bhattacharjee, Kamalika Chaudhuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning classifiers that are robust to adversarial examples has received a
great deal of recent attention. A major drawback of the standard robust
learning framework is there is an artificial robustness radius $r$ that applies
to all inputs. This ignores the fact that data may be highly heterogeneous, in
which case it is plausible that robustness regions should be larger in some
regions of data, and smaller in others. In this paper, we address this
limitation by proposing a new limit classifier, called the neighborhood optimal
classifier, that extends the Bayes optimal classifier outside its support by
using the label of the closest in-support point. We then argue that this
classifier maximizes the size of its robustness regions subject to the
constraint of having accuracy equal to the Bayes optimal. We then present
sufficient conditions under which general non-parametric methods that can be
represented as weight functions converge towards this limit, and show that both
nearest neighbors and kernel classifiers satisfy them under certain conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to Nuerips 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No-substitution k-means Clustering with Adversarial Order <span class="chip">ALT 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.14512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.14512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robi Bhattacharjee, Michal Moshkovitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate $k$-means clustering in the online no-substitution setting
when the input arrives in \emph{arbitrary} order. In this setting, points
arrive one after another, and the algorithm is required to instantly decide
whether to take the current point as a center before observing the next point.
Decisions are irrevocable. The goal is to minimize both the number of centers
and the $k$-means cost. Previous works in this setting assume that the input's
order is random, or that the input's aspect ratio is bounded. It is known that
if the order is arbitrary and there is no assumption on the input, then any
algorithm must take all points as centers. Moreover, assuming a bounded aspect
ratio is too restrictive -- it does not include natural input generated from
mixture models.
  We introduce a new complexity measure that quantifies the difficulty of
clustering a dataset arriving in arbitrary order. We design a new random
algorithm and prove that if applied on data with complexity $d$, the algorithm
takes $O(d\log(n) k\log(k))$ centers and is an $O(k^3)$-approximation. We also
prove that if the data is sampled from a ``natural" distribution, such as a
mixture of $k$ Gaussians, then the new complexity measure is equal to
$O(k^2\log(n))$. This implies that for data generated from those distributions,
our new algorithm takes only $\text{poly}(k\log(n))$ centers and is a
$\text{poly}(k)$-approximation. In terms of negative results, we prove that the
number of centers needed to achieve an $\alpha$-approximation is at least
$\Omega\left(\frac{d}{k\log(n\alpha)}\right)$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to ALT 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InstructPix2Pix: Learning to Follow Image Editing Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09800v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09800v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Brooks, Aleksander Holynski, Alexei A. Efros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method for editing images from human instructions: given an
input image and a written instruction that tells the model what to do, our
model follows these instructions to edit the image. To obtain training data for
this problem, we combine the knowledge of two large pretrained models -- a
language model (GPT-3) and a text-to-image model (Stable Diffusion) -- to
generate a large dataset of image editing examples. Our conditional diffusion
model, InstructPix2Pix, is trained on our generated data, and generalizes to
real images and user-written instructions at inference time. Since it performs
edits in the forward pass and does not require per example fine-tuning or
inversion, our model edits images quickly, in a matter of seconds. We show
compelling editing results for a diverse collection of input images and written
instructions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page with code:
  https://www.timothybrooks.com/instruct-pix2pix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparse<span class="highlight-title">GPT</span>: Massive Language Models Can Be Accurately Pruned in One-Shot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00774v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00774v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elias Frantar, Dan Alistarh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show for the first time that large-scale generative pretrained transformer
(GPT) family models can be pruned to at least 50% sparsity in one-shot, without
any retraining, at minimal loss of accuracy. This is achieved via a new pruning
method called SparseGPT, specifically designed to work efficiently and
accurately on massive GPT-family models. When executing SparseGPT on the
largest available open-source models, OPT-175B and BLOOM-176B, we can reach 60%
sparsity with negligible increase in perplexity: remarkably, more than 100
billion weights from these models can be ignored at inference time. SparseGPT
generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with
weight quantization approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Global Contrastive Batch Sampling via Optimization on Sample
  Permutations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.12874v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.12874v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vin Sachidananda, Ziyi Yang, Chenguang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Learning has recently achieved state-of-the-art performance in a
wide range of tasks. Many contrastive learning approaches use mined hard
negatives to make batches more informative during training but these approaches
are inefficient as they increase epoch length proportional to the number of
mined negatives and require frequent updates of nearest neighbor indices or
mining from recent batches. In this work, we provide an alternative to hard
negative mining, Global Contrastive Batch Sampling (GCBS), an efficient
approximation to the batch assignment problem that upper bounds the gap between
the global and training losses, $\mathcal{L}^{Global} - \mathcal{L}^{Train}$,
in contrastive learning settings. Through experimentation we find GCBS improves
state-of-the-art performance in sentence embedding and code-search tasks.
Additionally, GCBS is easy to implement as it requires only a few additional
lines of code, does not maintain external data structures such as nearest
neighbor indices, is more computationally efficient than the most minimal hard
negative mining approaches, and makes no changes to the model being trained.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal learning with graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.03299v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.03299v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasha Ektefaie, George Dasoulas, Ayush Noori, Maha Farhat, Marinka Zitnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence for graphs has achieved remarkable success in
modeling complex systems, ranging from dynamic networks in biology to
interacting particle systems in physics. However, the increasingly
heterogeneous graph datasets call for multimodal methods that can combine
different inductive biases: the set of assumptions that algorithms use to make
predictions for inputs they have not encountered during training. Learning on
multimodal datasets presents fundamental challenges because the inductive
biases can vary by data modality and graphs might not be explicitly given in
the input. To address these challenges, multimodal graph AI methods combine
different modalities while leveraging cross-modal dependencies using graphs.
Diverse datasets are combined using graphs and fed into sophisticated
multimodal architectures, specified as image-intensive, knowledge-grounded and
language-intensive models. Using this categorization, we introduce a blueprint
for multimodal graph learning, use it to study existing methods and provide
guidelines to design new models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 5 figures, 2 boxes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Theseus: A Library for Differentiable Nonlinear Optimization <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09442v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09442v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Pineda, Taosha Fan, Maurizio Monge, Shobha Venkataraman, Paloma Sodhi, Ricky T. Q. Chen, Joseph Ortiz, Daniel DeTone, Austin Wang, Stuart Anderson, Jing Dong, Brandon Amos, Mustafa Mukadam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Theseus, an efficient application-agnostic open source library for
differentiable nonlinear least squares (DNLS) optimization built on PyTorch,
providing a common framework for end-to-end structured learning in robotics and
vision. Existing DNLS implementations are application specific and do not
always incorporate many ingredients important for efficiency. Theseus is
application-agnostic, as we illustrate with several example applications that
are built using the same underlying differentiable components, such as
second-order optimizers, standard costs functions, and Lie groups. For
efficiency, Theseus incorporates support for sparse solvers, automatic
vectorization, batching, GPU acceleration, and gradient computation with
implicit differentiation and direct loss minimization. We do extensive
performance evaluation in a set of applications, demonstrating significant
efficiency gains and better scalability when these features are incorporated.
Project page: https://sites.google.com/view/theseus-ai
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Advances in Neural Information Processing Systems (NeurIPS), 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Bayesian Framework for Digital Twin-Based Control, Monitoring, and
  Data Collection in Wireless Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.01351v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.01351v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clement Ruah, Osvaldo Simeone, Bashir Al-Hashimi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Commonly adopted in the manufacturing and aerospace sectors, digital twin
(DT) platforms are increasingly seen as a promising paradigm to control,
monitor, and analyze software-based, "open", communication systems. Notably, DT
platforms provide a sandbox in which to test artificial intelligence (AI)
solutions for communication systems, potentially reducing the need to collect
data and test algorithms in the field, i.e., on the physical twin (PT). A key
challenge in the deployment of DT systems is to ensure that virtual control
optimization, monitoring, and analysis at the DT are safe and reliable,
avoiding incorrect decisions caused by "model exploitation". To address this
challenge, this paper presents a general Bayesian framework with the aim of
quantifying and accounting for model uncertainty at the DT that is caused by
limitations in the amount and quality of data available at the DT from the PT.
In the proposed framework, the DT builds a Bayesian model of the communication
system, which is leveraged to enable core DT functionalities such as control
via multi-agent reinforcement learning (MARL), monitoring of the PT for anomaly
detection, prediction, data-collection optimization, and counterfactual
analysis. To exemplify the application of the proposed framework, we
specifically investigate a case-study system encompassing multiple sensing
devices that report to a common receiver. Experimental results validate the
effectiveness of the proposed Bayesian framework as compared to standard
frequentist model-based solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extends and subsumes arXiv:2210.05582 Updates: - 18/01/2023: Updated
  reference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Digital Twin-Based Multiple Access Optimization and Monitoring via
  Model-Driven Bayesian Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.05582v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.05582v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clement Ruah, Osvaldo Simeone, Bashir Al-Hashimi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Commonly adopted in the manufacturing and aerospace sectors, digital twin
(DT) platforms are increasingly seen as a promising paradigm to control and
monitor software-based, "open", communication systems, which play the role of
the physical twin (PT). In the general framework presented in this work, the DT
builds a Bayesian model of the communication system, which is leveraged to
enable core DT functionalities such as control via multi-agent reinforcement
learning (MARL) and monitoring of the PT for anomaly detection. We specifically
investigate the application of the proposed framework to a simple case-study
system encompassing multiple sensing devices that report to a common receiver.
The Bayesian model trained at the DT has the key advantage of capturing
epistemic uncertainty regarding the communication system, e.g., regarding
current traffic conditions, which arise from limited PT-to-DT data transfer.
Experimental results validate the effectiveness of the proposed Bayesian
framework as compared to standard frequentist model-based solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for conference publication Updates: - 18/01/2023: updated
  reference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spiking Neural Network Decision Feedback Equalization <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.04756v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.04756v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eike-Manuel Bansbach, Alexander von Bank, Laurent Schmalen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the past years, artificial neural networks (ANNs) have become the de-facto
standard to solve tasks in communications engineering that are difficult to
solve with traditional methods. In parallel, the artificial intelligence
community drives its research to biology-inspired, brain-like spiking neural
networks (SNNs), which promise extremely energy-efficient computing. In this
paper, we investigate the use of SNNs in the context of channel equalization
for ultra-low complexity receivers. We propose an SNN-based equalizer with a
feedback structure akin to the decision feedback equalizer (DFE). For
conversion of real-world data into spike signals we introduce a novel ternary
encoding and compare it with traditional log-scale encoding. We show that our
approach clearly outperforms conventional linear equalizers for three different
exemplary channels. We highlight that mainly the conversion of the channel
output to spikes introduces a small performance penalty. The proposed SNN with
a decision feedback structure enables the path to competitive energy-efficient
transceivers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted for publication at SCC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Approximation Theory of Tree Tensor Networks: Tensorized Multivariate
  Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2101.11932v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2101.11932v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mazen Ali, Anthony Nouy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the approximation of multivariate functions with tensor networks
(TNs). The main conclusion of this work is an answer to the following two
questions: "What are the approximation capabilities of TNs?" and "What is an
appropriate model class of functions that can be approximated with TNs?" To
answer the former: we show that TNs can (near to) optimally replicate
$h$-uniform and $h$-adaptive approximation, for any smoothness order of the
target function. Tensor networks thus exhibit universal expressivity w.r.t.
isotropic, anisotropic and mixed smoothness spaces that is comparable with more
general neural networks families such as deep rectified linear unit (ReLU)
networks. Put differently, TNs have the capacity to (near to) optimally
approximate many function classes -- without being adapted to the particular
class in question. To answer the latter: as a candidate model class we consider
approximation classes of TNs and show that these are (quasi-)Banach spaces,
that many types of classical smoothness spaces are continuously embedded into
said approximation classes and that TN approximation classes are themselves not
embedded in any classical smoothness space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For part I see arXiv:2007.00118, for part II see arXiv:2007.00128</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Approximation Theory of Tree Tensor Networks: Tensorized Univariate
  Functions -- Part II 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2007.00128v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2007.00128v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mazen Ali, Anthony Nouy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the approximation by tensor networks (TNs) of functions from
classical smoothness classes. The considered approximation tool combines a
tensorization of functions in $L^p([0,1))$, which allows to identify a
univariate function with a multivariate function (or tensor), and the use of
tree tensor networks (the tensor train format) for exploiting low-rank
structures of multivariate functions. The resulting tool can be interpreted as
a feed-forward neural network, with first layers implementing the
tensorization, interpreted as a particular featuring step, followed by a
sum-product network with sparse architecture. In part I of this work, we
presented several approximation classes associated with different measures of
complexity of tensor networks and studied their properties. In this work (part
II), we show how classical approximation tools, such as polynomials or splines
(with fixed or free knots), can be encoded as a tensor network with controlled
complexity. We use this to derive direct (Jackson) inequalities for the
approximation spaces of tensor networks. This is then utilized to show that
Besov spaces are continuously embedded into these approximation spaces. In
other words, we show that arbitrary Besov functions can be approximated with
optimal or near to optimal rate. We also show that an arbitrary function in the
approximation class possesses no Besov smoothness, unless one limits the depth
of the tensor network.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For part I see arXiv:2007.00118, for part III see arXiv:2101.11932</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Approximation Theory of Tree Tensor Networks: Tensorized Univariate
  Functions -- Part I 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2007.00118v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2007.00118v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mazen Ali, Anthony Nouy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the approximation of functions by tensor networks (TNs). We show
that Lebesgue $L^p$-spaces in one dimension can be identified with tensor
product spaces of arbitrary order through tensorization. We use this tensor
product structure to define subsets of $L^p$ of rank-structured functions of
finite representation complexity. These subsets are then used to define
different approximation classes of tensor networks, associated with different
measures of complexity. These approximation classes are shown to be
quasi-normed linear spaces. We study some elementary properties and
relationships of said spaces. In part II of this work, we will show that
classical smoothness (Besov) spaces are continuously embedded into these
approximation classes. We will also show that functions in these approximation
classes do not possess any Besov smoothness, unless one restricts the depth of
the tensor networks. The results of this work are both an analysis of the
approximation spaces of TNs and a study of the expressivity of a particular
type of neural networks (NN) -- namely feed-forward sum-product networks with
sparse architecture. The input variables of this network result from the
tensorization step, interpreted as a particular featuring step which can also
be implemented with a neural network with a specific architecture. We point out
interesting parallels to recent results on the expressivity of rectified linear
unit (ReLU) networks -- currently one of the most popular type of NNs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For part II see arXiv:2007.00128, for part III see arXiv:2101.11932</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Antenna Array Calibration Via Gaussian Process Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06582v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06582v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sergey S. Tambovskiy, Gábor Fodor, Hugo Tullberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Antenna array calibration is necessary to maintain the high fidelity of beam
patterns across a wide range of advanced antenna systems and to ensure channel
reciprocity in time division duplexing schemes. Despite the continuous
development in this area, most existing solutions are optimised for specific
radio architectures, require standardised over-the-air data transmission, or
serve as extensions of conventional methods. The diversity of communication
protocols and hardware creates a problematic case, since this diversity
requires to design or update the calibration procedures for each new advanced
antenna system. In this study, we formulate antenna calibration in an
alternative way, namely as a task of functional approximation, and address it
via Bayesian machine learning. Our contributions are three-fold. Firstly, we
define a parameter space, based on near-field measurements, that captures the
underlying hardware impairments corresponding to each radiating element, their
positional offsets, as well as the mutual coupling effects between antenna
elements. Secondly, Gaussian process regression is used to form models from a
sparse set of the aforementioned near-field data. Once deployed, the learned
non-parametric models effectively serve to continuously transform the
beamforming weights of the system, resulting in corrected beam patterns.
Lastly, we demonstrate the viability of the described methodology for both
digital and analog beamforming antenna arrays of different scales and discuss
its further extension to support real-time operation with dynamic hardware
impairments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International ITG 26th Workshop on Smart Antennas and 13th Conference
  on Systems, Communications, and Coding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Many-Body Dispersion Correction through Random-phase
  Approximation for Chemically Accurate Density Functional Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.09784v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.09784v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pier Paolo Poier, Louis Lagardère, Jean-Philip Piquemal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We extend our recently proposed Deep Learning-aided many-body dispersion
(DNN-MBD) model to quadrupole polarizability (Q) terms using a generalized
Random Phase Approximation (RPA) formalism, thus enabling the inclusion of van
der Waals contributions beyond dipole. The resulting DNN-MBDQ model only relies
on ab initio-derived quantities as the introduced quadrupole polarizabilities
are recursively retrieved from dipole ones, in turn modelled via the
Tkatchenko-Scheffler method. A transferable and efficient deep-neuronal network
(DNN) provides atom in molecule volumes, while a single range-separation
parameter is used to couple the model to Density Functional Theory (DFT). Since
it can be computed at a negligible cost, the DNN-MBDQ approach can be coupled
with DFT functionals such as PBE,PBE0 and B86bPBE (dispersionless). The
DNN-MBQ-corrected functionals reach chemical accuracy while exhibiting lower
errors compared to their dipole-only counterparts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimization-based Block Coordinate Gradient Coding for Mitigating
  Partial Stragglers in Distributed Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.02450v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.02450v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Wang, Ying Cui, Chenglin Li, Junni Zou, Hongkai Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gradient coding schemes effectively mitigate full stragglers in distributed
learning by introducing identical redundancy in coded local partial derivatives
corresponding to all model parameters. However, they are no longer effective
for partial stragglers as they cannot utilize incomplete computation results
from partial stragglers. This paper aims to design a new gradient coding scheme
for mitigating partial stragglers in distributed learning. Specifically, we
consider a distributed system consisting of one master and N workers,
characterized by a general partial straggler model and focuses on solving a
general large-scale machine learning problem with L model parameters using
gradient coding. First, we propose a coordinate gradient coding scheme with L
coding parameters representing L possibly different diversities for the L
coordinates, which generates most gradient coding schemes. Then, we consider
the minimization of the expected overall runtime and the maximization of the
completion probability with respect to the L coding parameters for coordinates,
which are challenging discrete optimization problems. To reduce computational
complexity, we first transform each to an equivalent but much simpler discrete
problem with N\llL variables representing the partition of the L coordinates
into N blocks, each with identical redundancy. This indicates an equivalent but
more easily implemented block coordinate gradient coding scheme with N coding
parameters for blocks. Then, we adopt continuous relaxation to further reduce
computational complexity. For the resulting minimization of expected overall
runtime, we develop an iterative algorithm of computational complexity O(N^2)
to obtain an optimal solution and derive two closed-form approximate solutions
both with computational complexity O(N). For the resultant maximization of the
completion probability, we develop an iterative algorithm of...
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures, submitted to IEEE Trans. Signal Process. arXiv
  admin note: text overlap with arXiv:2109.08933</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-IID Quantum Federated Learning with One-shot Communication
  Complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.00768v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.00768v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haimeng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning refers to the task of machine learning based on
decentralized data from multiple clients with secured data privacy. Recent
studies show that quantum algorithms can be exploited to boost its performance.
However, when the clients' data are not independent and identically distributed
(IID), the performance of conventional federated algorithms is known to
deteriorate. In this work, we explore the non-IID issue in quantum federated
learning with both theoretical and numerical analysis. We further prove that a
global quantum channel can be exactly decomposed into local channels trained by
each client with the help of local density estimators. This observation leads
to a general framework for quantum federated learning on non-IID data with
one-shot communication complexity. Numerical simulations show that the proposed
algorithm outperforms the conventional ones significantly under non-IID
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages excluding appendices and references. Code available at
  https://github.com/JasonZHM/quantum-fed-infer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaussian Process Priors for Systems of Linear Partial Differential
  Equations with Constant Coefficients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.14319v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.14319v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Härkönen, Markus Lange-Hegermann, Bogdan Raiţă
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial differential equations (PDEs) are important tools to model physical
systems, and including them into machine learning models is an important way of
incorporating physical knowledge. Given any system of linear PDEs with constant
coefficients, we propose a family of Gaussian process (GP) priors, which we
call EPGP, such that all realizations are exact solutions of this system. We
apply the Ehrenpreis-Palamodov fundamental principle, which works like a
non-linear Fourier transform, to construct GP kernels mirroring standard
spectral methods for GPs. Our approach can infer probable solutions of linear
PDE systems from any data such as noisy measurements, or pointwise defined
initial and boundary conditions. Constructing EPGP-priors is algorithmic,
generally applicable, and comes with a sparse version (S-EPGP) that learns the
relevant spectral frequencies and works better for big data sets. We
demonstrate our approach on three families of systems of PDE, the heat
equation, wave equation, and Maxwell's equations, where we improve upon the
state of the art in computation time and precision, in some experiments by
several orders of magnitude.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 8 figures; updated with expanded appendices and ancillary
  files. Code available at https://github.com/haerski/EPGP. For animations, see
  https://mathrepo.mis.mpg.de/EPGP/index.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prediction of Red Wine Quality Using One-dimensional Convolutional
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.14008v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.14008v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. Di, Y. Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As an alcoholic beverage, wine has remained prevalent for thousands of years,
and the quality assessment of wines has been significant in wine production and
trade. Scholars have proposed various deep learning and machine learning
algorithms for wine quality prediction, such as Support vector machine (SVM),
Random Forest (RF), K-nearest neighbors (KNN), Deep neural network (DNN), and
Logistic regression (LR). However, these methods ignore the inner relationship
between the physical and chemical properties of the wine, for example, the
correlations between pH values, fixed acidity, citric acid, and so on. To fill
the gap, this paper conducts the Pearson correlation analysis, PCA analysis,
and Shapiro-Wilk test on those properties and incorporates 1D-CNN architecture
to capture the correlations among neighboring features. In addition, it
implemented dropout and batch normalization techniques to improve the
robustness of the proposed model. Massive experiments have shown that our
method can outperform baseline approaches in wine quality prediction. Moreover,
ablation experiments also demonstrate the effectiveness of incorporating the
1-D CNN module, Dropout, and normalization techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Task-Oriented Communication for Edge Inference: An Information
  Bottleneck Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.04170v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.04170v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Shao, Yuyi Mao, Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates task-oriented communication for edge inference, where
a low-end edge device transmits the extracted feature vector of a local data
sample to a powerful edge server for processing. It is critical to encode the
data into an informative and compact representation for low-latency inference
given the limited bandwidth. We propose a learning-based communication scheme
that jointly optimizes feature extraction, source coding, and channel coding in
a task-oriented manner, i.e., targeting the downstream inference task rather
than data reconstruction. Specifically, we leverage an information bottleneck
(IB) framework to formalize a rate-distortion tradeoff between the
informativeness of the encoded feature and the inference performance. As the IB
optimization is computationally prohibitive for the high-dimensional data, we
adopt a variational approximation, namely the variational information
bottleneck (VIB), to build a tractable upper bound. To reduce the communication
overhead, we leverage a sparsity-inducing distribution as the variational prior
for the VIB framework to sparsify the encoded feature vector. Furthermore,
considering dynamic channel conditions in practical communication systems, we
propose a variable-length feature encoding scheme based on dynamic neural
networks to adaptively adjust the activated dimensions of the encoded feature
to different channel conditions. Extensive experiments evidence that the
proposed task-oriented communication system achieves a better rate-distortion
tradeoff than baseline methods and significantly reduces the feature
transmission latency in dynamic channel conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted to the IEEE JSAC Series on Machine Learning
  for Communications and Networks and will be published in Jan 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>ing Large Language Model for Machine Translation: A Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07069v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07069v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biao Zhang, Barry Haddow, Alexandra Birch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on prompting has shown excellent performance with little or even no
supervised training across many tasks. However, prompting for machine
translation is still under-explored in the literature. We fill this gap by
offering a systematic study on prompting strategies for translation, examining
various factors for prompt template and demonstration example selection. We
further explore the use of monolingual data and the feasibility of
cross-lingual, cross-domain, and sentence-to-document transfer learning in
prompting. Extensive experiments with GLM-130B (Zeng et al., 2022) as the
testbed show that 1) the number and the quality of prompt examples matter,
where using suboptimal examples degenerates translation; 2) several features of
prompt examples, such as semantic similarity, show significant Spearman
correlation with their prompting performance; yet, none of the correlations are
strong enough; 3) using pseudo parallel prompt examples constructed from
monolingual data via zero-shot prompting could improve translation; and 4)
improved performance is achievable by transferring knowledge from prompt
examples selected in other settings. We finally provide an analysis on the
model outputs and discuss several problems that prompting still suffers from.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Landscape Complexity for the Empirical Risk of Generalized Linear Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1912.02143v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1912.02143v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Maillard, Gérard Ben Arous, Giulio Biroli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method to obtain the average and the typical value of the number
of critical points of the empirical risk landscape for generalized linear
estimation problems and variants. This represents a substantial extension of
previous applications of the Kac-Rice method since it allows to analyze the
critical points of high dimensional non-Gaussian random functions. Under a
technical hypothesis, we obtain a rigorous explicit variational formula for the
annealed complexity, which is the logarithm of the average number of critical
points at fixed value of the empirical risk. This result is simplified, and
extended, using the non-rigorous Kac-Rice replicated method from theoretical
physics. In this way we find an explicit variational formula for the quenched
complexity, which is generally different from its annealed counterpart, and
allows to obtain the number of critical points for typical instances up to
exponential accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages and 18 pages appendix. Update to match the published version
  (v2). Corrections of remaining small typos (v3). Simplification of a
  technical argument in Appendix A (v4) and clarification of a technical
  hypothesis (v5)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Concentration inequalities for leave-one-out cross validation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.02478v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.02478v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benny Avelin, Lauri Viitasaari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article we prove that estimator stability is enough to show that
leave-one-out cross validation is a sound procedure, by providing concentration
bounds in a general framework. In particular, we provide concentration bounds
beyond Lipschitz continuity assumptions on the loss or on the estimator. In
order to obtain our results, we rely on random variables with distribution
satisfying the logarithmic Sobolev inequality, providing us a relatively rich
class of distributions. We illustrate our method by considering several
interesting examples, including linear regression, kernel density estimation,
and stabilized / truncated estimators such as stabilized kernel regression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reverse Differentiation via Predictive Coding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.04689v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.04689v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tommaso Salvatori, Yuhang Song, Thomas Lukasiewicz, Rafal Bogacz, Zhenghua Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has redefined the field of artificial intelligence (AI) thanks
to the rise of artificial neural networks, which are architectures inspired by
their neurological counterpart in the brain. Through the years, this dualism
between AI and neuroscience has brought immense benefits to both fields,
allowing neural networks to be used in dozens of applications. These networks
use an efficient implementation of reverse differentiation, called
backpropagation (BP). This algorithm, however, is often criticized for its
biological implausibility (e.g., lack of local update rules for the
parameters). Therefore, biologically plausible learning methods that rely on
predictive coding (PC), a framework for describing information processing in
the brain, are increasingly studied. Recent works prove that these methods can
approximate BP up to a certain margin on multilayer perceptrons (MLPs), and
asymptotically on any other complex model, and that zero-divergence inference
learning (Z-IL), a variant of PC, is able to exactly implement BP on MLPs.
However, the recent literature shows also that there is no biologically
plausible method yet that can exactly replicate the weight update of BP on
complex models. To fill this gap, in this paper, we generalize (PC and) Z-IL by
directly defining them on computational graphs, and show that it can perform
exact reverse differentiation. What results is the first biologically plausible
algorithm that is equivalent to BP in the way of updating parameters on any
neural network, providing a bridge between the interdisciplinary research of
neuroscience and deep learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond ADMM: A Unified Client-variance-reduced Adaptive Federated
  Learning Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.01519v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.01519v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Wang, Yanqing Xu, Zhiguo Wang, Tsung-Hui Chang, Tony Q. S. Quek, Defeng Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a novel distributed learning paradigm, federated learning (FL) faces
serious challenges in dealing with massive clients with heterogeneous data
distribution and computation and communication resources. Various
client-variance-reduction schemes and client sampling strategies have been
respectively introduced to improve the robustness of FL. Among others,
primal-dual algorithms such as the alternating direction of method multipliers
(ADMM) have been found being resilient to data distribution and outperform most
of the primal-only FL algorithms. However, the reason behind remains a mystery
still. In this paper, we firstly reveal the fact that the federated ADMM is
essentially a client-variance-reduced algorithm. While this explains the
inherent robustness of federated ADMM, the vanilla version of it lacks the
ability to be adaptive to the degree of client heterogeneity. Besides, the
global model at the server under client sampling is biased which slows down the
practical convergence. To go beyond ADMM, we propose a novel primal-dual FL
algorithm, termed FedVRA, that allows one to adaptively control the
variance-reduction level and biasness of the global model. In addition, FedVRA
unifies several representative FL algorithms in the sense that they are either
special instances of FedVRA or are close to it. Extensions of FedVRA to
semi/un-supervised learning are also presented. Experiments based on
(semi-)supervised image classification tasks demonstrate superiority of FedVRA
over the existing schemes in learning scenarios with massive heterogeneous
clients and client sampling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Federated Learning Personalization via Model Agnostic Meta
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1909.12488v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1909.12488v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihan Jiang, Jakub Konečný, Keith Rush, Sreeram Kannan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) refers to learning a high quality global model based
on decentralized data storage, without ever copying the raw data. A natural
scenario arises with data created on mobile phones by the activity of their
users. Given the typical data heterogeneity in such situations, it is natural
to ask how can the global model be personalized for every such device,
individually. In this work, we point out that the setting of Model Agnostic
Meta Learning (MAML), where one optimizes for a fast, gradient-based, few-shot
adaptation to a heterogeneous distribution of tasks, has a number of
similarities with the objective of personalization for FL. We present FL as a
natural source of practical applications for MAML algorithms, and make the
following observations. 1) The popular FL algorithm, Federated Averaging, can
be interpreted as a meta learning algorithm. 2) Careful fine-tuning can yield a
global model with higher accuracy, which is at the same time easier to
personalize. However, solely optimizing for the global model accuracy yields a
weaker personalization result. 3) A model trained using a standard datacenter
optimization method is much harder to personalize, compared to one trained
using Federated Averaging, supporting the first claim. These results raise new
questions for FL, MAML, and broader ML research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Auxiliary Cross-Modal Representation Learning with Triplet Loss
  Functions for Online Handwriting Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.07901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.07901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Ott, David Rügamer, Lucas Heublein, Bernd Bischl, Christopher Mutschler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-modal representation learning learns a shared embedding between two or
more modalities to improve performance in a given task compared to using only
one of the modalities. Cross-modal representation learning from different data
types -- such as images and time-series data (e.g., audio or text data) --
requires a deep metric learning loss that minimizes the distance between the
modality embeddings. In this paper, we propose to use the triplet loss, which
uses positive and negative identities to create sample pairs with different
labels, for cross-modal representation learning between image and time-series
modalities (CMR-IS). By adapting the triplet loss for cross-modal
representation learning, higher accuracy in the main (time-series
classification) task can be achieved by exploiting additional information of
the auxiliary (image classification) task. Our experiments on synthetic data
and handwriting recognition data from sensor-enhanced pens show improved
classification accuracy, faster convergence, and better generalizability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ - Modelling Difference Between Censored and Uncensored Electric Vehicle
  Charging Demand 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06418v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06418v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frederik Boe Hüttel, Filipe Rodrigues, Francisco Câmara Pereira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electric vehicle charging demand models, with charging records as input, will
inherently be biased toward the supply of available chargers, as the data do
not include demand lost from occupied stations and competitors. This lost
demand implies that the records only observe a fraction of the total demand,
i.e. the observations are censored, and actual demand is likely higher than
what the data reflect. Machine learning models often neglect to account for
this censored demand when forecasting the charging demand, which limits models'
applications for future expansions and supply management. We address this gap
by modelling the charging demand with probabilistic censorship-aware graph
neural networks, which learn the latent demand distribution in both the spatial
and temporal dimensions. We use GPS trajectories from cars in Copenhagen,
Denmark, to study how censoring occurs and much demand is lost due to occupied
charging and competing services. We find that censorship varies throughout the
city and over time, encouraging spatial and temporal modelling. We find that in
some regions of Copenhagen, censorship occurs 61% of the time. Our results show
censorship-aware models provide better prediction and uncertainty estimation in
actual future demand than censorship-unaware models. Our results suggest that
future models based on charging records should account for the censoring to
expand the application areas of machine learning models in this supply
management and infrastructure expansion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Planning and Learning with Adaptive Lookahead 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.12403v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.12403v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aviv Rosenberg, Assaf Hallak, Shie Mannor, Gal Chechik, Gal Dalal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Some of the most powerful reinforcement learning frameworks use planning for
action selection. Interestingly, their planning horizon is either fixed or
determined arbitrarily by the state visitation history. Here, we expand beyond
the naive fixed horizon and propose a theoretically justified strategy for
adaptive selection of the planning horizon as a function of the state-dependent
value estimate. We propose two variants for lookahead selection and analyze the
trade-off between iteration count and computational complexity per iteration.
We then devise a corresponding deep Q-network algorithm with an adaptive tree
search horizon. We separate the value estimation per depth to compensate for
the off-policy discrepancy between depths. Lastly, we demonstrate the efficacy
of our adaptive lookahead method in a maze environment and Atari.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Operator Learning Framework for Digital Twin and Complex Engineering
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06701v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06701v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuma Kobayashi, James Daniell, Syed B. Alam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With modern computational advancements and statistical analysis methods,
machine learning algorithms have become a vital part of engineering modeling.
Neural Operator Networks (ONets) is an emerging machine learning algorithm as a
"faster surrogate" for approximating solutions to partial differential
equations (PDEs) due to their ability to approximate mathematical operators
versus the direct approximation of Neural Networks (NN). ONets use the
Universal Approximation Theorem to map finite-dimensional inputs to
infinite-dimensional space using the branch-trunk architecture, which encodes
domain and feature information separately before using a dot product to combine
the information. ONets are expected to occupy a vital niche for surrogate
modeling in physical systems and Digital Twin (DT) development. Three test
cases are evaluated using ONets for operator approximation, including a
1-dimensional ordinary differential equations (ODE), general diffusion system,
and convection-diffusion (Burger) system. Solutions for ODE and diffusion
systems yield accurate and reliable results (R2>0.95), while solutions for
Burger systems need further refinement in the ONet algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Domain Evaluation of a Deep Learning-Based Type Inference System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.09189v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.09189v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernd Gruner, Tim Sonnekalb, Thomas S. Heinze, Clemens-Alexander Brust
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optional type annotations allow for enriching dynamic programming languages
with static typing features like better Integrated Development Environment
(IDE) support, more precise program analysis, and early detection and
prevention of type-related runtime errors. Machine learning-based type
inference promises interesting results for automating this task. However, the
practical usage of such systems depends on their ability to generalize across
different domains, as they are often applied outside their training domain. In
this work, we investigate Type4Py as a representative of state-of-the-art deep
learning-based type inference systems, by conducting extensive cross-domain
experiments. Thereby, we address the following problems: class imbalances,
out-of-vocabulary words, dataset shifts, and unknown classes. To perform such
experiments, we use the datasets ManyTypes4Py and CrossDomainTypes4Py. The
latter we introduce in this paper. Our dataset enables the evaluation of type
inference systems in different domains of software projects and has over
1,000,000 type annotations mined on the platforms GitHub and Libraries. It
consists of data from the two domains web development and scientific
calculation. Through our experiments, we detect that the shifts in the dataset
and the long-tailed distribution with many rare and unknown data types decrease
the performance of the deep learning-based type inference system drastically.
In this context, we test unsupervised domain adaptation methods and fine-tuning
to overcome these issues. Moreover, we investigate the impact of
out-of-vocabulary words.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding the Spectral Bias of Coordinate Based MLPs Via Training
  Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.05816v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.05816v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Lazzari, Xiuwen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, multi-layer perceptrons (MLPs) with ReLU activations have enabled
new photo-realistic rendering techniques by encoding scene properties using
their weights. For these models, termed coordinate based MLPs, sinusoidal
encodings are necessary in allowing for convergence to the high frequency
components of the signal due to their severe spectral bias. Previous work has
explained this phenomenon using Neural Tangent Kernel (NTK) and Fourier
analysis. However, the kernel regime does not expose the properties of the
network that induce this behavior, and the Fourier decomposition is global, not
allowing for insight on the network's local dynamics. A new interpretation of
spectral bias directly through ReLU network computations would expose their
limitations in dense settings, while providing a clearer explanation as to how
this behavior emerges during the learning process. In this paper, we provide
the first study of spectral bias in a coordinate based MLP through its
activation regions and gradient descent dynamics, specifically using gradient
confusion. We relate the confusion between inputs to the distinctiveness of
their activation patterns, and find higher amounts of confusion when expressive
power is limited. This leads to slower convergence to the high frequency
components of the signal, which is magnified by the density of coordinates.
Additionally, this method allows us to analyze the properties of the activation
regions as spectral bias is reduced, in which we find distinct dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 10 figures, preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Teacher Forcing Recovers Reward Functions for Text Generation <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.08708v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.08708v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchang Hao, Yuxin Liu, Lili Mou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has been widely used in text generation to
alleviate the exposure bias issue or to utilize non-parallel datasets. The
reward function plays an important role in making RL training successful.
However, previous reward functions are typically task-specific and sparse,
restricting the use of RL. In our work, we propose a task-agnostic approach
that derives a step-wise reward function directly from a model trained with
teacher forcing. We additionally propose a simple modification to stabilize the
RL training on non-parallel datasets with our induced reward function.
Empirical results show that our method outperforms self-training and reward
regression methods on several text generation tasks, confirming the
effectiveness of our reward function.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Concrete Score Matching: Generalized Score Matching for Discrete Data <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.00802v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.00802v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenlin Meng, Kristy Choi, Jiaming Song, Stefano Ermon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representing probability distributions by the gradient of their density
functions has proven effective in modeling a wide range of continuous data
modalities. However, this representation is not applicable in discrete domains
where the gradient is undefined. To this end, we propose an analogous score
function called the "Concrete score", a generalization of the (Stein) score for
discrete settings. Given a predefined neighborhood structure, the Concrete
score of any input is defined by the rate of change of the probabilities with
respect to local directional changes of the input. This formulation allows us
to recover the (Stein) score in continuous domains when measuring such changes
by the Euclidean distance, while using the Manhattan distance leads to our
novel score function in discrete domains. Finally, we introduce a new framework
to learn such scores from samples called Concrete Score Matching (CSM), and
propose an efficient training objective to scale our approach to high
dimensions. Empirically, we demonstrate the efficacy of CSM on density
estimation tasks on a mixture of synthetic, tabular, and high-dimensional image
datasets, and demonstrate that it performs favorably relative to existing
baselines for modeling discrete data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022. First two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Good Is NLP? A Sober Look at NLP Tasks through the Lens of Social
  Impact <span class="chip">ACL 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.02359v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.02359v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijing Jin, Geeticka Chauhan, Brian Tse, Mrinmaya Sachan, Rada Mihalcea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have seen many breakthroughs in natural language processing
(NLP), transitioning it from a mostly theoretical field to one with many
real-world applications. Noting the rising number of applications of other
machine learning and AI techniques with pervasive societal impact, we
anticipate the rising importance of developing NLP technologies for social
good. Inspired by theories in moral philosophy and global priorities research,
we aim to promote a guideline for social good in the context of NLP. We lay the
foundations via the moral philosophy definition of social good, propose a
framework to evaluate the direct and indirect real-world impact of NLP tasks,
and adopt the methodology of global priorities research to identify priority
causes for NLP research. Finally, we use our theoretical framework to provide
some practical guidelines for future NLP research for social good. Our data and
code are available at http://github.com/zhijing-jin/nlp4sg_acl2021. In
addition, we curate a list of papers and resources on NLP for social good at
https://github.com/zhijing-jin/NLP4SocialGood_Papers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of ACL 2021; also accepted at the NLP for Positive Impact
  workshop@ACL 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamically Mitigating Data Discrepancy with Balanced Focal Loss for
  Replay Attack Detection <span class="chip">ICPR2020</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2006.14563v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2006.14563v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongqiang Dou, Haocheng Yang, Maolin Yang, Yanyan Xu, Dengfeng Ke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It becomes urgent to design effective anti-spoofing algorithms for vulnerable
automatic speaker verification systems due to the advancement of high-quality
playback devices. Current studies mainly treat anti-spoofing as a binary
classification problem between bonafide and spoofed utterances, while lack of
indistinguishable samples makes it difficult to train a robust spoofing
detector. In this paper, we argue that for anti-spoofing, it needs more
attention for indistinguishable samples over easily-classified ones in the
modeling process, to make correct discrimination a top priority. Therefore, to
mitigate the data discrepancy between training and inference, we propose D3M,
to leverage a balanced focal loss function as the training objective to
dynamically scale the loss based on the traits of the sample itself. Besides,
in the experiments, we select three kinds of features that contain both
magnitude-based and phase-based information to form complementary and
informative features. Experimental results on the ASVspoof2019 dataset
demonstrate the superiority of the proposed methods by comparison between our
systems and top-performing ones. Systems trained with the balanced focal loss
perform significantly better than conventional cross-entropy loss. With
complementary features, our fusion system with only three kinds of features
outperforms other systems containing five or more complex single models by
22.5% for min-tDCF and 7% for EER, achieving a min-tDCF and an EER of 0.0124
and 0.55% respectively. Furthermore, we present and discuss the evaluation
results on real replay data apart from the simulated ASVspoof2019 data,
indicating that research for anti-spoofing still has a long way to go. Source
code, analysis data, and other details are publicly available at
https://github.com/asvspoof/D3M.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 25th International Conference on Pattern Recognition (ICPR2020)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Pattern Extraction Multi-Task Learning for Multi-Step
  Conversion Estimations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.02494v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.02494v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuewen Tao, Mingming Ha, Xiaobo Guo, Qiongxu Ma, Hongwei Cheng, Wenfang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task learning (MTL) has been successfully implemented in many
real-world applications, which aims to simultaneously solve multiple tasks with
a single model. The general idea of multi-task learning is designing kinds of
global parameter sharing mechanism and task-specific feature extractor to
improve the performance of all tasks. However, sequential dependence between
tasks are rarely studied but frequently encountered in e-commence online
recommendation, e.g. impression, click and conversion on displayed product.
There is few theoretical work on this problem and biased optimization object
adopted in most MTL methods deteriorates online performance. Besides, challenge
still remains in balancing the trade-off between various tasks and effectively
learn common and specific representation. In this paper, we first analyze
sequential dependence MTL from rigorous mathematical perspective and design a
dependence task learning loss to provide an unbiased optimizing object. And we
propose a Task Aware Feature Extraction (TAFE) framework for sequential
dependence MTL, which enables to selectively reconstruct implicit shared
representations from a sample-wise view and extract explicit task-specific
information in an more efficient way. Extensive experiments on offline datasets
and online A/B implementation demonstrate the effectiveness of our proposed
TAFE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comprehensive Literature <span class="highlight-title">Survey</span> on Deep Learning used in Image
  Memorability Prediction and Modification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06080v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06080v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ananya Sadana, Nikita Thakur, Nikita Poria, Astika Anand, Seeja K. R
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As humans, we can remember certain visuals in great detail, and sometimes
even after viewing them once. What is even more interesting is that humans tend
to remember and forget the same things, suggesting that there might be some
general internal characteristics of an image to encode and discard similar
types of information. Research suggests that some pictures tend to be memorized
more than others. The ability of an image to be remembered by different viewers
is one of its intrinsic properties. In visualization and photography, creating
memorable images is a difficult task. Hence, to solve the problem, various
techniques predict visual memorability and manipulate images' memorability. We
present a comprehensive literature survey to assess the deep learning
techniques used to predict and modify memorability. In particular, we analyze
the use of Convolutional Neural Networks, Recurrent Neural Networks, and
Generative Adversarial Networks for image memorability prediction and
modification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Deep Graph Clustering with Random-walk based <span class="highlight-title">Self-supervised</span>
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.15530v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.15530v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Dong Li, Ruoming Jin, Gagan Agrawal, Rajiv Ramnath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Web-based interactions can be frequently represented by an attributed graph,
and node clustering in such graphs has received much attention lately. Multiple
efforts have successfully applied Graph Convolutional Networks (GCN), though
with some limits on accuracy as GCNs have been shown to suffer from
over-smoothing issues. Though other methods (particularly those based on
Laplacian Smoothing) have reported better accuracy, a fundamental limitation of
all the work is a lack of scalability. This paper addresses this open problem
by relating the Laplacian smoothing to the Generalized PageRank and applying a
random-walk based algorithm as a scalable graph filter. This forms the basis
for our scalable deep clustering algorithm, RwSL, where through a
self-supervised mini-batch training mechanism, we simultaneously optimize a
deep neural network for sample-cluster assignment distribution and an
autoencoder for a clustering-oriented embedding. Using 6 real-world datasets
and 6 clustering metrics, we show that RwSL achieved improved results over
several recent baselines. Most notably, we show that RwSL, unlike all other
deep clustering frameworks, can continue to scale beyond graphs with more than
one million nodes, i.e., handle web-scale. We also demonstrate how RwSL could
perform node clustering on a graph with 1.8 billion edges using only a single
GPU.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Short-time SSVEP data extension by a novel generative adversarial
  networks based framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.05599v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.05599v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yudong Pan, Ning Li, Yangsong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Steady-state visual evoked potentials (SSVEPs) based brain-computer interface
(BCI) has received considerable attention due to its high transfer rate and
available quantity of targets. However, the performance of frequency
identification methods heavily hinges on the amount of user calibration data
and data length, which hinders the deployment in real-world applications.
Recently, generative adversarial networks (GANs)-based data generation methods
have been widely adopted to create supplementary synthetic
electroencephalography (EEG) data, holds promise to address these issues. In
this paper, we proposed a GAN-based end-to-end signal transformation network
for data length window extension, termed as TEGAN. TEGAN transforms short-time
SSVEP signals into long-time artificial SSVEP signals. By incorporating a novel
U-Net generator architecture and auxiliary classifier into the network design,
the TEGAN could produce conditioned features in the synthetic data.
Additionally, to regularize the training process of GAN, we introduced a
two-stage training strategy and the LeCam-divergence regularization term during
the network implementation. The proposed TEGAN was evaluated on two public
SSVEP datasets. With the assistance of TEGAN, the performance of traditional
frequency recognition methods and deep learning-based methods have been
significantly improved under limited calibration data. This study substantiates
the feasibility of the proposed method to extend the data length for short-time
SSVEP signals to develop a high-performance BCI system. The proposed GAN-based
methods have the great potential of shortening the calibration time for various
real-world BCI-based applications, while the novelty of our augmentation
strategies shed some value light on understanding the subject-invariant
properties of SSVEPs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, 2 tables, submitted to IEEE TBME</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Concentration of polynomial random matrices via Efron-Stein inequalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.02655v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.02655v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Goutham Rajendran, Madhur Tulsiani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analyzing concentration of large random matrices is a common task in a wide
variety of fields. Given independent random variables, many tools are available
to analyze random matrices whose entries are linear in the variables, e.g. the
matrix-Bernstein inequality. However, in many applications, we need to analyze
random matrices whose entries are polynomials in the variables. These arise
naturally in the analysis of spectral algorithms, e.g., Hopkins et al. [STOC
2016], Moitra-Wein [STOC 2019]; and in lower bounds for semidefinite programs
based on the Sum of Squares hierarchy, e.g. Barak et al. [FOCS 2016], Jones et
al. [FOCS 2021]. In this work, we present a general framework to obtain such
bounds, based on the matrix Efron-Stein inequalities developed by
Paulin-Mackey-Tropp [Annals of Probability 2016]. The Efron-Stein inequality
bounds the norm of a random matrix by the norm of another simpler (but still
random) matrix, which we view as arising by "differentiating" the starting
matrix. By recursively differentiating, our framework reduces the main task to
analyzing far simpler matrices. For Rademacher variables, these simpler
matrices are in fact deterministic and hence, analyzing them is far easier. For
general non-Rademacher variables, the task reduces to scalar concentration,
which is much easier. Moreover, in the setting of polynomial matrices, our
results generalize the work of Paulin-Mackey-Tropp. Using our basic framework,
we recover known bounds in the literature for simple "tensor networks" and
"dense graph matrices". Using our general framework, we derive bounds for
"sparse graph matrices", which were obtained only recently by Jones et al.
[FOCS 2021] using a nontrivial application of the trace power method, and was a
core component in their work. We expect our framework to be helpful for other
applications involving concentration phenomena for nonlinear random matrices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at SODA 2023. 41 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with
  Multimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06267v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06267v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiu Lin, Samuel Yu, Zhiyi Kuang, Deepak Pathak, Deva Ramanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to quickly learn a new task with minimal instruction - known as
few-shot learning - is a central aspect of intelligent agents. Classical
few-shot benchmarks make use of few-shot samples from a single modality, but
such samples may not be sufficient to characterize an entire concept class. In
contrast, humans use cross-modal information to learn new concepts efficiently.
In this work, we demonstrate that one can indeed build a better ${\bf visual}$
dog classifier by ${\bf read}$ing about dogs and ${\bf listen}$ing to them
bark. To do so, we exploit the fact that recent multimodal foundation models
such as CLIP are inherently cross-modal, mapping different modalities to the
same representation space. Specifically, we propose a simple cross-modal
adaptation approach that learns from few-shot examples spanning different
modalities. By repurposing class names as additional one-shot training samples,
we achieve SOTA results with an embarrassingly simple linear classifier for
vision-language adaptation. Furthermore, we show that our approach can benefit
existing methods such as prefix tuning, adapters, and classifier ensembling.
Finally, to explore other modalities beyond vision and language, we construct
the first (to our knowledge) audiovisual few-shot benchmark and use cross-modal
training to improve the performance of both image and audio classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://linzhiqiu.github.io/papers/cross_modal/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Redaction from <span class="highlight-title">Pre-train</span>ed GANs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.14389v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.14389v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhifeng Kong, Kamalika Chaudhuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large pre-trained generative models are known to occasionally output
undesirable samples, which undermines their trustworthiness. The common way to
mitigate this is to re-train them differently from scratch using different data
or different regularization -- which uses a lot of computational resources and
does not always fully address the problem.
  In this work, we take a different, more compute-friendly approach and
investigate how to post-edit a model after training so that it ''redacts'', or
refrains from outputting certain kinds of samples. We show that redaction is a
fundamentally different task from data deletion, and data deletion may not
always lead to redaction. We then consider Generative Adversarial Networks
(GANs), and provide three different algorithms for data redaction that differ
on how the samples to be redacted are described. Extensive evaluations on
real-world image datasets show that our algorithms out-perform data deletion
baselines, and are capable of redacting data while retaining high generation
quality at a fraction of the cost of full re-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SaTML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An <span class="highlight-title">Overview</span> of Human Activity Recognition Using Wearable Sensors:
  Healthcare and Artificial Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.15990v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.15990v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rex Liu, Albara Ah Ramli, Huanle Zhang, Erik Henricson, Xin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of the internet of things (IoT) and artificial
intelligence (AI) technologies, human activity recognition (HAR) has been
applied in a variety of domains such as security and surveillance, human-robot
interaction, and entertainment. Even though a number of surveys and review
papers have been published, there is a lack of HAR overview papers focusing on
healthcare applications that use wearable sensors. Therefore, we fill in the
gap by presenting this overview paper. In particular, we present our projects
to illustrate the system design of HAR applications for healthcare. Our
projects include early mobility identification of human activities for
intensive care unit (ICU) patients and gait analysis of Duchenne muscular
dystrophy (DMD) patients. We cover essential components of designing HAR
systems including sensor factors (e.g., type, number, and placement location),
AI model selection (e.g., classical machine learning models versus deep
learning models), and feature engineering. In addition, we highlight the
challenges of such healthcare-oriented HAR systems and propose several research
opportunities for both the medical and the computer science community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Network Quantization for Efficient Inference: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.06126v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.06126v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivia Weng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As neural networks have become more powerful, there has been a rising desire
to deploy them in the real world; however, the power and accuracy of neural
networks is largely due to their depth and complexity, making them difficult to
deploy, especially in resource-constrained devices. Neural network quantization
has recently arisen to meet this demand of reducing the size and complexity of
neural networks by reducing the precision of a network. With smaller and
simpler networks, it becomes possible to run neural networks within the
constraints of their target hardware. This paper surveys the many neural
network quantization techniques that have been developed in the last decade.
Based on this survey and comparison of neural network quantization techniques,
we propose future directions of research in the area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved Differential Privacy for SGD via Optimal Private Linear
  Operators on Adaptive Streams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.08312v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.08312v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sergey Denisov, Brendan McMahan, Keith Rush, Adam Smith, Abhradeep Guha Thakurta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by recent applications requiring differential privacy over adaptive
streams, we investigate the question of optimal instantiations of the matrix
mechanism in this setting. We prove fundamental theoretical results on the
applicability of matrix factorizations to adaptive streams, and provide a
parameter-free fixed-point algorithm for computing optimal factorizations. We
instantiate this framework with respect to concrete matrices which arise
naturally in machine learning, and train user-level differentially private
models with the resulting optimal mechanisms, yielding significant improvements
in a notable problem in federated learning with user-level differential
privacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 6 figures. Associated code at
  https://github.com/google-research/federated/tree/master/dp_matrix_factorization</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Characterizing the Spectrum of the NTK via a Power Series Expansion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.07844v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.07844v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Murray, Hui Jin, Benjamin Bowman, Guido Montufar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Under mild conditions on the network initialization we derive a power series
expansion for the Neural Tangent Kernel (NTK) of arbitrarily deep feedforward
networks in the infinite width limit. We provide expressions for the
coefficients of this power series which depend on both the Hermite coefficients
of the activation function as well as the depth of the network. We observe
faster decay of the Hermite coefficients leads to faster decay in the NTK
coefficients and explore the role of depth. Using this series, first we relate
the effective rank of the NTK to the effective rank of the input-data Gram.
Second, for data drawn uniformly on the sphere we study the eigenvalues of the
NTK, analyzing the impact of the choice of activation function. Finally, for
generic data and activation functions with sufficiently fast Hermite
coefficient decay, we derive an asymptotic upper bound on the spectrum of the
NTK.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>55 pages, 3 Figures, 1 Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From One Hand to Multiple Hands: Imitation Learning for Dexterous
  Manipulation from Single-Camera Teleoperation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.12490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.12490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhe Qin, Hao Su, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose to perform imitation learning for dexterous manipulation with
multi-finger robot hand from human demonstrations, and transfer the policy to
the real robot hand. We introduce a novel single-camera teleoperation system to
collect the 3D demonstrations efficiently with only an iPad and a computer. One
key contribution of our system is that we construct a customized robot hand for
each user in the physical simulator, which is a manipulator resembling the same
kinematics structure and shape of the operator's hand. This provides an
intuitive interface and avoid unstable human-robot hand retargeting for data
collection, leading to large-scale and high quality data. Once the data is
collected, the customized robot hand trajectories can be converted to different
specified robot hands (models that are manufactured) to generate training
demonstrations. With imitation learning using our data, we show large
improvement over baselines with multiple complex manipulation tasks.
Importantly, we show our learned policy is significantly more robust when
transferring to the real robot. More videos can be found in the
https://yzqin.github.io/dex-teleop-imitation .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://yzqin.github.io/dex-teleop-imitation/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context-aware controller inference for stabilizing dynamical systems
  from scarce data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.11049v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.11049v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steffen W. R. Werner, Benjamin Peherstorfer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces a data-driven control approach for stabilizing
high-dimensional dynamical systems from scarce data. The proposed context-aware
controller inference approach is based on the observation that controllers need
to act locally only on the unstable dynamics to stabilize systems. This means
it is sufficient to learn the unstable dynamics alone, which are typically
confined to much lower dimensional spaces than the high-dimensional state
spaces of all system dynamics and thus few data samples are sufficient to
identify them. Numerical experiments demonstrate that context-aware controller
inference learns stabilizing controllers from orders of magnitude fewer data
samples than traditional data-driven control techniques and variants of
reinforcement learning. The experiments further show that the low data
requirements of context-aware controller inference are especially beneficial in
data-scarce engineering problems with complex physics, for which learning
complete system dynamics is often intractable in terms of data and training
costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Thermodynamics-informed neural networks for physically realistic mixed
  reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.13414v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.13414v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quercus Hernández, Alberto Badías, Francisco Chinesta, Elías Cueto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The imminent impact of immersive technologies in society urges for active
research in real-time and interactive physics simulation for virtual worlds to
be realistic. In this context, realistic means to be compliant to the laws of
physics. In this paper we present a method for computing the dynamic response
of (possibly non-linear and dissipative) deformable objects induced by
real-time user interactions in mixed reality using deep learning. The
graph-based architecture of the method ensures the thermodynamic consistency of
the predictions, whereas the visualization pipeline allows a natural and
realistic user experience. Two examples of virtual solids interacting with
virtual or physical solids in mixed reality scenarios are provided to prove the
performance of the method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Skeleton Clustering: Dimension-Free Density-based Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.10770v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.10770v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Wei, Yen-Chi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a density-based clustering method called skeleton clustering
that can detect clusters in multivariate and even high-dimensional data with
irregular shapes. To bypass the curse of dimensionality, we propose surrogate
density measures that are less dependent on the dimension but have intuitive
geometric interpretations. The clustering framework constructs a concise
representation of the given data as an intermediate step and can be thought of
as a combination of prototype methods, density-based clustering, and
hierarchical clustering. We show by theoretical analysis and empirical studies
that the skeleton clustering leads to reliable clusters in multivariate and
high-dimensional scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Causal Structure Learning: Scoping <span class="highlight-title">Review</span> of Traditional and
  Deep Learning Algorithms and New Opportunities in Biomedicine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.07785v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.07785v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pulakesh Upadhyaya, Kai Zhang, Can Li, Xiaoqian Jiang, Yejin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal structure learning refers to a process of identifying causal
structures from observational data, and it can have multiple applications in
biomedicine and health care. This paper provides a practical review and
tutorial on scalable causal structure learning models with examples of
real-world data to help health care audiences understand and apply them. We
reviewed traditional (combinatorial and score-based methods) for causal
structure discovery and machine learning-based schemes. We also highlighted
recent developments in biomedicine where causal structure learning can be
applied to discover structures such as gene networks, brain connectivity
networks, and those in cancer epidemiology. We also compared the performance of
traditional and machine learning-based algorithms for causal discovery over
some benchmark data sets. Machine learning-based approaches, including deep
learning, have many advantages over traditional approaches, such as
scalability, including a greater number of variables, and potentially being
applied in a wide range of biomedical applications, such as genetics, if
sufficient data are available. Furthermore, these models are more flexible than
traditional models and are poised to positively affect many applications in the
future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybrid thermal modeling of additive manufacturing processes using
  physics-informed neural networks for temperature prediction and parameter
  identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.07756v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.07756v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuheng Liao, Tianju Xue, Jihoon Jeong, Samantha Webster, Kornel Ehmann, Jian Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the thermal behavior of additive manufacturing (AM) processes
is crucial for enhancing the quality control and enabling customized process
design. Most purely physics-based computational models suffer from intensive
computational costs and the need of calibrating unknown parameters, thus not
suitable for online control and iterative design application. Data-driven
models taking advantage of the latest developed computational tools can serve
as a more efficient surrogate, but they are usually trained over a large amount
of simulation data and often fail to effectively use small but high-quality
experimental data. In this work, we developed a hybrid physics-based
data-driven thermal modeling approach of AM processes using physics-informed
neural networks. Specifically, partially observed temperature data measured
from an infrared camera is combined with the physics laws to predict full-field
temperature history and to discover unknown material and process parameters. In
the numerical and experimental examples, the effectiveness of adding auxiliary
training data and using the pretrained model on training efficiency and
prediction accuracy, as well as the ability to identify unknown parameters with
partially observed data, are demonstrated. The results show that the hybrid
thermal model can effectively identify unknown parameters and capture the
full-field temperature accurately, and thus it has the potential to be used in
iterative process design and real-time process control of AM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Salient Sign Detection In Safe Autonomous Driving: AI Which Reasons Over
  Full Visual Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.05804v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.05804v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ross Greer, Akshay Gopalkrishnan, Nachiket Deo, Akshay Rangesh, Mohan Trivedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting road traffic signs and accurately determining how they can affect
the driver's future actions is a critical task for safe autonomous driving
systems. However, various traffic signs in a driving scene have an unequal
impact on the driver's decisions, making detecting the salient traffic signs a
more important task. Our research addresses this issue, constructing a traffic
sign detection model which emphasizes performance on salient signs, or signs
that influence the decisions of a driver. We define a traffic sign salience
property and use it to construct the LAVA Salient Signs Dataset, the first
traffic sign dataset that includes an annotated salience property. Next, we use
a custom salience loss function, Salience-Sensitive Focal Loss, to train a
Deformable DETR object detection model in order to emphasize stronger
performance on salient signs. Results show that a model trained with
Salience-Sensitive Focal Loss outperforms a model trained without, with regards
to recall of both salient signs and all signs combined. Further, the
performance margin on salient signs compared to all signs is largest for the
model trained with Salience-Sensitive Focal Loss.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simultaneously Learning Robust Audio Embeddings and balanced Hash codes
  for Query-by-Example 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11060v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11060v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anup Singh, Kris Demuynck, Vipul Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio fingerprinting systems must efficiently and robustly identify query
snippets in an extensive database. To this end, state-of-the-art systems use
deep learning to generate compact audio fingerprints. These systems deploy
indexing methods, which quantize fingerprints to hash codes in an unsupervised
manner to expedite the search. However, these methods generate imbalanced hash
codes, leading to their suboptimal performance. Therefore, we propose a
self-supervised learning framework to compute fingerprints and balanced hash
codes in an end-to-end manner to achieve both fast and accurate retrieval
performance. We model hash codes as a balanced clustering process, which we
regard as an instance of the optimal transport problem. Experimental results
indicate that the proposed approach improves retrieval efficiency while
preserving high accuracy, particularly at high distortion levels, compared to
the competing methods. Moreover, our system is efficient and scalable in
computational load and memory storage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We need to rewrite the subsection 'Efficiency' section under section
  4 to make it more easy to follow for the readers and appreciate our results</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reduced-Reference Quality Assessment of Point Clouds via
  Content-Oriented Saliency Projection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Zhou, Guanghui Yue, Ruizeng Zhang, Yipeng Qin, Hantao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many dense 3D point clouds have been exploited to represent visual objects
instead of traditional images or videos. To evaluate the perceptual quality of
various point clouds, in this letter, we propose a novel and efficient
Reduced-Reference quality metric for point clouds, which is based on
Content-oriented sAliency Projection (RR-CAP). Specifically, we make the first
attempt to simplify reference and distorted point clouds into projected
saliency maps with a downsampling operation. Through this process, we tackle
the issue of transmitting large-volume original point clouds to user-ends for
quality assessment. Then, motivated by the characteristics of the human visual
system (HVS), the objective quality scores of distorted point clouds are
produced by combining content-oriented similarity and statistical correlation
measurements. Finally, extensive experiments are conducted on SJTU-PCQA and WPC
databases. The experimental results demonstrate that our proposed algorithm
outperforms existing reduced-reference and no-reference quality metrics, and
significantly reduces the performance gap between state-of-the-art
full-reference quality assessment methods. In addition, we show the performance
variation of each proposed technical component by ablation tests.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sharp Eyes: A Salient Object Detector Working The Same Way as Human
  Visual Characteristics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Zhu, Jinbao Li, Yahong Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current methods aggregate multi-level features or introduce edge and skeleton
to get more refined saliency maps. However, little attention is paid to how to
obtain the complete salient object in cluttered background, where the targets
are usually similar in color and texture to the background. To handle this
complex scene, we propose a sharp eyes network (SENet) that first seperates the
object from scene, and then finely segments it, which is in line with human
visual characteristics, i.e., to look first and then focus. Different from
previous methods which directly integrate edge or skeleton to supplement the
defects of objects, the proposed method aims to utilize the expanded objects to
guide the network obtain complete prediction. Specifically, SENet mainly
consists of target separation (TS) brach and object segmentation (OS) branch
trained by minimizing a new hierarchical difference aware (HDA) loss. In the TS
branch, we construct a fractal structure to produce saliency features with
expanded boundary via the supervision of expanded ground truth, which can
enlarge the detail difference between foreground and background. In the OS
branch, we first aggregate multi-level features to adaptively select
complementary components, and then feed the saliency features with expanded
boundary into aggregated features to guide the network obtain complete
prediction. Moreover, we propose the HDA loss to further improve the structural
integrity and local details of the salient objects, which assigns weight to
each pixel according to its distance from the boundary hierarchically. Hard
pixels with similar appearance in border region will be given more attention
hierarchically to emphasize their importance in completeness prediction.
Comprehensive experimental results on five datasets demonstrate that the
proposed approach outperforms the state-of-the-art methods both quantitatively
and qualitatively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Metaverse from a Multimedia Communications Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiwei Dong, Jeannie S. A. Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  eXtended reality (XR) technologies such as virtual reality and 360{\deg}
stereoscopic streaming enable the concept of the Metaverse, an immersive
virtual space for collaboration and interaction. To ensure high fidelity
display of immersive media, the bandwidth, latency and network traffic patterns
will need to be considered to ensure a user's Quality of Experience (QoE). In
this article, examples and calculations are explored to demonstrate the
requirements of the abovementioned parameters. Additionally, future methods
such as network-awareness using reinforcement learning (RL) and XR content
awareness using spatial or temporal difference in the frames could be explored
from a multimedia communications perspective.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-01-17T00:00:00Z">2023-01-17</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the State of German (Abstractive) Text Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Aumiller, Jing Fan, Michael Gertz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With recent advancements in the area of Natural Language Processing, the
focus is slowly shifting from a purely English-centric view towards more
language-specific solutions, including German. Especially practical for
businesses to analyze their growing amount of textual data are text
summarization systems, which transform long input documents into compressed and
more digestible summary texts. In this work, we assess the particular landscape
of German abstractive text summarization and investigate the reasons why
practically useful solutions for abstractive text summarization are still
absent in industry. Our focus is two-fold, analyzing a) training resources, and
b) publicly available summarization systems. We are able to show that popular
existing datasets exhibit crucial flaws in their assumptions about the original
sources, which frequently leads to detrimental effects on system generalization
and evaluation biases. We confirm that for the most popular training dataset,
MLSUM, over 50% of the training set is unsuitable for abstractive summarization
purposes. Furthermore, available systems frequently fail to compare to simple
baselines, and ignore more effective and efficient extractive summarization
approaches. We attribute poor evaluation quality to a variety of different
factors, which are investigated in more detail in this work: A lack of
qualitative (and diverse) gold data considered for training, understudied (and
untreated) positional biases in some of the existing datasets, and the lack of
easily accessible and streamlined pre-processing strategies or analysis tools.
We provide a comprehensive assessment of available models on the cleaned
datasets, and find that this can lead to a reduction of more than 20 ROUGE-1
points during evaluation. The code for dataset filtering and reproducing
results can be found online at https://github.com/dennlinger/summaries
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 20th Conference on Database Systems for Business,
  Technology and Web (BTW'23)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Customized Visual Models with Retrieval-Augmented Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotian Liu, Kilho Son, Jianwei Yang, Ce Liu, Jianfeng Gao, Yong Jae Lee, Chunyuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-text contrastive learning models such as CLIP have demonstrated strong
task transfer ability. The high generality and usability of these visual models
is achieved via a web-scale data collection process to ensure broad concept
coverage, followed by expensive pre-training to feed all the knowledge into
model weights. Alternatively, we propose REACT, REtrieval-Augmented
CusTomization, a framework to acquire the relevant web knowledge to build
customized visual models for target domains. We retrieve the most relevant
image-text pairs (~3% of CLIP pre-training data) from the web-scale database as
external knowledge, and propose to customize the model by only training new
modualized blocks while freezing all the original weights. The effectiveness of
REACT is demonstrated via extensive experiments on classification, retrieval,
detection and segmentation tasks, including zero, few, and full-shot settings.
Particularly, on the zero-shot classification task, compared with CLIP, it
achieves up to 5.4% improvement on ImageNet and 3.7% on the ELEVATER benchmark
(20 datasets).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLIGEN: Open-Set Grounded Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, Yong Jae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale text-to-image diffusion models have made amazing advances.
However, the status quo is to use text input alone, which can impede
controllability. In this work, we propose GLIGEN, Grounded-Language-to-Image
Generation, a novel approach that builds upon and extends the functionality of
existing pre-trained text-to-image diffusion models by enabling them to also be
conditioned on grounding inputs. To preserve the vast concept knowledge of the
pre-trained model, we freeze all of its weights and inject the grounding
information into new trainable layers via a gated mechanism. Our model achieves
open-world grounded text2img generation with caption and bounding box condition
inputs, and the grounding ability generalizes well to novel spatial
configuration and concepts. GLIGEN's zero-shot performance on COCO and LVIS
outperforms that of existing supervised layout-to-image baselines by a large
margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision Learners Meet Web Image-Text Pairs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingchen Zhao, Quan Cui, Hao Wu, Osamu Yoshie, Cheng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most recent self-supervised learning~(SSL) methods are pre-trained on the
well-curated ImageNet-1K dataset. In this work, we consider SSL pre-training on
noisy web image-text paired data due to the excellent scalability of web data.
First, we conduct a benchmark study of representative SSL pre-training methods
on large-scale web data in a fair condition. Methods include single-modal ones
such as MAE and multi-modal ones such as CLIP. We observe that multi-modal
methods cannot outperform single-modal ones on vision transfer learning tasks.
We derive an information-theoretical view to explain the benchmarking results,
which provides insights into designing novel vision learners. Inspired by the
above explorations, we present a visual representation pre-training method,
MUlti-modal Generator~(MUG), for scalable web image-text data. MUG achieves
state-of-the-art transferring performances on a variety of tasks and shows
promising scaling behavior. Models and codes will be made public. Demo
available at https://huggingface.co/spaces/tennant/MUG_caption
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://bzhao.me/MUG/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MooseNet: A trainable metric for synthesized speech with plda backend 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ondřej Plátek, Ondřej Dušek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present MooseNet, a trainable speech metric that predicts listeners' Mean
Opinion Score (MOS). We report improvements to the challenge baselines using
easy-to-use modeling techniques, which also scales for larger self-supervised
learning (SSL) model. We present two models. The first model is a Neural
Network (NN). As a second model, we propose a PLDA generative model on the top
layers of the first NN model, which improves the pure NN model. Ensembles from
our two models achieve the top 3 or 4 VoiceMOS leaderboard places on all system
and utterance level metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Language Models Worse than Humans at Following <span class="highlight-title">Prompt</span>s? It's
  Complicated 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Albert Webson, Alyssa Marie Loo, Qinan Yu, Ellie Pavlick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompts have been the center of progress in advancing language models'
zero-shot and few-shot performance. However, recent work finds that models can
perform surprisingly well when given intentionally irrelevant or misleading
prompts. Such results may be interpreted as evidence that model behavior is not
"human like". In this study, we challenge a central assumption in such work:
that humans would perform badly when given pathological instructions. We find
that humans are able to reliably ignore irrelevant instructions and thus, like
models, perform well on the underlying task despite an apparent lack of signal
regarding the task they are being asked to do. However, when given deliberately
misleading instructions, humans follow the instructions faithfully, whereas
models do not. Thus, our conclusion is mixed with respect to prior work. We
argue against the earlier claim that high performance with irrelevant prompts
constitutes evidence against models' instruction understanding, but we
reinforce the claim that models' failure to follow misleading instructions
raises concerns. More broadly, we caution that future research should not
idealize human behaviors as a monolith and should not train or evaluate models
to mimic assumptions about these behaviors without first validating humans'
behaviors empirically.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>ing Large Language Model for Machine Translation: A Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biao Zhang, Barry Haddow, Alexandra Birch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on prompting has shown excellent performance with little or even no
supervised training across many tasks. However, prompting for machine
translation is still under-explored in the literature. We fill this gap by
offering a systematic study on prompting strategies for translation, examining
various factors for prompt template and demonstration example selection. We
further explore the use of monolingual data and the feasibility of
cross-lingual, cross-domain, and sentence-to-document transfer learning in
prompting. Extensive experiments with GLM-130B (Zeng et al., 2022) as the
testbed show that 1) the number and the quality of prompt examples matter,
where using suboptimal examples degenerates translation; 2) several features of
prompt examples, such as semantic similarity, show significant Spearman
correlation with their prompting performance; yet, none of the correlations are
strong enough; 3) using pseudo parallel prompt examples constructed from
monolingual data via zero-shot prompting could improve translation; and 4)
improved performance is achievable by transferring knowledge from prompt
examples selected in other settings. We finally provide an analysis on the
model outputs and discuss several problems that prompting still suffers from.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>s as Algorithms: Generalization and Implicit Model Selection
  in In-context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingcong Li, M. Emrullah Ildiz, Dimitris Papailiopoulos, Samet Oymak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) is a type of prompting where a transformer model
operates on a sequence of (input, output) examples and performs inference
on-the-fly. This implicit training is in contrast to explicitly tuning the
model weights based on examples. In this work, we formalize in-context learning
as an algorithm learning problem, treating the transformer model as a learning
algorithm that can be specialized via training to implement-at
inference-time-another target algorithm. We first explore the statistical
aspects of this abstraction through the lens of multitask learning: We obtain
generalization bounds for ICL when the input prompt is (1) a sequence of i.i.d.
(input, label) pairs or (2) a trajectory arising from a dynamical system. The
crux of our analysis is relating the excess risk to the stability of the
algorithm implemented by the transformer, which holds under mild assumptions.
Secondly, we use our abstraction to show that transformers can act as an
adaptive learning algorithm and perform model selection across different
hypothesis classes. We provide numerical evaluations that (1) demonstrate
transformers can indeed implement near-optimal algorithms on classical
regression problems with i.i.d. and dynamic data, (2) identify an inductive
bias phenomenon where the transfer risk on unseen tasks is independent of the
transformer complexity, and (3) empirically verify our theoretical predictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span> Based Implementation for Automatic Book Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07057v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07057v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddhant Porwal, Laxmi Bewoor, Vivek Deshpande
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document Summarization is the procedure of generating a meaningful and
concise summary of a given document with the inclusion of relevant and
topic-important points. There are two approaches: one is picking up the most
relevant statements from the document itself and adding it to the Summary known
as Extractive and the other is generating sentences for the Summary known as
Abstractive Summarization. Training a machine learning model to perform tasks
that are time-consuming or very difficult for humans to evaluate is a major
challenge. Book Abstract generation is one of such complex tasks. Traditional
machine learning models are getting modified with pre-trained transformers.
Transformer based language models trained in a self-supervised fashion are
gaining a lot of attention; when fine-tuned for Natural Language
Processing(NLP) downstream task like text summarization. This work is an
attempt to use Transformer based techniques for Abstract generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at - https://ijisae.org/index.php/IJISAE/article/view/2421</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Which Model Shall I Choose? Cost/Quality Trade-offs for Text
  Classification Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Zong, Josh Seltzer,  Jiahua,  Pan, Kathy Cheng, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Industry practitioners always face the problem of choosing the appropriate
model for deployment under different considerations, such as to maximize a
metric that is crucial for production, or to reduce the total cost given
financial concerns. In this work, we focus on the text classification task and
present a quantitative analysis for this challenge. Using classification
accuracy as the main metric, we evaluate the classifiers' performances for a
variety of models, including large language models, along with their associated
costs, including the annotation cost, training (fine-tuning) cost, and
inference cost. We then discuss the model choices for situations like having a
large number of samples needed for inference. We hope our work will help people
better understand the cost/quality trade-offs for the text classification task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Statistical analysis of word flow among five Indo-European languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josué Ely Molina, Jorge Flores, Carlos Gershenson, Carlos Pineda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A recent increase in data availability has allowed the possibility to perform
different statistical linguistic studies. Here we use the Google Books Ngram
dataset to analyze word flow among English, French, German, Italian, and
Spanish. We study what we define as ``migrant words'', a type of loanwords that
do not change their spelling. We quantify migrant words from one language to
another for different decades, and notice that most migrant words can be
aggregated in semantic fields and associated to historic events. We also study
the statistical properties of accumulated migrant words and their rank
dynamics. We propose a measure of use of migrant words that could be used as a
proxy of cultural influence. Our methodology is not exempt of caveats, but our
results are encouraging to promote further studies in this direction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Algorithms for Acyclic Weighted Finite-State Automata with Failure Arcs <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anej Svete, Benjamin Dayan, Tim Vieira, Ryan Cotterell, Jason Eisner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weighted finite-state automata (WSFAs) are commonly used in NLP. Failure
transitions are a useful extension for compactly representing backoffs or
interpolation in $n$-gram models and CRFs, which are special cases of WFSAs.
The pathsum in ordinary acyclic WFSAs is efficiently computed by the backward
algorithm in time $O(|E|)$, where $E$ is the set of transitions. However, this
does not allow failure transitions, and preprocessing the WFSA to eliminate
failure transitions could greatly increase $|E|$. We extend the backward
algorithm to handle failure transitions directly. Our approach is efficient
when the average state has outgoing arcs for only a small fraction $s \ll 1$ of
the alphabet $\Sigma$. We propose an algorithm for general acyclic WFSAs which
runs in $O{\left(|E| + s |\Sigma| |Q| T_\text{max} \log{|\Sigma|}\right)}$,
where $Q$ is the set of states and $T_\text{max}$ is the size of the largest
connected component of failure transitions. When the failure transition
topology satisfies a condition exemplified by CRFs, the $T_\text{max}$ factor
can be dropped, and when the weight semiring is a ring, the $\log{|\Sigma|}$
factor can be dropped. In the latter case (ring-weighted acyclic WFSAs), we
also give an alternative algorithm with complexity $\displaystyle O{\left(|E| +
|\Sigma| |Q| \min(1,s\pi_\text{max}) \right)}$, where $\pi_\text{max}$ is the
size of the longest failure path.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, Proceedings of EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Syntactically Robust Training on Partially-Observed Data for Open
  Information Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Qi, Yuxiang Chen, Lei Hou, Juanzi Li, Bin Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open Information Extraction models have shown promising results with
sufficient supervision. However, these models face a fundamental challenge that
the syntactic distribution of training data is partially observable in
comparison to the real world. In this paper, we propose a syntactically robust
training framework that enables models to be trained on a syntactic-abundant
distribution based on diverse paraphrase generation. To tackle the intrinsic
problem of knowledge deformation of paraphrasing, two algorithms based on
semantic similarity matching and syntactic tree walking are used to restore the
expressionally transformed knowledge. The training framework can be generally
applied to other syntactic partial observable domains. Based on the proposed
framework, we build a new evaluation set called CaRB-AutoPara, a syntactically
diverse dataset consistent with the real-world setting for validating the
robustness of the models. Experiments including a thorough analysis show that
the performance of the model degrades with the increase of the difference in
syntactic distribution, while our framework gives a robust boundary. The source
code is publicly available at https://github.com/qijimrc/RobustOIE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HanoiT: Enhancing Context-aware Translation via Selective Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Yang, Yuwei Yin, Shuming Ma, Liqun Yang, Hongcheng Guo, Haoyang Huang, Dongdong Zhang, Yutao Zeng, Zhoujun Li, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context-aware neural machine translation aims to use the document-level
context to improve translation quality. However, not all words in the context
are helpful. The irrelevant or trivial words may bring some noise and distract
the model from learning the relationship between the current sentence and the
auxiliary context. To mitigate this problem, we propose a novel end-to-end
encoder-decoder model with a layer-wise selection mechanism to sift and refine
the long document context. To verify the effectiveness of our method, extensive
experiments and extra quantitative analysis are conducted on four
document-level machine translation benchmarks. The experimental results
demonstrate that our model significantly outperforms previous models on all
datasets via the soft selection mechanism.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 2nd Swiss German Speech to Standard German Text Shared Task at SwissText
  2022 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michel Plüss, Yanick Schraner, Christian Scheller, Manfred Vogel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the results and findings of the 2nd Swiss German speech to
Standard German text shared task at SwissText 2022. Participants were asked to
build a sentence-level Swiss German speech to Standard German text system
specialized on the Grisons dialect. The objective was to maximize the BLEU
score on a test set of Grisons speech. 3 teams participated, with the
best-performing system achieving a BLEU score of 70.1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 pages, 0 figures, to appear in proceedings of SwissText 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Dynamics of Coordinated Online Behavior: Stability, Archetypes,
  and Influence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Serena Tardelli, Leonardo Nizzoli, Maurizio Tesconi, Mauro Conti, Preslav Nakov, Giovanni Da San Martino, Stefano Cresci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale online campaigns, malicious or otherwise, require a significant
degree of coordination among participants, which sparked interest in the study
of coordinated online behavior. State-of-the-art methods for detecting
coordinated behavior perform static analyses, disregarding the temporal
dynamics of coordination. Here, we carry out the first dynamic analysis of
coordinated behavior. To reach our goal we build a multiplex temporal network
and we perform dynamic community detection to identify groups of users that
exhibited coordinated behaviors in time. Thanks to our novel approach we find
that: (i) coordinated communities feature variable degrees of temporal
instability; (ii) dynamic analyses are needed to account for such instability,
and results of static analyses can be unreliable and scarcely representative of
unstable communities; (iii) some users exhibit distinct archetypal behaviors
that have important practical implications; (iv) content and network
characteristics contribute to explaining why users leave and join coordinated
communities. Our results demonstrate the advantages of dynamic analyses and
open up new directions of research on the unfolding of online debates, on the
strategies of coordinated communities, and on the patterns of online influence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Recent Advances in Automatic Term Extraction: A <span class="highlight-title">survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanh Thi Hong Tran, Matej Martinc, Jaya Caporusso, Antoine Doucet, Senja Pollak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic term extraction (ATE) is a Natural Language Processing (NLP) task
that eases the effort of manually identifying terms from domain-specific
corpora by providing a list of candidate terms. As units of knowledge in a
specific field of expertise, extracted terms are not only beneficial for
several terminographical tasks, but also support and improve several complex
downstream tasks, e.g., information retrieval, machine translation, topic
detection, and sentiment analysis. ATE systems, along with annotated datasets,
have been studied and developed widely for decades, but recently we observed a
surge in novel neural systems for the task at hand. Despite a large amount of
new research on ATE, systematic survey studies covering novel neural approaches
are lacking. We present a comprehensive survey of deep learning-based
approaches to ATE, with a focus on Transformer-based neural models. The study
also offers a comparison between these systems and previous ATE approaches,
which were based on feature engineering and non-neural supervised learning
algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages,4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tracing and Manipulating Intermediate Values in Neural Math Problem
  Solvers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuta Matsumoto, Benjamin Heinzerling, Masashi Yoshikawa, Kentaro Inui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How language models process complex input that requires multiple steps of
inference is not well understood. Previous research has shown that information
about intermediate values of these inputs can be extracted from the activations
of the models, but it is unclear where that information is encoded and whether
that information is indeed used during inference. We introduce a method for
analyzing how a Transformer model processes these inputs by focusing on simple
arithmetic problems and their intermediate values. To trace where information
about intermediate values is encoded, we measure the correlation between
intermediate values and the activations of the model using principal component
analysis (PCA). Then, we perform a causal intervention by manipulating model
weights. This intervention shows that the weights identified via tracing are
not merely correlated with intermediate values, but causally related to model
predictions. Our findings show that the model has a locality to certain
intermediate values, and this is useful for enhancing the interpretability of
the models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, MathNLP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">BERT</span>-ERC: Fine-tuning <span class="highlight-title">BERT</span> is Enough for Emotion Recognition in
  Conversation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Qin, Zhiyu Wu, Jinshi Cui, Tingting Zhang, Yanran Li, Jian Luan, Bin Wang, Li Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous works on emotion recognition in conversation (ERC) follow a two-step
paradigm, which can be summarized as first producing context-independent
features via fine-tuning pretrained language models (PLMs) and then analyzing
contextual information and dialogue structure information among the extracted
features. However, we discover that this paradigm has several limitations.
Accordingly, we propose a novel paradigm, i.e., exploring contextual
information and dialogue structure information in the fine-tuning step, and
adapting the PLM to the ERC task in terms of input text, classification
structure, and training strategy. Furthermore, we develop our model BERT-ERC
according to the proposed paradigm, which improves ERC performance in three
aspects, namely suggestive text, fine-grained classification module, and
two-stage training. Compared to existing methods, BERT-ERC achieves substantial
improvement on four datasets, indicating its effectiveness and generalization
capability. Besides, we also set up the limited resources scenario and the
online prediction scenario to approximate real-world scenarios. Extensive
experiments demonstrate that the proposed paradigm significantly outperforms
the previous one and can be adapted to various scenes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Syllable Subword Tokens for Open Vocabulary Speech Recognition in
  Malayalam 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kavya Manohar, A. R. Jayan, Rajeev Rajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a hybrid automatic speech recognition (ASR) system, a pronunciation
lexicon (PL) and a language model (LM) are essential to correctly retrieve
spoken word sequences. Being a morphologically complex language, the vocabulary
of Malayalam is so huge and it is impossible to build a PL and an LM that cover
all diverse word forms. Usage of subword tokens to build PL and LM, and
combining them to form words after decoding, enables the recovery of many out
of vocabulary words. In this work we investigate the impact of using syllables
as subword tokens instead of words in Malayalam ASR, and evaluate the relative
improvement in lexicon size, model memory requirement and word error rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two Stage Contextual Word Filtering for Context bias in Unified
  Streaming and Non-streaming Transducer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanheng Yang, Sining Sun, Xiong Wang, Yike Zhang, Long Ma, Lei Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is difficult for an end-to-end (E2E) ASR system to recognize words such as
named entities appearing infrequently in the training data. A widely used
method to mitigate this issue is feeding contextual information into the
acoustic model. A contextual word list is necessary, which lists all possible
contextual word candidates. Previous works have proven that the size and
quality of the list are crucial. A compact and accurate list can boost the
performance significantly. In this paper, we propose an efficient approach to
obtain a high quality contextual word list for a unified streaming and
non-streaming based Conformer-Transducer (C-T) model. Specifically, we make use
of the phone-level streaming output to first filter the predefined contextual
word list. During the subsequent non-streaming inference, the words in the
filtered list are regarded as contextual information fused into non-casual
encoder and decoder to generate the final recognition results. Our approach can
take advantage of streaming recognition hypothesis, improve the accuracy of the
contextual ASR system and speed up the inference process as well. Experiments
on two datasets demonstrates over 20% relative character error rate reduction
(CERR) comparing to the baseline system. Meanwile, the RTF of our system can be
stabilized within 0.15 when the size of the contextual word list grows over
6,000.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VaxxHesitancy: A <span class="highlight-title">Dataset</span> for Studying Hesitancy Towards COVID-19
  Vaccination on Twitter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yida Mu, Mali Jin, Charlie Grimshaw, Carolina Scarton, Kalina Bontcheva, Xingyi Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vaccine hesitancy has been a common concern, probably since vaccines were
created and, with the popularisation of social media, people started to express
their concerns about vaccines online alongside those posting pro- and
anti-vaccine content. Predictably, since the first mentions of a COVID-19
vaccine, social media users posted about their fears and concerns or about
their support and belief into the effectiveness of these rapidly developing
vaccines. Identifying and understanding the reasons behind public hesitancy
towards COVID-19 vaccines is important for policy markers that need to develop
actions to better inform the population with the aim of increasing vaccine
take-up. In the case of COVID-19, where the fast development of the vaccines
was mirrored closely by growth in anti-vaxx disinformation, automatic means of
detecting citizen attitudes towards vaccination became necessary. This is an
important computational social sciences task that requires data analysis in
order to gain in-depth understanding of the phenomena at hand. Annotated data
is also necessary for training data-driven models for more nuanced analysis of
attitudes towards vaccination. To this end, we created a new collection of over
3,101 tweets annotated with users' attitudes towards COVID-19 vaccination
(stance). Besides, we also develop a domain-specific language model (VaxxBERT)
that achieves the best predictive performance (73.0 accuracy and 69.3 F1-score)
as compared to a robust set of baselines. To the best of our knowledge, these
are the first dataset and model that model vaccine hesitancy as a category
distinct from pro- and anti-vaccine stance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Curriculum Script Distillation for Multilingual Visual Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07227v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07227v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khyathi Raghavi Chandu, Alborz Geramifard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained models with dual and cross encoders have shown remarkable success
in propelling the landscape of several tasks in vision and language in Visual
Question Answering (VQA). However, since they are limited by the requirements
of gold annotated data, most of these advancements do not see the light of day
in other languages beyond English. We aim to address this problem by
introducing a curriculum based on the source and target language translations
to finetune the pre-trained models for the downstream task. Experimental
results demonstrate that script plays a vital role in the performance of these
models. Specifically, we show that target languages that share the same script
perform better (~6%) than other languages and mixed-script code-switched
languages perform better than their counterparts (~5-12%).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning a Formality-Aware Japanese Sentence Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henry Li Xinyuan, Ray Lee, Jerry Chen, Kelly Marchisio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the way intermediate representations are generated in encoder-decoder
sequence-to-sequence models typically allow them to preserve the semantics of
the input sentence, input features such as formality might be left out. On the
other hand, downstream tasks such as translation would benefit from working
with a sentence representation that preserves formality in addition to
semantics, so as to generate sentences with the appropriate level of social
formality -- the difference between speaking to a friend versus speaking with a
supervisor. We propose a sequence-to-sequence method for learning a
formality-aware representation for Japanese sentences, where sentence
generation is conditioned on both the original representation of the input
sentence, and a side constraint which guides the sentence representation
towards preserving formality information. Additionally, we propose augmenting
the sentence representation with a learned representation of formality which
facilitates the extraction of formality in downstream tasks. We address the
lack of formality-annotated parallel data by adapting previous works on
procedural formality classification of Japanese sentences. Experimental results
suggest that our techniques not only helps the decoder recover the formality of
the input sentence, but also slightly improves the preservation of input
sentence semantics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Newsbridge -Telecom SudParis VoxCeleb Speaker Recognition Challenge
  2022 System Description 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yannis Tevissen, Jérôme Boudy, Frédéric Petitpont
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe the system used by our team for the VoxCeleb Speaker Recognition
Challenge 2022 (VoxSRC 2022) in the speaker diarization track. Our solution was
designed around a new combination of voice activity detection algorithms that
uses the strengths of several systems. We introduce a novel multi stream
approach with a decision protocol based on classifiers entropy. We called this
method a multi-stream voice activity detection and used it with standard
baseline diarization embeddings, clustering and resegmentation. With this work,
we successfully demonstrated that using a strong baseline and working only on
voice activity detection, one can achieved close to state-of-theart results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Vision-Language Models for Granular Market Change Prediction <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Wimmer, Navid Rekabsaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting future direction of stock markets using the historical data has
been a fundamental component in financial forecasting. This historical data
contains the information of a stock in each specific time span, such as the
opening, closing, lowest, and highest price. Leveraging this data, the future
direction of the market is commonly predicted using various time-series models
such as Long-Short Term Memory networks. This work proposes modeling and
predicting market movements with a fundamentally new approach, namely by
utilizing image and byte-based number representation of the stock data
processed with the recently introduced Vision-Language models. We conduct a
large set of experiments on the hourly stock data of the German share index and
evaluate various architectures on stock price prediction using historical stock
data. We conduct a comprehensive evaluation of the results with various metrics
to accurately depict the actual performance of various approaches. Our
evaluation results show that our novel approach based on representation of
stock data as text (bytes) and image significantly outperforms strong deep
learning-based baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Multimodal AI for Financial Forecasting Workshop (Muffin)
  at AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models as Corporate Lobbyists 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.01181v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.01181v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John J. Nay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate a proof-of-concept of a large language model conducting
corporate lobbying related activities. An autoregressive large language model
(OpenAI's text-davinci-003) determines if proposed U.S. Congressional bills are
relevant to specific public companies and provides explanations and confidence
levels. For the bills the model deems as relevant, the model drafts a letter to
the sponsor of the bill in an attempt to persuade the congressperson to make
changes to the proposed legislation. We use hundreds of novel ground-truth
labels of the relevance of a bill to a company to benchmark the performance of
the model, which outperforms the baseline of predicting the most common outcome
of irrelevance. We also benchmark the performance of the previous OpenAI GPT-3
model (text-davinci-002), which was the state-of-the-art model on many academic
natural language tasks until text-davinci-003 was recently released. The
performance of text-davinci-002 is worse than a simple benchmark. These results
suggest that, as large language models continue to exhibit improved natural
language understanding capabilities, performance on lobbying related tasks will
continue to improve. Longer-term, if AI begins to influence law in a manner
that is not a direct extension of human intentions, this threatens the critical
role that law as information could play in aligning AI with humans. Initially,
AI is being used to simply augment human lobbyists for a small portion of their
daily tasks. However, firms have an incentive to use less and less human
oversight over automated assessments of policy ideas and the written
communication to regulatory agencies and Congressional staffers. The core
question raised is where to draw the line between human-driven and AI-driven
policy influence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our open-source code available here:
  https://github.com/JohnNay/llm-lobbyist</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Dementia from Speech and Transcripts using <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.14769v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.14769v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Loukas Ilias, Dimitris Askounis, John Psarras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alzheimer's disease (AD) constitutes a neurodegenerative disease with serious
consequences to peoples' everyday lives, if it is not diagnosed early since
there is no available cure. Alzheimer's is the most common cause of dementia,
which constitutes a general term for loss of memory. Due to the fact that
dementia affects speech, existing research initiatives focus on detecting
dementia from spontaneous speech. However, little work has been done regarding
the conversion of speech data to Log-Mel spectrograms and Mel-frequency
cepstral coefficients (MFCCs) and the usage of pretrained models. Concurrently,
little work has been done in terms of both the usage of transformer networks
and the way the two modalities, i.e., speech and transcripts, are combined in a
single neural network. To address these limitations, first we represent speech
signal as an image and employ several pretrained models, with Vision
Transformer (ViT) achieving the highest evaluation results. Secondly, we
propose multimodal models. More specifically, our introduced models include
Gated Multimodal Unit in order to control the influence of each modality
towards the final classification and crossmodal attention so as to capture in
an effective way the relationships between the two modalities. Extensive
experiments conducted on the ADReSS Challenge dataset demonstrate the
effectiveness of the proposed models and their superiority over
state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Computer Speech & Language (Accepted)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Why do Nearest Neighbor Language Models Work? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.02828v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.02828v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frank F. Xu, Uri Alon, Graham Neubig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) compute the probability of a text by sequentially
computing a representation of an already-seen context and using this
representation to predict the next word. Currently, most LMs calculate these
representations through a neural network consuming the immediate previous
context. However recently, retrieval-augmented LMs have shown to improve over
standard neural LMs, by accessing information retrieved from a large datastore,
in addition to their standard, parametric, next-word prediction. In this paper,
we set out to understand why retrieval-augmented language models, and
specifically why k-nearest neighbor language models (kNN-LMs) perform better
than standard parametric LMs, even when the k-nearest neighbor component
retrieves examples from the same training set that the LM was originally
trained on. To this end, we perform a careful analysis of the various
dimensions over which kNN-LM diverges from standard LMs, and investigate these
dimensions one by one. Empirically, we identify three main reasons why kNN-LM
performs better than standard LMs: using a different input representation for
predicting the next tokens, approximate kNN search, and the importance of
softmax temperature for the kNN distribution. Further, we incorporate these
insights into the model architecture or the training procedure of the standard
parametric LM, improving its results without the need for an explicit retrieval
component. The code is available at https://github.com/frankxu2004/knnlm-why.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, 21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Vocal Fatigue with Neural Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.03428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.03428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian P. Bayerl, Dominik Wagner, Ilja Baumann, Korbinian Riedhammer, Tobias Bocklet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vocal fatigue refers to the feeling of tiredness and weakness of voice due to
extended utilization. This paper investigates the effectiveness of neural
embeddings for the detection of vocal fatigue. We compare x-vectors,
ECAPA-TDNN, and wav2vec 2.0 embeddings on a corpus of academic spoken English.
Low-dimensional mappings of the data reveal that neural embeddings capture
information about the change in vocal characteristics of a speaker during
prolonged voice usage. We show that vocal fatigue can be reliably predicted
using all three kinds of neural embeddings after only 50 minutes of continuous
speaking when temporal smoothing and normalization are applied to the extracted
embeddings. We employ support vector machines for classification and achieve
accuracy scores of 81% using x-vectors, 85% using ECAPA-TDNN embeddings, and
82% using wav2vec 2.0 embeddings as input features. We obtain an accuracy score
of 76%, when the trained system is applied to a different speaker and recording
environment without any adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for Publication in the Journal of Voice</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Generation Meets Real People: Building a Social, Informative
  Open-Domain Dialogue Agent <span class="chip">SIGDIAL '22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.12021v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.12021v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan A. Chi, Ashwin Paranjape, Abigail See, Caleb Chiam, Trenton Chang, Kathleen Kenealy, Swee Kiat Lim, Amelia Hardy, Chetanya Rastogi, Haojun Li, Alexander Iyabor, Yutong He, Hari Sowrirajan, Peng Qi, Kaushik Ram Sadagopan, Nguyet Minh Phu, Dilara Soylu, Jillian Tang, Avanika Narayan, Giovanni Campagna, Christopher D. Manning
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Chirpy Cardinal, an open-domain social chatbot. Aiming to be both
informative and conversational, our bot chats with users in an authentic,
emotionally intelligent way. By integrating controlled neural generation with
scaffolded, hand-written dialogue, we let both the user and bot take turns
driving the conversation, producing an engaging and socially fluent experience.
Deployed in the fourth iteration of the Alexa Prize Socialbot Grand Challenge,
Chirpy Cardinal handled thousands of conversations per day, placing second out
of nine bots with an average user rating of 3.58/5.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGDIAL '22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Word-Graph2vec: An efficient word embedding approach on word
  co-occurrence graph using random walk sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.04312v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.04312v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenting Li, Yuanzhe Cai, Jiahong Xue, Zeyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Word embedding has become ubiquitous and is widely used in various text
mining and natural language processing (NLP) tasks, such as information
retrieval, semantic analysis, and machine translation, among many others.
Unfortunately, it is prohibitively expensive to train the word embedding in a
relatively large corpus. We propose a graph-based word embedding algorithm,
called Word-Graph2vec, which converts the large corpus into a word
co-occurrence graph, then takes the word sequence samples from this graph by
randomly traveling and trains the word embedding on this sampling corpus in the
end. We posit that because of the stable vocabulary, relative idioms, and fixed
expressions in English, the size and density of the word co-occurrence graph
change slightly with the increase in the training corpus. So that
Word-Graph2vec has stable runtime on the large scale data set, and its
performance advantage becomes more and more obvious with the growth of the
training corpus. Extensive experiments conducted on real-world datasets show
that the proposed algorithm outperforms traditional Skip-Gram by four-five
times in terms of efficiency, while the error generated by the random walk
sampling is small.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topic Modelling of Swedish Newspaper Articles about Coronavirus: a Case
  Study using Latent Dirichlet Allocation Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.03029v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.03029v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernadeta Griciūtė, Lifeng Han, Goran Nenadic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic Modelling (TM) is from the research branches of natural language
understanding (NLU) and natural language processing (NLP) that is to facilitate
insightful analysis from large documents and datasets, such as a summarisation
of main topics and the topic changes. This kind of discovery is getting more
popular in real-life applications due to its impact on big data analytics. In
this study, from the social-media and healthcare domain, we apply popular
Latent Dirichlet Allocation (LDA) methods to model the topic changes in Swedish
newspaper articles about Coronavirus. We describe the corpus we created
including 6515 articles, methods applied, and statistics on topic changes over
approximately 1 year and two months period of time from 17th January 2020 to
13th March 2021. We hope this work can be an asset for grounding applications
of topic modelling and can be inspiring for similar case studies in an era with
pandemics, to support socio-economic impact research as well as clinical and
healthcare analytics. Our data and source code are openly available at
https://github. com/poethan/Swed_Covid_TM Keywords: Latent Dirichlet Allocation
(LDA); Topic Modelling; Coronavirus; Pandemics; Natural Language Understanding
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Named Tensor Notation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.13196v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.13196v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Chiang, Alexander M. Rush, Boaz Barak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a notation for tensors with named axes, which relieves the author,
reader, and future implementers of machine learning models from the burden of
keeping track of the order of axes and the purpose of each. The notation makes
it easy to lift operations on low-order tensors to higher order ones, for
example, from images to minibatches of images, or from an attention mechanism
to multiple attention heads.
  After a brief overview and formal definition of the notation, we illustrate
it through several examples from modern machine learning, from building blocks
like attention and convolution to full models like Transformers and LeNet. We
then discuss differential calculus in our notation and compare with some
alternative notations. Our proposals build on ideas from many previous papers
and software libraries. We hope that our notation will encourage more authors
to use named tensors, resulting in clearer papers and more precise
implementations.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Customized Visual Models with Retrieval-Augmented Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotian Liu, Kilho Son, Jianwei Yang, Ce Liu, Jianfeng Gao, Yong Jae Lee, Chunyuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-text contrastive learning models such as CLIP have demonstrated strong
task transfer ability. The high generality and usability of these visual models
is achieved via a web-scale data collection process to ensure broad concept
coverage, followed by expensive pre-training to feed all the knowledge into
model weights. Alternatively, we propose REACT, REtrieval-Augmented
CusTomization, a framework to acquire the relevant web knowledge to build
customized visual models for target domains. We retrieve the most relevant
image-text pairs (~3% of CLIP pre-training data) from the web-scale database as
external knowledge, and propose to customize the model by only training new
modualized blocks while freezing all the original weights. The effectiveness of
REACT is demonstrated via extensive experiments on classification, retrieval,
detection and segmentation tasks, including zero, few, and full-shot settings.
Particularly, on the zero-shot classification task, compared with CLIP, it
achieves up to 5.4% improvement on ImageNet and 3.7% on the ELEVATER benchmark
(20 datasets).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLIGEN: Open-Set Grounded Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, Yong Jae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale text-to-image diffusion models have made amazing advances.
However, the status quo is to use text input alone, which can impede
controllability. In this work, we propose GLIGEN, Grounded-Language-to-Image
Generation, a novel approach that builds upon and extends the functionality of
existing pre-trained text-to-image diffusion models by enabling them to also be
conditioned on grounding inputs. To preserve the vast concept knowledge of the
pre-trained model, we freeze all of its weights and inject the grounding
information into new trainable layers via a gated mechanism. Our model achieves
open-world grounded text2img generation with caption and bounding box condition
inputs, and the grounding ability generalizes well to novel spatial
configuration and concepts. GLIGEN's zero-shot performance on COCO and LVIS
outperforms that of existing supervised layout-to-image baselines by a large
margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision Learners Meet Web Image-Text Pairs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingchen Zhao, Quan Cui, Hao Wu, Osamu Yoshie, Cheng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most recent self-supervised learning~(SSL) methods are pre-trained on the
well-curated ImageNet-1K dataset. In this work, we consider SSL pre-training on
noisy web image-text paired data due to the excellent scalability of web data.
First, we conduct a benchmark study of representative SSL pre-training methods
on large-scale web data in a fair condition. Methods include single-modal ones
such as MAE and multi-modal ones such as CLIP. We observe that multi-modal
methods cannot outperform single-modal ones on vision transfer learning tasks.
We derive an information-theoretical view to explain the benchmarking results,
which provides insights into designing novel vision learners. Inspired by the
above explorations, we present a visual representation pre-training method,
MUlti-modal Generator~(MUG), for scalable web image-text data. MUG achieves
state-of-the-art transferring performances on a variety of tasks and shows
promising scaling behavior. Models and codes will be made public. Demo
available at https://huggingface.co/spaces/tennant/MUG_caption
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://bzhao.me/MUG/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SegViz: A Federated Learning Framework for Medical Image Segmentation
  from Distributed <span class="highlight-title">Dataset</span>s with Different and Incomplete Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adway U. Kanhere, Pranav Kulkarni, Paul H. Yi, Vishwa S. Parekh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmentation is one of the primary tasks in the application of deep learning
in medical imaging, owing to its multiple downstream clinical applications. As
a result, many large-scale segmentation datasets have been curated and released
for the segmentation of different anatomical structures. However, these
datasets focus on the segmentation of a subset of anatomical structures in the
body, therefore, training a model for each dataset would potentially result in
hundreds of models and thus limit their clinical translational utility.
Furthermore, many of these datasets share the same field of view but have
different subsets of annotations, thus making individual dataset annotations
incomplete. To that end, we developed SegViz, a federated learning framework
for aggregating knowledge from distributed medical image segmentation datasets
with different and incomplete annotations into a `global` meta-model. The
SegViz framework was trained to build a single model capable of segmenting both
liver and spleen aggregating knowledge from both these nodes by aggregating the
weights after every 10 epochs. The global SegViz model was tested on an
external dataset, Beyond the Cranial Vault (BTCV), comprising both liver and
spleen annotations using the dice similarity (DS) metric. The baseline
individual segmentation models for spleen and liver trained on their respective
datasets produced a DS score of 0.834 and 0.878 on the BTCV test set. In
comparison, the SegViz model produced comparable mean DS scores of 0.829 and
0.899 for the segmentation of the spleen and liver respectively. Our results
demonstrate SegViz as an essential first step towards training clinically
translatable multi-task segmentation models from distributed datasets with
disjoint incomplete annotations with excellent performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preserving Privacy in Surgical Video Analysis Using Artificial
  Intelligence: A Deep Learning Classifier to Identify Out-of-Body Scenes in
  Endoscopic Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joël L. Lavanchy, Armine Vardazaryan, Pietro Mascagni, AI4SafeChole Consortium, Didier Mutter, Nicolas Padoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: To develop and validate a deep learning model for the
identification of out-of-body images in endoscopic videos. Background: Surgical
video analysis facilitates education and research. However, video recordings of
endoscopic surgeries can contain privacy-sensitive information, especially if
out-of-body scenes are recorded. Therefore, identification of out-of-body
scenes in endoscopic videos is of major importance to preserve the privacy of
patients and operating room staff. Methods: A deep learning model was trained
and evaluated on an internal dataset of 12 different types of laparoscopic and
robotic surgeries. External validation was performed on two independent
multicentric test datasets of laparoscopic gastric bypass and cholecystectomy
surgeries. All images extracted from the video datasets were annotated as
inside or out-of-body. Model performance was evaluated compared to human ground
truth annotations measuring the receiver operating characteristic area under
the curve (ROC AUC). Results: The internal dataset consisting of 356,267 images
from 48 videos and the two multicentric test datasets consisting of 54,385 and
58,349 images from 10 and 20 videos, respectively, were annotated. Compared to
ground truth annotations, the model identified out-of-body images with 99.97%
ROC AUC on the internal test dataset. Mean $\pm$ standard deviation ROC AUC on
the multicentric gastric bypass dataset was 99.94$\pm$0.07% and 99.71$\pm$0.40%
on the multicentric cholecystectomy dataset, respectively. Conclusion: The
proposed deep learning model can reliably identify out-of-body images in
endoscopic videos. The trained model is publicly shared. This facilitates
privacy preservation in surgical video analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Jo\"el L. Lavanchy and Armine Vardazaryan contributed equally and
  share first co-authorship</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explain What You See: Open-Ended Segmentation and Recognition of
  Occluded 3D Objects <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        H. Ayoobi, H. Kasaei, M. Cao, R. Verbrugge, B. Verheij
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Local-HDP (for Local Hierarchical Dirichlet Process) is a hierarchical
Bayesian method that has recently been used for open-ended 3D object category
recognition. This method has been proven to be efficient in real-time robotic
applications. However, the method is not robust to a high degree of occlusion.
We address this limitation in two steps. First, we propose a novel semantic 3D
object-parts segmentation method that has the flexibility of Local-HDP. This
method is shown to be suitable for open-ended scenarios where the number of 3D
objects or object parts is not fixed and can grow over time. We show that the
proposed method has a higher percentage of mean intersection over union, using
a smaller number of learning instances. Second, we integrate this technique
with a recently introduced argumentation-based online incremental learning
method, thereby enabling the model to handle a high degree of occlusion. We
show that the resulting model produces an explicit set of explanations for the
3D object category recognition task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICRA 2023 Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Opti-CAM: Optimizing saliency maps for interpretability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanwei Zhang, Felipe Torres, Ronan Sicre, Yannis Avrithis, Stephane Ayache
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Methods based on class activation maps (CAM) provide a simple mechanism to
interpret predictions of convolutional neural networks by using linear
combinations of feature maps as saliency maps. By contrast, masking-based
methods optimize a saliency map directly in the image space or learn it by
training another network on additional data.
  In this work we introduce Opti-CAM, combining ideas from CAM-based and
masking-based approaches. Our saliency map is a linear combination of feature
maps, where weights are optimized per image such that the logit of the masked
image for a given class is maximized. We also fix a fundamental flaw in two of
the most common evaluation metrics of attribution methods. On several datasets,
Opti-CAM largely outperforms other CAM-based approaches according to the most
relevant classification metrics. We provide empirical evidence supporting that
localization and classifier interpretability are not necessarily aligned.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision Based Machine Learning Algorithms for Out-of-Distribution
  Generalisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamza Riaz, Alan F. Smeaton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are many computer vision applications including object segmentation,
classification, object detection, and reconstruction for which machine learning
(ML) shows state-of-the-art performance. Nowadays, we can build ML tools for
such applications with real-world accuracy. However, each tool works well
within the domain in which it has been trained and developed. Often, when we
train a model on a dataset in one specific domain and test on another unseen
domain known as an out of distribution (OOD) dataset, models or ML tools show a
decrease in performance. For instance, when we train a simple classifier on
real-world images and apply that model on the same classes but with a different
domain like cartoons, paintings or sketches then the performance of ML tools
disappoints. This presents serious challenges of domain generalisation (DG),
domain adaptation (DA), and domain shifting. To enhance the power of ML tools,
we can rebuild and retrain models from scratch or we can perform transfer
learning. In this paper, we present a comparison study between vision-based
technologies for domain-specific and domain-generalised methods. In this
research we highlight that simple convolutional neural network (CNN) based deep
learning methods perform poorly when they have to tackle domain shifting.
Experiments are conducted on two popular vision-based benchmarks, PACS and
Office-Home. We introduce an implementation pipeline for domain generalisation
methods and conventional deep learning models. The outcome confirms that
CNN-based deep learning models show poor generalisation compare to other
extensive methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Computing Conference, 22-23 June 2023, London, United Kingdom. 15
  pages, 5 Figures, 3 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long Range Pooling for 3D Large-Scale Scene Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06962v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06962v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang-Li Li, Meng-Hao Guo, Tai-Jiang Mu, Ralph R. Martin, Shi-Min Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the success of recent vision transformers and large kernel design
in convolutional neural networks (CNNs), in this paper, we analyze and explore
essential reasons for their success. We claim two factors that are critical for
3D large-scale scene understanding: a larger receptive field and operations
with greater non-linearity. The former is responsible for providing long range
contexts and the latter can enhance the capacity of the network. To achieve the
above properties, we propose a simple yet effective long range pooling (LRP)
module using dilation max pooling, which provides a network with a large
adaptive receptive field. LRP has few parameters, and can be readily added to
current CNNs. Also, based on LRP, we present an entire network architecture,
LRPNet, for 3D understanding. Ablation studies are presented to support our
claims, and show that the LRP module achieves better results than large kernel
convolution yet with reduced computation, due to its nonlinearity. We also
demonstrate the superiority of LRPNet on various benchmarks: LRPNet performs
the best on ScanNet and surpasses other CNN-based methods on S3DIS and
Matterport3D. Code will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixed Attention with Deep Supervision for Delineation of COVID Infection
  in Lung CT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pallabi Dutta, Sushmita Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The COVID-19 pandemic, with its multiple variants, has placed immense
pressure on the global healthcare system. An early effective screening and
grading become imperative towards optimizing the limited available resources of
the medical facilities. Computed tomography (CT) provides a significant
non-invasive screening mechanism for COVID-19 infection. An automated
segmentation of the infected volumes in lung CT is expected to significantly
aid in the diagnosis and care of patients. However, an accurate demarcation of
lesions remains problematic due to their irregular structure and location(s)
within the lung. A novel deep learning architecture, Mixed Attention Deeply
Supervised Network (MiADS-Net), is proposed for delineating the infected
regions of the lung from CT images. Incorporating dilated convolutions with
varying dilation rates, into a mixed attention framework, allows capture of
multi-scale features towards improved segmentation of lesions having different
sizes and textures. Mixed attention helps prioritise relevant feature maps to
be probed, along with those regions containing crucial information within these
maps. Deep supervision facilitates discovery of robust and discriminatory
characteristics in the hidden layers at shallower levels, while overcoming the
vanishing gradient. This is followed by estimating the severity of the disease,
based on the ratio of the area of infected region in each lung with respect to
its entire volume. Experimental results, on three publicly available datasets,
indicate that the MiADS-Net outperforms several state-of-the-art architectures
in the COVID-19 lesion segmentation task; particularly in defining structures
involving complex geometries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Masked Visual Reconstruction in Language Semantic Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06958v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06958v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shusheng Yang, Yixiao Ge, Kun Yi, Dian Li, Ying Shan, Xiaohu Qie, Xinggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Both masked image modeling (MIM) and natural language supervision have
facilitated the progress of transferable visual pre-training. In this work, we
seek the synergy between two paradigms and study the emerging properties when
MIM meets natural language supervision. To this end, we present a novel masked
visual Reconstruction In Language semantic Space (RILS) pre-training framework,
in which sentence representations, encoded by the text encoder, serve as
prototypes to transform the vision-only signals into patch-sentence
probabilities as semantically meaningful MIM reconstruction targets. The vision
models can therefore capture useful components with structured information by
predicting proper semantic of masked tokens. Better visual representations
could, in turn, improve the text encoder via the image-text alignment
objective, which is essential for the effective MIM target transformation.
Extensive experimental results demonstrate that our method not only enjoys the
best of previous MIM and CLIP but also achieves further improvements on various
tasks due to their mutual benefits. RILS exhibits advanced transferability on
downstream classification, detection, and segmentation, especially for low-shot
regimes. Code will be made available at https://github.com/hustvl/RILS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DR-WLC: Dimensionality Reduction cognition for object detection and pose
  estimation by Watching, Learning and Checking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Gao, Xi Xu, Tianji Jiang, Siyuan Chen, Yi Yang, Yufeng Yue, Mengyin Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection and pose estimation are difficult tasks in robotics and
autonomous driving. Existing object detection and pose estimation methods
mostly adopt the same-dimensional data for training. For example, 2D object
detection usually requires a large amount of 2D annotation data with high cost.
Using high-dimensional information to supervise lower-dimensional tasks is a
feasible way to reduce datasets size. In this work, the DR-WLC, a
dimensionality reduction cognitive model, which can perform both object
detection and pose estimation tasks at the same time is proposed. The model
only requires 3D model of objects and unlabeled environment images (with or
without objects) to finish the training. In addition, a bounding boxes
generation strategy is also proposed to build the relationship between 3D model
and 2D object detection task. Experiments show that our method can qualify the
work without any manual annotations and it is easy to deploy for practical
applications. Source code is at https://github.com/IN2-ViAUn/DR-WLC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-supervised</span> Domain Adaptation for Breaking the Limits of Low-quality
  Fundus Image Quality Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingshan Hou, Peng Cao, Jiaqi Wang, Xiaoli Liu, Jinzhu Yang, Osmar R. Zaiane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retinal fundus images have been applied for the diagnosis and screening of
eye diseases, such as Diabetic Retinopathy (DR) or Diabetic Macular Edema
(DME). However, both low-quality fundus images and style inconsistency
potentially increase uncertainty in the diagnosis of fundus disease and even
lead to misdiagnosis by ophthalmologists. Most of the existing image
enhancement methods mainly focus on improving the image quality by leveraging
the guidance of high-quality images, which is difficult to be collected in
medical applications. In this paper, we tackle image quality enhancement in a
fully unsupervised setting, i.e., neither paired images nor high-quality
images. To this end, we explore the potential of the self-supervised task for
improving the quality of fundus images without the requirement of high-quality
reference images. Specifically, we construct multiple patch-wise domains via an
auxiliary pre-trained quality assessment network and a style clustering. To
achieve robust low-quality image enhancement and address style inconsistency,
we formulate two self-supervised domain adaptation tasks to disentangle the
features of image content, low-quality factor and style information by
exploring intrinsic supervision signals within the low-quality images.
Extensive experiments are conducted on EyeQ and Messidor datasets, and results
show that our DASQE method achieves new state-of-the-art performance when only
low-quality images are available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BSNet: Lane Detection via Draw B-spline Curves Nearby 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxin Chen, Mengmeng Wang, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Curve-based methods are one of the classic lane detection methods. They learn
the holistic representation of lane lines, which is intuitive and concise.
However, their performance lags behind the recent state-of-the-art methods due
to the limitation of their lane representation and optimization. In this paper,
we revisit the curve-based lane detection methods from the perspectives of the
lane representations' globality and locality. The globality of lane
representation is the ability to complete invisible parts of lanes with visible
parts. The locality of lane representation is the ability to modify lanes
locally which can simplify parameter optimization. Specifically, we first
propose to exploit the b-spline curve to fit lane lines since it meets the
locality and globality. Second, we design a simple yet efficient network BSNet
to ensure the acquisition of global and local features. Third, we propose a new
curve distance to make the lane detection optimization objective more
reasonable and alleviate ill-conditioned problems. The proposed methods achieve
state-of-the-art performance on the Tusimple, CULane, and LLAMAS datasets,
which dramatically improved the accuracy of curve-based methods in the lane
detection task while running far beyond real-time (197FPS).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cooperation Learning Enhanced Colonic Polyp Segmentation Based on
  <span class="highlight-title">Transformer</span>-CNN Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanyuan Wang, Zhaohong Deng, Qiongdan Lou, Shudong Hu, Kup-sze Choi, Shitong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional segmentation methods for colonic polyps are mainly designed based
on low-level features. They could not accurately extract the location of small
colonic polyps. Although the existing deep learning methods can improve the
segmentation accuracy, their effects are still unsatisfied. To meet the above
challenges, we propose a hybrid network called Fusion-Transformer-HardNetMSEG
(i.e., Fu-TransHNet) in this study. Fu-TransHNet uses deep learning of
different mechanisms to fuse each other and is enhanced with multi-view
collaborative learning techniques. Firstly, the Fu-TransHNet utilizes the
Transformer branch and the CNN branch to realize the global feature learning
and local feature learning, respectively. Secondly, a fusion module is designed
to integrate the features from two branches. The fusion module consists of two
parts: 1) the Global-Local Feature Fusion (GLFF) part and 2) the Dense Fusion
of Multi-scale features (DFM) part. The former is built to compensate the
feature information mission from two branches at the same scale; the latter is
constructed to enhance the feature representation. Thirdly, the above two
branches and fusion modules utilize multi-view cooperative learning techniques
to obtain their respective weights that denote their importance and then make a
final decision comprehensively. Experimental results showed that the
Fu-TransHNet network was superior to the existing methods on five widely used
benchmark datasets. In particular, on the ETIS-LaribPolypDB dataset containing
many small-target colonic polyps, the mDice obtained by Fu-TransHNet were 12.4%
and 6.2% higher than the state-of-the-art methods HardNet-MSEG and TransFuse-s,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been submitted to a journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Biometric Fuzzy Vault based on Face and Fingerprints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06882v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06882v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Rathgeb, Benjamin Tams, Johannes Merkle, Vanessa Nesterowicz, Ulrike Korte, Matthias Neu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fuzzy vault scheme has been established as cryptographic primitive
suitable for privacy-preserving biometric authentication. To improve accuracy
and privacy protection, biometric information of multiple characteristics can
be fused at feature level prior to locking it in a fuzzy vault. We construct a
multi-biometric fuzzy vault based on face and multiple fingerprints. On a
multi-biometric database constructed from the FRGCv2 face and the MCYT-100
fingerprint databases, a perfect recognition accuracy is achieved at a false
accept security above 30 bits. Further, we provide a formalisation of
feature-level fusion in multi-biometric fuzzy vaults, on the basis of which
relevant security issues are elaborated. Said security issues, for which we
define countermeasures, are commonly ignored and may impair the overall
system's security.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Methods of Multi-label Prediction Classifiers for Hyperspectral
  Remote Sensing Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06874v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06874v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salma Haidar, José Oramas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With their combined spectral depth and geometric resolution, hyperspectral
remote sensing images embed a wealth of complex, non-linear information that
challenges traditional computer vision techniques. Yet, deep learning methods
known for their representation learning capabilities prove more suitable for
handling such complexities. Unlike applications that focus on single-label,
pixel-level classification methods for hyperspectral remote sensing images, we
propose a multi-label, patch-level classification method based on a
two-component deep-learning network. We use patches of reduced spatial
dimension and a complete spectral depth extracted from the remote sensing
images. Additionally, we investigate three training schemes for our network:
Iterative, Joint, and Cascade. Experiments suggest that the Joint scheme is the
best-performing scheme; however, its application requires an expensive search
for the best weight combination of the loss constituents. The Iterative scheme
enables the sharing of features between the two parts of the network at the
early stages of training. It performs better on complex data with multi-labels.
Further experiments showed that methods designed with different architectures
performed well when trained on patches extracted and labeled according to our
sampling method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Denoising Diffusion Probabilistic Models as a Defense against
  Adversarial Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06871v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06871v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lars Lien Ankile, Anna Midgley, Sebastian Weisshaar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Networks are infamously sensitive to small perturbations in their
inputs, making them vulnerable to adversarial attacks. This project evaluates
the performance of Denoising Diffusion Probabilistic Models (DDPM) as a
purification technique to defend against adversarial attacks. This works by
adding noise to an adversarial example before removing it through the reverse
process of the diffusion model. We evaluate the approach on the PatchCamelyon
data set for histopathologic scans of lymph node sections and find an
improvement of the robust accuracy by up to 88\% of the original model's
accuracy, constituting a considerable improvement over the vanilla model and
our baselines. The project code is located at
https://github.com/ankile/Adversarial-Diffusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAT: Size-Aware <span class="highlight-title">Transformer</span> for 3D Point Cloud Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Zhou, Yongping Xiong, Chinwai Chiu, Fangyu Liu, Xiangyang Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer models have achieved promising performances in point cloud
segmentation. However, most existing attention schemes provide the same feature
learning paradigm for all points equally and overlook the enormous difference
in size among scene objects. In this paper, we propose the Size-Aware
Transformer (SAT) that can tailor effective receptive fields for objects of
different sizes. Our SAT achieves size-aware learning via two steps: introduce
multi-scale features to each attention layer and allow each point to choose its
attentive fields adaptively. It contains two key designs: the Multi-Granularity
Attention (MGA) scheme and the Re-Attention module. The MGA addresses two
challenges: efficiently aggregating tokens from distant areas and preserving
multi-scale features within one attention layer. Specifically, point-voxel
cross attention is proposed to address the first challenge, and the shunted
strategy based on the standard multi-head self attention is applied to solve
the second. The Re-Attention module dynamically adjusts the attention scores to
the fine- and coarse-grained features output by MGA for each point. Extensive
experimental results demonstrate that SAT achieves state-of-the-art
performances on S3DIS and ScanNetV2 datasets. Our SAT also achieves the most
balanced performance on categories among all referred methods, which
illustrates the superiority of modelling categories of different sizes. Our
code and model will be released after the acceptance of this paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building Scalable Video Understanding Benchmarks through Sports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06866v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06866v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniket Agarwal, Alex Zhang, Karthik Narasimhan, Igor Gilitschenski, Vishvak Murahari, Yash Kant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing benchmarks for evaluating long video understanding falls short on
multiple aspects, either lacking in scale or quality of annotations. These
limitations arise from the difficulty in collecting dense annotations for long
videos (e.g. actions, dialogues, etc.), which are often obtained by manually
labeling many frames per second. In this work, we introduce an automated
Annotation and Video Stream Alignment Pipeline (abbreviated ASAP). We
demonstrate the generality of ASAP by aligning unlabeled videos of four
different sports (Cricket, Football, Basketball, and American Football) with
their corresponding dense annotations (i.e. commentary) freely available on the
web. Our human studies indicate that ASAP can align videos and annotations with
high fidelity, precision, and speed. We then leverage ASAP scalability to
create LCric, a large-scale long video understanding benchmark, with over 1000
hours of densely annotated long Cricket videos (with an average sample length
of 50 mins) collected at virtually zero annotation cost. We benchmark and
analyze state-of-the-art video understanding models on LCric through a large
set of compositional multi-choice and regression queries. We establish a human
baseline that indicates significant room for new research to explore.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Event-based Shape from Polarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manasi Muglikar, Leonard Bauersfeld, Diederik Paul Moeys, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art solutions for Shape-from-Polarization (SfP) suffer from a
speed-resolution tradeoff: they either sacrifice the number of polarization
angles measured or necessitate lengthy acquisition times due to framerate
constraints, thus compromising either accuracy or latency. We tackle this
tradeoff using event cameras. Event cameras operate at microseconds resolution
with negligible motion blur, and output a continuous stream of events that
precisely measures how light changes over time asynchronously. We propose a
setup that consists of a linear polarizer rotating at high-speeds in front of
an event camera. Our method uses the continuous event stream caused by the
rotation to reconstruct relative intensities at multiple polarizer angles.
Experiments demonstrate that our method outperforms physics-based baselines
using frames, reducing the MAE by 25% in synthetic and real-world dataset. In
the real world, we observe, however, that the challenging conditions (i.e.,
when few events are generated) harm the performance of physics-based solutions.
To overcome this, we propose a learning-based approach that learns to estimate
surface normals even at low event-rates, improving the physics-based approach
by 52% on the real world dataset. The proposed system achieves an acquisition
speed equivalent to 50 fps (>twice the framerate of the commercial polarization
sensor) while retaining the spatial resolution of 1MP. Our evaluation is based
on the first large-scale dataset for event-based SfP
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ USER: Unified Semantic Enhancement with Momentum Contrast for Image-Text
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Zhang, Zhong Ji, Di Wang, Yanwei Pang, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a fundamental and challenging task in bridging language and vision
domains, Image-Text Retrieval (ITR) aims at searching for the target instances
that are semantically relevant to the given query from the other modality, and
its key challenge is to measure the semantic similarity across different
modalities. Although significant progress has been achieved, existing
approaches typically suffer from two major limitations: (1) It hurts the
accuracy of the representation by directly exploiting the bottom-up attention
based region-level features where each region is equally treated. (2) It limits
the scale of negative sample pairs by employing the mini-batch based end-to-end
training mechanism. To address these limitations, we propose a Unified Semantic
Enhancement Momentum Contrastive Learning (USER) method for ITR. Specifically,
we delicately design two simple but effective Global representation based
Semantic Enhancement (GSE) modules. One learns the global representation via
the self-attention algorithm, noted as Self-Guided Enhancement (SGE) module.
The other module benefits from the pre-trained CLIP module, which provides a
novel scheme to exploit and transfer the knowledge from an off-the-shelf model,
noted as CLIP-Guided Enhancement (CGE) module. Moreover, we incorporate the
training mechanism of MoCo into ITR, in which two dynamic queues are employed
to enrich and enlarge the scale of negative sample pairs. Meanwhile, a Unified
Training Objective (UTO) is developed to learn from mini-batch based and
dynamic queue based samples. Extensive experiments on the benchmark MSCOCO and
Flickr30K datasets demonstrate the superiority of both retrieval accuracy and
inference efficiency. Our source code will be released at
https://github.com/zhangy0822/USER.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Acute ischemic stroke lesion segmentation in non-contrast CT images
  using 3D convolutional neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06793v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06793v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. V. Dobshik, S. K. Verbitskiy, I. A. Pestunov, K. M. Sherman, Yu. N. Sinyavskiy, A. A. Tulupov, V. B. Berikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, an automatic algorithm aimed at volumetric segmentation of
acute ischemic stroke lesion in non-contrast computed tomography brain 3D
images is proposed. Our deep-learning approach is based on the popular 3D U-Net
convolutional neural network architecture, which was modified by adding the
squeeze-and-excitation blocks and residual connections. Robust pre-processing
methods were implemented to improve the segmentation accuracy. Moreover, a
specific patches sampling strategy was used to address the large size of
medical images, to smooth out the effect of the class imbalance problem and to
stabilize neural network training. All experiments were performed using
five-fold cross-validation on the dataset containing non-contrast computed
tomography volumetric brain scans of 81 patients diagnosed with acute ischemic
stroke. Two radiology experts manually segmented images independently and then
verified the labeling results for inconsistencies. The quantitative results of
the proposed algorithm and obtained segmentation were measured by the Dice
similarity coefficient, sensitivity, specificity and precision metrics. Our
proposed model achieves an average Dice of $0.628\pm0.033$, sensitivity of
$0.699\pm0.039$, specificity of $0.9965\pm0.0016$ and precision of
$0.619\pm0.036$, showing promising segmentation results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 4 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Large-Scale Outdoor Multi-modal <span class="highlight-title">Dataset</span> and Benchmark for Novel View
  Synthesis and Implicit Scene Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chongshan Lu, Fukun Yin, Xin Chen, Tao Chen, Gang YU, Jiayuan Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRF) has achieved impressive results in single
object scene reconstruction and novel view synthesis, which have been
demonstrated on many single modality and single object focused indoor scene
datasets like DTU, BMVS, and NeRF Synthetic.However, the study of NeRF on
large-scale outdoor scene reconstruction is still limited, as there is no
unified outdoor scene dataset for large-scale NeRF evaluation due to expensive
data acquisition and calibration costs. In this paper, we propose a large-scale
outdoor multi-modal dataset, OMMO dataset, containing complex land objects and
scenes with calibrated images, point clouds and prompt annotations. Meanwhile,
a new benchmark for several outdoor NeRF-based tasks is established, such as
novel view synthesis, surface reconstruction, and multi-modal NeRF. To create
the dataset, we capture and collect a large number of real fly-view videos and
select high-quality and high-resolution clips from them. Then we design a
quality review module to refine images, remove low-quality frames and
fail-to-calibrate scenes through a learning-based automatic evaluation plus
manual review. Finally, a number of volunteers are employed to add the text
descriptions for each scene and key-frame to meet the potential multi-modal
requirements in the future. Compared with existing NeRF datasets, our dataset
contains abundant real-world urban and natural scenes with various scales,
camera trajectories, and lighting conditions. Experiments show that our dataset
can benchmark most state-of-the-art NeRF methods on different tasks. We will
release the dataset and model weights very soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Face Inverse Rendering via Hierarchical Decoupling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Wang, Xiaojie Guo, Wenjing Dai, Jiawan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous face inverse rendering methods often require synthetic data with
ground truth and/or professional equipment like a lighting stage. However, a
model trained on synthetic data or using pre-defined lighting priors is
typically unable to generalize well for real-world situations, due to the gap
between synthetic data/lighting priors and real data. Furthermore, for common
users, the professional equipment and skill make the task expensive and
complex. In this paper, we propose a deep learning framework to disentangle
face images in the wild into their corresponding albedo, normal, and lighting
components. Specifically, a decomposition network is built with a hierarchical
subdivision strategy, which takes image pairs captured from arbitrary
viewpoints as input. In this way, our approach can greatly mitigate the
pressure from data preparation, and significantly broaden the applicability of
face inverse rendering. Extensive experiments are conducted to demonstrate the
efficacy of our design, and show its superior performance in face relighting
over other state-of-the-art alternatives. {Our code is available at
\url{https://github.com/AutoHDR/HD-Net.git}}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bag of States: A Non-sequential Approach to Video-based Engagement
  Measurement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Abedi, Chinchu Thomas, Dinesh Babu Jayagopi, Shehroz S. Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic measurement of student engagement provides helpful information for
instructors to meet learning program objectives and individualize program
delivery. Students' behavioral and emotional states need to be analyzed at
fine-grained time scales in order to measure their level of engagement. Many
existing approaches have developed sequential and spatiotemporal models, such
as recurrent neural networks, temporal convolutional networks, and
three-dimensional convolutional neural networks, for measuring student
engagement from videos. These models are trained to incorporate the order of
behavioral and emotional states of students into video analysis and output
their level of engagement. In this paper, backed by educational psychology, we
question the necessity of modeling the order of behavioral and emotional states
of students in measuring their engagement. We develop bag-of-words-based models
in which only the occurrence of behavioral and emotional states of students is
modeled and analyzed and not the order in which they occur. Behavioral and
affective features are extracted from videos and analyzed by the proposed
models to determine the level of engagement in an ordinal-output classification
setting. Compared to the existing sequential and spatiotemporal approaches for
engagement measurement, the proposed non-sequential approach improves the
state-of-the-art results. According to experimental results, our method
significantly improved engagement level classification accuracy on the IIITB
Online SE dataset by 26% compared to sequential models and achieved engagement
level classification accuracy as high as 66.58% on the DAiSEE student
engagement dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FemtoDet: An Object Detection Baseline for Energy Versus Performance
  Tradeoffs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Tu, Xu Xie, Ming Ling, Min Yang, Guo AI, Yawen Huang, Yefeng Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient detectors for edge devices are often optimized for metrics like
parameters or speed counts, which remain weak correlation with the energy of
detectors. However, among vision applications of convolutional neural networks
(CNNs), some, such as always-on surveillance cameras, are critical for energy
constraints. This paper aims to serve as a baseline by designing detectors to
reach tradeoffs between energy and performance from two perspectives: 1) We
extensively analyze various CNNs to identify low-energy architectures,
including the selection of activation functions, convolutions operators, and
feature fusion structures on necks. These underappreciated details in past
works seriously affect the energy consumption of detectors; 2) To break through
the dilemmatic energy-performance problem, we propose a balanced detector
driven by energy using discovered low-energy components named
\textit{FemtoDet}. In addition to the novel construction, we further improve
FemtoDet by considering convolutions and training strategy optimizations.
Specifically, we develop a new instance boundary enhancement (IBE) module for
convolution optimization to overcome the contradiction between the limited
capacity of CNNs and detection tasks in diverse spatial representations, and
propose a recursive warm-restart (RecWR) for optimizing training strategy to
escape the sub-optimization of light-weight detectors, considering the data
shift produced in popular augmentations. As a result, FemtoDet with only 68.77k
parameters achieves a competitive score of 46.3 AP50 on PASCAL VOC and power of
7.83W on RTX 3090. Extensive experiments on COCO and TJU-DHD datasets indicate
that the proposed method achieves competitive results in diverse scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SwinDepth: Unsupervised Depth Estimation using Monocular Sequences via
  Swin <span class="highlight-title">Transformer</span> and Densely Cascaded Network <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongseok Shim, H. Jin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular depth estimation plays a critical role in various computer vision
and robotics applications such as localization, mapping, and 3D object
detection. Recently, learning-based algorithms achieve huge success in depth
estimation by training models with a large amount of data in a supervised
manner. However, it is challenging to acquire dense ground truth depth labels
for supervised training, and the unsupervised depth estimation using monocular
sequences emerges as a promising alternative. Unfortunately, most studies on
unsupervised depth estimation explore loss functions or occlusion masks, and
there is little change in model architecture in that ConvNet-based
encoder-decoder structure becomes a de-facto standard for depth estimation. In
this paper, we employ a convolution-free Swin Transformer as an image feature
extractor so that the network can capture both local geometric features and
global semantic features for depth estimation. Also, we propose a Densely
Cascaded Multi-scale Network (DCMNet) that connects every feature map directly
with another from different scales via a top-down cascade pathway. This densely
cascaded connectivity reinforces the interconnection between decoding layers
and produces high-quality multi-scale depth outputs. The experiments on two
different datasets, KITTI and Make3D, demonstrate that our proposed method
outperforms existing state-of-the-art unsupervised algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio2Gestures: Generating Diverse Gestures from Audio 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Li, Di Kang, Wenjie Pei, Xuefei Zhe, Ying Zhang, Linchao Bao, Zhenyu He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  People may perform diverse gestures affected by various mental and physical
factors when speaking the same sentences. This inherent one-to-many
relationship makes co-speech gesture generation from audio particularly
challenging. Conventional CNNs/RNNs assume one-to-one mapping, and thus tend to
predict the average of all possible target motions, easily resulting in
plain/boring motions during inference. So we propose to explicitly model the
one-to-many audio-to-motion mapping by splitting the cross-modal latent code
into shared code and motion-specific code. The shared code is expected to be
responsible for the motion component that is more correlated to the audio while
the motion-specific code is expected to capture diverse motion information that
is more independent of the audio. However, splitting the latent code into two
parts poses extra training difficulties. Several crucial training
losses/strategies, including relaxed motion loss, bicycle constraint, and
diversity loss, are designed to better train the VAE.
  Experiments on both 3D and 2D motion datasets verify that our method
generates more realistic and diverse motions than previous state-of-the-art
methods, quantitatively and qualitatively. Besides, our formulation is
compatible with discrete cosine transformation (DCT) modeling and other popular
backbones (\textit{i.e.} RNN, Transformer). As for motion losses and
quantitative motion evaluation, we find structured losses/metrics
(\textit{e.g.} STFT) that consider temporal and/or spatial context complement
the most commonly used point-wise losses (\textit{e.g.} PCK), resulting in
better motion dynamics and more nuanced motion details. Finally, we demonstrate
that our method can be readily used to generate motion sequences with
user-specified motion clips on the timeline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2108.06720</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distribution Aligned Feature Clustering for Zero-Shot Sketch-Based Image
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Wu, Kun Song, Fangzheng Zhao, Jiansheng Chen, Huimin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR) is a challenging cross-modal
retrieval task. In prior arts, the retrieval is conducted by sorting the
distance between the query sketch and each image in the gallery. However, the
domain gap and the zero-shot setting make neural networks hard to generalize.
This paper tackles the challenges from a new perspective: utilizing gallery
image features. We propose a Cluster-then-Retrieve (ClusterRetri) method that
performs clustering on the gallery images and uses the cluster centroids as
proxies for retrieval. Furthermore, a distribution alignment loss is proposed
to align the image and sketch features with a common Gaussian distribution,
reducing the domain gap. Despite its simplicity, our proposed method
outperforms the state-of-the-art methods by a large margin on popular datasets,
e.g., up to 31% and 39% relative improvement of mAP@all on the Sketchy and
TU-Berlin datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Surgical Aggregation: A Federated Learning Framework for Harmonizing
  Distributed <span class="highlight-title">Dataset</span>s with Diverse Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Kulkarni, Adway Kanhere, Paul H. Yi, Vishwa S. Parekh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-assisted characterization of chest x-rays (CXR) has the potential to
provide substantial benefits across many clinical applications. Many
large-scale public CXR datasets have been curated for detection of
abnormalities using deep learning. However, each of these datasets focus on
detecting a subset of disease labels that could be present in a CXR, thus
limiting their clinical utility. Furthermore, the distributed nature of these
datasets, along with data sharing regulations, make it difficult to share and
create a complete representation of disease labels. We propose surgical
aggregation, a federated learning framework for aggregating knowledge from
distributed datasets with different disease labels into a 'global' deep
learning model. We randomly divided the NIH Chest X-Ray 14 dataset into
training (70%), validation (10%), and test (20%) splits with no patient overlap
and conducted two experiments. In the first experiment, we pruned the disease
labels to create two 'toy' datasets containing 11 and 8 labels respectively
with 4 overlapping labels. For the second experiment, we pruned the disease
labels to create two disjoint 'toy' datasets with 7 labels each. We observed
that the surgically aggregated 'global' model resulted in excellent performance
across both experiments when compared to a 'baseline' model trained on complete
disease labels. The overlapping and disjoint experiments had an AUROC of 0.87
and 0.86 respectively, compared to the baseline AUROC of 0.87. We used surgical
aggregation to harmonize the NIH Chest X-Ray 14 and CheXpert datasets into a
'global' model with an AUROC of 0.85 and 0.83 respectively. Our results show
that surgical aggregation could be used to develop clinically useful deep
learning models by aggregating knowledge from distributed datasets with diverse
tasks, a step forward towards bridging the gap from bench to bedside.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures, 5 tables, submitted to MIDL 2023 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-domain Unsupervised Reconstruction with Equivariance for
  Photoacoustic Computed Tomography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengrong Lan, Lijie Huang, Liming Nie, Jianwen Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate image reconstruction is crucial for photoacoustic (PA) computed
tomography (PACT). Recently, deep learning has been used to reconstruct the PA
image with a supervised scheme, which requires high-quality images as ground
truth labels. In practice, there are inevitable trade-offs between cost and
performance since the use of more channels is an expensive strategy to access
more measurements. Here, we propose a cross-domain unsupervised reconstruction
(CDUR) strategy with a pure transformer model, which overcomes the lack of
ground truth labels from limited PA measurements. The proposed approach
exploits the equivariance of PACT to achieve high performance with a smaller
number of channels. We implement a self-supervised reconstruction in a
model-based form. Meanwhile, we also leverage the self-supervision to enforce
the measurement and image consistency on three partitions of measured PA data,
by randomly masking different channels. We find that dynamically masking a high
proportion of the channels, e.g., 80%, yields nontrivial self-supervisors in
both image and signal domains, which decrease the multiplicity of the pseudo
solution to efficiently reconstruct the image from fewer PA measurements with
minimum error of the image. Experimental results on in-vivo PACT dataset of
mice demonstrate the potential of our unsupervised framework. In addition, our
method shows a high performance (0.83 structural similarity index (SSIM) in the
extreme sparse case with 13 channels), which is close to that of supervised
scheme (0.77 SSIM with 16 channels). On top of all the advantages, our method
may be deployed on different trainable models in an end-to-end manner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DIGITOUR: Automatic Digital Tours for Real-Estate Properties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prateek Chhikara, Harshul Kuhar, Anil Goyal, Chirag Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A virtual or digital tour is a form of virtual reality technology which
allows a user to experience a specific location remotely. Currently, these
virtual tours are created by following a 2-step strategy. First, a photographer
clicks a 360 degree equirectangular image; then, a team of annotators manually
links these images for the "walkthrough" user experience. The major challenge
in the mass adoption of virtual tours is the time and cost involved in manual
annotation/linking of images. Therefore, this paper presents an end-to-end
pipeline to automate the generation of 3D virtual tours using equirectangular
images for real-estate properties. We propose a novel HSV-based coloring scheme
for paper tags that need to be placed at different locations before clicking
the equirectangular images using 360 degree cameras. These tags have two
characteristics: i) they are numbered to help the photographer for placement of
tags in sequence and; ii) bi-colored, which allows better learning of tag
detection (using YOLOv5 architecture) in an image and digit recognition (using
custom MobileNet architecture) tasks. Finally, we link/connect all the
equirectangular images based on detected tags. We show the efficiency of the
proposed pipeline on a real-world equirectangular image dataset collected from
the Housing.com database.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at CODS-COMAD '23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Lightweight Salient Object Detection via Network Depth-Width
  Tradeoff 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Li, Shengye Qiao, Zhirui Zhao, Chenxi Xie, Xiaowu Chen, Changqun Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing salient object detection methods often adopt deeper and wider
networks for better performance, resulting in heavy computational burden and
slow inference speed. This inspires us to rethink saliency detection to achieve
a favorable balance between efficiency and accuracy. To this end, we design a
lightweight framework while maintaining satisfying competitive accuracy.
Specifically, we propose a novel trilateral decoder framework by decoupling the
U-shape structure into three complementary branches, which are devised to
confront the dilution of semantic context, loss of spatial structure and
absence of boundary detail, respectively. Along with the fusion of three
branches, the coarse segmentation results are gradually refined in structure
details and boundary quality. Without adding additional learnable parameters,
we further propose Scale-Adaptive Pooling Module to obtain multi-scale
receptive filed. In particular, on the premise of inheriting this framework, we
rethink the relationship among accuracy, parameters and speed via network
depth-width tradeoff. With these insightful considerations, we comprehensively
design shallower and narrower models to explore the maximum potential of
lightweight SOD. Our models are purposed for different application
environments: 1) a tiny version CTD-S (1.7M, 125FPS) for resource constrained
devices, 2) a fast version CTD-M (12.6M, 158FPS) for speed-demanding scenarios,
3) a standard version CTD-L (26.5M, 84FPS) for high-performance platforms.
Extensive experiments validate the superiority of our method, which achieves
better efficiency-accuracy balance across five benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature-based Image Matching for Identifying Individual Kākā 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fintan O'Sullivan, Kirita-Rose Escott, Rachael Shaw, Andrew Lensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report investigates an unsupervised, feature-based image matching
pipeline for the novel application of identifying individual k\=ak\=a. Applied
with a similarity network for clustering, this addresses a weakness of current
supervised approaches to identifying individual birds which struggle to handle
the introduction of new individuals to the population. Our approach uses object
localisation to locate k\=ak\=a within images and then extracts local features
that are invariant to rotation and scale. These features are matched between
images with nearest neighbour matching techniques and mismatch removal to
produce a similarity score for image match comparison. The results show that
matches obtained via the image matching pipeline achieve high accuracy of true
matches. We conclude that feature-based image matching could be used with a
similarity network to provide a viable alternative to existing supervised
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, honour's report from Victoria University of Wellington</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Artificial intelligence as a gateway to scientific discovery: Uncovering
  features in retinal fundus images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parsa Delavari, Gulcenur Ozturan, Ozgur Yilmaz, Ipek Oruc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: Convolutional neural networks can be trained to detect various
conditions or patient traits based on retinal fundus photographs, some of
which, such as the patient sex, are invisible to the expert human eye. Here we
propose a methodology for explainable classification of fundus images to
uncover the mechanism(s) by which CNNs successfully predict the labels. We used
patient sex as a case study to validate our proposed methodology.
  Approach: First, we used a set of 4746 fundus images, including training,
validation and test partitions, to fine-tune a pre-trained CNN on the sex
classification task. Next, we utilized deep learning explainability tools to
hypothesize possible ways sex differences in the retina manifest. We measured
numerous retinal properties relevant to our hypotheses through image
segmentation to identify those significantly different between males and
females. To tackle the multiple comparisons problem, we shortlisted the
parameters by testing them on a set of 100 fundus images distinct from the
images used for fine-tuning. Finally, we used an additional 400 images, not
included in any previous set, to reveal significant sex differences in the
retina.
  Results: We observed that the peripapillary area is darker in males compared
to females ($p=.023, d=.243$). We also observed that males have richer retinal
vasculature networks by showing a higher number of branches ($p=.016, d=.272$)
and nodes ($p=.014, d=.299$) and a larger total length of branches ($p=.045,
d=.206$) in the vessel graph. Also, vessels cover a greater area in the
superior temporal quadrant of the retina in males compared to females
($p=0.048, d=.194$).
  Conclusions: Our methodology reveals retinal features in fundus photographs
that allow CNNs to predict traits currently unknown, but meaningful to experts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi Kernel Positional Embedding ConvNeXt for Polyp Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trong-Hieu Nguyen Mau, Quoc-Huy Trinh, Nhat-Tan Bui, Minh-Triet Tran, Hai-Dang Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation is the technique that helps doctor view and has a
precise diagnosis, particularly in Colorectal Cancer. Specifically, with the
increase in cases, the diagnosis and identification need to be faster and more
accurate for many patients; in endoscopic images, the segmentation task has
been vital to helping the doctor identify the position of the polyps or the
ache in the system correctly. As a result, many efforts have been made to apply
deep learning to automate polyp segmentation, mostly to ameliorate the U-shape
structure. However, the simple skip connection scheme in UNet leads to
deficient context information and the semantic gap between feature maps from
the encoder and decoder. To deal with this problem, we propose a novel
framework composed of ConvNeXt backbone and Multi Kernel Positional Embedding
block. Thanks to the suggested module, our method can attain better accuracy
and generalization in the polyps segmentation task. Extensive experiments show
that our model achieves the Dice coefficient of 0.8818 and the IOU score of
0.8163 on the Kvasir-SEG dataset. Furthermore, on various datasets, we make
competitive achievement results with other previous state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Free Lunch for Generating Effective Outlier Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sen Pei, Jiaxi Sun, Richard Yi Da Xu, Bin Fan, Shiming Xiang, Gaofeng Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When deployed in practical applications, computer vision systems will
encounter numerous unexpected images (\emph{{i.e.}}, out-of-distribution data).
Due to the potentially raised safety risks, these aforementioned unseen data
should be carefully identified and handled. Generally, existing approaches in
dealing with out-of-distribution (OOD) detection mainly focus on the
statistical difference between the features of OOD and in-distribution (ID)
data extracted by the classifiers. Although many of these schemes have brought
considerable performance improvements, reducing the false positive rate (FPR)
when processing open-set images, they necessarily lack reliable theoretical
analysis and generalization guarantees. Unlike the observed ways, in this
paper, we investigate the OOD detection problem based on the Bayes rule and
present a convincing description of the reason for failures encountered by
conventional classifiers. Concretely, our analysis reveals that refining the
probability distribution yielded by the vanilla neural networks is necessary
for OOD detection, alleviating the issues of assigning high confidence to OOD
data. To achieve this effortlessly, we propose an ultra-effective method to
generate near-realistic outlier supervision. Extensive experiments on
large-scale benchmarks reveal that our proposed \texttt{BayesAug} significantly
reduces the FPR95 over 12.50\% compared with the previous schemes, boosting the
reliability of machine learning systems. The code will be made publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>pre-print</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ YeLan: Event Camera-Based 3D Human Pose Estimation for
  Technology-Mediated Dancing in Challenging Environments with Comprehensive
  Motion-to-Event Simulator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongyang Zhang, Kaidong Chai, Haowen Yu, Ramzi Majaj, Francesca Walsh, Edward Wang, Upal Mahbub, Hava Siegelmann, Donghyun Kim, Tauhidur Rahman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a beloved sport worldwide, dancing is getting integrated into traditional
and virtual reality-based gaming platforms nowadays. It opens up new
opportunities in the technology-mediated dancing space. These platforms
primarily rely on passive and continuous human pose estimation as an input
capture mechanism. Existing solutions are mainly based on RGB or RGB-Depth
cameras for dance games. The former suffers in low-lighting conditions due to
the motion blur and low sensitivity, while the latter is too power-hungry, has
a low frame rate, and has limited working distance. With ultra-low latency,
energy efficiency, and wide dynamic range characteristics, the event camera is
a promising solution to overcome these shortcomings. We propose YeLan, an event
camera-based 3-dimensional human pose estimation(HPE) system that survives
low-lighting and dynamic background contents. We collected the world's first
event camera dance dataset and developed a fully customizable motion-to-event
physics-aware simulator. YeLan outperforms the baseline models in these
challenging conditions and demonstrated robustness against different types of
clothing, background motion, viewing angle, occlusion, and lighting
fluctuations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCARP: 3D Shape Completion in ARbitrary Poses for Improved Grasping <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bipasha Sen, Aditya Agarwal, Gaurav Singh, Brojeshwar B., Srinath Sridhar, Madhava Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recovering full 3D shapes from partial observations is a challenging task
that has been extensively addressed in the computer vision community. Many deep
learning methods tackle this problem by training 3D shape generation networks
to learn a prior over the full 3D shapes. In this training regime, the methods
expect the inputs to be in a fixed canonical form, without which they fail to
learn a valid prior over the 3D shapes. We propose SCARP, a model that performs
Shape Completion in ARbitrary Poses. Given a partial pointcloud of an object,
SCARP learns a disentangled feature representation of pose and shape by relying
on rotationally equivariant pose features and geometric shape features trained
using a multi-tasking objective. Unlike existing methods that depend on an
external canonicalization, SCARP performs canonicalization, pose estimation,
and shape completion in a single network, improving the performance by 45% over
the existing baselines. In this work, we use SCARP for improving grasp
proposals on tabletop objects. By completing partial tabletop objects directly
in their observed poses, SCARP enables a SOTA grasp proposal network improve
their proposals by 71.2% on partial shapes. Project page:
https://bipashasen.github.io/scarp
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robotic Navigation Autonomy for Subretinal Injection via Intelligent
  Real-Time Virtual iOCT Volume Slicing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shervin Dehghani, Michael Sommersperger, Peiyao Zhang, Alejandro Martin-Gomez, Benjamin Busam, Peter Gehlbach, Nassir Navab, M. Ali Nasseri, Iulian Iordachita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the last decade, various robotic platforms have been introduced that could
support delicate retinal surgeries. Concurrently, to provide semantic
understanding of the surgical area, recent advances have enabled
microscope-integrated intraoperative Optical Coherent Tomography (iOCT) with
high-resolution 3D imaging at near video rate. The combination of robotics and
semantic understanding enables task autonomy in robotic retinal surgery, such
as for subretinal injection. This procedure requires precise needle insertion
for best treatment outcomes. However, merging robotic systems with iOCT
introduces new challenges. These include, but are not limited to high demands
on data processing rates and dynamic registration of these systems during the
procedure. In this work, we propose a framework for autonomous robotic
navigation for subretinal injection, based on intelligent real-time processing
of iOCT volumes. Our method consists of an instrument pose estimation method,
an online registration between the robotic and the iOCT system, and trajectory
planning tailored for navigation to an injection target. We also introduce
intelligent virtual B-scans, a volume slicing approach for rapid instrument
pose estimation, which is enabled by Convolutional Neural Networks (CNNs). Our
experiments on ex-vivo porcine eyes demonstrate the precision and repeatability
of the method. Finally, we discuss identified challenges in this work and
suggest potential solutions to further the development of such systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Large Text-to-Image Models with Structured <span class="highlight-title">Prompt</span>s for Skin
  Disease Identification: A Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sajith Rajapaksa, Jean Marie Uwabeza Vianney, Renell Castro, Farzad Khalvati, Shubhra Aich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the potential usage of large text-to-image (LTI)
models for the automated diagnosis of a few skin conditions with rarity or a
serious lack of annotated datasets. As the input to the LTI model, we provide
the targeted instantiation of a generic but succinct prompt structure designed
upon careful observations of the conditional narratives from the standard
medical textbooks. In this regard, we pave the path to utilizing accessible
textbook descriptions for automated diagnosis of conditions with data scarcity
through the lens of LTI models. Experiments show the efficacy of the proposed
framework, including much better localization of the infected regions.
Moreover, it has the immense possibility for generalization across the medical
sub-domains, not only to mitigate the data scarcity issue but also to debias
automated diagnostics from the all-pervasive racial biases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embodied Agents for Efficient Exploration and Smart Scene Description <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roberto Bigazzi, Marcella Cornia, Silvia Cascianelli, Lorenzo Baraldi, Rita Cucchiara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of embodied agents that can communicate with humans in
natural language has gained increasing interest over the last years, as it
facilitates the diffusion of robotic platforms in human-populated environments.
As a step towards this objective, in this work, we tackle a setting for visual
navigation in which an autonomous agent needs to explore and map an unseen
indoor environment while portraying interesting scenes with natural language
descriptions. To this end, we propose and evaluate an approach that combines
recent advances in visual robotic exploration and image captioning on images
generated through agent-environment interaction. Our approach can generate
smart scene descriptions that maximize semantic knowledge of the environment
and avoid repetitions. Further, such descriptions offer user-understandable
insights into the robot's representation of the environment by highlighting the
prominent objects and the correlation between them as encountered during the
exploration. To quantitatively assess the performance of the proposed approach,
we also devise a specific score that takes into account both exploration and
description skills. The experiments carried out on both photorealistic
simulated environments and real-world ones demonstrate that our approach can
effectively describe the robot's point of view during exploration, improving
the human-friendly interpretability of its observations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE International Conference on Robotics and Automation
  (ICRA 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COVINS-G: A Generic Back-end for Collaborative Visual-Inertial SLAM <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manthan Patel, Marco Karrer, Philipp Bänninger, Margarita Chli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative SLAM is at the core of perception in multi-robot systems as it
enables the co-localization of the team of robots in a common reference frame,
which is of vital importance for any coordination amongst them. The paradigm of
a centralized architecture is well established, with the robots (i.e. agents)
running Visual-Inertial Odometry (VIO) onboard while communicating relevant
data, such as e.g. Keyframes (KFs), to a central back-end (i.e. server), which
then merges and optimizes the joint maps of the agents. While these frameworks
have proven to be successful, their capability and performance are highly
dependent on the choice of the VIO front-end, thus limiting their flexibility.
In this work, we present COVINS-G, a generalized back-end building upon the
COVINS framework, enabling the compatibility of the server-back-end with any
arbitrary VIO front-end, including, for example, off-the-shelf cameras with
odometry capabilities, such as the Realsense T265. The COVINS-G back-end
deploys a multi-camera relative pose estimation algorithm for computing the
loop-closure constraints allowing the system to work purely on 2D image data.
In the experimental evaluation, we show on-par accuracy with state-of-the-art
multi-session and collaborative SLAM systems, while demonstrating the
flexibility and generality of our approach by employing different front-ends
onboard collaborating agents within the same mission. The COVINS-G codebase
along with a generalized front-end wrapper to allow any existing VIO front-end
to be readily used in combination with the proposed collaborative back-end is
open-sourced. Video: https://youtu.be/FoJfXCfaYDw
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6+1 Pages, 5 Figures, 2 Tables, Accepted at ICRA 2023, London</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Keypoint-GraspNet: Keypoint-based 6-DoF Grasp Generation from the
  Monocular RGB-D input <span class="chip">ICRA2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.08752v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.08752v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiye Chen, Yunzhi Lin, Patricio Vela
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Great success has been achieved in the 6-DoF grasp learning from the point
cloud input, yet the computational cost due to the point set orderlessness
remains a concern. Alternatively, we explore the grasp generation from the
RGB-D input in this paper. The proposed solution, Keypoint-GraspNet, detects
the projection of the gripper keypoints in the image space and then recover the
SE(3) poses with a PnP algorithm. A synthetic dataset based on the primitive
shape and the grasp family is constructed to examine our idea. Metric-based
evaluation reveals that our method outperforms the baselines in terms of the
grasp proposal accuracy, diversity, and the time cost. Finally, robot
experiments show high success rate, demonstrating the potential of the idea in
the real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthetic <span class="highlight-title">Dataset</span> Generation for Privacy-Preserving Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.03205v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.03205v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Efstathia Soufleri, Gobinda Saha, Kaushik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Learning (ML) has achieved enormous success in solving a variety of
problems in computer vision, speech recognition, object detection, to name a
few. The principal reason for this success is the availability of huge datasets
for training deep neural networks (DNNs). However, datasets cannot be publicly
released if they contain sensitive information such as medical records, and
data privacy becomes a major concern. Encryption methods could be a possible
solution, however their deployment on ML applications seriously impacts
classification accuracy and results in substantial computational overhead.
Alternatively, obfuscation techniques could be used, but maintaining a good
trade-off between visual privacy and accuracy is challenging. In this paper, we
propose a method to generate secure synthetic datasets from the original
private datasets. Given a network with Batch Normalization (BN) layers
pretrained on the original dataset, we first record the class-wise BN layer
statistics. Next, we generate the synthetic dataset by optimizing random noise
such that the synthetic data match the layer-wise statistical distribution of
original images. We evaluate our method on image classification datasets
(CIFAR10, ImageNet) and show that synthetic data can be used in place of the
original CIFAR10/ImageNet data for training networks from scratch, producing
comparable classification performance. Further, to analyze visual privacy
provided by our method, we use Image Quality Metrics and show high degree of
visual dissimilarity between the original and synthetic images. Moreover, we
show that our proposed method preserves data-privacy under various
privacy-leakage attacks including Gradient Matching Attack, Model Memorization
Attack, and GAN-based Attack.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>There was a bug in the code. An updated version will be archived soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compositional Visual Generation with Composable Diffusion Models <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.01714v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.01714v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, Joshua B. Tenenbaum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large text-guided diffusion models, such as DALLE-2, are able to generate
stunning photorealistic images given natural language descriptions. While such
models are highly flexible, they struggle to understand the composition of
certain concepts, such as confusing the attributes of different objects or
relations between objects. In this paper, we propose an alternative structured
approach for compositional generation using diffusion models. An image is
generated by composing a set of diffusion models, with each of them modeling a
certain component of the image. To do this, we interpret diffusion models as
energy-based models in which the data distributions defined by the energy
functions may be explicitly combined. The proposed method can generate scenes
at test time that are substantially more complex than those seen in training,
composing sentence descriptions, object relations, human facial attributes, and
even generalizing to new combinations that are rarely seen in the real world.
We further illustrate how our approach may be used to compose pre-trained
text-guided diffusion models and generate photorealistic images containing all
the details described in the input descriptions, including the binding of
certain object attributes that have been shown difficult for DALLE-2. These
results point to the effectiveness of the proposed method in promoting
structured generalization for visual generation. Project page:
https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022. First three authors contributed equally. Project website:
  https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Approaching Peak Ground Truth 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00243v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00243v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Kofler, Johannes Wahle, Ivan Ezhov, Sophia Wagner, Rami Al-Maskari, Emilia Gryska, Mihail Todorov, Christina Bukas, Felix Meissen, Tingying Peng, Ali Ertürk, Daniel Rueckert, Rolf Heckemann, Jan Kirschke, Claus Zimmer, Benedikt Wiestler, Bjoern Menze, Marie Piraud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models are typically evaluated by computing similarity with
reference annotations and trained by maximizing similarity with such.
Especially in the bio-medical domain, annotations are subjective and suffer
from low inter- and intra-rater reliability. Since annotations only reflect the
annotation entity's interpretation of the real world, this can lead to
sub-optimal predictions even though the model achieves high similarity scores.
Here, the theoretical concept of Peak Ground Truth (PGT) is introduced. PGT
marks the point beyond which an increase in similarity with the reference
annotation stops translating to better Real World Model Performance (RWMP).
Additionally, a quantitative technique to approximate PGT by computing inter-
and intra-rater reliability is proposed. Finally, three categories of PGT-aware
strategies to evaluate and improve model performance are reviewed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7pages, 2 figures (this updates just affiliations and corrects figure
  rendering)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Skip-Attention: Improving Vision <span class="highlight-title">Transformer</span>s by Paying Less Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.02240v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.02240v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashanka Venkataramanan, Amir Ghodrati, Yuki M. Asano, Fatih Porikli, Amirhossein Habibian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work aims to improve the efficiency of vision transformers (ViT). While
ViTs use computationally expensive self-attention operations in every layer, we
identify that these operations are highly correlated across layers -- a key
redundancy that causes unnecessary computations. Based on this observation, we
propose SkipAt, a method to reuse self-attention computation from preceding
layers to approximate attention at one or more subsequent layers. To ensure
that reusing self-attention blocks across layers does not degrade the
performance, we introduce a simple parametric function, which outperforms the
baseline transformer's performance while running computationally faster. We
show the effectiveness of our method in image classification and
self-supervised learning on ImageNet-1K, semantic segmentation on ADE20K, image
denoising on SIDD, and video denoising on DAVIS. We achieve improved throughput
at the same-or-higher accuracy levels in all these tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CurveFormer: 3D Lane Detection by Curve Propagation with Curve Queries
  and Attention <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.07989v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.07989v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifeng Bai, Zhirong Chen, Zhangjie Fu, Lang Peng, Pengpeng Liang, Erkang Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D lane detection is an integral part of autonomous driving systems. Previous
CNN and Transformer-based methods usually first generate a bird's-eye-view
(BEV) feature map from the front view image, and then use a sub-network with
BEV feature map as input to predict 3D lanes. Such approaches require an
explicit view transformation between BEV and front view, which itself is still
a challenging problem. In this paper, we propose CurveFormer, a single-stage
Transformer-based method that directly calculates 3D lane parameters and can
circumvent the difficult view transformation step. Specifically, we formulate
3D lane detection as a curve propagation problem by using curve queries. A 3D
lane query is represented by a dynamic and ordered anchor point set. In this
way, queries with curve representation in Transformer decoder iteratively
refine the 3D lane detection results. Moreover, a curve cross-attention module
is introduced to compute the similarities between curve queries and image
features. Additionally, a context sampling module that can capture more
relative image features of a curve query is provided to further boost the 3D
lane detection performance. We evaluate our method for 3D lane detection on
both synthetic and real-world datasets, and the experimental results show that
our method achieves promising performance compared with the state-of-the-art
approaches. The effectiveness of each component is validated via ablation
studies as well.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the IEEE Conference on Robotics and Automation, ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3DOS: Towards 3D Open Set Learning -- Benchmarking and Understanding
  Semantic Novelty Detection on Point Clouds <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.11554v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.11554v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Alliegro, Francesco Cappio Borlino, Tatiana Tommasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years there has been significant progress in the field of 3D
learning on classification, detection and segmentation problems. The vast
majority of the existing studies focus on canonical closed-set conditions,
neglecting the intrinsic open nature of the real-world. This limits the
abilities of robots and autonomous systems involved in safety-critical
applications that require managing novel and unknown signals. In this context
exploiting 3D data can be a valuable asset since it provides rich information
about the geometry of perceived objects and scenes. With this paper we provide
the first broad study on 3D Open Set learning. We introduce 3DOS: a novel
testbed for semantic novelty detection that considers several settings with
increasing difficulties in terms of semantic (category) shift, and covers both
in-domain (synthetic-to-synthetic, real-to-real) and cross-domain
(synthetic-to-real) scenarios. Moreover, we investigate the related 2D Open Set
literature to understand if and how its recent improvements are effective on 3D
data. Our extensive benchmark positions several algorithms in the same coherent
picture, revealing their strengths and limitations. The results of our analysis
may serve as a reliable foothold for future tailored 3D Open Set methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2022 Datasets and Benchmarks Track. Code:
  https://github.com/antoalli/3D_OS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shrinking the Semantic Gap: Spatial Pooling of Local Moment Invariants
  for Copy-Move Forgery Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09135v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09135v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Wang, Zhiqiu Huang, Shuren Qi, Yaoshen Yu, Guohua Shen, Yushu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Copy-move forgery is a manipulation of copying and pasting specific patches
from and to an image, with potentially illegal or unethical uses. Recent
advances in the forensic methods for copy-move forgery have shown increasing
success in detection accuracy and robustness. However, for images with high
self-similarity or strong signal corruption, the existing algorithms often
exhibit inefficient processes and unreliable results. This is mainly due to the
inherent semantic gap between low-level visual representation and high-level
semantic concept. In this paper, we present a very first study of trying to
mitigate the semantic gap problem in copy-move forgery detection, with spatial
pooling of local moment invariants for midlevel image representation. Our
detection method expands the traditional works on two aspects: 1) we introduce
the bag-of-visual-words model into this field for the first time, may meaning a
new perspective of forensic study; 2) we propose a word-to-phrase feature
description and matching pipeline, covering the spatial structure and visual
saliency information of digital images. Extensive experimental results show the
superior performance of our framework over state-of-the-art algorithms in
overcoming the related problems caused by the semantic gap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Information Forensics and Security,
  2023, https://ieeexplore.ieee.org/document/10007894</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Prune-and-Select: Class-incremental learning with specialized
  subnetworks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.04952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.04952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandr Dekhovich, David M. J. Tax, Marcel H. F. Sluiter, Miguel A. Bessa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The human brain is capable of learning tasks sequentially mostly without
forgetting. However, deep neural networks (DNNs) suffer from catastrophic
forgetting when learning one task after another. We address this challenge
considering a class-incremental learning scenario where the DNN sees test data
without knowing the task from which this data originates. During training,
Continual-Prune-and-Select (CP&S) finds a subnetwork within the DNN that is
responsible for solving a given task. Then, during inference, CP&S selects the
correct subnetwork to make predictions for that task. A new task is learned by
training available neuronal connections of the DNN (previously untrained) to
create a new subnetwork by pruning, which can include previously trained
connections belonging to other subnetwork(s) because it does not update shared
connections. This enables to eliminate catastrophic forgetting by creating
specialized regions in the DNN that do not conflict with each other while still
allowing knowledge transfer across them. The CP&S strategy is implemented with
different subnetwork selection strategies, revealing superior performance to
state-of-the-art continual learning methods tested on various datasets
(CIFAR-100, CUB-200-2011, ImageNet-100 and ImageNet-1000). In particular, CP&S
is capable of sequentially learning 10 tasks from ImageNet-1000 keeping an
accuracy around 94% with negligible forgetting, a first-of-its-kind result in
class-incremental learning. To the best of the authors' knowledge, this
represents an improvement in accuracy above 10% when compared to the best
alternative method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AnimeSR: Learning Real-World Super-Resolution Models for Animation
  Videos <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.07038v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.07038v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanze Wu, Xintao Wang, Gen Li, Ying Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the problem of real-world video super-resolution (VSR) for
animation videos, and reveals three key improvements for practical animation
VSR. First, recent real-world super-resolution approaches typically rely on
degradation simulation using basic operators without any learning capability,
such as blur, noise, and compression. In this work, we propose to learn such
basic operators from real low-quality animation videos, and incorporate the
learned ones into the degradation generation pipeline. Such
neural-network-based basic operators could help to better capture the
distribution of real degradations. Second, a large-scale high-quality animation
video dataset, AVC, is built to facilitate comprehensive training and
evaluations for animation VSR. Third, we further investigate an efficient
multi-scale network structure. It takes advantage of the efficiency of
unidirectional recurrent networks and the effectiveness of sliding-window-based
methods. Thanks to the above delicate designs, our method, AnimeSR, is capable
of restoring real-world low-quality animation videos effectively and
efficiently, achieving superior performance to previous state-of-the-art
methods. Codes and models are available at
https://github.com/TencentARC/AnimeSR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022. Codes and models are available at
  https://github.com/TencentARC/AnimeSR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Dementia from Speech and Transcripts using <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.14769v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.14769v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Loukas Ilias, Dimitris Askounis, John Psarras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alzheimer's disease (AD) constitutes a neurodegenerative disease with serious
consequences to peoples' everyday lives, if it is not diagnosed early since
there is no available cure. Alzheimer's is the most common cause of dementia,
which constitutes a general term for loss of memory. Due to the fact that
dementia affects speech, existing research initiatives focus on detecting
dementia from spontaneous speech. However, little work has been done regarding
the conversion of speech data to Log-Mel spectrograms and Mel-frequency
cepstral coefficients (MFCCs) and the usage of pretrained models. Concurrently,
little work has been done in terms of both the usage of transformer networks
and the way the two modalities, i.e., speech and transcripts, are combined in a
single neural network. To address these limitations, first we represent speech
signal as an image and employ several pretrained models, with Vision
Transformer (ViT) achieving the highest evaluation results. Secondly, we
propose multimodal models. More specifically, our introduced models include
Gated Multimodal Unit in order to control the influence of each modality
towards the final classification and crossmodal attention so as to capture in
an effective way the relationships between the two modalities. Extensive
experiments conducted on the ADReSS Challenge dataset demonstrate the
effectiveness of the proposed models and their superiority over
state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Computer Speech & Language (Accepted)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Progressive Self-Distillation for Ground-to-Aerial Perception Knowledge
  Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.13404v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.13404v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Hu, Chenyou Fan, Mete Ozay, Hua Feng, Yuan Gao, Tin Lun Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a practical yet hasn't been explored problem: how a drone can
perceive in an environment from different flight heights. Unlike autonomous
driving, where the perception is always conducted from a ground viewpoint, a
flying drone may flexibly change its flight height due to specific tasks,
requiring the capability for viewpoint invariant perception. Tackling the such
problem with supervised learning will incur tremendous costs for data
annotation of different flying heights. On the other hand, current
semi-supervised learning methods are not effective under viewpoint differences.
In this paper, we introduce the ground-to-aerial perception knowledge transfer
and propose a progressive semi-supervised learning framework that enables drone
perception using only labeled data of ground viewpoint and unlabeled data of
flying viewpoints. Our framework has four core components: i) a dense viewpoint
sampling strategy that splits the range of vertical flight height into a set of
small pieces with evenly-distributed intervals, ii) nearest neighbor
pseudo-labeling that infers labels of the nearest neighbor viewpoint with a
model learned on the preceding viewpoint, iii) MixView that generates augmented
images among different viewpoints to alleviate viewpoint differences, and iv) a
progressive distillation strategy to gradually learn until reaching the maximum
flying height. We collect a synthesized and a real-world dataset, and we
perform extensive experimental analyses to show that our method yields 22.2%
and 16.9% accuracy improvement for the synthesized dataset and the real world.
Code and datasets are available on
https://github.com/FreeformRobotics/Progressive-Self-Distillation-for-Ground-to-Aerial-Perception-Knowledge-Transfer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision-Based Lane Detection and Tracking under Different Challenging
  Environmental Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.10233v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.10233v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samia Sultana, Boshir Ahmed, Manoranjan Paul, Muhammad Rafiqul Islam, Shamim Ahmad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lane marking detection is fundamental for both advanced driving assistance
systems and traffic surveillance systems. However, detecting lane is highly
challenging when the visibility of a road lane marking is low, obscured or
often invisible due to real-life challenging environment and adverse weather.
Most of the lane detection methods suffer from four types of challenges: (i)
light effects i.e. shadow, glare of light, reflection etc. created by different
light sources like streetlamp, tunnel-light, sun, wet road etc.; (ii) Obscured
visibility of eroded, blurred, dashed, colored and cracked lane caused by
natural disasters and adverse weather; (iii) lane marking occlusion by
different objects from surroundings; and (iv) presence of confusing lines e.g.,
guardrails, pavement marking, road divider etc. In this paper, we proposed a
simple, real-time, and robust lane detection and tracking method to detect and
track lane marking. Here, we introduced three key technologies. First, we
introduce a comprehensive intensity threshold range (CITR) to improve the
performance of the canny operator in detecting lane edges of different
intensity. Second, we propose a robust lane verification technique, the angle
and length-based geometric constraint (ALGC) followed by Hough Transform, to
verify the characteristics of lane marking and to prevent incorrect lane
detection. Finally, we propose a novel lane tracking technique, to predict the
lane position of next frame by defining a range of horizontal lane position
which will be updating with respect to the lane position of previous frame. To
evaluate the performance of the proposed method we used the DSDLDE [1] dataset
with 1080x1920 resolutions at 24 frames/sec. Experimental results show that the
average detection rate is 97.36%, and the average detection time is 29.06msec
per frame, which outperformed the state-of-the-art method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 10 figures, submitted to IEEE Access</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DytanVO: Joint Refinement of Visual Odometry and Motion Segmentation in
  Dynamic Environments <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.08430v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.08430v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihao Shen, Yilin Cai, Wenshan Wang, Sebastian Scherer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning-based visual odometry (VO) algorithms achieve remarkable performance
on common static scenes, benefiting from high-capacity models and massive
annotated data, but tend to fail in dynamic, populated environments. Semantic
segmentation is largely used to discard dynamic associations before estimating
camera motions but at the cost of discarding static features and is hard to
scale up to unseen categories. In this paper, we leverage the mutual dependence
between camera ego-motion and motion segmentation and show that both can be
jointly refined in a single learning-based framework. In particular, we present
DytanVO, the first supervised learning-based VO method that deals with dynamic
environments. It takes two consecutive monocular frames in real-time and
predicts camera ego-motion in an iterative fashion. Our method achieves an
average improvement of 27.7% in ATE over state-of-the-art VO solutions in
real-world dynamic environments, and even performs competitively among dynamic
visual SLAM systems which optimize the trajectory on the backend. Experiments
on plentiful unseen environments also demonstrate our method's
generalizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at ICRA 2023 and for inclusion in the
  conference proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ APANet: Adaptive Prototypes Alignment Network for Few-Shot Semantic
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.12263v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.12263v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Chen, Bin-Bin Gao, Zongqing Lu, Jing-Hao Xue, Chengjie Wang, Qingmin Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot semantic segmentation aims to segment novel-class objects in a given
query image with only a few labeled support images. Most advanced solutions
exploit a metric learning framework that performs segmentation through matching
each query feature to a learned class-specific prototype. However, this
framework suffers from biased classification due to incomplete feature
comparisons. To address this issue, we present an adaptive prototype
representation by introducing class-specific and class-agnostic prototypes and
thus construct complete sample pairs for learning semantic alignment with query
features. The complementary features learning manner effectively enriches
feature comparison and helps yield an unbiased segmentation model in the
few-shot setting. It is implemented with a two-branch end-to-end network (i.e.,
a class-specific branch and a class-agnostic branch), which generates
prototypes and then combines query features to perform comparisons. In
addition, the proposed class-agnostic branch is simple yet effective. In
practice, it can adaptively generate multiple class-agnostic prototypes for
query images and learn feature alignment in a self-contrastive manner.
Extensive experiments on PASCAL-5$^i$ and COCO-20$^i$ demonstrate the
superiority of our method. At no expense of inference efficiency, our model
achieves state-of-the-art results in both 1-shot and 5-shot settings for
semantic segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures, Accepted to IEEE Trans. on Multimedia. arXiv
  admin note: substantial text overlap with arXiv:2104.09216</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D-C2FT: Coarse-to-fine <span class="highlight-title">Transformer</span> for Multi-view 3D Reconstruction <span class="chip">ACCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.14575v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.14575v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leslie Ching Ow Tiong, Dick Sigmund, Andrew Beng Jin Teoh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the transformer model has been successfully employed for the
multi-view 3D reconstruction problem. However, challenges remain on designing
an attention mechanism to explore the multiview features and exploit their
relations for reinforcing the encoding-decoding modules. This paper proposes a
new model, namely 3D coarse-to-fine transformer (3D-C2FT), by introducing a
novel coarse-to-fine(C2F) attention mechanism for encoding multi-view features
and rectifying defective 3D objects. C2F attention mechanism enables the model
to learn multi-view information flow and synthesize 3D surface correction in a
coarse to fine-grained manner. The proposed model is evaluated by ShapeNet and
Multi-view Real-life datasets. Experimental results show that 3D-C2FT achieves
notable results and outperforms several competing models on these datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Asian Conference on Computer Vision (ACCV) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Imperceptible Adversarial Attack via Invertible Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15030v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15030v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Chen, Ziyue Wang, Junjie Huang, Wentao Zhao, Xiao Liu, Dejian Guan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adding perturbations via utilizing auxiliary gradient information or
discarding existing details of the benign images are two common approaches for
generating adversarial examples. Though visual imperceptibility is the desired
property of adversarial examples, conventional adversarial attacks still
generate traceable adversarial perturbations. In this paper, we introduce a
novel Adversarial Attack via Invertible Neural Networks (AdvINN) method to
produce robust and imperceptible adversarial examples. Specifically, AdvINN
fully takes advantage of the information preservation property of Invertible
Neural Networks and thereby generates adversarial examples by simultaneously
adding class-specific semantic information of the target class and dropping
discriminant information of the original class. Extensive experiments on
CIFAR-10, CIFAR-100, and ImageNet-1K demonstrate that the proposed AdvINN
method can produce less imperceptible adversarial images than the
state-of-the-art methods and AdvINN yields more robust adversarial examples
with high confidence compared to other adversarial attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Privacy Preserving Method with a Random Orthogonal Matrix for
  ConvMixer Models <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.03843v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.03843v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rei Aso, Tatsuya Chuman, Hitoshi Kiya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a privacy preserving image classification method is proposed
under the use of ConvMixer models. To protect the visual information of test
images, a test image is divided into blocks, and then every block is encrypted
by using a random orthogonal matrix. Moreover, a ConvMixer model trained with
plain images is transformed by the random orthogonal matrix used for encrypting
test images, on the basis of the embedding structure of ConvMixer. The proposed
method allows us not only to use the same classification accuracy as that of
ConvMixer models without considering privacy protection but to also enhance
robustness against various attacks compared to conventional privacy-preserving
learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in 2023 RISP International Workshop on Nonlinear Circuits,
  Communications and Signal Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MultiAct: Long-Term 3D Human Motion Generation from Multiple Action
  Labels <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05897v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05897v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taeryung Lee, Gyeongsik Moon, Kyoung Mu Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle the problem of generating long-term 3D human motion from multiple
action labels. Two main previous approaches, such as action- and
motion-conditioned methods, have limitations to solve this problem. The
action-conditioned methods generate a sequence of motion from a single action.
Hence, it cannot generate long-term motions composed of multiple actions and
transitions between actions. Meanwhile, the motion-conditioned methods generate
future motions from initial motion. The generated future motions only depend on
the past, so they are not controllable by the user's desired actions. We
present MultiAct, the first framework to generate long-term 3D human motion
from multiple action labels. MultiAct takes account of both action and motion
conditions with a unified recurrent generation system. It repetitively takes
the previous motion and action label; then, it generates a smooth transition
and the motion of the given action. As a result, MultiAct produces realistic
long-term motion controlled by the given sequence of multiple action labels.
Codes are available here at https://github.com/TaeryungLee/MultiAct_RELEASE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2023 (Oral presentation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WS-3D-Lane: Weakly Supervised 3D Lane Detection With 2D Lane Labels <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.11523v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.11523v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyong Ai, Wenbo Ding, Jiuhua Zhao, Jiachen Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared to 2D lanes, real 3D lane data is difficult to collect accurately.
In this paper, we propose a novel method for training 3D lanes with only 2D
lane labels, called weakly supervised 3D lane detection WS-3D-Lane. By
assumptions of constant lane width and equal height on adjacent lanes, we
indirectly supervise 3D lane heights in the training. To overcome the problem
of the dynamic change of the camera pitch during data collection, a camera
pitch self-calibration method is proposed. In anchor representation, we propose
a double-layer anchor with a improved non-maximum suppression (NMS) method,
which enables the anchor-based method to predict two lane lines that are close.
Experiments are conducted on the base of 3D-LaneNet under two supervision
methods. Under weakly supervised setting, our WS-3D-Lane outperforms previous
3D-LaneNet: F-score rises to 92.3% on Apollo 3D synthetic dataset, and F1 rises
to 74.5% on ONCE-3DLanes. Meanwhile, WS-3D-Lane in purely supervised setting
makes more increments and outperforms state-of-the-art. To the best of our
knowledge, WS-3D-Lane is the first try of 3D lane detection under weakly
supervised setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 8 figures. Accepted by ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ERNAS: An Evolutionary Neural Architecture Search for Magnetic Resonance
  Image Reconstructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.07280v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.07280v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samira Vafay Eslahi, Jian Tao, Jim Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic resonance imaging (MRI) is one of the noninvasive imaging modalities
that can produce high-quality images. However, the scan procedure is relatively
slow, which causes patient discomfort and motion artifacts in images.
Accelerating MRI hardware is constrained by physical and physiological
limitations. A popular alternative approach to accelerated MRI is to
undersample the k-space data. While undersampling speeds up the scan procedure,
it generates artifacts in the images, and advanced reconstruction algorithms
are needed to produce artifact-free images. Recently deep learning has emerged
as a promising MRI reconstruction method to address this problem. However,
straightforward adoption of the existing deep learning neural network
architectures in MRI reconstructions is not usually optimal in terms of
efficiency and reconstruction quality. In this work, MRI reconstruction from
undersampled data was carried out using an optimized neural network using a
novel evolutionary neural architecture search algorithm. Brain and knee MRI
datasets show that the proposed algorithm outperforms manually designed neural
network-based MR reconstruction models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures, and 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coarse-to-Fine Video Denoising with Dual-Stage Spatial-Channel
  <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.00214v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.00214v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wulian Yun, Mengshi Qi, Chuanming Wang, Huiyuan Fu, Huadong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video denoising aims to recover high-quality frames from the noisy video.
While most existing approaches adopt convolutional neural networks~(CNNs) to
separate the noise from the original visual content, however, CNNs focus on
local information and ignore the interactions between long-range regions in the
frame. Furthermore, most related works directly take the output after basic
spatio-temporal denoising as the final result, leading to neglect the
fine-grained denoising process. In this paper, we propose a Dual-stage
Spatial-Channel Transformer for coarse-to-fine video denoising, which inherits
the advantages of both Transformer and CNNs. Specifically, DSCT is proposed
based on a progressive dual-stage architecture, namely a coarse-level and a
fine-level stage to extract dynamic features and static features, respectively.
At both stages, a Spatial-Channel Encoding Module is designed to model the
long-range contextual dependencies at both spatial and channel levels.
Meanwhile, we design a Multi-Scale Residual Structure to preserve multiple
aspects of information at different stages, which contains a Temporal Features
Aggregation Module to summarize the dynamic representation. Extensive
experiments on four publicly available datasets demonstrate our proposed method
achieves significant improvements compared to the state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Motion-based Post-Processing: Using Kalman Filter to Exclude Similar
  Targets in Underwater Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.01482v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.01482v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfeng Li, Bo Wang, Ye Li, Wei Huo, Zhuoyan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual tracker includes network and post-processing. Despite the color
distortion and low contrast of underwater images, advanced trackers can still
be very competitive in underwater object tracking because deep learning
empowers the networks to discriminate the appearance features of the target.
However, underwater object tracking also faces another problem. Underwater
targets such as fish and dolphins, usually appear in groups, and creatures of
the same species usually have similar expressions of appearance features, so it
is challenging to distinguish the weak differences characteristics only by the
network itself. The existing detection-based post-processing only reflects the
results of single frame detection, but cannot locate real targets among similar
targets. In this paper, we propose a new post-processing strategy based on
motion, which uses Kalman filter (KF) to maintain the motion information of the
target and exclude similar targets around. Specifically, we use the KF
predicted box and the candidate boxes in the response map and their confidence
to calculate the candidate location score to find the real target. Our method
does not change the network structure, nor does it perform additional training
for the tracker. It can be quickly applied to other tracking fields with
similar target problem. We improved SOTA trackers based on our method, and
proved the effectiveness of our method on UOT100 and UTB180. The AUC of our
method for OSTrack on similar subsequences is improved by more than 3% on
average, and the precision and normalization precision are improved by more
than 3.5% on average. It has been proved that our method has good compatibility
in dealing with similar target problems and can enhance performance of the
tracker together with other methods. More details can be found in:
https://github.com/LiYunfengLYF/KF_in_underwater_trackers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking the Data Annotation Process for Multi-view 3D Pose Estimation
  with Active Learning and Self-Training <span class="chip">WACV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.13709v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.13709v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Feng, Kun He, He Wen, Cem Keskin, Yuting Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pose estimation of the human body and hands is a fundamental problem in
computer vision, and learning-based solutions require a large amount of
annotated data. In this work, we improve the efficiency of the data annotation
process for 3D pose estimation problems with Active Learning (AL) in a
multi-view setting. AL selects examples with the highest value to annotate
under limited annotation budgets (time and cost), but choosing the selection
strategy is often nontrivial. We present a framework to efficiently extend
existing single-view AL strategies. We then propose two novel AL strategies
that make full use of multi-view geometry. Moreover, we demonstrate additional
performance gains by incorporating pseudo-labels computed during the AL
process, which is a form of self-training. Our system significantly outperforms
simulated annotation baselines in 3D body and hand pose estimation on two
large-scale benchmarks: CMU Panoptic Studio and InterHand2.6M. Notably, on CMU
Panoptic Studio, we are able to reduce the turn-around time by 60% and
annotation cost by 80% when compared to the conventional annotation process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE WACV 2023 algorithms track. Code:
  https://github.com/facebookresearch/multi_view_active_learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature Alignment as a Generative Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.12562v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.12562v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiago de Souza Farias, Jonas Maziero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reversibility in artificial neural networks allows us to retrieve the input
given an output. We present feature alignment, a method for approximating
reversibility in arbitrary neural networks. We train a network by minimizing
the distance between the output of a data point and the random output with
respect to a random input. We applied the technique to the MNIST, CIFAR-10,
CelebA and STL-10 image datasets. We demonstrate that this method can roughly
recover images from just their latent representation without the need of a
decoder. By utilizing the formulation of variational autoencoders, we
demonstrate that it is possible to produce new images that are statistically
comparable to the training data. Furthermore, we demonstrate that the quality
of the images can be improved by coupling a generator and a discriminator
together. In addition, we show how this method, with a few minor modifications,
can be used to train networks locally, which has the potential to save
computational memory resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Improving the Explainability of Text-based Information Retrieval
  with Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boqi Chen, Kua Chen, Yujing Yang, Afshin Amini, Bharat Saxena, Cecilia Chávez-García, Majid Babaei, Amir Feizpour, Dániel Varró
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thanks to recent advancements in machine learning, vector-based methods have
been adopted in many modern information retrieval (IR) systems. While showing
promising retrieval performance, these approaches typically fail to explain why
a particular document is retrieved as a query result to address explainable
information retrieval(XIR). Knowledge graphs record structured information
about entities and inherently explainable relationships. Most of existing XIR
approaches focus exclusively on the retrieval model with little consideration
on using existing knowledge graphs for providing an explanation. In this paper,
we propose a general architecture to incorporate knowledge graphs for XIR in
various steps of the retrieval process. Furthermore, we create two instances of
the architecture for different types of explanation. We evaluate our approaches
on well-known IR benchmarks using standard metrics and compare them with
vector-based methods as baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, The 1st Workshop on Trustworthy Learning on Graphs
  (TrustLOG)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Öffentliche Daten auf die nächste Stufe heben -- Vom RESTful
  Webservice für Pflanzenschutzmittelregistrierungsdaten zur
  anwendungsunabhängigen Ontologie (erweiterte Version) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katharina Albrecht, Kristoffer Janis Schneider, Daniel Martini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During the application of chemical pesticides, distance requirements have to
be considered. However, these have to be determined and considered by the
farmer manually. To support the farmer the Pesticide Application Manager
(PAM)-Projects were conducted and a decision support system was developed. A
part of this system, the distance requirements service was developed at the
KTBL. To provide this service, the PAM-prozess is conducted: the pesticide
registration data provided via REST-API is crawled, then the data is mapped
into an ontology using rmlmapper and made availabe in a machine readable and
application-independent form via a SPARQL-Endpoint.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 Pages, in German language, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Follow Us and Become Famous! Insights and Guidelines From Instagram
  Engagement Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pier Paolo Tricomi, Marco Chilese, Mauro Conti, Ahmad-Reza Sadeghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With 1.3 billion users, Instagram (IG) has also become a business tool. IG
influencer marketing, expected to generate $33.25 billion in 2022, encourages
companies and influencers to create trending content. Various methods have been
proposed for predicting a post's popularity, i.e., how much engagement (e.g.,
Likes) it will generate. However, these methods are limited: first, they focus
on forecasting the likes, ignoring the number of comments, which became crucial
in 2021. Secondly, studies often use biased or limited data. Third, researchers
focused on Deep Learning models to increase predictive performance, which are
difficult to interpret. As a result, end-users can only estimate engagement
after a post is created, which is inefficient and expensive. A better approach
is to generate a post based on what people and IG like, e.g., by following
guidelines.
  In this work, we uncover part of the underlying mechanisms driving IG
engagement. To achieve this goal, we rely on statistical analysis and
interpretable models rather than Deep Learning (black-box) approaches. We
conduct extensive experiments using a worldwide dataset of 10 million posts
created by 34K global influencers in nine different categories. With our simple
yet powerful algorithms, we can predict engagement up to 94% of F1-Score,
making us comparable and even superior to Deep Learning-based method.
Furthermore, we propose a novel unsupervised algorithm for finding highly
engaging topics on IG. Thanks to our interpretable approaches, we conclude by
outlining guidelines for creating successful posts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reusable Self-Attention Recommender Systems in Fashion Industry
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marjan Celikik, Jacek Wasilewski, Ana Peleteiro Ramallo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A large number of empirical studies on applying self-attention models in the
domain of recommender systems are based on offline evaluation and metrics
computed on standardized datasets. Moreover, many of them do not consider side
information such as item and customer metadata although deep-learning
recommenders live up to their full potential only when numerous features of
heterogeneous type are included. Also, normally the model is used only for a
single use case. Due to these shortcomings, even if relevant, previous works
are not always representative of their actual effectiveness in real-world
industry applications. In this talk, we contribute to bridging this gap by
presenting live experimental results demonstrating improvements in user
retention of up to 30\%. Moreover, we share our learnings and challenges from
building a re-usable and configurable recommender system for various
applications from the fashion industry. In particular, we focus on fashion
inspiration use-cases, such as outfit ranking, outfit recommendation and
real-time personalized outfit generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards the design of user-centric strategy recommendation systems for
  collaborative Human-AI tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lakshita Dodeja, Pradyumna Tambwekar, Erin Hedlund-Botti, Matthew Gombolay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence is being employed by humans to collaboratively solve
complicated tasks for search and rescue, manufacturing, etc. Efficient teamwork
can be achieved by understanding user preferences and recommending different
strategies for solving the particular task to humans. Prior work has focused on
personalization of recommendation systems for relatively well-understood tasks
in the context of e-commerce or social networks. In this paper, we seek to
understand the important factors to consider while designing user-centric
strategy recommendation systems for decision-making. We conducted a
human-subjects experiment (n=60) for measuring the preferences of users with
different personality types towards different strategy recommendation systems.
We conducted our experiment across four types of strategy recommendation
modalities that have been established in prior work: (1) Single strategy
recommendation, (2) Multiple similar recommendations, (3) Multiple diverse
recommendations, (4) All possible strategies recommendations. While these
strategy recommendation schemes have been explored independently in prior work,
our study is novel in that we employ all of them simultaneously and in the
context of strategy recommendations, to provide us an in-depth overview of the
perception of different strategy recommendation systems. We found that certain
personality traits, such as conscientiousness, notably impact the preference
towards a particular type of system (p < 0.01). Finally, we report an
interesting relationship between usability, alignment and perceived
intelligence wherein greater perceived alignment of recommendations with one's
own preferences leads to higher perceived intelligence (p < 0.01) and higher
usability (p < 0.01).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Episodes Discovery Recommendation with Multi-Source Augmentations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.01737v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.01737v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwei Fan, Alice Wang, Zahra Nazari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems (RS) commonly retrieve potential candidate items for
users from a massive number of items by modeling user interests based on
historical interactions. However, historical interaction data is highly sparse,
and most items are long-tail items, which limits the representation learning
for item discovery. This problem is further augmented by the discovery of novel
or cold-start items. For example, after a user displays interest in bitcoin
financial investment shows in the podcast space, a recommender system may want
to suggest, e.g., a newly released blockchain episode from a more technical
show. Episode correlations help the discovery, especially when interaction data
of episodes is limited. Accordingly, we build upon the classical Two-Tower
model and introduce the novel Multi-Source Augmentations using a Contrastive
Learning framework (MSACL) to enhance episode embedding learning by
incorporating positive episodes from numerous correlated semantics. Extensive
experiments on a real-world podcast recommendation dataset from a large audio
streaming platform demonstrate the effectiveness of the proposed framework for
user podcast exploration and cold-start episode recommendation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages long for episodes discovery recommendation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LIMEADE: From AI Explanations to Advice Taking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2003.04315v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2003.04315v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Charles Germain Lee, Doug Downey, Kyle Lo, Daniel S. Weld
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research in human-centered AI has shown the benefits of systems that can
explain their predictions. Methods that allow an AI to take advice from humans
in response to explanations are similarly useful. While both capabilities are
well-developed for transparent learning models (e.g., linear models and
GA$^2$Ms), and recent techniques (e.g., LIME and SHAP) can generate
explanations for opaque models, little attention has been given to advice
methods for opaque models. This paper introduces LIMEADE, the first general
framework that translates both positive and negative advice (expressed using
high-level vocabulary such as that employed by post-hoc explanations) into an
update to an arbitrary, underlying opaque model. We demonstrate the generality
of our approach with case studies on seventy real-world models across two broad
domains: image classification and text recommendation. We show our method
improves accuracy compared to a rigorous baseline on the image classification
domains. For the text modality, we apply our framework to a neural recommender
system for scientific papers on a public website; our user study shows that our
framework leads to significantly higher perceived user control, trust, and
satisfaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Refined Edge Usage of Graph Neural Networks for Edge Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.12970v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.12970v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Jin, Yangkun Wang, Weinan Zhang, Quan Gan, Xiang Song, Yong Yu, Zheng Zhang, David Wipf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs), originally proposed for node classification,
have also motivated many recent works on edge prediction (a.k.a., link
prediction). However, existing methods lack elaborate design regarding the
distinctions between two tasks that have been frequently overlooked: (i) edges
only constitute the topology in the node classification task but can be used as
both the topology and the supervisions (i.e., labels) in the edge prediction
task; (ii) the node classification makes prediction over each individual node,
while the edge prediction is determinated by each pair of nodes. To this end,
we propose a novel edge prediction paradigm named Edge-aware Message PassIng
neuRal nEtworks (EMPIRE). Concretely, we first introduce an edge splitting
technique to specify use of each edge where each edge is solely used as either
the topology or the supervision (named as topology edge or supervision edge).
We then develop a new message passing mechanism that generates the messages to
source nodes (through topology edges) being aware of target nodes (through
supervision edges). In order to emphasize the differences between pairs
connected by supervision edges and pairs unconnected, we further weight the
messages to highlight the relative ones that can reflect the differences. In
addition, we design a novel negative node-pair sampling trick that efficiently
samples 'hard' negative instances in the supervision instances, and can
significantly improve the performance. Experimental results verify that the
proposed method can significantly outperform existing state-of-the-art models
regarding the edge prediction task on multiple homogeneous and heterogeneous
graph datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Optimal Algorithm for Finding Champions in Tournament Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.13621v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.13621v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Beretta, Franco Maria Nardini, Roberto Trani, Rossano Venturini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A tournament graph is a complete directed graph, which can be used to model a
round-robin tournament between $n$ players. In this paper, we address the
problem of finding a champion of the tournament, also known as Copeland winner,
which is a player that wins the highest number of matches. In detail, we aim to
investigate algorithms that find the champion by playing a low number of
matches. Solving this problem allows us to speed up several Information
Retrieval and Recommender System applications, including question answering,
conversational search, etc. Indeed, these applications often search for the
champion inducing a round-robin tournament among the players by employing a
machine learning model to estimate who wins each pairwise comparison. Our
contribution, thus, allows finding the champion by performing a low number of
model inferences. We prove that any deterministic or randomized algorithm
finding a champion with constant success probability requires $\Omega(\ell n)$
comparisons, where $\ell$ is the number of matches lost by the champion. We
then present an asymptotically-optimal deterministic algorithm matching this
lower bound without knowing $\ell$, and we extend our analysis to three
variants of the problem. Lastly, we conduct a comprehensive experimental
assessment of the proposed algorithms on a question answering task on public
data. Results show that our proposed algorithms speed up the retrieval of the
champion up to $13\times$ with respect to the state-of-the-art algorithm that
perform the full tournament.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Customized Visual Models with Retrieval-Augmented Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotian Liu, Kilho Son, Jianwei Yang, Ce Liu, Jianfeng Gao, Yong Jae Lee, Chunyuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-text contrastive learning models such as CLIP have demonstrated strong
task transfer ability. The high generality and usability of these visual models
is achieved via a web-scale data collection process to ensure broad concept
coverage, followed by expensive pre-training to feed all the knowledge into
model weights. Alternatively, we propose REACT, REtrieval-Augmented
CusTomization, a framework to acquire the relevant web knowledge to build
customized visual models for target domains. We retrieve the most relevant
image-text pairs (~3% of CLIP pre-training data) from the web-scale database as
external knowledge, and propose to customize the model by only training new
modualized blocks while freezing all the original weights. The effectiveness of
REACT is demonstrated via extensive experiments on classification, retrieval,
detection and segmentation tasks, including zero, few, and full-shot settings.
Particularly, on the zero-shot classification task, compared with CLIP, it
achieves up to 5.4% improvement on ImageNet and 3.7% on the ELEVATER benchmark
(20 datasets).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLIGEN: Open-Set Grounded Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, Yong Jae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale text-to-image diffusion models have made amazing advances.
However, the status quo is to use text input alone, which can impede
controllability. In this work, we propose GLIGEN, Grounded-Language-to-Image
Generation, a novel approach that builds upon and extends the functionality of
existing pre-trained text-to-image diffusion models by enabling them to also be
conditioned on grounding inputs. To preserve the vast concept knowledge of the
pre-trained model, we freeze all of its weights and inject the grounding
information into new trainable layers via a gated mechanism. Our model achieves
open-world grounded text2img generation with caption and bounding box condition
inputs, and the grounding ability generalizes well to novel spatial
configuration and concepts. GLIGEN's zero-shot performance on COCO and LVIS
outperforms that of existing supervised layout-to-image baselines by a large
margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision Learners Meet Web Image-Text Pairs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingchen Zhao, Quan Cui, Hao Wu, Osamu Yoshie, Cheng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most recent self-supervised learning~(SSL) methods are pre-trained on the
well-curated ImageNet-1K dataset. In this work, we consider SSL pre-training on
noisy web image-text paired data due to the excellent scalability of web data.
First, we conduct a benchmark study of representative SSL pre-training methods
on large-scale web data in a fair condition. Methods include single-modal ones
such as MAE and multi-modal ones such as CLIP. We observe that multi-modal
methods cannot outperform single-modal ones on vision transfer learning tasks.
We derive an information-theoretical view to explain the benchmarking results,
which provides insights into designing novel vision learners. Inspired by the
above explorations, we present a visual representation pre-training method,
MUlti-modal Generator~(MUG), for scalable web image-text data. MUG achieves
state-of-the-art transferring performances on a variety of tasks and shows
promising scaling behavior. Models and codes will be made public. Demo
available at https://huggingface.co/spaces/tennant/MUG_caption
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://bzhao.me/MUG/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Fast Algorithm for Adaptive Private Mean Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Duchi, Saminul Haque, Rohith Kuditipudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We design an $(\varepsilon, \delta)$-differentially private algorithm to
estimate the mean of a $d$-variate distribution, with unknown covariance
$\Sigma$, that is adaptive to $\Sigma$. To within polylogarithmic factors, the
estimator achieves optimal rates of convergence with respect to the induced
Mahalanobis norm $||\cdot||_\Sigma$, takes time $\tilde{O}(n d^2)$ to compute,
has near linear sample complexity for sub-Gaussian distributions, allows
$\Sigma$ to be degenerate or low rank, and adaptively extends beyond
sub-Gaussianity. Prior to this work, other methods required exponential
computation time or the superlinear scaling $n = \Omega(d^{3/2})$ to achieve
non-trivial error with respect to the norm $||\cdot||_\Sigma$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, no figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SegViz: A Federated Learning Framework for Medical Image Segmentation
  from Distributed <span class="highlight-title">Dataset</span>s with Different and Incomplete Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adway U. Kanhere, Pranav Kulkarni, Paul H. Yi, Vishwa S. Parekh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmentation is one of the primary tasks in the application of deep learning
in medical imaging, owing to its multiple downstream clinical applications. As
a result, many large-scale segmentation datasets have been curated and released
for the segmentation of different anatomical structures. However, these
datasets focus on the segmentation of a subset of anatomical structures in the
body, therefore, training a model for each dataset would potentially result in
hundreds of models and thus limit their clinical translational utility.
Furthermore, many of these datasets share the same field of view but have
different subsets of annotations, thus making individual dataset annotations
incomplete. To that end, we developed SegViz, a federated learning framework
for aggregating knowledge from distributed medical image segmentation datasets
with different and incomplete annotations into a `global` meta-model. The
SegViz framework was trained to build a single model capable of segmenting both
liver and spleen aggregating knowledge from both these nodes by aggregating the
weights after every 10 epochs. The global SegViz model was tested on an
external dataset, Beyond the Cranial Vault (BTCV), comprising both liver and
spleen annotations using the dice similarity (DS) metric. The baseline
individual segmentation models for spleen and liver trained on their respective
datasets produced a DS score of 0.834 and 0.878 on the BTCV test set. In
comparison, the SegViz model produced comparable mean DS scores of 0.829 and
0.899 for the segmentation of the spleen and liver respectively. Our results
demonstrate SegViz as an essential first step towards training clinically
translatable multi-task segmentation models from distributed datasets with
disjoint incomplete annotations with excellent performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>ing Large Language Model for Machine Translation: A Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biao Zhang, Barry Haddow, Alexandra Birch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on prompting has shown excellent performance with little or even no
supervised training across many tasks. However, prompting for machine
translation is still under-explored in the literature. We fill this gap by
offering a systematic study on prompting strategies for translation, examining
various factors for prompt template and demonstration example selection. We
further explore the use of monolingual data and the feasibility of
cross-lingual, cross-domain, and sentence-to-document transfer learning in
prompting. Extensive experiments with GLM-130B (Zeng et al., 2022) as the
testbed show that 1) the number and the quality of prompt examples matter,
where using suboptimal examples degenerates translation; 2) several features of
prompt examples, such as semantic similarity, show significant Spearman
correlation with their prompting performance; yet, none of the correlations are
strong enough; 3) using pseudo parallel prompt examples constructed from
monolingual data via zero-shot prompting could improve translation; and 4)
improved performance is achievable by transferring knowledge from prompt
examples selected in other settings. We finally provide an analysis on the
model outputs and discuss several problems that prompting still suffers from.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The #DNN-Verification problem: Counting Unsafe Inputs for Deep Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Marzari, Davide Corsi, Ferdinando Cicalese, Alessandro Farinelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks are increasingly adopted in critical tasks that require
a high level of safety, e.g., autonomous driving. While state-of-the-art
verifiers can be employed to check whether a DNN is unsafe w.r.t. some given
property (i.e., whether there is at least one unsafe input configuration),
their yes/no output is not informative enough for other purposes, such as
shielding, model selection, or training improvements. In this paper, we
introduce the #DNN-Verification problem, which involves counting the number of
input configurations of a DNN that result in a violation of a particular safety
property. We analyze the complexity of this problem and propose a novel
approach that returns the exact count of violations. Due to the #P-completeness
of the problem, we also propose a randomized, approximate method that provides
a provable probabilistic bound of the correct count while significantly
reducing computational requirements. We present experimental results on a set
of safety-critical benchmarks that demonstrate the effectiveness of our
approximate method and evaluate the tightness of the bound.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Marzari and Corsi contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>s as Algorithms: Generalization and Implicit Model Selection
  in In-context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingcong Li, M. Emrullah Ildiz, Dimitris Papailiopoulos, Samet Oymak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) is a type of prompting where a transformer model
operates on a sequence of (input, output) examples and performs inference
on-the-fly. This implicit training is in contrast to explicitly tuning the
model weights based on examples. In this work, we formalize in-context learning
as an algorithm learning problem, treating the transformer model as a learning
algorithm that can be specialized via training to implement-at
inference-time-another target algorithm. We first explore the statistical
aspects of this abstraction through the lens of multitask learning: We obtain
generalization bounds for ICL when the input prompt is (1) a sequence of i.i.d.
(input, label) pairs or (2) a trajectory arising from a dynamical system. The
crux of our analysis is relating the excess risk to the stability of the
algorithm implemented by the transformer, which holds under mild assumptions.
Secondly, we use our abstraction to show that transformers can act as an
adaptive learning algorithm and perform model selection across different
hypothesis classes. We provide numerical evaluations that (1) demonstrate
transformers can indeed implement near-optimal algorithms on classical
regression problems with i.i.d. and dynamic data, (2) identify an inductive
bias phenomenon where the transfer risk on unseen tasks is independent of the
transformer complexity, and (3) empirically verify our theoretical predictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Monotonicity for AI ethics and society: An empirical study of the
  monotonic neural additive model in criminology, education, health care, and
  finance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dangxing Chen, Luyao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Algorithm fairness in the application of artificial intelligence (AI) is
essential for a better society. As the foundational axiom of social mechanisms,
fairness consists of multiple facets. Although the machine learning (ML)
community has focused on intersectionality as a matter of statistical parity,
especially in discrimination issues, an emerging body of literature addresses
another facet -- monotonicity. Based on domain expertise, monotonicity plays a
vital role in numerous fairness-related areas, where violations could misguide
human decisions and lead to disastrous consequences. In this paper, we first
systematically evaluate the significance of applying monotonic neural additive
models (MNAMs), which use a fairness-aware ML algorithm to enforce both
individual and pairwise monotonicity principles, for the fairness of AI ethics
and society. We have found, through a hybrid method of theoretical reasoning,
simulation, and extensive empirical analysis, that considering monotonicity
axioms is essential in all areas of fairness, including criminology, education,
health care, and finance. Our research contributes to the interdisciplinary
research at the interface of AI ethics, explainable AI (XAI), and
human-computer interactions (HCIs). By evidencing the catastrophic consequences
if monotonicity is not met, we address the significance of monotonicity
requirements in AI applications. Furthermore, we demonstrate that MNAMs are an
effective fairness-aware ML approach by imposing monotonicity restrictions
integrating human intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span> Based Implementation for Automatic Book Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07057v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07057v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddhant Porwal, Laxmi Bewoor, Vivek Deshpande
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document Summarization is the procedure of generating a meaningful and
concise summary of a given document with the inclusion of relevant and
topic-important points. There are two approaches: one is picking up the most
relevant statements from the document itself and adding it to the Summary known
as Extractive and the other is generating sentences for the Summary known as
Abstractive Summarization. Training a machine learning model to perform tasks
that are time-consuming or very difficult for humans to evaluate is a major
challenge. Book Abstract generation is one of such complex tasks. Traditional
machine learning models are getting modified with pre-trained transformers.
Transformer based language models trained in a self-supervised fashion are
gaining a lot of attention; when fine-tuned for Natural Language
Processing(NLP) downstream task like text summarization. This work is an
attempt to use Transformer based techniques for Abstract generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at - https://ijisae.org/index.php/IJISAE/article/view/2421</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ActSafe: Predicting Violations of Medical Temporal Constraints for
  Medication Adherence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parker Seegmiller, Joseph Gatto, Abdullah Mamun, Hassan Ghasemzadeh, Diane Cook, John Stankovic, Sarah Masud Preum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prescription medications often impose temporal constraints on regular health
behaviors (RHBs) of patients, e.g., eating before taking medication. Violations
of such medical temporal constraints (MTCs) can result in adverse effects.
Detecting and predicting such violations before they occur can help alert the
patient. We formulate the problem of modeling MTCs and develop a
proof-of-concept solution, ActSafe, to predict violations of MTCs well ahead of
time. ActSafe utilizes a context-free grammar based approach for extracting and
mapping MTCs from patient education materials. It also addresses the challenges
of accurately predicting RHBs central to MTCs (e.g., medication intake). Our
novel behavior prediction model, HERBERT , utilizes a basis vectorization of
time series that is generalizable across temporal scale and duration of
behaviors, explicitly capturing the dependency between temporally collocated
behaviors. Based on evaluation using a real-world RHB dataset collected from 28
patients in uncontrolled environments, HERBERT outperforms baseline models with
an average of 51% reduction in root mean square error. Based on an evaluation
involving patients with chronic conditions, ActSafe can predict MTC violations
a day ahead of time with an average F1 score of 0.86.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Energy-Efficient Reconfigurable Autoencoder Implementation on FPGA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Murat Isik, Matthew Oldland, Lifeng Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoencoders are unsupervised neural networks that are used to process and
compress input data and then reconstruct the data back to the original data
size. This allows autoencoders to be used for different processing applications
such as data compression, image classification, image noise reduction, and
image coloring. Hardware-wise, re-configurable architectures like Field
Programmable Gate Arrays (FPGAs) have been used for accelerating computations
from several domains because of their unique combination of flexibility,
performance, and power efficiency. In this paper, we look at the different
autoencoders available and use the convolutional autoencoder in both FPGA and
GPU-based implementations to process noisy static MNIST images. We compare the
different results achieved with the FPGA and GPU-based implementations and then
discuss the pros and cons of each implementation. The evaluation of the
proposed design achieved 80%accuracy and our experimental results show that the
proposed accelerator achieves a throughput of 21.12 Giga-Operations Per Second
(GOP/s) with a 5.93 W on-chip power consumption at 100 MHz. The comparison
results with off-the-shelf devices and recent state-of-the-art implementations
illustrate that the proposed accelerator has obvious advantages in terms of
energy efficiency and design flexibility. We also discuss future work that can
be done with the use of our proposed accelerator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Intelligent Systems Conference (IntelliSys) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Algorithms for Latent Bandits with Cluster Structure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumyabrata Pal, Arun Sai Suggala, Karthikeyan Shanmugam, Prateek Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of latent bandits with cluster structure where there
are multiple users, each with an associated multi-armed bandit problem. These
users are grouped into \emph{latent} clusters such that the mean reward vectors
of users within the same cluster are identical. At each round, a user, selected
uniformly at random, pulls an arm and observes a corresponding noisy reward.
The goal of the users is to maximize their cumulative rewards. This problem is
central to practical recommendation systems and has received wide attention of
late \cite{gentile2014online, maillard2014latent}. Now, if each user acts
independently, then they would have to explore each arm independently and a
regret of $\Omega(\sqrt{\mathsf{MNT}})$ is unavoidable, where $\mathsf{M},
\mathsf{N}$ are the number of arms and users, respectively. Instead, we propose
LATTICE (Latent bAndiTs via maTrIx ComplEtion) which allows exploitation of the
latent cluster structure to provide the minimax optimal regret of
$\widetilde{O}(\sqrt{(\mathsf{M}+\mathsf{N})\mathsf{T}})$, when the number of
clusters is $\widetilde{O}(1)$. This is the first algorithm to guarantee such a
strong regret bound. LATTICE is based on a careful exploitation of arm
information within a cluster while simultaneously clustering users.
Furthermore, it is computationally efficient and requires only
$O(\log{\mathsf{T}})$ calls to an offline matrix completion oracle across all
$\mathsf{T}$ rounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Consciousness is learning: predictive processing systems that learn by
  binding may perceive themselves as conscious 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        V. A. Aksyuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning algorithms have achieved superhuman performance in specific
complex domains. Yet learning online from few examples and efficiently
generalizing across domains remains elusive. In humans such learning proceeds
via declarative memory formation and is closely associated with consciousness.
Predictive processing has been advanced as a principled Bayesian inference
framework for understanding the cortex as implementing deep generative
perceptual models for both sensory data and action control. However, predictive
processing offers little direct insight into fast compositional learning or the
mystery of consciousness. Here we propose that through implementing online
learning by hierarchical binding of unpredicted inferences, a predictive
processing system may flexibly generalize in novel situations by forming
working memories for perceptions and actions from single examples, which can
become short- and long-term declarative memories retrievable by associative
recall. We argue that the contents of such working memories are unified yet
differentiated, can be maintained by selective attention and are consistent
with observations of masking, postdictive perceptual integration, and other
paradigm cases of consciousness research. We describe how the brain could have
evolved to use perceptual value prediction for reinforcement learning of
complex action policies simultaneously implementing multiple survival and
reproduction strategies. 'Conscious experience' is how such a learning system
perceptually represents its own functioning, suggesting an answer to the meta
problem of consciousness. Our proposal naturally unifies feature binding,
recurrent processing, and predictive processing with global workspace, and, to
a lesser extent, the higher order theories of consciousness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simplistic Collection and Labeling Practices Limit the Utility of
  Benchmark <span class="highlight-title">Dataset</span>s for Twitter Bot Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Hays, Zachary Schutzman, Manish Raghavan, Erin Walk, Philipp Zimmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate bot detection is necessary for the safety and integrity of online
platforms. It is also crucial for research on the influence of bots in
elections, the spread of misinformation, and financial market manipulation.
Platforms deploy infrastructure to flag or remove automated accounts, but their
tools and data are not publicly available. Thus, the public must rely on
third-party bot detection. These tools employ machine learning and often
achieve near perfect performance for classification on existing datasets,
suggesting bot detection is accurate, reliable and fit for use in downstream
applications. We provide evidence that this is not the case and show that high
performance is attributable to limitations in dataset collection and labeling
rather than sophistication of the tools. Specifically, we show that simple
decision rules -- shallow decision trees trained on a small number of features
-- achieve near-state-of-the-art performance on most available datasets and
that bot detection datasets, even when combined together, do not generalize
well to out-of-sample datasets. Our findings reveal that predictions are highly
dependent on each dataset's collection and labeling procedures rather than
fundamental differences between bots and humans. These results have important
implications for both transparency in sampling and labeling procedures and
potential biases in research using existing bot detection tools for
pre-processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Dataset</span> Distillation: A Comprehensive <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruonan Yu, Songhua Liu, Xinchao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent success of deep learning can be largely attributed to the huge amount
of data used for training deep neural networks. However, the sheer amount of
data significantly increase the burden on storage and transmission. It would
also consume considerable time and computational resources to train models on
such large datasets. Moreover, directly publishing raw data inevitably raise
concerns on privacy and copyright. Focusing on these inconveniences, dataset
distillation (DD), also known as dataset condensation (DC), has become a
popular research topic in recent years. Given an original large dataset, DD
aims at a much smaller dataset containing several synthetic samples, such that
models trained on the synthetic dataset can have comparable performance with
those trained on the original real one. This paper presents a comprehensive
review and summary for recent advances in DD and its application. We first
introduce the task in formal and propose an overall algorithmic framework
followed by all existing DD methods. Then, we provide a systematic taxonomy of
current methodologies in this area. Their theoretical relationship will also be
discussed. We also point out current challenges in DD through extensive
experiments and envision possible directions for future works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 167 references, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Negative Flux Aggregation to Estimate Feature Attributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Li, Deng Pan, Chengyin Li, Yao Qiang, Dongxiao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are increasing demands for understanding deep neural networks' (DNNs)
behavior spurred by growing security and/or transparency concerns. Due to
multi-layer nonlinearity of the deep neural network architectures, explaining
DNN predictions still remains as an open problem, preventing us from gaining a
deeper understanding of the mechanisms. To enhance the explainability of DNNs,
we estimate the input feature's attributions to the prediction task using
divergence and flux. Inspired by the divergence theorem in vector analysis, we
develop a novel Negative Flux Aggregation (NeFLAG) formulation and an efficient
approximation algorithm to estimate attribution map. Unlike the previous
techniques, ours doesn't rely on fitting a surrogate model nor need any path
integration of gradients. Both qualitative and quantitative experiments
demonstrate a superior performance of NeFLAG in generating more faithful
attribution maps than the competing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The SwaNNFlight System: On-the-Fly Sim-to-Real Adaptation via Anchored
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06987v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06987v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bassel El Mabsout, Shahin Roozkhosh, Siddharth Mysore, Kate Saenko, Renato Mancuso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) agents trained in simulated environments and then
deployed in the real world are often sensitive to the differences in dynamics
presented, commonly termed the sim-to-real gap. With the goal of minimizing
this gap on resource-constrained embedded systems, we train and live-adapt
agents on quadrotors built from off-the-shelf hardware. In achieving this we
developed three novel contributions. (i) SwaNNFlight, an open-source firmware
enabling wireless data capture and transfer of agents' observations.
Fine-tuning agents with new data, and receiving and swapping onboard NN
controllers -- all while in flight. We also design SwaNNFlight System (SwaNNFS)
allowing new research in training and live-adapting learning agents on similar
systems. (ii) Multiplicative value composition, a technique for preserving the
importance of each policy optimization criterion, improving training
performance and variability in learnt behavior. And (iii) anchor critics to
help stabilize the fine-tuning of agents during sim-to-real transfer, online
learning from real data while retaining behavior optimized in simulation. We
train consistently flight-worthy control policies in simulation and deploy them
on real quadrotors. We then achieve live controller adaptation via over-the-air
updates of the onboard control policy from a ground station. Our results
indicate that live adaptation unlocks a near-50\% reduction in power
consumption, attributed to the sim-to-real gap. Finally, we tackle the issues
of catastrophic forgetting and controller instability, showing the
effectiveness of our novel methods.
  Project Website: https://github.com/BU-Cyber-Physical-Systems-Lab/SwaNNFS
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision Based Machine Learning Algorithms for Out-of-Distribution
  Generalisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamza Riaz, Alan F. Smeaton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are many computer vision applications including object segmentation,
classification, object detection, and reconstruction for which machine learning
(ML) shows state-of-the-art performance. Nowadays, we can build ML tools for
such applications with real-world accuracy. However, each tool works well
within the domain in which it has been trained and developed. Often, when we
train a model on a dataset in one specific domain and test on another unseen
domain known as an out of distribution (OOD) dataset, models or ML tools show a
decrease in performance. For instance, when we train a simple classifier on
real-world images and apply that model on the same classes but with a different
domain like cartoons, paintings or sketches then the performance of ML tools
disappoints. This presents serious challenges of domain generalisation (DG),
domain adaptation (DA), and domain shifting. To enhance the power of ML tools,
we can rebuild and retrain models from scratch or we can perform transfer
learning. In this paper, we present a comparison study between vision-based
technologies for domain-specific and domain-generalised methods. In this
research we highlight that simple convolutional neural network (CNN) based deep
learning methods perform poorly when they have to tackle domain shifting.
Experiments are conducted on two popular vision-based benchmarks, PACS and
Office-Home. We introduce an implementation pipeline for domain generalisation
methods and conventional deep learning models. The outcome confirms that
CNN-based deep learning models show poor generalisation compare to other
extensive methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Computing Conference, 22-23 June 2023, London, United Kingdom. 15
  pages, 5 Figures, 3 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FewSOME: Few Shot Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niamh Belton, Misgina Tsighe Hagos, Aonghus Lawlor, Kathleen M. Curran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have seen considerable progress in the field of Anomaly
Detection but at the cost of increasingly complex training pipelines. Such
techniques require large amounts of training data, resulting in computationally
expensive algorithms. We propose Few Shot anomaly detection (FewSOME), a deep
One-Class Anomaly Detection algorithm with the ability to accurately detect
anomalies having trained on 'few' examples of the normal class and no examples
of the anomalous class. We describe FewSOME to be of low complexity given its
low data requirement and short training time. FewSOME is aided by pretrained
weights with an architecture based on Siamese Networks. By means of an ablation
study, we demonstrate how our proposed loss, 'Stop Loss', improves the
robustness of FewSOME. Our experiments demonstrate that FewSOME performs at
state-of-the-art level on benchmark datasets MNIST, CIFAR-10, F-MNIST and MVTec
AD while training on only 30 normal samples, a minute fraction of the data that
existing methods are trained on. Most notably, we found that FewSOME
outperforms even highly complex models in the setting where only few examples
of the normal class exist. Moreover, our extensive experiments show FewSOME to
be robust to contaminated datasets. We also report F1 score and Balanced
Accuracy in addition to AUC as a benchmark for future techniques to be compared
against.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Expected Gradients of Maxout Networks and Consequences to Parameter
  Initialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanna Tseran, Guido Montúfar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the gradients of a maxout network with respect to inputs and
parameters and obtain bounds for the moments depending on the architecture and
the parameter distribution. We observe that the distribution of the
input-output Jacobian depends on the input, which complicates a stable
parameter initialization. Based on the moments of the gradients, we formulate
parameter initialization strategies that avoid vanishing and exploding
gradients in wide networks. Experiments with deep fully-connected and
convolutional networks show that this strategy improves SGD and Adam training
of deep maxout networks. In addition, we obtain refined bounds on the expected
number of linear regions, results on the expected curve length distortion, and
results on the NTK.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Estimating Transferability using Hard Subsets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tarun Ram Menta, Surgan Jandial, Akash Patil, Vimal KB, Saketh Bachu, Balaji Krishnamurthy, Vineeth N. Balasubramanian, Chirag Agarwal, Mausoom Sarkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As transfer learning techniques are increasingly used to transfer knowledge
from the source model to the target task, it becomes important to quantify
which source models are suitable for a given target task without performing
computationally expensive fine tuning. In this work, we propose HASTE (HArd
Subset TransfErability), a new strategy to estimate the transferability of a
source model to a particular target task using only a harder subset of target
data. By leveraging the internal and output representations of model, we
introduce two techniques, one class agnostic and another class specific, to
identify harder subsets and show that HASTE can be used with any existing
transferability metric to improve their reliability. We further analyze the
relation between HASTE and the optimal average log likelihood as well as
negative conditional entropy and empirically validate our theoretical bounds.
Our experimental results across multiple source model architectures, target
datasets, and transfer learning tasks show that HASTE modified metrics are
consistently better or on par with the state of the art transferability
metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First three authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Memory-Augmented Theory of Mind Network <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dung Nguyen, Phuoc Nguyen, Hung Le, Kien Do, Svetha Venkatesh, Truyen Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social reasoning necessitates the capacity of theory of mind (ToM), the
ability to contextualise and attribute mental states to others without having
access to their internal cognitive structure. Recent machine learning
approaches to ToM have demonstrated that we can train the observer to read the
past and present behaviours of other agents and infer their beliefs (including
false beliefs about things that no longer exist), goals, intentions and future
actions. The challenges arise when the behavioural space is complex, demanding
skilful space navigation for rapidly changing contexts for an extended period.
We tackle the challenges by equipping the observer with novel neural memory
mechanisms to encode, and hierarchical attention to selectively retrieve
information about others. The memories allow rapid, selective querying of
distal related past behaviours of others to deliberatively reason about their
current mental state, beliefs and future behaviours. This results in ToMMY, a
theory of mind model that learns to reason while making little assumptions
about the underlying mental processes. We also construct a new suite of
experiments to demonstrate that memories facilitate the learning process and
achieve better theory of mind performance, especially for high-demand
false-belief tasks that require inferring through multiple steps of changes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Data Poison Attacks on Human Emotion Evaluation Systems
  based on EEG Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhibo Zhang, Sani Umar, Ahmed Y. Al Hammadi, Sangyoung Yoon, Ernesto Damiani, Claudio Agostino Ardagna, Nicola Bena, Chan Yeob Yeun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The major aim of this paper is to explain the data poisoning attacks using
label-flipping during the training stage of the electroencephalogram (EEG)
signal-based human emotion evaluation systems deploying Machine Learning models
from the attackers' perspective. Human emotion evaluation using EEG signals has
consistently attracted a lot of research attention. The identification of human
emotional states based on EEG signals is effective to detect potential internal
threats caused by insider individuals. Nevertheless, EEG signal-based human
emotion evaluation systems have shown several vulnerabilities to data poison
attacks. The findings of the experiments demonstrate that the suggested data
poison assaults are model-independently successful, although various models
exhibit varying levels of resilience to the attacks. In addition, the data
poison attacks on the EEG signal-based human emotion evaluation systems are
explained with several Explainable Artificial Intelligence (XAI) methods,
including Shapley Additive Explanation (SHAP) values, Local Interpretable
Model-agnostic Explanations (LIME), and Generated Decision Trees. And the codes
of this paper are publicly available on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAFUS: a Framework to predict mortality risk in MAFLD subjects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06908v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06908v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Domenico Lofù, Paolo Sorino, Tommaso Colafiglio, Caterina Bonfiglio, Fedelucio Narducci, Tommaso Di Noia, Eugenio Di Sciascio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metabolic (dysfunction) associated fatty liver disease (MAFLD) establishes
new criteria for diagnosing fatty liver disease independent of alcohol
consumption and concurrent viral hepatitis infection. However, the long-term
outcome of MAFLD subjects is sparse. Few articles are focused on mortality in
MAFLD subjects, and none investigate how to predict a fatal outcome. In this
paper, we propose an artificial intelligence-based framework named MAFUS that
physicians can use for predicting mortality in MAFLD subjects. The framework
uses data from various anthropometric and biochemical sources based on Machine
Learning (ML) algorithms. The framework has been tested on a state-of-the-art
dataset on which five ML algorithms are trained. Support Vector Machines
resulted in being the best model. Furthermore, an Explainable Artificial
Intelligence (XAI) analysis has been performed to understand the SVM diagnostic
reasoning and the contribution of each feature to the prediction. The MAFUS
framework is easy to apply, and the required parameters are readily available
in the dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Conditional Measure Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Turinici
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quantization of a (probability) measure is replacing it by a sum of Dirac
masses that is close enough to it (in some metric space of probability
measures). Various methods exists to do so, but the situation of quantizing a
conditional law has been less explored. We propose a method, called DCMQ,
involving a Huber-energy kernel-based approach coupled with a deep neural
network architecture. The method is tested on several examples and obtains
promising results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Filtering over Expanding Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bishwadeep Das, Elvin Isufi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data processing tasks over graphs couple the data residing over the nodes
with the topology through graph signal processing tools. Graph filters are one
such prominent tool, having been used in applications such as denoising,
interpolation, and classification. However, they are mainly used on fixed
graphs although many networks grow in practice, with nodes continually
attaching to the topology. Re-training the filter every time a new node
attaches is computationally demanding; hence an online learning solution that
adapts to the evolving graph is needed. We propose an online update of the
filter, based on the principles of online machine learning. To update the
filter, we perform online gradient descent, which has a provable regret bound
with respect to the filter computed offline. We show the performance of our
method for signal interpolation at the incoming nodes. Numerical results on
synthetic and graph-based recommender systems show that the proposed approach
compares well to the offline baseline filter while outperforming competitive
approaches. These findings lay the foundation for efficient filtering over
expanding graphs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The quantum cost function concentration dependency on the
  parametrization expressivity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Friedrich, Jonas Maziero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although we are currently in the era of noisy intermediate scale quantum
devices, several studies are being conducted with the aim of bringing machine
learning to the quantum domain. Currently, quantum variational circuits are one
of the main strategies used to build such models. However, despite its
widespread use, we still do not know what are the minimum resources needed to
create a quantum machine learning model. In this article, we analyze how the
expressiveness of the parametrization affects the cost function. We
analytically show that the more expressive the parametrization is, the more the
cost function will tend to concentrate around a value that depends both on the
chosen observable and on the number of qubits used. For this, we initially
obtain a relationship between the expressiveness of the parametrization and the
mean value of the cost function. Afterwards, we relate the expressivity of the
parametrization with the variance of the cost function. Finally, we show some
numerical simulation results that confirm our theoretical-analytical
predictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CS-lol: a <span class="highlight-title">Dataset</span> of Viewer Comment with Scene in E-sports
  Live-streaming <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06876v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06876v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie H. Xu, Yu Nakano, Lingrong Kong, Kojiro Iizuka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Billions of live-streaming viewers share their opinions on scenes they are
watching in real-time and interact with the event, commentators as well as
other viewers via text comments. Thus, there is necessary to explore viewers'
comments with scenes in E-sport live-streaming events. In this paper, we
developed CS-lol, a new large-scale dataset containing comments from viewers
paired with descriptions of game scenes in E-sports live-streaming. Moreover,
we propose a task, namely viewer comment retrieval, to retrieve the viewer
comments for the scene of the live-streaming event. Results on a series of
baseline retrieval methods derived from typical IR evaluation methods show our
task as a challenging task. Finally, we release CS-lol and baseline
implementation to the research community as a resource.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, In ACM SIGIR Conference on Human Information
  Interaction and Retrieval (CHIIR 23)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Denoising Diffusion Probabilistic Models as a Defense against
  Adversarial Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06871v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06871v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lars Lien Ankile, Anna Midgley, Sebastian Weisshaar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Networks are infamously sensitive to small perturbations in their
inputs, making them vulnerable to adversarial attacks. This project evaluates
the performance of Denoising Diffusion Probabilistic Models (DDPM) as a
purification technique to defend against adversarial attacks. This works by
adding noise to an adversarial example before removing it through the reverse
process of the diffusion model. We evaluate the approach on the PatchCamelyon
data set for histopathologic scans of lymph node sections and find an
improvement of the robust accuracy by up to 88\% of the original model's
accuracy, constituting a considerable improvement over the vanilla model and
our baselines. The project code is located at
https://github.com/ankile/Adversarial-Diffusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to solve arithmetic problems with a virtual abacus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Flavio Petruzzellis, Ling Xuan Chen, Alberto Testolin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acquiring mathematical skills is considered a key challenge for modern
Artificial Intelligence systems. Inspired by the way humans discover numerical
knowledge, here we introduce a deep reinforcement learning framework that
allows to simulate how cognitive agents could gradually learn to solve
arithmetic problems by interacting with a virtual abacus. The proposed model
successfully learn to perform multi-digit additions and subtractions, achieving
an error rate below 1% even when operands are much longer than those observed
during training. We also compare the performance of learning agents receiving a
different amount of explicit supervision, and we analyze the most common error
patterns to better understand the limitations and biases resulting from our
design choices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Northern Lights Deep Learning Conference 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Show me what you want: Inverse reinforcement learning to automatically
  design robot swarms by demonstration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06864v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06864v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilyes Gharbi, Jonas Kuckling, David Garzón Ramos, Mauro Birattari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic design is a promising approach to generating control software for
robot swarms. So far, automatic design has relied on mission-specific objective
functions to specify the desired collective behavior. In this paper, we explore
the possibility to specify the desired collective behavior via demonstrations.
We develop Demo-Cho, an automatic design method that combines inverse
reinforcement learning with automatic modular design of control software for
robot swarms. We show that, only on the basis of demonstrations and without the
need to be provided with an explicit objective function, Demo-Cho successfully
generated control software to perform four missions. We present results
obtained in simulation and with physical robots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A reinforcement learning path planning approach for range-only
  underwater target localization with autonomous vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06863v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06863v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Masmitja, Mario Martin, Kakani Katija, Spartacus Gomariz, Joan Navarro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underwater target localization using range-only and single-beacon (ROSB)
techniques with autonomous vehicles has been used recently to improve the
limitations of more complex methods, such as long baseline and ultra-short
baseline systems. Nonetheless, in ROSB target localization methods, the
trajectory of the tracking vehicle near the localized target plays an important
role in obtaining the best accuracy of the predicted target position. Here, we
investigate a Reinforcement Learning (RL) approach to find the optimal path
that an autonomous vehicle should follow in order to increase and optimize the
overall accuracy of the predicted target localization, while reducing time and
power consumption. To accomplish this objective, different experimental tests
have been designed using state-of-the-art deep RL algorithms. Our study also
compares the results obtained with the analytical Fisher information matrix
approach used in previous studies. The results revealed that the policy learned
by the RL agent outperforms trajectories based on these analytical solutions,
e.g. the median predicted error at the beginning of the target's localisation
is 17% less. These findings suggest that using deep RL for localizing acoustic
targets could be successfully applied to in-water applications that include
tracking of acoustically tagged marine animals by autonomous underwater
vehicles. This is envisioned as a first necessary step to validate the use of
RL to tackle such problems, which could be used later on in a more complex
scenarios
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CASE2022. Code at this Github repository
  https://github.com/imasmitja/RLforUTracking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Syntactically Robust Training on Partially-Observed Data for Open
  Information Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Qi, Yuxiang Chen, Lei Hou, Juanzi Li, Bin Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open Information Extraction models have shown promising results with
sufficient supervision. However, these models face a fundamental challenge that
the syntactic distribution of training data is partially observable in
comparison to the real world. In this paper, we propose a syntactically robust
training framework that enables models to be trained on a syntactic-abundant
distribution based on diverse paraphrase generation. To tackle the intrinsic
problem of knowledge deformation of paraphrasing, two algorithms based on
semantic similarity matching and syntactic tree walking are used to restore the
expressionally transformed knowledge. The training framework can be generally
applied to other syntactic partial observable domains. Based on the proposed
framework, we build a new evaluation set called CaRB-AutoPara, a syntactically
diverse dataset consistent with the real-world setting for validating the
robustness of the models. Experiments including a thorough analysis show that
the performance of the model degrades with the increase of the difference in
syntactic distribution, while our framework gives a robust boundary. The source
code is publicly available at https://github.com/qijimrc/RobustOIE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pathfinding Neural Cellular Automata 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam Earle, Ozlem Yildiz, Julian Togelius, Chinmay Hegde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pathfinding makes up an important sub-component of a broad range of complex
tasks in AI, such as robot path planning, transport routing, and game playing.
While classical algorithms can efficiently compute shortest paths, neural
networks could be better suited to adapting these sub-routines to more complex
and intractable tasks. As a step toward developing such networks, we hand-code
and learn models for Breadth-First Search (BFS), i.e. shortest path finding,
using the unified architectural framework of Neural Cellular Automata, which
are iterative neural networks with equal-size inputs and outputs. Similarly, we
present a neural implementation of Depth-First Search (DFS), and outline how it
can be combined with neural BFS to produce an NCA for computing diameter of a
graph. We experiment with architectural modifications inspired by these
hand-coded NCAs, training networks from scratch to solve the diameter problem
on grid mazes while exhibiting strong generalization ability. Finally, we
introduce a scheme in which data points are mutated adversarially during
training. We find that adversarially evolving mazes leads to increased
generalization on out-of-distribution examples, while at the same time
generating data-sets with significantly more complex solutions for reasoning
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Follow Us and Become Famous! Insights and Guidelines From Instagram
  Engagement Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pier Paolo Tricomi, Marco Chilese, Mauro Conti, Ahmad-Reza Sadeghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With 1.3 billion users, Instagram (IG) has also become a business tool. IG
influencer marketing, expected to generate $33.25 billion in 2022, encourages
companies and influencers to create trending content. Various methods have been
proposed for predicting a post's popularity, i.e., how much engagement (e.g.,
Likes) it will generate. However, these methods are limited: first, they focus
on forecasting the likes, ignoring the number of comments, which became crucial
in 2021. Secondly, studies often use biased or limited data. Third, researchers
focused on Deep Learning models to increase predictive performance, which are
difficult to interpret. As a result, end-users can only estimate engagement
after a post is created, which is inefficient and expensive. A better approach
is to generate a post based on what people and IG like, e.g., by following
guidelines.
  In this work, we uncover part of the underlying mechanisms driving IG
engagement. To achieve this goal, we rely on statistical analysis and
interpretable models rather than Deep Learning (black-box) approaches. We
conduct extensive experiments using a worldwide dataset of 10 million posts
created by 34K global influencers in nine different categories. With our simple
yet powerful algorithms, we can predict engagement up to 94% of F1-Score,
making us comparable and even superior to Deep Learning-based method.
Furthermore, we propose a novel unsupervised algorithm for finding highly
engaging topics on IG. Thanks to our interpretable approaches, we conclude by
outlining guidelines for creating successful posts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convergence of First-Order Algorithms for Meta-Learning with Moreau
  Envelopes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantin Mishchenko, Slavomír Hanzely, Peter Richtárik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we consider the problem of minimizing the sum of Moreau
envelopes of given functions, which has previously appeared in the context of
meta-learning and personalized federated learning. In contrast to the existing
theory that requires running subsolvers until a certain precision is reached,
we only assume that a finite number of gradient steps is taken at each
iteration. As a special case, our theory allows us to show the convergence of
First-Order Model-Agnostic Meta-Learning (FO-MAML) to the vicinity of a
solution of Moreau objective. We also study a more general family of
first-order algorithms that can be viewed as a generalization of FO-MAML. Our
main theoretical achievement is a theoretical improvement upon the inexact SGD
framework. In particular, our perturbed-iterate analysis allows for tighter
guarantees that improve the dependency on the problem's conditioning. In
contrast to the related work on meta-learning, ours does not require any
assumptions on the Hessian smoothness, and can leverage smoothness and
convexity of the reformulation based on Moreau envelopes. Furthermore, to fill
the gaps in the comparison of FO-MAML to the Implicit MAML (iMAML), we show
that the objective of iMAML is neither smooth nor convex, implying that it has
no convergence guarantees based on the existing theory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subgraph Centralization: A Necessary Step for Graph Anomaly Detection <span class="chip">SDM2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhong Zhuang, Kai Ming Ting, Guansong Pang, Shuaibin Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph anomaly detection has attracted a lot of interest recently. Despite
their successes, existing detectors have at least two of the three weaknesses:
(a) high computational cost which limits them to small-scale networks only; (b)
existing treatment of subgraphs produces suboptimal detection accuracy; and (c)
unable to provide an explanation as to why a node is anomalous, once it is
identified. We identify that the root cause of these weaknesses is a lack of a
proper treatment for subgraphs. A treatment called Subgraph Centralization for
graph anomaly detection is proposed to address all the above weaknesses. Its
importance is shown in two ways. First, we present a simple yet effective new
framework called Graph-Centric Anomaly Detection (GCAD). The key advantages of
GCAD over existing detectors including deep-learning detectors are: (i) better
anomaly detection accuracy; (ii) linear time complexity with respect to the
number of nodes; and (iii) it is a generic framework that admits an existing
point anomaly detector to be used to detect node anomalies in a network.
Second, we show that Subgraph Centralization can be incorporated into two
existing detectors to overcome the above-mentioned weaknesses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in SDM2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multicenter automatic detection of invasive carcinoma on breast whole
  slide images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rémy Peyret, Nicolas Pozin, Stéphane Sockeel, Solène-Florence Kammerer-Jacquet, Julien Adam, Claire Bocciarelli, Yoan Ditchi, Christophe Bontoux, Thomas Depoilly, Loris Guichard, Elisabeth Lanteri, Marie Sockeel, Sophie Prévot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Breast cancer is one of the most prevalent cancers worldwide and pathologists
are closely involved in establishing a diagnosis. Tools to assist in making a
diagnosis are required to manage the increasing workload. In this context,
artificial intelligence (AI) and deep-learning based tools may be used in daily
pathology practice. However, it is challenging to develop fast and reliable
algorithms that can be trusted by practitioners, whatever the medical center.
We describe a patch-based algorithm that incorporates a convolutional neural
network to detect and locate invasive carcinoma on breast whole-slide images.
The network was trained on a dataset extracted from a reference acquisition
center. We then performed a calibration step based on transfer learning to
maintain the performance when translating on a new target acquisition center by
using a limited amount of additional training data. Performance was evaluated
using classical binary measures (accuracy, recall, precision) for both centers
(referred to as test reference dataset and test target dataset) and at two
levels: patch and slide level. At patch level, accuracy, recall, and precision
of the model on the reference and target test sets were 92.1\% and 96.3\%, 95\%
and 87.8\%, and 73.9\% and 70.6\%, respectively. At slide level, accuracy,
recall, and precision were 97.6\% and 92.0\%, 90.9\% and 100\%, and 100\% and
70.8\% for test sets 1 and 2, respectively. The high performance of the
algorithm at both centers shows that the calibration process is efficient. This
is performed using limited training data from the new target acquisition center
and requires that the model is trained beforehand on a large database from a
reference center. This methodology allows the implementation of AI diagnostic
tools to help in routine pathology practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ensemble Reservoir Computing for Dynamical Systems: Prediction of
  Phase-Space Stable Region for Hadron Storage Rings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Casanova, Barbara Dalena, Luca Bonaventura, Massimo Giovannozzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the ability of an ensemble reservoir computing approach to
predict the long-term behaviour of the phase-space region in which the motion
of charged particles in hadron storage rings is bounded, the so-called dynamic
aperture. Currently, the calculation of the phase-space stability region of
hadron storage rings is performed through direct computer simulations, which
are resource- and time-intensive processes. Echo State Networks (ESN) are a
class of recurrent neural networks that are computationally effective, since
they avoid backpropagation and require only cross-validation. Furthermore, they
have been proven to be universal approximants of dynamical systems. In this
paper, we present the performance reached by ESN based on an ensemble approach
for the prediction of the phase-space stability region and compare it with
analytical scaling laws based on the stability-time estimate of the Nekhoroshev
theorem for Hamiltonian systems. We observe that the proposed ESN approach is
capable of effectively predicting the time evolution of the extent of the
dynamic aperture, improving the predictions by analytical scaling laws, thus
providing an efficient surrogate model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reusable Self-Attention Recommender Systems in Fashion Industry
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marjan Celikik, Jacek Wasilewski, Ana Peleteiro Ramallo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A large number of empirical studies on applying self-attention models in the
domain of recommender systems are based on offline evaluation and metrics
computed on standardized datasets. Moreover, many of them do not consider side
information such as item and customer metadata although deep-learning
recommenders live up to their full potential only when numerous features of
heterogeneous type are included. Also, normally the model is used only for a
single use case. Due to these shortcomings, even if relevant, previous works
are not always representative of their actual effectiveness in real-world
industry applications. In this talk, we contribute to bridging this gap by
presenting live experimental results demonstrating improvements in user
retention of up to 30\%. Moreover, we share our learnings and challenges from
building a re-usable and configurable recommender system for various
applications from the fashion industry. In particular, we focus on fashion
inspiration use-cases, such as outfit ranking, outfit recommendation and
real-time personalized outfit generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometric ergodicity of SGLD via reflection coupling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Li, Jian-Guo Liu, Yuliang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the geometric ergodicity of the Stochastic Gradient Langevin
Dynamics (SGLD) algorithm under nonconvexity settings. Via the technique of
reflection coupling, we prove the Wasserstein contraction of SGLD when the
target distribution is log-concave only outside some compact set. The time
discretization and the minibatch in SGLD introduce several difficulties when
applying the reflection coupling, which are addressed by a series of careful
estimates of conditional expectations. As a direct corollary, the SGLD with
constant step size has an invariant distribution and we are able to obtain its
geometric ergodicity in terms of $W_1$ distance. The generalization to
non-gradient drifts is also included.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedCliP: Federated Learning with Client Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beibei Li, Zerui Shao, Ao Liu, Peiran Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is a newly emerging distributed learning paradigm
that allows numerous participating clients to train machine learning models
collaboratively, each with its data distribution and without sharing their
data. One fundamental bottleneck in FL is the heavy communication overheads of
high-dimensional models between the distributed clients and the central server.
Previous works often condense models into compact formats by gradient
compression or distillation to overcome communication limitations. In contrast,
we propose FedCliP in this work, the first communication efficient FL training
framework from a macro perspective, which can position valid clients
participating in FL quickly and constantly prune redundant clients.
Specifically, We first calculate the reliability score based on the training
loss and model divergence as an indicator to measure the client pruning. We
propose a valid client determination approximation framework based on the
reliability score with Gaussian Scale Mixture (GSM) modeling for federated
participating clients pruning. Besides, we develop a communication efficient
client pruning training method in the FL scenario. Experimental results on
MNIST dataset show that FedCliP has up to 10%~70% communication costs for
converged models at only a 0.2% loss in accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1 figure, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tracing and Manipulating Intermediate Values in Neural Math Problem
  Solvers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuta Matsumoto, Benjamin Heinzerling, Masashi Yoshikawa, Kentaro Inui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How language models process complex input that requires multiple steps of
inference is not well understood. Previous research has shown that information
about intermediate values of these inputs can be extracted from the activations
of the models, but it is unclear where that information is encoded and whether
that information is indeed used during inference. We introduce a method for
analyzing how a Transformer model processes these inputs by focusing on simple
arithmetic problems and their intermediate values. To trace where information
about intermediate values is encoded, we measure the correlation between
intermediate values and the activations of the model using principal component
analysis (PCA). Then, we perform a causal intervention by manipulating model
weights. This intervention shows that the weights identified via tracing are
not merely correlated with intermediate values, but causally related to model
predictions. Our findings show that the model has a locality to certain
intermediate values, and this is useful for enhancing the interpretability of
the models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, MathNLP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Coronal Hole Analysis and Prediction using Computer Vision and LSTM
  Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juyoung Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As humanity has begun to explore space, the significance of space weather has
become apparent. It has been established that coronal holes, a type of space
weather phenomenon, can impact the operation of aircraft and satellites. The
coronal hole is an area on the sun characterized by open magnetic field lines
and relatively low temperatures, which result in the emission of the solar wind
at higher than average rates. In this study, To prepare for the impact of
coronal holes on the Earth, we use computer vision to detect the coronal hole
region and calculate its size based on images from the Solar Dynamics
Observatory (SDO). We then implement deep learning techniques, specifically the
Long Short-Term Memory (LSTM) method, to analyze trends in the coronal hole
area data and predict its size for different sun regions over 7 days. By
analyzing time series data on the coronal hole area, this study aims to
identify patterns and trends in coronal hole behavior and understand how they
may impact space weather events. This research represents an important step
towards improving our ability to predict and prepare for space weather events
that can affect Earth and technological systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SwinDepth: Unsupervised Depth Estimation using Monocular Sequences via
  Swin <span class="highlight-title">Transformer</span> and Densely Cascaded Network <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongseok Shim, H. Jin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular depth estimation plays a critical role in various computer vision
and robotics applications such as localization, mapping, and 3D object
detection. Recently, learning-based algorithms achieve huge success in depth
estimation by training models with a large amount of data in a supervised
manner. However, it is challenging to acquire dense ground truth depth labels
for supervised training, and the unsupervised depth estimation using monocular
sequences emerges as a promising alternative. Unfortunately, most studies on
unsupervised depth estimation explore loss functions or occlusion masks, and
there is little change in model architecture in that ConvNet-based
encoder-decoder structure becomes a de-facto standard for depth estimation. In
this paper, we employ a convolution-free Swin Transformer as an image feature
extractor so that the network can capture both local geometric features and
global semantic features for depth estimation. Also, we propose a Densely
Cascaded Multi-scale Network (DCMNet) that connects every feature map directly
with another from different scales via a top-down cascade pathway. This densely
cascaded connectivity reinforces the interconnection between decoding layers
and produces high-quality multi-scale depth outputs. The experiments on two
different datasets, KITTI and Make3D, demonstrate that our proposed method
outperforms existing state-of-the-art unsupervised algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Operator Framework for Digital Twin and Complex Engineering
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuma Kobayashi, James Daniell, Syed B. Alam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With modern computational advancements and statistical analysis methods,
machine learning algorithms have become a vital part of engineering modeling.
Neural Operator Networks (ONets) is an emerging machine learning algorithm as a
"faster surrogate" for approximating solutions to partial differential
equations (PDEs) due to their ability to approximate mathematical operators
versus the direct approximation of Neural Networks (NN). ONets use the
Universal Approximation Theorem to map finite-dimensional inputs to
infinite-dimensional space using the branch-trunk architecture, which encodes
domain and feature information separately before using a dot product to combine
the information. ONets are expected to occupy a vital niche for surrogate
modeling in physical systems and Digital Twin (DT) development. Three test
cases are evaluated using ONets for operator approximation, including a
1-dimensional ordinary differential equations (ODE), general diffusion system,
and convection-diffusion (Burger) system. Solutions for ODE and diffusion
systems yield accurate and reliable results (R2>0.95), while solutions for
Burger systems need further refinement in the ONet algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying and Managing Impacts of Concept Drifts on IoT Traffic
  Inference in Residential ISP Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aarman Pashamokhtari, Norihiro Okui, Masataka Nakahara, Ayumu Kubota, Gustavo Batista, Hassan Habibi Gharakheili
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Millions of vulnerable consumer IoT devices in home networks are the enabler
for cyber crimes putting user privacy and Internet security at risk. Internet
service providers (ISPs) are best poised to play key roles in mitigating risks
by automatically inferring active IoT devices per household and notifying users
of vulnerable ones. Developing a scalable inference method that can perform
robustly across thousands of home networks is a non-trivial task. This paper
focuses on the challenges of developing and applying data-driven inference
models when labeled data of device behaviors is limited and the distribution of
data changes (concept drift) across time and space domains. Our contributions
are three-fold: (1) We collect and analyze network traffic of 24 types of
consumer IoT devices from 12 real homes over six weeks to highlight the
challenge of temporal and spatial concept drifts in network behavior of IoT
devices; (2) We analyze the performance of two inference strategies, namely
"global inference" (a model trained on a combined set of all labeled data from
training homes) and "contextualized inference" (several models each trained on
the labeled data from a training home) in the presence of concept drifts; and
(3) To manage concept drifts, we develop a method that dynamically applies the
``closest'' model (from a set) to network traffic of unseen homes during the
testing phase, yielding better performance in 20% of scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE IoT Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DQNAS: Neural Architecture Search using Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anshumaan Chauhan, Siddhartha Bhattacharyya, S. Vadivel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional Neural Networks have been used in a variety of image related
applications after their rise in popularity due to ImageNet competition.
Convolutional Neural Networks have shown remarkable results in applications
including face recognition, moving target detection and tracking,
classification of food based on the calorie content and many more. Designing of
Convolutional Neural Networks requires experts having a cross domain knowledge
and it is laborious, which requires a lot of time for testing different values
for different hyperparameter along with the consideration of different
configurations of existing architectures. Neural Architecture Search is an
automated way of generating Neural Network architectures which saves
researchers from all the brute-force testing trouble, but with the drawback of
consuming a lot of computational resources for a prolonged period. In this
paper, we propose an automated Neural Architecture Search framework DQNAS,
guided by the principles of Reinforcement Learning along with One-shot Training
which aims to generate neural network architectures that show superior
performance and have minimum scalability problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 Pages, 6 Tables, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Surgical Aggregation: A Federated Learning Framework for Harmonizing
  Distributed <span class="highlight-title">Dataset</span>s with Diverse Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Kulkarni, Adway Kanhere, Paul H. Yi, Vishwa S. Parekh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-assisted characterization of chest x-rays (CXR) has the potential to
provide substantial benefits across many clinical applications. Many
large-scale public CXR datasets have been curated for detection of
abnormalities using deep learning. However, each of these datasets focus on
detecting a subset of disease labels that could be present in a CXR, thus
limiting their clinical utility. Furthermore, the distributed nature of these
datasets, along with data sharing regulations, make it difficult to share and
create a complete representation of disease labels. We propose surgical
aggregation, a federated learning framework for aggregating knowledge from
distributed datasets with different disease labels into a 'global' deep
learning model. We randomly divided the NIH Chest X-Ray 14 dataset into
training (70%), validation (10%), and test (20%) splits with no patient overlap
and conducted two experiments. In the first experiment, we pruned the disease
labels to create two 'toy' datasets containing 11 and 8 labels respectively
with 4 overlapping labels. For the second experiment, we pruned the disease
labels to create two disjoint 'toy' datasets with 7 labels each. We observed
that the surgically aggregated 'global' model resulted in excellent performance
across both experiments when compared to a 'baseline' model trained on complete
disease labels. The overlapping and disjoint experiments had an AUROC of 0.87
and 0.86 respectively, compared to the baseline AUROC of 0.87. We used surgical
aggregation to harmonize the NIH Chest X-Ray 14 and CheXpert datasets into a
'global' model with an AUROC of 0.85 and 0.83 respectively. Our results show
that surgical aggregation could be used to develop clinically useful deep
learning models by aggregating knowledge from distributed datasets with diverse
tasks, a step forward towards bridging the gap from bench to bedside.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures, 5 tables, submitted to MIDL 2023 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DIGITOUR: Automatic Digital Tours for Real-Estate Properties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prateek Chhikara, Harshul Kuhar, Anil Goyal, Chirag Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A virtual or digital tour is a form of virtual reality technology which
allows a user to experience a specific location remotely. Currently, these
virtual tours are created by following a 2-step strategy. First, a photographer
clicks a 360 degree equirectangular image; then, a team of annotators manually
links these images for the "walkthrough" user experience. The major challenge
in the mass adoption of virtual tours is the time and cost involved in manual
annotation/linking of images. Therefore, this paper presents an end-to-end
pipeline to automate the generation of 3D virtual tours using equirectangular
images for real-estate properties. We propose a novel HSV-based coloring scheme
for paper tags that need to be placed at different locations before clicking
the equirectangular images using 360 degree cameras. These tags have two
characteristics: i) they are numbered to help the photographer for placement of
tags in sequence and; ii) bi-colored, which allows better learning of tag
detection (using YOLOv5 architecture) in an image and digit recognition (using
custom MobileNet architecture) tasks. Finally, we link/connect all the
equirectangular images based on detected tags. We show the efficiency of the
proposed pipeline on a real-world equirectangular image dataset collected from
the Housing.com database.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at CODS-COMAD '23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature-based Image Matching for Identifying Individual Kākā 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fintan O'Sullivan, Kirita-Rose Escott, Rachael Shaw, Andrew Lensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report investigates an unsupervised, feature-based image matching
pipeline for the novel application of identifying individual k\=ak\=a. Applied
with a similarity network for clustering, this addresses a weakness of current
supervised approaches to identifying individual birds which struggle to handle
the introduction of new individuals to the population. Our approach uses object
localisation to locate k\=ak\=a within images and then extracts local features
that are invariant to rotation and scale. These features are matched between
images with nearest neighbour matching techniques and mismatch removal to
produce a similarity score for image match comparison. The results show that
matches obtained via the image matching pipeline achieve high accuracy of true
matches. We conclude that feature-based image matching could be used with a
similarity network to provide a viable alternative to existing supervised
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, honour's report from Victoria University of Wellington</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable, Interpretable & Trustworthy AI for Intelligent Digital
  Twin: Case Study on Remaining Useful Life 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuma Kobayashi, Bader Almutairi, Md Nazmus Sakib, Souvik Chakraborty, Syed B. Alam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) and Artificial Intelligence (AI) are increasingly used
in energy and engineering systems, but these models must be fair, unbiased, and
explainable. It is critical to have confidence in AI's trustworthiness. ML
techniques have been useful in predicting important parameters and improving
model performance. However, for these AI techniques to be useful for making
decisions, they need to be audited, accounted for, and easy to understand.
Therefore, the use of Explainable AI (XAI) and interpretable machine learning
(IML) is crucial for the accurate prediction of prognostics, such as remaining
useful life (RUL) in a digital twin system to make it intelligent while
ensuring that the AI model is transparent in its decision-making processes and
that the predictions it generates can be understood and trusted by users. By
using AI that is explainable, interpretable, and trustworthy, intelligent
digital twin systems can make more accurate predictions of RUL, leading to
better maintenance and repair planning and, ultimately, improved system
performance. The objective of this paper is to understand the idea of XAI and
IML and justify the important role of ML/AI in the Digital Twin framework and
components, which requires XAI to understand the prediction better. This paper
explains the importance of XAI and IML in both local and global aspects to
ensure the use of trustworthy ML/AI applications for RUL prediction. This paper
used the RUL prediction for the XAI and IML studies and leveraged the
integrated python toolbox for interpretable machine learning (PiML).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-fidelity surrogate modeling for temperature field prediction using
  deep convolution neural network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunyang Zhang, Zhiqiang Gong, Weien Zhou, Xiaoyu Zhao, Xiaohu Zheng, Wen Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temperature field prediction is of great importance in the thermal design of
systems engineering, and building the surrogate model is an effective way for
the task. Generally, large amounts of labeled data are required to guarantee a
good prediction performance of the surrogate model, especially the deep
learning model, which have more parameters and better representational ability.
However, labeled data, especially high-fidelity labeled data, are usually
expensive to obtain and sometimes even impossible. To solve this problem, this
paper proposes a pithy deep multi-fidelity model (DMFM) for temperature field
prediction, which takes advantage of low-fidelity data to boost the performance
with less high-fidelity data. First, a pre-train and fine-tune paradigm are
developed in DMFM to train the low-fidelity and high-fidelity data, which
significantly reduces the complexity of the deep surrogate model. Then, a
self-supervised learning method for training the physics-driven deep
multi-fidelity model (PD-DMFM) is proposed, which fully utilizes the physics
characteristics of the engineering systems and reduces the dependence on large
amounts of labeled low-fidelity data in the training process. Two diverse
temperature field prediction problems are constructed to validate the
effectiveness of DMFM and PD-DMFM, and the result shows that the proposed
method can greatly reduce the dependence of the model on high-fidelity data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Topology Learning Under Privacy Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Zhang. Qiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph learning, which aims to infer the underlying topology behind high
dimension data, has attracted intense attention. In this study, we shed a new
light on graph learning by considering a pragmatic scenario where data are
privacy sensitive and located in separated clients (devices or organizations).
The main difficulty in learning graphs in this scenario is that we cannot
process all the data in a central server, because the data are not allowed to
leave the local clients due to privacy concerns. The problem becomes more
challenging when data of different clients are non-IID, since it is
unreasonable to learn a global graph for heterogeneous data. To address these
issues, we propose a novel framework in which a personalized graph for each
client and a consensus graph are jointly learned in a federated fashion.
Specifically, we commute model updates instead of raw data to the central
server in the proposed federated algorithm. A provable convergence analysis
shows that the algorithm enjoys $\mathcal{O}(1/T)$ convergence rate. To further
enhance privacy, we design a deferentially privacy algorithm to prevent the
information of the raw data from being leaked when transferring model updates.
A theoretical guidance is provided on how to ensure that the algorithm
satisfies differential privacy. We also analyze the impact of differential
privacy on the convergence of our algorithm. Finally, extensive experiments on
both synthetic and real world data are carried out to validate the proposed
models and algorithms. Experimental results illustrate that our framework is
able to learn graphs effectively in the target scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Deep Traffic Forecasting Models with Dynamic Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Zhihao Zheng, Seongjin Choi, Lijun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A common assumption in deep learning-based multivariate and multistep traffic
time series forecasting models is that residuals are independent, isotropic,
and uncorrelated in space and time. While this assumption provides a
straightforward loss function (such as MAE/MSE), it is inevitable that residual
processes will exhibit strong autocorrelation and structured spatiotemporal
correlation. In this paper, we propose a complementary dynamic regression (DR)
framework to enhance existing deep multistep traffic forecasting frameworks
through structured specifications and learning for the residual process.
Specifically, we assume the residuals of the base model (i.e., a well-developed
traffic forecasting model) are governed by a matrix-variate seasonal
autoregressive (AR) model, which can be seamlessly integrated into the training
process by redesigning the overall loss function. Parameters in the DR
framework can be jointly learned with the base model. We evaluate the
effectiveness of the proposed framework in enhancing several state-of-the-art
deep traffic forecasting models on both speed and flow datasets. Our experiment
results show that the DR framework not only improves existing traffic
forecasting models but also offers interpretable regression coefficients and
spatiotemporal covariance matrices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Async-HFL: Efficient and Robust Asynchronous Federated Learning in
  Hierarchical IoT Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofan Yu, Ludmila Cherkasova, Harsh Vardhan, Quanling Zhao, Emily Ekaireb, Xiyuan Zhang, Arya Mazumdar, Tajana Rosing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) has gained increasing interest in recent years as a
distributed on-device learning paradigm. However, multiple challenges remain to
be addressed for deploying FL in real-world Internet-of-Things (IoT) networks
with hierarchies. Although existing works have proposed various approaches to
account data heterogeneity, system heterogeneity, unexpected stragglers and
scalibility, none of them provides a systematic solution to address all of the
challenges in a hierarchical and unreliable IoT network. In this paper, we
propose an asynchronous and hierarchical framework (Async-HFL) for performing
FL in a common three-tier IoT network architecture. In response to the largely
varied delays, Async-HFL employs asynchronous aggregations at both the gateway
and the cloud levels thus avoids long waiting time. To fully unleash the
potential of Async-HFL in converging speed under system heterogeneities and
stragglers, we design device selection at the gateway level and device-gateway
association at the cloud level. Device selection chooses edge devices to
trigger local training in real-time while device-gateway association determines
the network topology periodically after several cloud epochs, both satisfying
bandwidth limitation. We evaluate Async-HFL's convergence speedup using
large-scale simulations based on ns-3 and a network topology from NYCMesh. Our
results show that Async-HFL converges 1.08-1.31x faster in wall-clock time and
saves up to 21.6% total communication cost compared to state-of-the-art
asynchronous FL algorithms (with client selection). We further validate
Async-HFL on a physical deployment and observe robust convergence under
unexpected stragglers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Deep Networks with the Mesh Adaptive Direct Search algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dounia Lakhmiri, Mahdi Zolnouri, Vahid Partovi Nia, Christophe Tribes, Sébastien Le Digabel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are getting larger. Their implementation on edge and IoT
devices becomes more challenging and moved the community to design lighter
versions with similar performance. Standard automatic design tools such as
\emph{reinforcement learning} and \emph{evolutionary computing} fundamentally
rely on cheap evaluations of an objective function. In the neural network
design context, this objective is the accuracy after training, which is
expensive and time-consuming to evaluate. We automate the design of a light
deep neural network for image classification using the \emph{Mesh Adaptive
Direct Search}(MADS) algorithm, a mature derivative-free optimization method
that effectively accounts for the expensive blackbox nature of the objective
function to explore the design space, even in the presence of constraints.Our
tests show competitive compression rates with reduced numbers of trials.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Falsification of Digital Twins 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rob Cornish, Muhammad Faaiz Taufiq, Arnaud Doucet, Chris Holmes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital twins hold substantial promise in many applications, but rigorous
procedures for assessing their accuracy are essential for their widespread
deployment in safety-critical settings. By formulating this task within the
framework of causal inference, we show it is not possible to certify that a
twin is "correct" using real-world observational data unless potentially
tenuous assumptions are made about the data-generating process. To avoid these
assumptions, we propose an assessment strategy that instead aims to find cases
where the twin is not correct, and present a general-purpose statistical
procedure for doing so that may be used across a wide variety of applications
and twin models. Our approach yields reliable and actionable information about
the twin under only the assumption of an i.i.d. dataset of real-world
observations, and in particular remains sound even in the presence of arbitrary
unmeasured confounding. We demonstrate the effectiveness of our methodology via
a large-scale case study involving sepsis modelling within the Pulse Physiology
Engine, which we assess using the MIMIC-III dataset of ICU patients.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning a Formality-Aware Japanese Sentence Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henry Li Xinyuan, Ray Lee, Jerry Chen, Kelly Marchisio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the way intermediate representations are generated in encoder-decoder
sequence-to-sequence models typically allow them to preserve the semantics of
the input sentence, input features such as formality might be left out. On the
other hand, downstream tasks such as translation would benefit from working
with a sentence representation that preserves formality in addition to
semantics, so as to generate sentences with the appropriate level of social
formality -- the difference between speaking to a friend versus speaking with a
supervisor. We propose a sequence-to-sequence method for learning a
formality-aware representation for Japanese sentences, where sentence
generation is conditioned on both the original representation of the input
sentence, and a side constraint which guides the sentence representation
towards preserving formality information. Additionally, we propose augmenting
the sentence representation with a learned representation of formality which
facilitates the extraction of formality in downstream tasks. We address the
lack of formality-annotated parallel data by adapting previous works on
procedural formality classification of Japanese sentences. Experimental results
suggest that our techniques not only helps the decoder recover the formality of
the input sentence, but also slightly improves the preservation of input
sentence semantics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual-sPLS: a family of Dual Sparse Partial Least Squares regressions for
  feature selection and prediction with tunable sparsity; evaluation on
  simulated and near-infrared (NIR) data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Louna Alsouki, Laurent Duval, Clément Marteau, Rami El Haddad, François Wahl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relating a set of variables X to a response y is crucial in chemometrics. A
quantitative prediction objective can be enriched by qualitative data
interpretation, for instance by locating the most influential features. When
high-dimensional problems arise, dimension reduction techniques can be used.
Most notable are projections (e.g. Partial Least Squares or PLS ) or variable
selections (e.g. lasso). Sparse partial least squares combine both strategies,
by blending variable selection into PLS. The variant presented in this paper,
Dual-sPLS, generalizes the classical PLS1 algorithm. It provides balance
between accurate prediction and efficient interpretation. It is based on
penalizations inspired by classical regression methods (lasso, group lasso,
least squares, ridge) and uses the dual norm notion. The resulting sparsity is
enforced by an intuitive shrinking ratio parameter. Dual-sPLS favorably
compares to similar regression methods, on simulated and real chemical data.
Code is provided as an open-source package in R:
\url{https://CRAN.R-project.org/package=dual.spls}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Artificial Neuronal Ensembles with Learned Context Dependent Gating 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew James Tilley, Michelle Miller, David Freedman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biological neural networks are capable of recruiting different sets of
neurons to encode different memories. However, when training artificial neural
networks on a set of tasks, typically, no mechanism is employed for selectively
producing anything analogous to these neuronal ensembles. Further, artificial
neural networks suffer from catastrophic forgetting, where the network's
performance rapidly deteriorates as tasks are learned sequentially. By
contrast, sequential learning is possible for a range of biological organisms.
We introduce Learned Context Dependent Gating (LXDG), a method to flexibly
allocate and recall `artificial neuronal ensembles', using a particular network
structure and a new set of regularization terms. Activities in the hidden
layers of the network are modulated by gates, which are dynamically produced
during training. The gates are outputs of networks themselves, trained with a
sigmoid output activation. The regularization terms we have introduced
correspond to properties exhibited by biological neuronal ensembles. The first
term penalizes low gate sparsity, ensuring that only a specified fraction of
the network is used. The second term ensures that previously learned gates are
recalled when the network is presented with input from previously learned
tasks. Finally, there is a regularization term responsible for ensuring that
new tasks are encoded in gates that are as orthogonal as possible from
previously used ones. We demonstrate the ability of this method to alleviate
catastrophic forgetting on continual learning benchmarks. When the new
regularization terms are included in the model along with Elastic Weight
Consolidation (EWC) it achieves better performance on the benchmark `permuted
MNIST' than with EWC alone. The benchmark `rotated MNIST' demonstrates how
similar tasks recruit similar neurons to the artificial neuronal ensemble.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Combinatorial Semi-Bandit Approach to Charging Station Selection for
  Electric Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Åkerblom, Morteza Haghir Chehreghani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we address the problem of long-distance navigation for battery
electric vehicles (BEVs), where one or more charging sessions are required to
reach the intended destination. We consider the availability and performance of
the charging stations to be unknown and stochastic, and develop a combinatorial
semi-bandit framework for exploring the road network to learn the parameters of
the queue time and charging power distributions. Within this framework, we
first outline a pre-processing for the road network graph to handle the
constrained combinatorial optimization problem in an efficient way. Then, for
the pre-processed graph, we use a Bayesian approach to model the stochastic
edge weights, utilizing conjugate priors for the one-parameter exponential and
two-parameter gamma distributions, the latter of which is novel to multi-armed
bandit literature. Finally, we apply combinatorial versions of Thompson
Sampling, BayesUCB and Epsilon-greedy to the problem. We demonstrate the
performance of our framework on long-distance navigation problem instances in
country-sized road networks, with simulation experiments in Norway, Sweden and
Finland.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting mass-radius relationships for exoplanet populations: a
  machine learning insight 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahdiyar Mousavi-Sadr, Davood M. Jassur, Ghassem Gozaliasl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing number of exoplanet discoveries and advances in machine learning
techniques allow us to find, explore, and understand characteristics of these
new worlds beyond our Solar System. We analyze the dataset of 762 confirmed
exoplanets and eight Solar System planets using efficient machine-learning
approaches to characterize their fundamental quantities. By adopting different
unsupervised clustering algorithms, the data are divided into two main classes:
planets with $\log R_{p}\leq0.91R_{\oplus}$ and $\log M_{p}\leq1.72M_{\oplus}$
as class 1 and those with $\log R_{p}>0.91R_{\oplus}$ and $\log
M_{p}>1.72M_{\oplus}$ as class 2. Various regression models are used to reveal
correlations between physical parameters and evaluate their performance. We
find that planetary mass, orbital period, and stellar mass play preponderant
roles in predicting exoplanet radius. The validation metrics (RMSE, MAE, and
$R^{2}$) suggest that the Support Vector Regression has, by and large, better
performance than other models and is a promising model for obtaining planetary
radius. Not only do we improve the prediction accuracy in logarithmic space,
but also we derive parametric equations using the M5P and Markov Chain Monte
Carlo methods. Planets of class 1 are shown to be consistent with a positive
linear mass-radius relation, while for planets of class 2, the planetary radius
represents a strong correlation with their host stars' masses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to MNRAS. 15 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Heterogeneous Multi-Robot Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Bettini, Ajay Shankar, Amanda Prorok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cooperative multi-robot tasks can benefit from heterogeneity in the robots'
physical and behavioral traits. In spite of this, traditional Multi-Agent
Reinforcement Learning (MARL) frameworks lack the ability to explicitly
accommodate policy heterogeneity, and typically constrain agents to share
neural network parameters. This enforced homogeneity limits application in
cases where the tasks benefit from heterogeneous behaviors. In this paper, we
crystallize the role of heterogeneity in MARL policies. Towards this end, we
introduce Heterogeneous Graph Neural Network Proximal Policy Optimization
(HetGPPO), a paradigm for training heterogeneous MARL policies that leverages a
Graph Neural Network for differentiable inter-agent communication. HetGPPO
allows communicating agents to learn heterogeneous behaviors while enabling
fully decentralized training in partially observable environments. We
complement this with a taxonomical overview that exposes more heterogeneity
classes than previously identified. To motivate the need for our model, we
present a characterization of techniques that homogeneous models can leverage
to emulate heterogeneous behavior, and show how this "apparent heterogeneity"
is brittle in real-world conditions. Through simulations and real-world
experiments, we show that: (i) when homogeneous methods fail due to strong
heterogeneous requirements, HetGPPO succeeds, and, (ii) when homogeneous
methods are able to learn apparently heterogeneous behaviors, HetGPPO achieves
higher resilience to both training and deployment noise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Robust Deep Reinforcement Learning Requires Redefining
  Robustness <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ezgi Korkmaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning from raw high dimensional data via interaction with a given
environment has been effectively achieved through the utilization of deep
neural networks. Yet the observed degradation in policy performance caused by
imperceptible worst-case policy dependent translations along high sensitivity
directions (i.e. adversarial perturbations) raises concerns on the robustness
of deep reinforcement learning policies. In our paper, we show that these high
sensitivity directions do not lie only along particular worst-case directions,
but rather are more abundant in the deep neural policy landscape and can be
found via more natural means in a black-box setting. Furthermore, we show that
vanilla training techniques intriguingly result in learning more robust
policies compared to the policies learnt via the state-of-the-art adversarial
training techniques. We believe our work lays out intriguing properties of the
deep reinforcement learning policy manifold and our results can help to build
robust and generalizable deep reinforcement learning policies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Keypoint-GraspNet: Keypoint-based 6-DoF Grasp Generation from the
  Monocular RGB-D input <span class="chip">ICRA2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.08752v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.08752v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiye Chen, Yunzhi Lin, Patricio Vela
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Great success has been achieved in the 6-DoF grasp learning from the point
cloud input, yet the computational cost due to the point set orderlessness
remains a concern. Alternatively, we explore the grasp generation from the
RGB-D input in this paper. The proposed solution, Keypoint-GraspNet, detects
the projection of the gripper keypoints in the image space and then recover the
SE(3) poses with a PnP algorithm. A synthetic dataset based on the primitive
shape and the grasp family is constructed to examine our idea. Metric-based
evaluation reveals that our method outperforms the baselines in terms of the
grasp proposal accuracy, diversity, and the time cost. Finally, robot
experiments show high success rate, demonstrating the potential of the idea in
the real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Elastic Similarity and Distance Measures for Multivariate Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.10231v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.10231v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Shifaz, Charlotte Pelletier, Francois Petitjean, Geoffrey I. Webb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper contributes multivariate versions of seven commonly used elastic
similarity and distance measures for time series data analytics. Elastic
similarity and distance measures are a class of similarity measures that can
compensate for misalignments in the time axis of time series data. We adapt two
existing strategies used in a multivariate version of the well-known Dynamic
Time Warping (DTW), namely, Independent and Dependent DTW, to these seven
measures.
  While these measures can be applied to various time series analysis tasks, we
demonstrate their utility on multivariate time series classification using the
nearest neighbor classifier. On 23 well-known datasets, we demonstrate that
each of the measures but one achieves the highest accuracy relative to others
on at least one dataset, supporting the value of developing a suite of
multivariate similarity and distance measures. We also demonstrate that there
are datasets for which either the dependent versions of all measures are more
accurate than their independent counterparts or vice versa. In addition, we
also construct a nearest neighbor-based ensemble of the measures and show that
it is competitive to other state-of-the-art single-strategy multivariate time
series classifiers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recipes for when Physics Fails: Recovering Robust Learning of Physics
  Informed Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.13330v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.13330v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chandrajit Bajaj, Luke McLennan, Timothy Andeen, Avik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-informed Neural Networks (PINNs) have been shown to be effective in
solving partial differential equations by capturing the physics induced
constraints as a part of the training loss function. This paper shows that a
PINN can be sensitive to errors in training data and overfit itself in
dynamically propagating these errors over the domain of the solution of the
PDE. It also shows how physical regularizations based on continuity criteria
and conservation laws fail to address this issue and rather introduce problems
of their own causing the deep network to converge to a physics-obeying local
minimum instead of the global minimum. We introduce Gaussian Process (GP) based
smoothing that recovers the performance of a PINN and promises a robust
architecture against noise/errors in measurements. Additionally, we illustrate
an inexpensive method of quantifying the evolution of uncertainty based on the
variance estimation of GPs on boundary data. Robust PINN performance is also
shown to be achievable by choice of sparse sets of inducing points based on
sparsely induced GPs. We demonstrate the performance of our proposed methods
and compare the results from existing benchmark models in literature for
time-dependent Schr\"odinger and Burgers' equations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Machine Learning: Science and Technology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthetic <span class="highlight-title">Dataset</span> Generation for Privacy-Preserving Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.03205v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.03205v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Efstathia Soufleri, Gobinda Saha, Kaushik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Learning (ML) has achieved enormous success in solving a variety of
problems in computer vision, speech recognition, object detection, to name a
few. The principal reason for this success is the availability of huge datasets
for training deep neural networks (DNNs). However, datasets cannot be publicly
released if they contain sensitive information such as medical records, and
data privacy becomes a major concern. Encryption methods could be a possible
solution, however their deployment on ML applications seriously impacts
classification accuracy and results in substantial computational overhead.
Alternatively, obfuscation techniques could be used, but maintaining a good
trade-off between visual privacy and accuracy is challenging. In this paper, we
propose a method to generate secure synthetic datasets from the original
private datasets. Given a network with Batch Normalization (BN) layers
pretrained on the original dataset, we first record the class-wise BN layer
statistics. Next, we generate the synthetic dataset by optimizing random noise
such that the synthetic data match the layer-wise statistical distribution of
original images. We evaluate our method on image classification datasets
(CIFAR10, ImageNet) and show that synthetic data can be used in place of the
original CIFAR10/ImageNet data for training networks from scratch, producing
comparable classification performance. Further, to analyze visual privacy
provided by our method, we use Image Quality Metrics and show high degree of
visual dissimilarity between the original and synthetic images. Moreover, we
show that our proposed method preserves data-privacy under various
privacy-leakage attacks including Gradient Matching Attack, Model Memorization
Attack, and GAN-based Attack.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>There was a bug in the code. An updated version will be archived soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compositional Visual Generation with Composable Diffusion Models <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.01714v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.01714v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, Joshua B. Tenenbaum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large text-guided diffusion models, such as DALLE-2, are able to generate
stunning photorealistic images given natural language descriptions. While such
models are highly flexible, they struggle to understand the composition of
certain concepts, such as confusing the attributes of different objects or
relations between objects. In this paper, we propose an alternative structured
approach for compositional generation using diffusion models. An image is
generated by composing a set of diffusion models, with each of them modeling a
certain component of the image. To do this, we interpret diffusion models as
energy-based models in which the data distributions defined by the energy
functions may be explicitly combined. The proposed method can generate scenes
at test time that are substantially more complex than those seen in training,
composing sentence descriptions, object relations, human facial attributes, and
even generalizing to new combinations that are rarely seen in the real world.
We further illustrate how our approach may be used to compose pre-trained
text-guided diffusion models and generate photorealistic images containing all
the details described in the input descriptions, including the binding of
certain object attributes that have been shown difficult for DALLE-2. These
results point to the effectiveness of the proposed method in promoting
structured generalization for visual generation. Project page:
https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022. First three authors contributed equally. Project website:
  https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Approaching Peak Ground Truth 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00243v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00243v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Kofler, Johannes Wahle, Ivan Ezhov, Sophia Wagner, Rami Al-Maskari, Emilia Gryska, Mihail Todorov, Christina Bukas, Felix Meissen, Tingying Peng, Ali Ertürk, Daniel Rueckert, Rolf Heckemann, Jan Kirschke, Claus Zimmer, Benedikt Wiestler, Bjoern Menze, Marie Piraud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models are typically evaluated by computing similarity with
reference annotations and trained by maximizing similarity with such.
Especially in the bio-medical domain, annotations are subjective and suffer
from low inter- and intra-rater reliability. Since annotations only reflect the
annotation entity's interpretation of the real world, this can lead to
sub-optimal predictions even though the model achieves high similarity scores.
Here, the theoretical concept of Peak Ground Truth (PGT) is introduced. PGT
marks the point beyond which an increase in similarity with the reference
annotation stops translating to better Real World Model Performance (RWMP).
Additionally, a quantitative technique to approximate PGT by computing inter-
and intra-rater reliability is proposed. Finally, three categories of PGT-aware
strategies to evaluate and improve model performance are reviewed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7pages, 2 figures (this updates just affiliations and corrects figure
  rendering)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Random Planted Forest: a directly interpretable tree ensemble 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.14563v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.14563v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Munir Hiabu, Enno Mammen, Joseph T. Meyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel interpretable, tree based algorithm for prediction in a
regression setting in which each tree in a classical random forest is replaced
by a family of planted trees that grow simultaneously. The motivation for our
algorithm is to estimate the unknown regression function from a functional
decomposition perspective, where each tree corresponds to a function within
that decomposition. The maximal order of approximation in the decomposition can
be specified or left unlimited. If a first order approximation is chosen, the
result is an additive model. In the other extreme case, if the order of
approximation is not limited, the resulting model places no restrictions on the
form of the regression function. In a simulation study we find encouraging
prediction and visualisation properties of our random planted forest method. We
also develop theory for an idealised version of random planted forests in cases
where the maximal order of approximation is low. We show that if the order is
smaller than three, the idealised version achieves asymptotically optimal
convergence rates up to a logarithmic factor. ode is available on
https://github.com/PlantedML/randomPlantedForest
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Prominence of Artificial Intelligence in COVID-19 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.09537v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.09537v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        MD Abdullah Al Nasim, Aditi Dhali, Faria Afrin, Noshin Tasnim Zaman, Nazmul Karim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In December 2019, a novel virus called COVID-19 had caused an enormous number
of causalities to date. The battle with the novel Coronavirus is baffling and
horrifying after the Spanish Flu 2019. While the front-line doctors and medical
researchers have made significant progress in controlling the spread of the
highly contiguous virus, technology has also proved its significance in the
battle. Moreover, Artificial Intelligence has been adopted in many medical
applications to diagnose many diseases, even baffling experienced doctors.
Therefore, this survey paper explores the methodologies proposed that can aid
doctors and researchers in early and inexpensive methods of diagnosis of the
disease. Most developing countries have difficulties carrying out tests using
the conventional manner, but a significant way can be adopted with Machine and
Deep Learning. On the other hand, the access to different types of medical
images has motivated the researchers. As a result, a mammoth number of
techniques are proposed. This paper first details the background knowledge of
the conventional methods in the Artificial Intelligence domain. Following that,
we gather the commonly used datasets and their use cases to date. In addition,
we also show the percentage of researchers adopting Machine Learning over Deep
Learning. Thus we provide a thorough analysis of this scenario. Lastly, in the
research challenges, we elaborate on the problems faced in COVID-19 research,
and we address the issues with our understanding to build a bright and healthy
environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>63 pages, 3 tables, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Max Entrywise Error Bounds for Tensor Estimation from Sparse
  Observations via Similarity Based Collaborative Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1908.01241v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1908.01241v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devavrat Shah, Christina Lee Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consider the task of estimating a 3-order $n \times n \times n$ tensor from
noisy observations of randomly chosen entries in the sparse regime. We
introduce a similarity based collaborative filtering algorithm for estimating a
tensor from sparse observations and argue that it achieves sample complexity
that nearly matches the conjectured computationally efficient lower bound on
the sample complexity for the setting of low-rank tensors. Our algorithm uses
the matrix obtained from the flattened tensor to compute similarity, and
estimates the tensor entries using a nearest neighbor estimator. We prove that
the algorithm recovers a finite rank tensor with maximum entry-wise error (MEE)
and mean-squared-error (MSE) decaying to $0$ as long as each entry is observed
independently with probability $p = \Omega(n^{-3/2 + \kappa})$ for any
arbitrarily small $\kappa > 0$. More generally, we establish robustness of the
estimator, showing that when arbitrary noise bounded by $\varepsilon \geq 0$ is
added to each observation, the estimation error with respect to MEE and MSE
degrades by $\text{poly}(\varepsilon)$. Consequently, even if the tensor may
not have finite rank but can be approximated within $\varepsilon \geq 0$ by a
finite rank tensor, then the estimation error converges to
$\text{poly}(\varepsilon)$. Our analysis sheds insight into the conjectured
sample complexity lower bound, showing that it matches the connectivity
threshold of the graph used by our algorithm for estimating similarity between
coordinates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Betting the system: Using lineups to predict football scores <span class="chip">SDM23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.06327v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.06327v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Peters, Diogo Pacheco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper aims to reduce randomness in football by analysing the role of
lineups in final scores using machine learning prediction models we have
developed. Football clubs invest millions of dollars on lineups and knowing how
individual statistics translate to better outcomes can optimise investments.
Moreover, sports betting is growing exponentially and being able to predict the
future is profitable and desirable. We use machine learning models and
historical player data from English Premier League (2020-2022) to predict
scores and to understand how individual performance can improve the outcome of
a match. We compared different prediction techniques to maximise the
possibility of finding useful models. We created heuristic and machine learning
models predicting football scores to compare different techniques. We used
different sets of features and shown goalkeepers stats are more important than
attackers stats to predict goals scored. We applied a broad evaluation process
to assess the efficacy of the models in real world applications. We managed to
predict correctly all relegated teams after forecast 100 consecutive matches.
We show that Support Vector Regression outperformed other techniques predicting
final scores and that lineups do not improve predictions. Finally, our model
was profitable (42% return) when emulating a betting system using real world
odds data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 Page paper submitted for review for SDM23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Genetic algorithm for feature selection of EEG heterogeneous data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.07117v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.07117v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aurora Saibene, Francesca Gasparini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The electroencephalographic (EEG) signals provide highly informative data on
brain activities and functions. However, their heterogeneity and high
dimensionality may represent an obstacle for their interpretation. The
introduction of a priori knowledge seems the best option to mitigate high
dimensionality problems, but could lose some information and patterns present
in the data, while data heterogeneity remains an open issue that often makes
generalization difficult. In this study, we propose a genetic algorithm (GA)
for feature selection that can be used with a supervised or unsupervised
approach. Our proposal considers three different fitness functions without
relying on expert knowledge. Starting from two publicly available datasets on
cognitive workload and motor movement/imagery, the EEG signals are processed,
normalized and their features computed in the time, frequency and
time-frequency domains. The feature vector selection is performed by applying
our GA proposal and compared with two benchmarking techniques. The results show
that different combinations of our proposal achieve better results in respect
to the benchmark in terms of overall performance and feature reduction.
Moreover, the proposed GA, based on a novel fitness function here presented,
outperforms the benchmark when the two different datasets considered are merged
together, showing the effectiveness of our proposal on heterogeneous data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by Expert Systems with Applications (see
  https://www.sciencedirect.com/science/article/abs/pii/S0957417422025076)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward Explainable AI for Regression Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.11407v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.11407v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Letzgus, Patrick Wagner, Jonas Lederer, Wojciech Samek, Klaus-Robert Müller, Gregoire Montavon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In addition to the impressive predictive power of machine learning (ML)
models, more recently, explanation methods have emerged that enable an
interpretation of complex non-linear learning models such as deep neural
networks. Gaining a better understanding is especially important e.g. for
safety-critical ML applications or medical diagnostics etc. While such
Explainable AI (XAI) techniques have reached significant popularity for
classifiers, so far little attention has been devoted to XAI for regression
models (XAIR). In this review, we clarify the fundamental conceptual
differences of XAI for regression and classification tasks, establish novel
theoretical insights and analysis for XAIR, provide demonstrations of XAIR on
genuine practical regression problems, and finally discuss the challenges
remaining for the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 10 figures, published; changes: 1. references to code and
  xai-regression.org added (p. 1/2, end of introduction), 2. adjustment of
  sign-error in restructuring section (p. 8, just above Fig. 4)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Split-kl and PAC-Bayes-split-kl Inequalities for Ternary Random
  Variables <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.00706v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.00706v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Shan Wu, Yevgeny Seldin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new concentration of measure inequality for sums of independent
bounded random variables, which we name a split-kl inequality. The inequality
is particularly well-suited for ternary random variables, which naturally show
up in a variety of problems, including analysis of excess losses in
classification, analysis of weighted majority votes, and learning with
abstention. We demonstrate that for ternary random variables the inequality is
simultaneously competitive with the kl inequality, the Empirical Bernstein
inequality, and the Unexpected Bernstein inequality, and in certain regimes
outperforms all of them. It resolves an open question by Tolstikhin and Seldin
[2013] and Mhammedi et al. [2019] on how to match simultaneously the
combinatorial power of the kl inequality when the distribution happens to be
close to binary and the power of Bernstein inequalities to exploit low variance
when the probability mass is concentrated on the middle value. We also derive a
PAC-Bayes-split-kl inequality and compare it with the PAC-Bayes-kl,
PAC-Bayes-Empirical-Bennett, and PAC-Bayes-Unexpected-Bernstein inequalities in
an analysis of excess losses and in an analysis of a weighted majority vote for
several UCI datasets. Last but not least, our study provides the first direct
comparison of the Empirical Bernstein and Unexpected Bernstein inequalities and
their PAC-Bayes extensions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>aligned with the camera-ready version published to NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Prune-and-Select: Class-incremental learning with specialized
  subnetworks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.04952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.04952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandr Dekhovich, David M. J. Tax, Marcel H. F. Sluiter, Miguel A. Bessa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The human brain is capable of learning tasks sequentially mostly without
forgetting. However, deep neural networks (DNNs) suffer from catastrophic
forgetting when learning one task after another. We address this challenge
considering a class-incremental learning scenario where the DNN sees test data
without knowing the task from which this data originates. During training,
Continual-Prune-and-Select (CP&S) finds a subnetwork within the DNN that is
responsible for solving a given task. Then, during inference, CP&S selects the
correct subnetwork to make predictions for that task. A new task is learned by
training available neuronal connections of the DNN (previously untrained) to
create a new subnetwork by pruning, which can include previously trained
connections belonging to other subnetwork(s) because it does not update shared
connections. This enables to eliminate catastrophic forgetting by creating
specialized regions in the DNN that do not conflict with each other while still
allowing knowledge transfer across them. The CP&S strategy is implemented with
different subnetwork selection strategies, revealing superior performance to
state-of-the-art continual learning methods tested on various datasets
(CIFAR-100, CUB-200-2011, ImageNet-100 and ImageNet-1000). In particular, CP&S
is capable of sequentially learning 10 tasks from ImageNet-1000 keeping an
accuracy around 94% with negligible forgetting, a first-of-its-kind result in
class-incremental learning. To the best of the authors' knowledge, this
represents an improvement in accuracy above 10% when compared to the best
alternative method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chebyshev-Cantelli PAC-Bayes-Bennett Inequality for the Weighted
  Majority Vote <span class="chip">NeurIPS 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.13624v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.13624v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Shan Wu, Andrés R. Masegosa, Stephan S. Lorenzen, Christian Igel, Yevgeny Seldin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new second-order oracle bound for the expected risk of a
weighted majority vote. The bound is based on a novel parametric form of the
Chebyshev- Cantelli inequality (a.k.a. one-sided Chebyshev's), which is
amenable to efficient minimization. The new form resolves the optimization
challenge faced by prior oracle bounds based on the Chebyshev-Cantelli
inequality, the C-bounds [Germain et al., 2015], and, at the same time, it
improves on the oracle bound based on second order Markov's inequality
introduced by Masegosa et al. [2020]. We also derive a new concentration of
measure inequality, which we name PAC-Bayes-Bennett, since it combines
PAC-Bayesian bounding with Bennett's inequality. We use it for empirical
estimation of the oracle bound. The PAC-Bayes-Bennett inequality improves on
the PAC-Bayes-Bernstein inequality of Seldin et al. [2012]. We provide an
empirical evaluation demonstrating that the new bounds can improve on the work
of Masegosa et al. [2020]. Both the parametric form of the Chebyshev-Cantelli
inequality and the PAC-Bayes-Bennett inequality may be of independent interest
for the study of concentration of measure in other domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>aligned with the camera-ready version published at NeurIPS 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Necessary and Sufficient Conditions for Inverse Reinforcement Learning
  of Bayesian Stopping Time Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2007.03481v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2007.03481v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunal Pattanayak, Vikram Krishnamurthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an inverse reinforcement learning~(IRL) framework for
Bayesian stopping time problems. By observing the actions of a Bayesian
decision maker, we provide a necessary and sufficient condition to identify if
these actions are consistent with optimizing a cost function. In a Bayesian
(partially observed) setting, the inverse learner can at best identify
optimality wrt the observed actions. Our IRL algorithm identifies optimality
and then constructs set valued estimates of the cost function. To achieve this
IRL objective, we use novel ideas from Bayesian revealed preferences stemming
from microeconomics. We illustrate the proposed IRL scheme using two important
examples of stopping time problems, namely, sequential hypothesis testing and
Bayesian search, and also on a real-world YouTube dataset. Finally, for finite
datasets, we propose an IRL detection algorithm and give finite sample bounds
on its error probabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian statistical learning using density operators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.14715v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.14715v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yann Berquin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This short study reformulates the statistical Bayesian learning problem using
a quantum mechanics framework. Density operators representing ensembles of pure
states of sample wave functions are used in place probability densities. We
show that such representation allows to formulate the statistical Bayesian
learning problem in different coordinate systems on the sample space. We
further show that such representation allows to learn projections of density
operators using a kernel trick. In particular, the study highlights that
decomposing wave functions rather than probability densities, as it is done in
kernel embedding, allows to preserve the nature of probability operators.
Results are illustrated with a simple example using discrete orthogonal wavelet
transform of density operators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantized Training of Gradient Boosting Decision Trees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09682v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09682v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Shi, Guolin Ke, Zhuoming Chen, Shuxin Zheng, Tie-Yan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed significant success in Gradient Boosting Decision
Trees (GBDT) for a wide range of machine learning applications. Generally, a
consensus about GBDT's training algorithms is gradients and statistics are
computed based on high-precision floating points. In this paper, we investigate
an essentially important question which has been largely ignored by the
previous literature: how many bits are needed for representing gradients in
training GBDT? To solve this mystery, we propose to quantize all the
high-precision gradients in a very simple yet effective way in the GBDT's
training algorithm. Surprisingly, both our theoretical analysis and empirical
studies show that the necessary precisions of gradients without hurting any
performance can be quite low, e.g., 2 or 3 bits. With low-precision gradients,
most arithmetic operations in GBDT training can be replaced by integer
operations of 8, 16, or 32 bits. Promisingly, these findings may pave the way
for much more efficient training of GBDT from several aspects: (1) speeding up
the computation of gradient statistics in histograms; (2) compressing the
communication cost of high-precision statistical information during distributed
training; (3) the inspiration of utilization and development of hardware
architectures which well support low-precision computation for GBDT training.
Benchmarked on CPUs, GPUs, and distributed clusters, we observe up to 2$\times$
speedup of our simple quantization strategy compared with SOTA GBDT systems on
extensive datasets, demonstrating the effectiveness and potential of the
low-precision training of GBDT. The code will be released to the official
repository of LightGBM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ torchode: A Parallel ODE Solver for PyTorch <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.12375v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.12375v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marten Lienen, Stephan Günnemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce an ODE solver for the PyTorch ecosystem that can solve multiple
ODEs in parallel independently from each other while achieving significant
performance gains. Our implementation tracks each ODE's progress separately and
is carefully optimized for GPUs and compatibility with PyTorch's JIT compiler.
Its design lets researchers easily augment any aspect of the solver and collect
and analyze internal solver statistics. In our experiments, our implementation
is up to 4.3 times faster per step than other ODE solvers and it is robust
against within-batch interactions that lead other solvers to take up to 4 times
as many steps.
  Code available at https://github.com/martenlienen/torchode
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at The Symbiosis of Deep Learning and Differential Equations
  Workshop, NeurIPS, 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Deep Reinforcement Learning Approach for Online Parcel Assignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.03467v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.03467v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zeng, Qiong Wu, Kunpeng Han, Junying He, Haoyuan Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the online parcel assignment (OPA) problem, in
which each stochastically generated parcel needs to be assigned to a candidate
route for delivery to minimize the total cost subject to certain business
constraints. The OPA problem is challenging due to its stochastic nature: each
parcel's candidate routes, which depends on the parcel's origin, destination,
weight, etc., are unknown until its order is placed, and the total parcel
volume is uncertain in advance. To tackle this challenge, we propose the
PPO-OPA algorithm based on deep reinforcement learning that shows competitive
performance. More specifically, we introduce a novel Markov Decision Process
(MDP) framework to model the OPA problem, and develop a policy gradient
algorithm that adopts attention networks for policy evaluation. By designing a
dedicated reward function, our proposed algorithm can achieve a lower total
cost with smaller violation of constraints, comparing to the traditional method
which assigns parcels to candidate routes proportionally. In addition, the
performances of our proposed algorithm and the Primal-Dual algorithm are
comparable, while the later assumes a known total parcel volume in advance,
which is unrealistic in practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D-C2FT: Coarse-to-fine <span class="highlight-title">Transformer</span> for Multi-view 3D Reconstruction <span class="chip">ACCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.14575v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.14575v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leslie Ching Ow Tiong, Dick Sigmund, Andrew Beng Jin Teoh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the transformer model has been successfully employed for the
multi-view 3D reconstruction problem. However, challenges remain on designing
an attention mechanism to explore the multiview features and exploit their
relations for reinforcing the encoding-decoding modules. This paper proposes a
new model, namely 3D coarse-to-fine transformer (3D-C2FT), by introducing a
novel coarse-to-fine(C2F) attention mechanism for encoding multi-view features
and rectifying defective 3D objects. C2F attention mechanism enables the model
to learn multi-view information flow and synthesize 3D surface correction in a
coarse to fine-grained manner. The proposed model is evaluated by ShapeNet and
Multi-view Real-life datasets. Experimental results show that 3D-C2FT achieves
notable results and outperforms several competing models on these datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Asian Conference on Computer Vision (ACCV) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Why do Nearest Neighbor Language Models Work? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.02828v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.02828v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frank F. Xu, Uri Alon, Graham Neubig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) compute the probability of a text by sequentially
computing a representation of an already-seen context and using this
representation to predict the next word. Currently, most LMs calculate these
representations through a neural network consuming the immediate previous
context. However recently, retrieval-augmented LMs have shown to improve over
standard neural LMs, by accessing information retrieved from a large datastore,
in addition to their standard, parametric, next-word prediction. In this paper,
we set out to understand why retrieval-augmented language models, and
specifically why k-nearest neighbor language models (kNN-LMs) perform better
than standard parametric LMs, even when the k-nearest neighbor component
retrieves examples from the same training set that the LM was originally
trained on. To this end, we perform a careful analysis of the various
dimensions over which kNN-LM diverges from standard LMs, and investigate these
dimensions one by one. Empirically, we identify three main reasons why kNN-LM
performs better than standard LMs: using a different input representation for
predicting the next tokens, approximate kNN search, and the importance of
softmax temperature for the kNN distribution. Further, we incorporate these
insights into the model architecture or the training procedure of the standard
parametric LM, improving its results without the need for an explicit retrieval
component. The code is available at https://github.com/frankxu2004/knnlm-why.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, 21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RenyiCL: Contrastive Representation Learning with Skew Renyi Divergence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.06270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.06270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyungmin Lee, Jinwoo Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive representation learning seeks to acquire useful representations
by estimating the shared information between multiple views of data. Here, the
choice of data augmentation is sensitive to the quality of learned
representations: as harder the data augmentations are applied, the views share
more task-relevant information, but also task-irrelevant one that can hinder
the generalization capability of representation. Motivated by this, we present
a new robust contrastive learning scheme, coined R\'enyiCL, which can
effectively manage harder augmentations by utilizing R\'enyi divergence. Our
method is built upon the variational lower bound of R\'enyi divergence, but a
na\"ive usage of a variational method is impractical due to the large variance.
To tackle this challenge, we propose a novel contrastive objective that
conducts variational estimation of a skew R\'enyi divergence and provide a
theoretical guarantee on how variational estimation of skew divergence leads to
stable training. We show that R\'enyi contrastive learning objectives perform
innate hard negative sampling and easy positive sampling simultaneously so that
it can selectively learn useful features and ignore nuisance features. Through
experiments on ImageNet, we show that R\'enyi contrastive learning with
stronger augmentations outperforms other self-supervised methods without extra
regularization or computational overhead. Moreover, we also validate our method
on other domains such as graph and tabular, showing empirical gain over other
contrastive methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Labels, Information, and Computation: Efficient Learning Using
  Sufficient Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.09015v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.09015v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyu Duan, Spencer Chang, Jose C. Principe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In supervised learning, obtaining a large set of fully-labeled training data
is expensive. We show that we do not always need full label information on
every single training example to train a competent classifier. Specifically,
inspired by the principle of sufficiency in statistics, we present a statistic
(a summary) of the fully-labeled training set that captures almost all the
relevant information for classification but at the same time is easier to
obtain directly. We call this statistic "sufficiently-labeled data" and prove
its sufficiency and efficiency for finding the optimal hidden representations,
on which competent classifier heads can be trained using as few as a single
randomly-chosen fully-labeled example per class. Sufficiently-labeled data can
be obtained from annotators directly without collecting the fully-labeled data
first. And we prove that it is easier to directly obtain sufficiently-labeled
data than obtaining fully-labeled data. Furthermore, sufficiently-labeled data
is naturally more secure since it stores relative, instead of absolute,
information. Extensive experimental results are provided to support our theory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tighter Regret Analysis and Optimization of Online Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.06491v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.06491v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dohyeok Kwon, Jonghwan Park, Songnam Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In federated learning (FL), it is commonly assumed that all data are placed
at clients in the beginning of machine learning (ML) optimization (i.e.,
offline learning). However, in many real-world applications, it is expected to
proceed in an online fashion. To this end, online FL (OFL) has been introduced,
which aims at learning a sequence of global models from decentralized streaming
data such that the so-called cumulative regret is minimized. Combining online
gradient descent and model averaging, in this framework, FedOGD is constructed
as the counterpart of FedSGD in FL. While it can enjoy an optimal sublinear
regret, FedOGD suffers from heavy communication costs. In this paper, we
present a communication-efficient method (named OFedIQ) by means of
intermittent transmission (enabled by client subsampling and periodic
transmission) and quantization. For the first time, we derive the regret bound
that captures the impact of data-heterogeneity and the communication-efficient
techniques. Through this, we efficiently optimize the parameters of OFedIQ such
as sampling rate, transmission period, and quantization levels. Also, it is
proved that the optimized OFedIQ can asymptotically achieve the performance of
FedOGD while reducing the communication costs by 99%. Via experiments with real
datasets, we demonstrate the effectiveness of the optimized OFedIQ.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigation of a Machine learning methodology for the SKA pulsar
  search pipeline 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.04430v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.04430v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashank Sanjay Bhat, Thiagaraj Prabu, Ben Stappers, Atul Ghalame, Snehanshu Saha, T. S. B Sudarshan, Zafiirah Hosenie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The SKA pulsar search pipeline will be used for real time detection of
pulsars. Modern radio telescopes such as SKA will be generating petabytes of
data in their full scale of operation. Hence experience-based and data-driven
algorithms become indispensable for applications such as candidate detection.
Here we describe our findings from testing a state of the art object detection
algorithm called Mask R-CNN to detect candidate signatures in the SKA pulsar
search pipeline. We have trained the Mask R-CNN model to detect candidate
images. A custom annotation tool was developed to mark the regions of interest
in large datasets efficiently. We have successfully demonstrated this algorithm
by detecting candidate signatures on a simulation dataset. The paper presents
details of this work with a highlight on the future prospects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KoopmanLab: machine learning for solving complex physics equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.01104v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.01104v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Xiong, Muyuan Ma, Ziyang Zhang, Pei Sun, Yang Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous physics theories are rooted in partial differential equations
(PDEs). However, the increasingly intricate physics equations, especially those
that lack analytic solutions or closed forms, have impeded the further
development of physics. Computationally solving PDEs by classic numerical
approaches suffers from the trade-off between accuracy and efficiency and is
not applicable to the empirical data generated by unknown latent PDEs. To
overcome this challenge, we present KoopmanLab, an efficient module of the
Koopman neural operator family, for learning PDEs without analytic solutions or
closed forms. Our module consists of multiple variants of the Koopman neural
operator (KNO), a kind of mesh-independent neural-network-based PDE solvers
developed following dynamic system theory. The compact variants of KNO can
accurately solve PDEs with small model sizes while the large variants of KNO
are more competitive in predicting highly complicated dynamic systems govern by
unknown, high-dimensional, and non-linear PDEs. All variants are validated by
mesh-independent and long-term prediction experiments implemented on
representative PDEs (e.g., the Navier-Stokes equation and the Bateman-Burgers
equation in fluid mechanics) and ERA5 (i.e., one of the largest high-resolution
global-scale climate data sets in earth physics). These demonstrations suggest
the potential of KoopmanLab to be a fundamental tool in diverse physics studies
related to equations or dynamic systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Observer with Lyapunov Stability Guarantee for Uncertain
  Nonlinear Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.13006v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.13006v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Song Chen, Shengze Cai, Tehuan Chen, Chao Xu, Jian Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel nonlinear observer based on neural
networks, called neural observer, for observation tasks of linear
time-invariant (LTI) systems and uncertain nonlinear systems. In particular,
the neural observer designed for uncertain systems is inspired by the active
disturbance rejection control, which can measure the uncertainty in real-time.
The stability analysis (e.g., exponential convergence rate) of LTI and
uncertain nonlinear systems (involving neural observers) are presented and
guaranteed, where it is shown that the observation problems can be solved only
using the linear matrix inequalities (LMIs). Also, it is revealed that the
observability and controllability of the system matrices are required to
demonstrate the existence of solutions of LMIs. Finally, the effectiveness of
neural observers is verified on three simulation cases, including the X-29A
aircraft model, the nonlinear pendulum, and the four-wheel steering vehicle.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, submitted to IEEE journal for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedDAG: Federated DAG Structure Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.03555v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.03555v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erdun Gao, Junjia Chen, Li Shen, Tongliang Liu, Mingming Gong, Howard Bondell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To date, most directed acyclic graphs (DAGs) structure learning approaches
require data to be stored in a central server. However, due to the
consideration of privacy protection, data owners gradually refuse to share
their personalized raw data to avoid private information leakage, making this
task more troublesome by cutting off the first step. Thus, a puzzle arises:
\textit{how do we discover the underlying DAG structure from decentralized
data?} In this paper, focusing on the additive noise models (ANMs) assumption
of data generation, we take the first step in developing a gradient-based
learning framework named FedDAG, which can learn the DAG structure without
directly touching the local data and also can naturally handle the data
heterogeneity. Our method benefits from a two-level structure of each local
model. The first level structure learns the edges and directions of the graph
and communicates with the server to get the model information from other
clients during the learning procedure, while the second level structure
approximates the mechanisms among variables and personally updates on its own
data to accommodate the data heterogeneity. Moreover, FedDAG formulates the
overall learning task as a continuous optimization problem by taking advantage
of an equality acyclicity constraint, which can be solved by gradient descent
methods to boost the searching efficiency. Extensive experiments on both
synthetic and real-world datasets verify the efficacy of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Transactions on Machine Learning Research</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MANDERA: Malicious Node Detection in Federated Learning via Ranking <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.11736v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.11736v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanchuang Zhu, Benjamin Zi Hao Zhao, Simon Luo, Tongliang Liu, Ke Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Byzantine attacks hinder the deployment of federated learning algorithms.
Although we know that the benign gradients and Byzantine attacked gradients are
distributed differently, to detect the malicious gradients is challenging due
to (1) the gradient is high-dimensional and each dimension has its unique
distribution and (2) the benign gradients and the attacked gradients are always
mixed (two-sample test methods cannot apply directly). To address the above,
for the first time, we propose MANDERA which is theoretically guaranteed to
efficiently detect all malicious gradients under Byzantine attacks with no
prior knowledge or history about the number of attacked nodes. More
specifically, we transfer the original updating gradient space into a ranking
matrix. By such an operation, the scales of different dimensions of the
gradients in the ranking space become identical. The high-dimensional benign
gradients and the malicious gradients can be easily separated. The
effectiveness of MANDERA is further confirmed by experimentation on four
Byzantine attack implementations (Gaussian, Zero Gradient, Sign Flipping,
Shifted Mean), comparing with state-of-the-art defenses. The experiments cover
both IID and Non-IID datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 11 figures, ICML</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MissDAG: Causal Discovery in the Presence of Missing Data with
  Continuous Additive Noise Models <span class="chip">NeurIPS22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.13869v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.13869v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erdun Gao, Ignavier Ng, Mingming Gong, Li Shen, Wei Huang, Tongliang Liu, Kun Zhang, Howard Bondell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art causal discovery methods usually assume that the
observational data is complete. However, the missing data problem is pervasive
in many practical scenarios such as clinical trials, economics, and biology.
One straightforward way to address the missing data problem is first to impute
the data using off-the-shelf imputation methods and then apply existing causal
discovery methods. However, such a two-step method may suffer from
suboptimality, as the imputation algorithm may introduce bias for modeling the
underlying data distribution. In this paper, we develop a general method, which
we call MissDAG, to perform causal discovery from data with incomplete
observations. Focusing mainly on the assumptions of ignorable missingness and
the identifiable additive noise models (ANMs), MissDAG maximizes the expected
likelihood of the visible part of observations under the
expectation-maximization (EM) framework. In the E-step, in cases where
computing the posterior distributions of parameters in closed-form is not
feasible, Monte Carlo EM is leveraged to approximate the likelihood. In the
M-step, MissDAG leverages the density transformation to model the noise
distributions with simpler and specific formulations by virtue of the ANMs and
uses a likelihood-based causal discovery algorithm with directed acyclic graph
constraint. We demonstrate the flexibility of MissDAG for incorporating various
causal discovery algorithms and its efficacy through extensive simulations and
real data experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Counterfactual Fairness with Partially Known Causal Graph <span class="chip">NeurIPS22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.13972v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.13972v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aoqi Zuo, Susan Wei, Tongliang Liu, Bo Han, Kun Zhang, Mingming Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fair machine learning aims to avoid treating individuals or sub-populations
unfavourably based on \textit{sensitive attributes}, such as gender and race.
Those methods in fair machine learning that are built on causal inference
ascertain discrimination and bias through causal effects. Though
causality-based fair learning is attracting increasing attention, current
methods assume the true causal graph is fully known. This paper proposes a
general method to achieve the notion of counterfactual fairness when the true
causal graph is unknown. To be able to select features that lead to
counterfactual fairness, we derive the conditions and algorithms to identify
ancestral relations between variables on a \textit{Partially Directed Acyclic
Graph (PDAG)}, specifically, a class of causal DAGs that can be learned from
observational data combined with domain knowledge. Interestingly, we find that
counterfactual fairness can be achieved as if the true causal graph were fully
known, when specific background knowledge is provided: the sensitive attributes
do not have ancestors in the causal graph. Results on both simulated and
real-world datasets demonstrate the effectiveness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding the Generalization Benefit of Normalization Layers:
  Sharpness Reduction <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.07085v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.07085v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaifeng Lyu, Zhiyuan Li, Sanjeev Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Normalization layers (e.g., Batch Normalization, Layer Normalization) were
introduced to help with optimization difficulties in very deep nets, but they
clearly also help generalization, even in not-so-deep nets. Motivated by the
long-held belief that flatter minima lead to better generalization, this paper
gives mathematical analysis and supporting experiments suggesting that
normalization (together with accompanying weight-decay) encourages GD to reduce
the sharpness of loss surface. Here "sharpness" is carefully defined given that
the loss is scale-invariant, a known consequence of normalization.
Specifically, for a fairly broad class of neural nets with normalization, our
theory explains how GD with a finite learning rate enters the so-called Edge of
Stability (EoS) regime, and characterizes the trajectory of GD in this regime
via a continuous sharpness-reduction flow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>76 pages, many figures; NeurIPS 2022 camera-ready version; fixes
  minor typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Universal Prediction Band via Semi-Definite Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.17203v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.17203v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tengyuan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a computationally efficient method to construct nonparametric,
heteroscedastic prediction bands for uncertainty quantification, with or
without any user-specified predictive model. Our approach provides an
alternative to the now-standard conformal prediction for uncertainty
quantification, with novel theoretical insights and computational advantages.
The data-adaptive prediction band is universally applicable with minimal
distributional assumptions, has strong non-asymptotic coverage properties, and
is easy to implement using standard convex programs. Our approach can be viewed
as a novel variance interpolation with confidence and further leverages
techniques from semi-definite programming and sum-of-squares optimization.
Theoretical and numerical performances for the proposed approach for
uncertainty quantification are analyzed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LB-SimTSC: An Efficient Similarity-Aware Graph Neural Network for
  Semi-Supervised Time Series Classification <span class="chip">AAAI'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.04838v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.04838v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjie Xi, Arnav Jain, Li Zhang, Jessica Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series classification is an important data mining task that has received
a lot of interest in the past two decades. Due to the label scarcity in
practice, semi-supervised time series classification with only a few labeled
samples has become popular. Recently, Similarity-aware Time Series
Classification (SimTSC) is proposed to address this problem by using a graph
neural network classification model on the graph generated from pairwise
Dynamic Time Warping (DTW) distance of batch data. It shows excellent accuracy
and outperforms state-of-the-art deep learning models in several few-label
settings. However, since SimTSC relies on pairwise DTW distances, the quadratic
complexity of DTW limits its usability to only reasonably sized datasets. To
address this challenge, we propose a new efficient semi-supervised time series
classification technique, LB-SimTSC, with a new graph construction module.
Instead of using DTW, we propose to utilize a lower bound of DTW, LB_Keogh, to
approximate the dissimilarity between instances in linear time, while retaining
the relative proximity relationships one would have obtained via computing DTW.
We construct the pairwise distance matrix using LB_Keogh and build a graph for
the graph neural network. We apply this approach to the ten largest datasets
from the well-known UCR time series classification archive. The results
demonstrate that this approach can be up to 104x faster than SimTSC when
constructing the graph on large datasets without significantly decreasing
classification accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accpeted by DLG-AAAI'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphTheta: A Distributed Graph Neural Network Learning System With
  Flexible Training Strategy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.10569v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.10569v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchao Liu, Houyi Li, Guowei Zhang, Xintan Zeng, Yongyong Li, Bin Huang, Peng Zhang, Zhao Li, Xiaowei Zhu, Changhua He, Wenguang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) have been demonstrated as a powerful tool for
analyzing non-Euclidean graph data. However, the lack of efficient distributed
graph learning systems severely hinders applications of GNNs, especially when
graphs are big and GNNs are relatively deep. Herein, we present GraphTheta, the
first distributed and scalable graph learning system built upon vertex-centric
distributed graph processing with neural network operators implemented as
user-defined functions. This system supports multiple training strategies and
enables efficient and scalable big-graph learning on distributed (virtual)
machines with low memory. To facilitate graph convolutions, GraphTheta puts
forward a new graph learning abstraction named NN-TGAR to bridge the gap
between graph processing and graph deep learning. A distributed graph engine is
proposed to conduct the stochastic gradient descent optimization with a
hybrid-parallel execution, and a new cluster-batched training strategy is
supported. We evaluate GraphTheta using several datasets with network sizes
ranging from small-, modest- to large-scale. Experimental results show that
GraphTheta can scale well to 1,024 workers for training an in-house developed
GNN on an industry-scale Alipay dataset of 1.4 billion nodes and 4.1 billion
attributed edges, with a cluster of CPU virtual machines (dockers) of small
memory each (5$\sim$12GB). Moreover, GraphTheta can outperform DistDGL by up to
$2.02\times$, with better scalability, and GraphLearn by up to $30.56\times$.
As for model accuracy, GraphTheta is capable of learning as good GNNs as
existing frameworks. To the best of our knowledge, this work presents the
largest edge-attributed GNN learning task in the literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 13 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Episodes Discovery Recommendation with Multi-Source Augmentations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.01737v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.01737v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwei Fan, Alice Wang, Zahra Nazari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems (RS) commonly retrieve potential candidate items for
users from a massive number of items by modeling user interests based on
historical interactions. However, historical interaction data is highly sparse,
and most items are long-tail items, which limits the representation learning
for item discovery. This problem is further augmented by the discovery of novel
or cold-start items. For example, after a user displays interest in bitcoin
financial investment shows in the podcast space, a recommender system may want
to suggest, e.g., a newly released blockchain episode from a more technical
show. Episode correlations help the discovery, especially when interaction data
of episodes is limited. Accordingly, we build upon the classical Two-Tower
model and introduce the novel Multi-Source Augmentations using a Contrastive
Learning framework (MSACL) to enhance episode embedding learning by
incorporating positive episodes from numerous correlated semantics. Extensive
experiments on a real-world podcast recommendation dataset from a large audio
streaming platform demonstrate the effectiveness of the proposed framework for
user podcast exploration and cold-start episode recommendation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages long for episodes discovery recommendation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantification of geogrid lateral restraint using transparent sand and
  deep learning-based image segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02939v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02939v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Marx, Krishna Kumar, Jorge Zornberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An experimental technique is presented to quantify the lateral restraint
provided by a geogrid embedded in granular soil at the particle level. Repeated
load triaxial tests were done on transparent sand specimens with geosynthetic
inclusions simulating geogrids. Particle outlines on laser illuminated planes
through the specimens were segmented using a deep learning-based segmentation
algorithm. The particle outlines were characterized in terms of Fourier shape
descriptors and tracked across sequentially captured images. The accuracy of
the particle displacement measurements was validated against Digital Image
Correlation (DIC) measurements. In addition, the method's resolution and
repeatability is presented. Based on the measured particle displacements and
rotations, a state boundary line between probable and improbable particle
motions was identified for each test. The size of the zone of probable motions
could be used to quantify the lateral restraint provided by the inclusions.
Overall, the tests results revealed that the geosynthetic inclusions restricted
both particle displacements and rotations. However, the particle displacements
were found to be restrained more significantly than the rotations. Finally, a
unique relationship was found between the magnitude of the permanent strains of
the specimens and the size of the zone of probable motions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LIMEADE: From AI Explanations to Advice Taking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2003.04315v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2003.04315v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Charles Germain Lee, Doug Downey, Kyle Lo, Daniel S. Weld
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research in human-centered AI has shown the benefits of systems that can
explain their predictions. Methods that allow an AI to take advice from humans
in response to explanations are similarly useful. While both capabilities are
well-developed for transparent learning models (e.g., linear models and
GA$^2$Ms), and recent techniques (e.g., LIME and SHAP) can generate
explanations for opaque models, little attention has been given to advice
methods for opaque models. This paper introduces LIMEADE, the first general
framework that translates both positive and negative advice (expressed using
high-level vocabulary such as that employed by post-hoc explanations) into an
update to an arbitrary, underlying opaque model. We demonstrate the generality
of our approach with case studies on seventy real-world models across two broad
domains: image classification and text recommendation. We show our method
improves accuracy compared to a rigorous baseline on the image classification
domains. For the text modality, we apply our framework to a neural recommender
system for scientific papers on a public website; our user study shows that our
framework leads to significantly higher perceived user control, trust, and
satisfaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Refined Edge Usage of Graph Neural Networks for Edge Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.12970v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.12970v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Jin, Yangkun Wang, Weinan Zhang, Quan Gan, Xiang Song, Yong Yu, Zheng Zhang, David Wipf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs), originally proposed for node classification,
have also motivated many recent works on edge prediction (a.k.a., link
prediction). However, existing methods lack elaborate design regarding the
distinctions between two tasks that have been frequently overlooked: (i) edges
only constitute the topology in the node classification task but can be used as
both the topology and the supervisions (i.e., labels) in the edge prediction
task; (ii) the node classification makes prediction over each individual node,
while the edge prediction is determinated by each pair of nodes. To this end,
we propose a novel edge prediction paradigm named Edge-aware Message PassIng
neuRal nEtworks (EMPIRE). Concretely, we first introduce an edge splitting
technique to specify use of each edge where each edge is solely used as either
the topology or the supervision (named as topology edge or supervision edge).
We then develop a new message passing mechanism that generates the messages to
source nodes (through topology edges) being aware of target nodes (through
supervision edges). In order to emphasize the differences between pairs
connected by supervision edges and pairs unconnected, we further weight the
messages to highlight the relative ones that can reflect the differences. In
addition, we design a novel negative node-pair sampling trick that efficiently
samples 'hard' negative instances in the supervision instances, and can
significantly improve the performance. Experimental results verify that the
proposed method can significantly outperform existing state-of-the-art models
regarding the edge prediction task on multiple homogeneous and heterogeneous
graph datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weight Matrix Dimensionality Reduction in Deep Learning via Kronecker
  Multi-layer Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.04273v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.04273v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jarom D. Hogue, Robert M. Kirby, Akil Narayan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning using neural networks is an effective technique for generating
models of complex data. However, training such models can be expensive when
networks have large model capacity resulting from a large number of layers and
nodes. For training in such a computationally prohibitive regime,
dimensionality reduction techniques ease the computational burden, and allow
implementations of more robust networks. We propose a novel type of such
dimensionality reduction via a new deep learning architecture based on fast
matrix multiplication of a Kronecker product decomposition; in particular our
network construction can be viewed as a Kronecker product-induced
sparsification of an "extended" fully connected network. Analysis and practical
examples show that this architecture allows a neural network to be trained and
implemented with a significant reduction in computational time and resources,
while achieving a similar error level compared to a traditional feedforward
neural network.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature Alignment as a Generative Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.12562v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.12562v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiago de Souza Farias, Jonas Maziero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reversibility in artificial neural networks allows us to retrieve the input
given an output. We present feature alignment, a method for approximating
reversibility in arbitrary neural networks. We train a network by minimizing
the distance between the output of a data point and the random output with
respect to a random input. We applied the technique to the MNIST, CIFAR-10,
CelebA and STL-10 image datasets. We demonstrate that this method can roughly
recover images from just their latent representation without the need of a
decoder. By utilizing the formulation of variational autoencoders, we
demonstrate that it is possible to produce new images that are statistically
comparable to the training data. Furthermore, we demonstrate that the quality
of the images can be improved by coupling a generator and a discriminator
together. In addition, we show how this method, with a few minor modifications,
can be used to train networks locally, which has the potential to save
computational memory resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Named Tensor Notation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.13196v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.13196v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Chiang, Alexander M. Rush, Boaz Barak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a notation for tensors with named axes, which relieves the author,
reader, and future implementers of machine learning models from the burden of
keeping track of the order of axes and the purpose of each. The notation makes
it easy to lift operations on low-order tensors to higher order ones, for
example, from images to minibatches of images, or from an attention mechanism
to multiple attention heads.
  After a brief overview and formal definition of the notation, we illustrate
it through several examples from modern machine learning, from building blocks
like attention and convolution to full models like Transformers and LeNet. We
then discuss differential calculus in our notation and compare with some
alternative notations. Our proposals build on ideas from many previous papers
and software libraries. We hope that our notation will encourage more authors
to use named tensors, resulting in clearer papers and more precise
implementations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nostradamus: Weathering Worth 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05933v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05933v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alapan Chaudhuri, Zeeshan Ahmed, Ashwin Rao, Shivansh Subramanian, Shreyas Pradhan, Abhishek Mittal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nostradamus, inspired by the French astrologer and reputed seer, is a
detailed study exploring relations between environmental factors and changes in
the stock market. In this paper, we analyze associative correlation and
causation between environmental elements (including natural disasters, climate
and weather conditions) and stock prices, using historical stock market data,
historical climate data, and various climate indicators such as carbon dioxide
emissions. We have conducted our study based on the US financial market, global
climate trends, and daily weather records to demonstrate a significant
relationship between climate and stock price fluctuation. Our analysis covers
both short-term and long-term rises and dips in company stock performances.
Lastly, we take four natural disasters as a case study to observe the effect
they have on people's emotional state and their influence on the stock market.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 13 figures; updated abstract; updated format to Springer
  LNCS</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Your Day in Your Pocket: Complex Activity Recognition from Smartphone
  Accelerometers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emma Bouton--Bessac, Lakmal Meegahapola, Daniel Gatica-Perez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human Activity Recognition (HAR) enables context-aware user experiences where
mobile apps can alter content and interactions depending on user activities.
Hence, smartphones have become valuable for HAR as they allow large, and
diversified data collection. Although previous work in HAR managed to detect
simple activities (i.e., sitting, walking, running) with good accuracy using
inertial sensors (i.e., accelerometer), the recognition of complex daily
activities remains an open problem, specially in remote work/study settings
when people are more sedentary. Moreover, understanding the everyday activities
of a person can support the creation of applications that aim to support their
well-being. This paper investigates the recognition of complex activities
exclusively using smartphone accelerometer data. We used a large smartphone
sensing dataset collected from over 600 users in five countries during the
pandemic and showed that deep learning-based, binary classification of eight
complex activities (sleeping, eating, watching videos, online communication,
attending a lecture, sports, shopping, studying) can be achieved with AUROC
scores up to 0.76 with partially personalized models. This shows encouraging
signs toward assessing complex activities only using phone accelerometer data
in the post-pandemic world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16th EAI International Conference on Pervasive Computing Technologies
  for Healthcare (PervasiveHealth) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CS-lol: a <span class="highlight-title">Dataset</span> of Viewer Comment with Scene in E-sports
  Live-streaming <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06876v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06876v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie H. Xu, Yu Nakano, Lingrong Kong, Kojiro Iizuka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Billions of live-streaming viewers share their opinions on scenes they are
watching in real-time and interact with the event, commentators as well as
other viewers via text comments. Thus, there is necessary to explore viewers'
comments with scenes in E-sport live-streaming events. In this paper, we
developed CS-lol, a new large-scale dataset containing comments from viewers
paired with descriptions of game scenes in E-sports live-streaming. Moreover,
we propose a task, namely viewer comment retrieval, to retrieve the viewer
comments for the scene of the live-streaming event. Results on a series of
baseline retrieval methods derived from typical IR evaluation methods show our
task as a challenging task. Finally, we release CS-lol and baseline
implementation to the research community as a resource.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, In ACM SIGIR Conference on Human Information
  Interaction and Retrieval (CHIIR 23)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2023-01-25T05:21:26.458737974Z">
            2023-01-25 05:21:26 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
